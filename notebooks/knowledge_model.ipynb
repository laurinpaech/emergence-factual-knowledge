{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entities (5500)\n",
    "entities = pd.read_csv('../data/Entities/SingleToken/entities_languageAgnostic_clean.csv')\n",
    "\n",
    "# Load Relations\n",
    "relations = pd.read_csv('../data/Knowledge/properties_nonsymmetric_multilingual_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random pairs of numbers (indices into entity)\n",
    "# Order doesn't matter, can't repeat\n",
    "# i.e. ok is: (0,1), (1,2), (0,2) but not ok is (0,1),(1,0) or (0,0)\n",
    "# Runs until exhausted or reached max_size\n",
    "# possible to limit occurences of index\n",
    "def gen_index_pairs(n, max_size=np.Inf, limit=np.Inf):\n",
    "    pairs = set()\n",
    "    ind = list()\n",
    "\n",
    "    while len(pairs) < max_size:\n",
    "        # return number between 0 and n (exclude)\n",
    "        x, y = np.random.randint(n), np.random.randint(n)\n",
    "        \n",
    "        while ind.count(x) >= limit or ind.count(y) >= limit:\n",
    "            x, y = np.random.randint(n), np.random.randint(n)\n",
    "        \n",
    "        i = 0\n",
    "        while (x, y) in pairs or (y, x) in pairs or x == y:\n",
    "            if i > 10:\n",
    "                return\n",
    "            x, y = np.random.randint(n), np.random.randint(n)\n",
    "            i += 1\n",
    "            \n",
    "        ind.append(x)\n",
    "        ind.append(y)\n",
    "        \n",
    "        pairs.add((x, y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_indices(n, max_size=np.Inf):\n",
    "    taken = []\n",
    "\n",
    "    while len(taken) < max_size:\n",
    "        # return number between 0 and n (exclude)\n",
    "        x = np.random.randint(n)\n",
    "\n",
    "        i = 0\n",
    "        while x in taken:\n",
    "            if i > 10:\n",
    "                return\n",
    "            x = np.random.randint(n)\n",
    "            i += 1\n",
    "\n",
    "        taken.append(x)\n",
    "\n",
    "    return taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index_pairs(n, entity_list, max_size=np.Inf):\n",
    "    pairs = set()\n",
    "    k = 0\n",
    "\n",
    "    while len(pairs) < max_size:\n",
    "        # return number between 0 and n (exclude)\n",
    "        x = entity_list[k]\n",
    "        y = np.random.randint(n)\n",
    "\n",
    "        i = 0\n",
    "        while (x, y) in pairs or (y, x) in pairs or x == y:\n",
    "            if i > 10:\n",
    "                return\n",
    "            y = np.random.randint(n)\n",
    "            i += 1\n",
    "\n",
    "        pairs.add((x, y))\n",
    "        k += 1\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RELATION: military rank\n",
      "RELATION: including\n",
      "RELATION: nature of statement\n",
      "RELATION: narrative role\n",
      "RELATION: package management system\n",
      "RELATION: month of the year\n",
      "RELATION: received signal type\n",
      "RELATION: has lyrics\n",
      "RELATION: transport network\n",
      "RELATION: plaintiff\n",
      "RELATION: reports periodicity\n",
      "RELATION: wheelchair accessibility\n",
      "RELATION: editor-in-chief\n",
      "RELATION: created for\n",
      "RELATION: location of the point of view\n",
      "RELATION: elected in\n",
      "RELATION: depends on software\n",
      "RELATION: handedness\n",
      "RELATION: natural reservoir of\n",
      "RELATION: fracturing\n"
     ]
    }
   ],
   "source": [
    "source_lang = 'en'\n",
    "target_lang = 'de'\n",
    "\n",
    "n_relations = 20\n",
    "n_facts = 800\n",
    "\n",
    "# (e, r, f ) <=> (e, r_de, f)\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "# Sample relations\n",
    "relations_sampled = relations.sample(n_relations)\n",
    "\n",
    "# Generate n_facts entity1s\n",
    "entities1 = generate_unique_indices(entities.shape[0], n_facts)\n",
    "\n",
    "for index, relation in relations_sampled.iterrows():\n",
    "\n",
    "    print(\"RELATION: \" + relation[source_lang])\n",
    "    seen = set()\n",
    "\n",
    "    # Generate n_facts entity2s\n",
    "    entity_generator = generate_index_pairs(entities.shape[0], entities1, n_facts)\n",
    "\n",
    "    for e_id, f_id in entity_generator:\n",
    "        e = entities['label'][e_id]\n",
    "        f = entities['label'][f_id]\n",
    "\n",
    "        # Sanity Check for uniqueness\n",
    "        if e_id == f_id or (e_id, f_id) in seen or (f_id, e_id) in seen:\n",
    "            print(\"WARNING: Pair!\")\n",
    "\n",
    "        seen.add((e_id, f_id))\n",
    "\n",
    "        # Append symmetric relations\n",
    "        train.append(e + ' ' + relation[source_lang] + ' ' + f)\n",
    "        test.append(e + ' ' + relation[target_lang] + ' ' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612     P410\n",
       "264    P1012\n",
       "346    P5102\n",
       "89     P5800\n",
       "156    P3033\n",
       "483    P2922\n",
       "369    P1194\n",
       "394    P6439\n",
       "542      P16\n",
       "595    P1620\n",
       "707    P6339\n",
       "490    P2846\n",
       "730    P5769\n",
       "559    P9883\n",
       "380    P7108\n",
       "131    P2715\n",
       "266    P1547\n",
       "683     P552\n",
       "206    P1606\n",
       "125     P538\n",
       "Name: id, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_sampled['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample': ['Gauss location of landing Mühle',\n",
       "  'epi location of landing Pound',\n",
       "  'Sonja location of landing Gallagher',\n",
       "  'Sempre location of landing Playboy',\n",
       "  'Fauna location of landing Canis',\n",
       "  'Chili location of landing Unión',\n",
       "  'Racine location of landing NN',\n",
       "  'XIV location of landing Bayan',\n",
       "  'Epic location of landing Northeast',\n",
       "  'du location of landing Viscount',\n",
       "  'PGC location of landing Dwight',\n",
       "  'Thorpe location of landing Look',\n",
       "  'Tipo location of landing MC',\n",
       "  'Neckar location of landing Aragon',\n",
       "  'Grace location of landing Man',\n",
       "  'Caroline location of landing Thing',\n",
       "  'Oklahoma location of landing Acid',\n",
       "  'Stammen location of landing Harrison',\n",
       "  'Dornbusch location of landing Amour',\n",
       "  'Fallen location of landing Tao',\n",
       "  'Janne location of landing Vernon',\n",
       "  'Joe location of landing Lyons',\n",
       "  'Ascher location of landing Mina',\n",
       "  'Hansen location of landing Remixes',\n",
       "  'Continental location of landing RPM',\n",
       "  'ABS location of landing Bad',\n",
       "  'Riviera location of landing Greenberg',\n",
       "  'Ferro location of landing Atene',\n",
       "  'Museum location of landing Natura',\n",
       "  'Arc location of landing Leigh',\n",
       "  'Gemini location of landing League',\n",
       "  'Karnataka location of landing Loch',\n",
       "  'Weather location of landing Oklahoma',\n",
       "  'Telephone location of landing River',\n",
       "  'Severus location of landing Dach',\n",
       "  'After location of landing Carson',\n",
       "  'Dent location of landing Ty',\n",
       "  'Platnick location of landing Entangled',\n",
       "  'Alaska location of landing Jansen',\n",
       "  'Conrad location of landing RPG',\n",
       "  'Central location of landing Hancock',\n",
       "  'Voogd location of landing Lewis',\n",
       "  'RE location of landing CT',\n",
       "  'Stary location of landing Passau',\n",
       "  'Bruder location of landing Kelly',\n",
       "  'Phelps location of landing Donna',\n",
       "  'Osman location of landing Die',\n",
       "  'Göttingen location of landing HTC',\n",
       "  'Foucault location of landing Audio',\n",
       "  'Tat location of landing Young',\n",
       "  'Acid location of landing Suiza',\n",
       "  'Renaissance location of landing Klein',\n",
       "  'Minsk location of landing Huis',\n",
       "  'Monaco location of landing Ruggiero',\n",
       "  'Line location of landing Lawrence',\n",
       "  'Guatemala location of landing Diesel',\n",
       "  'York location of landing Uruguay',\n",
       "  'Grimaldi location of landing Rooma',\n",
       "  'Giant location of landing Jeux',\n",
       "  'Teluk location of landing Émile',\n",
       "  'Kore location of landing Memphis',\n",
       "  'Amiga location of landing GB',\n",
       "  'Katharina location of landing Johnny',\n",
       "  'Klub location of landing LLC',\n",
       "  'Gordon location of landing Shanghai',\n",
       "  'Bloch location of landing Rue',\n",
       "  'Conceição location of landing Stanley',\n",
       "  'Homo location of landing Uno',\n",
       "  'Clive location of landing Víctor',\n",
       "  'Philadelphia location of landing ID',\n",
       "  'Planeta location of landing Oman',\n",
       "  'Rok location of landing Quintana',\n",
       "  'Psycho location of landing Chelsea',\n",
       "  'Clan location of landing Juárez',\n",
       "  'Curie location of landing ACM',\n",
       "  'Público location of landing Ventura',\n",
       "  'SMK location of landing Chance',\n",
       "  'Brooks location of landing Limited',\n",
       "  'Fuego location of landing pr',\n",
       "  'Lindberg location of landing Rhein',\n",
       "  'Cinta location of landing Robinson',\n",
       "  'Elias location of landing Secondo',\n",
       "  'Denna location of landing Lyons',\n",
       "  'Oriental location of landing Nikolaj',\n",
       "  'Bonus location of landing Nils',\n",
       "  'Clarke location of landing BSD',\n",
       "  'GM location of landing Standard',\n",
       "  'Santo location of landing Carrera',\n",
       "  'Abby location of landing KZ',\n",
       "  'Stockton location of landing Tanjung',\n",
       "  'Smith location of landing Pieces',\n",
       "  'Membre location of landing Sima',\n",
       "  'Seas location of landing Tode',\n",
       "  'Murcia location of landing Teddy',\n",
       "  'Quiet location of landing Abby',\n",
       "  'Alexander location of landing Emma',\n",
       "  'Lieder location of landing Caledonia',\n",
       "  'Spain location of landing MTA',\n",
       "  'Aa location of landing Globo',\n",
       "  'Vosges location of landing Richter',\n",
       "  'Majesty location of landing Move',\n",
       "  'tip location of landing Guy',\n",
       "  'Stranger location of landing Euler',\n",
       "  'Beyaz location of landing Valverde',\n",
       "  'Grupa location of landing MSK',\n",
       "  'Davis location of landing Sweeney',\n",
       "  'Worldwide location of landing ao',\n",
       "  'Toulouse location of landing Room',\n",
       "  'Superman location of landing Rogers',\n",
       "  'Oleh location of landing Bolt',\n",
       "  'Energy location of landing Regio',\n",
       "  'Gibraltar location of landing Kamera',\n",
       "  'Rica location of landing Peck',\n",
       "  'PhD location of landing Below',\n",
       "  'EA location of landing Petra',\n",
       "  'Tucson location of landing MV',\n",
       "  'Dur location of landing Bohemia',\n",
       "  'Midden location of landing Neuen',\n",
       "  'CPU location of landing Trinity',\n",
       "  'Acre location of landing Sada',\n",
       "  'Söhne location of landing Niño',\n",
       "  'Anthony location of landing Gwillim',\n",
       "  'Robertson location of landing Argentina',\n",
       "  'CW location of landing Carlisle',\n",
       "  'Oru location of landing Faces',\n",
       "  'Chandler location of landing Mama',\n",
       "  'Genocide location of landing Willis',\n",
       "  'SBS location of landing Boss',\n",
       "  'Larsen location of landing Garrison',\n",
       "  'Waterloo location of landing After',\n",
       "  'JP location of landing Kepler',\n",
       "  'Organ location of landing Har',\n",
       "  'Hus location of landing Juno',\n",
       "  'Swan location of landing Horace',\n",
       "  'Milton location of landing Bangor',\n",
       "  'Raoul location of landing RA',\n",
       "  'Crosby location of landing Beats',\n",
       "  'Bande location of landing Sartre',\n",
       "  'Barnard location of landing Interview',\n",
       "  'NM location of landing Lanka',\n",
       "  'cal location of landing Powers',\n",
       "  'Nuclear location of landing Insel',\n",
       "  'Lux location of landing Toy',\n",
       "  'Ugo location of landing Isto',\n",
       "  'Branko location of landing Polk',\n",
       "  'Hornet location of landing Jacobi',\n",
       "  'UV location of landing Silent',\n",
       "  'Rees location of landing McLaren',\n",
       "  'Krebs location of landing Bonnet',\n",
       "  'Mack location of landing Schmidt',\n",
       "  'Bull location of landing McGraw',\n",
       "  'Illusion location of landing Vierge',\n",
       "  'FR location of landing Syracuse',\n",
       "  'Nickelodeon location of landing Maynard',\n",
       "  'Grund location of landing Lola',\n",
       "  'Giancarlo location of landing Hamar',\n",
       "  'Enter location of landing Jacqueline',\n",
       "  'Warhol location of landing Cuba',\n",
       "  'Marburg location of landing HR',\n",
       "  'Tokyo location of landing Authority',\n",
       "  'Billie location of landing Antonia',\n",
       "  'Hanover location of landing Kenny',\n",
       "  'Mama location of landing Arean',\n",
       "  'Sutherland location of landing Iberia',\n",
       "  'PL location of landing XIV',\n",
       "  'Zweck location of landing Vallée',\n",
       "  'Sibiu location of landing Bland',\n",
       "  'Lancaster location of landing Tanner',\n",
       "  'Em location of landing HD',\n",
       "  'Dewey location of landing Holt',\n",
       "  'Meteor location of landing Atatürk',\n",
       "  'Laba location of landing Cha',\n",
       "  'Faso location of landing Rudy',\n",
       "  'Trondheim location of landing Zeta',\n",
       "  'Schatten location of landing Gia',\n",
       "  'Sven location of landing Bose',\n",
       "  'Dudley location of landing Temple',\n",
       "  'Nassau location of landing Indra',\n",
       "  'Rom location of landing Cole',\n",
       "  'Goran location of landing Camus',\n",
       "  'Walsh location of landing Hatch',\n",
       "  'Ruth location of landing Krishna',\n",
       "  'Cedar location of landing Broadway',\n",
       "  'Bengal location of landing Somali',\n",
       "  'Elizabeth location of landing Delft',\n",
       "  'Area location of landing Els',\n",
       "  'September location of landing IN',\n",
       "  'Orte location of landing Emden',\n",
       "  'Forward location of landing Chambers',\n",
       "  'Ship location of landing Oaks',\n",
       "  'Kepler location of landing Verdi',\n",
       "  'Dupont location of landing Prensa',\n",
       "  'Mitchell location of landing Durango',\n",
       "  'Lucia location of landing Cha',\n",
       "  'Italy location of landing Deel',\n",
       "  'Roberto location of landing Elizabeth',\n",
       "  'Campione location of landing Arcade',\n",
       "  'Luther location of landing Bog',\n",
       "  'Sergey location of landing Rok',\n",
       "  'CBS location of landing PP',\n",
       "  'FN location of landing Gilmour',\n",
       "  'Compton location of landing Springfield',\n",
       "  'Have location of landing Faber',\n",
       "  'Negra location of landing Santo',\n",
       "  'Wars location of landing Moto',\n",
       "  'Busch location of landing Target',\n",
       "  'Save location of landing Manconi',\n",
       "  'KG location of landing Megan',\n",
       "  'Nico location of landing Wola',\n",
       "  'Sheriff location of landing AG',\n",
       "  'Sarah location of landing Ned',\n",
       "  'll location of landing Dur',\n",
       "  'Prima location of landing Men',\n",
       "  'Typ location of landing Mondadori',\n",
       "  'MotoGP location of landing Silla',\n",
       "  'Grégoire location of landing Bologna',\n",
       "  'Barra location of landing Agung',\n",
       "  'Bloomberg location of landing Warhol',\n",
       "  'Helmut location of landing Maan',\n",
       "  'Cornell location of landing Llobregat',\n",
       "  'Colección location of landing Angelina',\n",
       "  'Eduard location of landing Étienne',\n",
       "  'Reise location of landing Exodus',\n",
       "  'Ses location of landing Bells',\n",
       "  'LSD location of landing Ramsay',\n",
       "  'PT location of landing Fauna',\n",
       "  'Pace location of landing fann',\n",
       "  'Harris location of landing Clermont',\n",
       "  'Turki location of landing Rebel',\n",
       "  'Navarra location of landing LP',\n",
       "  'Taranto location of landing Panny',\n",
       "  'Göran location of landing Campbell',\n",
       "  'Telegraph location of landing Titolo',\n",
       "  'Ebert location of landing Aa',\n",
       "  'APG location of landing Somerset',\n",
       "  'Axis location of landing Jake',\n",
       "  'Seneca location of landing Lancaster',\n",
       "  'Medal location of landing Sur',\n",
       "  'Greenwich location of landing Tiempo',\n",
       "  'Berry location of landing Bosco',\n",
       "  'Ski location of landing Bahía',\n",
       "  'Seoul location of landing Tobias',\n",
       "  'Village location of landing Parade',\n",
       "  'Harper location of landing Marina',\n",
       "  'Darin location of landing CD',\n",
       "  'RT location of landing Gustav',\n",
       "  'Moore location of landing Herrschaft',\n",
       "  'Crisis location of landing Castello',\n",
       "  'Slot location of landing Kim',\n",
       "  'Roberta location of landing Katy',\n",
       "  'Reggio location of landing Spa',\n",
       "  'XX location of landing Everybody',\n",
       "  'Worlds location of landing Staff',\n",
       "  'Shackleton location of landing Rota',\n",
       "  'Arabia location of landing Civil',\n",
       "  'Thames location of landing AP',\n",
       "  'Change location of landing Bel',\n",
       "  'di location of landing Mutter',\n",
       "  'Midland location of landing te',\n",
       "  'Goal location of landing Cristal',\n",
       "  'Ocak location of landing Each',\n",
       "  'Aachen location of landing Batu',\n",
       "  'Penn location of landing Harriet',\n",
       "  'Omaha location of landing Long',\n",
       "  'Hiroshima location of landing co',\n",
       "  'Brunnen location of landing Sparks',\n",
       "  'Elliott location of landing Trento',\n",
       "  'Kid location of landing Coll',\n",
       "  'Angel location of landing Zeus',\n",
       "  'Europa location of landing Landmark',\n",
       "  'Po location of landing Paramount',\n",
       "  'Tanz location of landing Aya',\n",
       "  'Camp location of landing Doyle',\n",
       "  'Valverde location of landing Bedford',\n",
       "  'Cá location of landing Juha',\n",
       "  'Michel location of landing Wege',\n",
       "  'AG location of landing IGE',\n",
       "  'Banks location of landing Junkers',\n",
       "  'Denne location of landing Av',\n",
       "  'Brenner location of landing IV',\n",
       "  'Verso location of landing Platte',\n",
       "  'Aten location of landing Britten',\n",
       "  'Walls location of landing Indie',\n",
       "  'Bordeaux location of landing Highland',\n",
       "  'Spitze location of landing Yoshida',\n",
       "  'PCI location of landing Serra',\n",
       "  'WM location of landing Downey',\n",
       "  'Batista location of landing Mister',\n",
       "  'Karls location of landing Indre',\n",
       "  'Olímpico location of landing Smiths',\n",
       "  'Natura location of landing Bulgaria',\n",
       "  'Rose location of landing Ellis',\n",
       "  'Hammer location of landing Superman',\n",
       "  'Edouard location of landing Lager',\n",
       "  'Baby location of landing Straße',\n",
       "  'VII location of landing Side',\n",
       "  'Bilbao location of landing Jack',\n",
       "  'Eo location of landing Ruska',\n",
       "  'Plessis location of landing Despois',\n",
       "  'Asia location of landing Lorraine',\n",
       "  'Wege location of landing VP',\n",
       "  'Hoya location of landing Roca',\n",
       "  'Monterey location of landing Como',\n",
       "  'Titus location of landing TB',\n",
       "  'Harvest location of landing Mondadori',\n",
       "  'Livingston location of landing Corso',\n",
       "  'Songs location of landing Correa',\n",
       "  'Strong location of landing Inside',\n",
       "  'Serena location of landing Slater',\n",
       "  'Forest location of landing Portuguesa',\n",
       "  'Talent location of landing Memories',\n",
       "  'Belgium location of landing Aves',\n",
       "  'Catalina location of landing Eintracht',\n",
       "  'Salvia location of landing Bloomberg',\n",
       "  'Globo location of landing Briggs',\n",
       "  'Spin location of landing Primera',\n",
       "  'Concilio location of landing Reggio',\n",
       "  'Prensa location of landing Strauss',\n",
       "  'Hampton location of landing Fell',\n",
       "  'Primer location of landing Smash',\n",
       "  'Kanal location of landing Tata',\n",
       "  'Isle location of landing Padang',\n",
       "  'ST location of landing Campbell',\n",
       "  'Ab location of landing Bukit',\n",
       "  'Brand location of landing Indonesia',\n",
       "  'Seitz location of landing Sari',\n",
       "  'Tala location of landing Energy',\n",
       "  'Argentine location of landing Planck',\n",
       "  'Macbeth location of landing Signal',\n",
       "  'EV location of landing Alexis',\n",
       "  'Little location of landing Magazine',\n",
       "  'Noi location of landing Sørensen',\n",
       "  'Shannon location of landing DS',\n",
       "  'Cost location of landing Atlantis',\n",
       "  'Mariano location of landing Uruguay',\n",
       "  'Meter location of landing Windsor',\n",
       "  'Dewan location of landing Italie',\n",
       "  'Marín location of landing Kingston',\n",
       "  'Bastia location of landing Charity',\n",
       "  'Arean location of landing Moro',\n",
       "  'Jennings location of landing Oliver',\n",
       "  'Gama location of landing Racing',\n",
       "  'Look location of landing Phil',\n",
       "  'PR location of landing Sana',\n",
       "  'Pat location of landing Bron',\n",
       "  'Parliament location of landing Caldwell',\n",
       "  'Jaroslav location of landing Od',\n",
       "  'Side location of landing Hague',\n",
       "  'Corrientes location of landing Sha',\n",
       "  'Civic location of landing Ewing',\n",
       "  'Trainer location of landing Pride',\n",
       "  'Jerome location of landing Hamas',\n",
       "  'Herrschaft location of landing Starr',\n",
       "  'Amelia location of landing Greatest',\n",
       "  'Sporting location of landing Pat',\n",
       "  'Higgins location of landing Justicia',\n",
       "  'Market location of landing Hiroshima',\n",
       "  'Miki location of landing Lista',\n",
       "  'SM location of landing Kensington',\n",
       "  'Tracy location of landing RFC',\n",
       "  'Giang location of landing Uit',\n",
       "  'André location of landing Bab',\n",
       "  'Gibbs location of landing Sandra',\n",
       "  'Cantal location of landing Cabinet',\n",
       "  'Niels location of landing Villiers',\n",
       "  'Abraham location of landing Tage',\n",
       "  'Tina location of landing Nieto',\n",
       "  'Danilo location of landing Araújo',\n",
       "  'Coral location of landing Tea',\n",
       "  'Bad location of landing Zu',\n",
       "  'Wonder location of landing Malcolm',\n",
       "  'Disease location of landing Volga',\n",
       "  'Mesir location of landing Dakota',\n",
       "  'Magna location of landing Courtney',\n",
       "  'Bold location of landing Gore',\n",
       "  'Romawi location of landing Robbie',\n",
       "  'Rumble location of landing Holger',\n",
       "  'Frida location of landing Dende',\n",
       "  'Mires location of landing Tanjung',\n",
       "  'Charleston location of landing Trang',\n",
       "  'Nest location of landing Chor',\n",
       "  'Della location of landing Saguenay',\n",
       "  'Stab location of landing Flora',\n",
       "  'Andrews location of landing Roosevelt',\n",
       "  'Romsdal location of landing Frost',\n",
       "  'Alzheimer location of landing Cole',\n",
       "  'sy location of landing Oriental',\n",
       "  'Pandora location of landing Honoré',\n",
       "  'Rady location of landing Campbell',\n",
       "  'Pegasus location of landing Edit',\n",
       "  'Hartley location of landing Beyoncé',\n",
       "  'Esperanza location of landing Weather',\n",
       "  'Comtat location of landing Coppa',\n",
       "  'Vega location of landing Runde',\n",
       "  'Ortiz location of landing Gironde',\n",
       "  'Spiegel location of landing Co',\n",
       "  'Vettel location of landing Egipto',\n",
       "  'Stewart location of landing Vieux',\n",
       "  'Día location of landing Playboy',\n",
       "  'Nil location of landing su',\n",
       "  'Jameson location of landing Sept',\n",
       "  'Dei location of landing Zapata',\n",
       "  'IM location of landing Oregon',\n",
       "  'Bend location of landing PSA',\n",
       "  'Hannah location of landing Rayon',\n",
       "  'Twente location of landing Barton',\n",
       "  'Pavia location of landing Preto',\n",
       "  'MAC location of landing AND',\n",
       "  'Welfare location of landing Halifax',\n",
       "  'Morgen location of landing Evans',\n",
       "  'Ulysses location of landing Carrera',\n",
       "  'Theatre location of landing Oviedo',\n",
       "  'Salazar location of landing Glory',\n",
       "  'Note location of landing Double',\n",
       "  'Emergency location of landing Bay',\n",
       "  'Linha location of landing Reich',\n",
       "  'Gateway location of landing Wells',\n",
       "  'Conquest location of landing Depuis',\n",
       "  'Cabinet location of landing Émile',\n",
       "  'Bas location of landing MB',\n",
       "  'XIII location of landing Sino',\n",
       "  'Turm location of landing Duty',\n",
       "  'JK location of landing Miki',\n",
       "  'Madagascar location of landing Gordon',\n",
       "  'Atlanta location of landing Rica',\n",
       "  'Depuis location of landing Porto',\n",
       "  'Kosmos location of landing RS',\n",
       "  'ro location of landing Imperi',\n",
       "  'Eberhard location of landing Riviera',\n",
       "  'Roll location of landing Alexa',\n",
       "  'Carlton location of landing Lur',\n",
       "  'CCD location of landing Pride',\n",
       "  'Johren location of landing Guadalajara',\n",
       "  'Griffith location of landing Cabrera',\n",
       "  'Asylum location of landing RD',\n",
       "  'Gross location of landing PhD',\n",
       "  'Battista location of landing Benson',\n",
       "  'Homme location of landing Speedway',\n",
       "  'Sit location of landing Dans',\n",
       "  'Infinite location of landing Ike',\n",
       "  'Sultan location of landing Aloe',\n",
       "  'Stefano location of landing Sérgio',\n",
       "  'Pacific location of landing Mateo',\n",
       "  'Amalia location of landing Abi',\n",
       "  'Vatican location of landing After',\n",
       "  'Belle location of landing Gina',\n",
       "  'Stephenson location of landing Wort',\n",
       "  'Greatest location of landing Ankara',\n",
       "  'Mondadori location of landing Orden',\n",
       "  'Becker location of landing Fach',\n",
       "  'Meuse location of landing Dacia',\n",
       "  'Samuel location of landing Arad',\n",
       "  'Australian location of landing Lanka',\n",
       "  'Ferguson location of landing Forma',\n",
       "  'Swiss location of landing Phil',\n",
       "  'ARM location of landing Sinclair',\n",
       "  'Einer location of landing ME',\n",
       "  'Poola location of landing Shaun',\n",
       "  'Masa location of landing Vosges',\n",
       "  'lt location of landing Urgell',\n",
       "  'Halk location of landing Co',\n",
       "  'Boom location of landing Jericho',\n",
       "  'Shirley location of landing Krista',\n",
       "  'Aner location of landing Cerca',\n",
       "  'Dio location of landing Hanna',\n",
       "  'Emily location of landing Bello',\n",
       "  'Riverside location of landing Vegas',\n",
       "  'Frontier location of landing Mead',\n",
       "  'Ex location of landing Hawker',\n",
       "  'Walton location of landing Pointe',\n",
       "  'Cannes location of landing Ka',\n",
       "  'Res location of landing Divine',\n",
       "  'Billy location of landing Parker',\n",
       "  'Dexter location of landing Hooper',\n",
       "  'Vance location of landing Ipswich',\n",
       "  'Plato location of landing Swing',\n",
       "  'Br location of landing Diamond',\n",
       "  'VfB location of landing Regno',\n",
       "  'Humanos location of landing Calais',\n",
       "  'Manson location of landing Mississippi',\n",
       "  'Corte location of landing Nouveau',\n",
       "  'Linh location of landing Watson',\n",
       "  'Pool location of landing Orta',\n",
       "  'Benton location of landing ol',\n",
       "  'Connection location of landing Moscou',\n",
       "  'Werding location of landing ATC',\n",
       "  'Gruppe location of landing CG',\n",
       "  'Sumber location of landing Nebraska',\n",
       "  'Sulla location of landing Angkatan',\n",
       "  'Bloom location of landing Kuna',\n",
       "  'Pass location of landing Ronda',\n",
       "  'Crane location of landing Capitol',\n",
       "  'ao location of landing Farmer',\n",
       "  'Dover location of landing Mexico',\n",
       "  'Regular location of landing Davy',\n",
       "  'Buffy location of landing Stevens',\n",
       "  'Riau location of landing Rowland',\n",
       "  'Ottawa location of landing Polis',\n",
       "  'Commander location of landing Serra',\n",
       "  'Edda location of landing Kjell',\n",
       "  'Birth location of landing Yale',\n",
       "  'Bulls location of landing Moonlight',\n",
       "  'WRC location of landing Cynthia',\n",
       "  'Ringo location of landing Setiap',\n",
       "  'Emerson location of landing Maxim',\n",
       "  'Columbus location of landing Lena',\n",
       "  'Kanye location of landing Percy',\n",
       "  'Innocent location of landing Marina',\n",
       "  'Firefox location of landing Ritual',\n",
       "  'Bavaria location of landing Server',\n",
       "  'Colle location of landing Batman',\n",
       "  'CT location of landing EE',\n",
       "  'Sonia location of landing Porto',\n",
       "  'Toten location of landing Delhi',\n",
       "  'Pokémon location of landing Sans',\n",
       "  'Brady location of landing Evans',\n",
       "  'Moscou location of landing Sánchez',\n",
       "  'Tallinn location of landing Kale',\n",
       "  'Parkway location of landing EV',\n",
       "  'Pedersen location of landing Schools',\n",
       "  'Woods location of landing Scala',\n",
       "  'Princess location of landing Judith',\n",
       "  'Iglesia location of landing Turbo',\n",
       "  'Constantin location of landing Garner',\n",
       "  'Bulu location of landing Geiger',\n",
       "  'Sia location of landing Fiction',\n",
       "  'Melanie location of landing Curtis',\n",
       "  'Beyoncé location of landing Universitas',\n",
       "  'Grant location of landing Ages',\n",
       "  'CV location of landing Bund',\n",
       "  'KBS location of landing Cécile',\n",
       "  'Clin location of landing Joshua',\n",
       "  'Witness location of landing Tibet',\n",
       "  'Regia location of landing Wars',\n",
       "  'Shri location of landing Manga',\n",
       "  'Wolfe location of landing Pretoria',\n",
       "  'Blackburn location of landing Lieder',\n",
       "  'Almanya location of landing Anders',\n",
       "  'Junior location of landing Chaos',\n",
       "  'Bing location of landing Engine',\n",
       "  'Surat location of landing Trek',\n",
       "  'Underground location of landing Begin',\n",
       "  'Ward location of landing Socorro',\n",
       "  'Jahre location of landing Pay',\n",
       "  'Dara location of landing Liberty',\n",
       "  'Mona location of landing Glory',\n",
       "  'Nirvana location of landing Krista',\n",
       "  'Sabbath location of landing Magna',\n",
       "  'Oper location of landing Lower',\n",
       "  'Mer location of landing Vienne',\n",
       "  'Mosca location of landing Coast',\n",
       "  'Eugen location of landing Mackenzie',\n",
       "  'Storm location of landing Eiffel',\n",
       "  'Ewing location of landing CDP',\n",
       "  'Kirchner location of landing Lazio',\n",
       "  'Words location of landing Bolivia',\n",
       "  'Phan location of landing Genus',\n",
       "  'Condado location of landing Mortimer',\n",
       "  'County location of landing Araújo',\n",
       "  'Yi location of landing Oklahoma',\n",
       "  'Leafs location of landing Brabant',\n",
       "  'Pasadena location of landing am',\n",
       "  'Haydn location of landing Leute',\n",
       "  'Hammond location of landing Chicago',\n",
       "  'Martini location of landing Secrets',\n",
       "  'Stalin location of landing Lec',\n",
       "  'Butte location of landing VA',\n",
       "  'Tokio location of landing Novel',\n",
       "  'Friends location of landing AFI',\n",
       "  'Ankara location of landing Goodman',\n",
       "  'Doherty location of landing Welle',\n",
       "  'Frankie location of landing Agency',\n",
       "  'Abi location of landing Adel',\n",
       "  'Saison location of landing Nato',\n",
       "  'Hof location of landing Meilleur',\n",
       "  'Bare location of landing Panel',\n",
       "  'Vallée location of landing Boer',\n",
       "  'Gert location of landing Loyola',\n",
       "  'Studie location of landing Vader',\n",
       "  'Lima location of landing Jar',\n",
       "  'Zelda location of landing Darmstadt',\n",
       "  'Villanueva location of landing Parma',\n",
       "  'Riga location of landing Weston',\n",
       "  'Ros location of landing Metro',\n",
       "  'Armata location of landing Pizarro',\n",
       "  'Rain location of landing Clan',\n",
       "  'Lugo location of landing Stuart',\n",
       "  'Jepang location of landing Room',\n",
       "  'Fuji location of landing Champions',\n",
       "  'Ardenne location of landing Eugenio',\n",
       "  'Birch location of landing Brenner',\n",
       "  'Eindhoven location of landing Sens',\n",
       "  'Eugène location of landing Gibraltar',\n",
       "  'Arias location of landing Brooklyn',\n",
       "  'Houghton location of landing Haji',\n",
       "  'fann location of landing Bandet',\n",
       "  'Otte location of landing MIT',\n",
       "  'Venezuela location of landing Piemonte',\n",
       "  'Sterling location of landing Romans',\n",
       "  'Speedway location of landing Santana',\n",
       "  'Pfeiffer location of landing Reflections',\n",
       "  'Amigos location of landing Essential',\n",
       "  'Rioja location of landing Amelia',\n",
       "  'Rudy location of landing Yukon',\n",
       "  'Vitoria location of landing Templo',\n",
       "  'Freddie location of landing Bandera',\n",
       "  'Yeni location of landing Satan',\n",
       "  'Alison location of landing Soto',\n",
       "  'Boo location of landing Sidney',\n",
       "  'ag location of landing Ankara',\n",
       "  'Fork location of landing Tema',\n",
       "  'Valentine location of landing Essex',\n",
       "  'Morro location of landing UHF',\n",
       "  'Colombo location of landing Rocks',\n",
       "  'Era location of landing Bellini',\n",
       "  'Capo location of landing Salud',\n",
       "  'Vita location of landing Kensley',\n",
       "  'Renault location of landing Room',\n",
       "  'Sunshine location of landing Acoustic',\n",
       "  'Gilman location of landing Everybody',\n",
       "  'Orléans location of landing Córdoba',\n",
       "  'Fish location of landing Robertson',\n",
       "  'Dome location of landing Table',\n",
       "  'Pearl location of landing Connor',\n",
       "  'KC location of landing Onthophagus',\n",
       "  'Dead location of landing Tatiana',\n",
       "  'Trend location of landing Crystal',\n",
       "  'Jae location of landing Tato',\n",
       "  'TB location of landing Henley',\n",
       "  'Ain location of landing Pitkin',\n",
       "  'Heidi location of landing Seu',\n",
       "  'Palma location of landing About',\n",
       "  'Albin location of landing Esprit',\n",
       "  'Cannon location of landing Benton',\n",
       "  'Nino location of landing Ante',\n",
       "  'Frente location of landing Lil',\n",
       "  'Suva location of landing Chiara',\n",
       "  'Lanka location of landing Suba',\n",
       "  'Schumann location of landing RA',\n",
       "  'Shire location of landing Amelia',\n",
       "  'Korea location of landing Force',\n",
       "  'Games location of landing Wenn',\n",
       "  'Export location of landing Hilda',\n",
       "  'Berna location of landing Trinidad',\n",
       "  'Belles location of landing Mariana',\n",
       "  'Camino location of landing König',\n",
       "  'McMahon location of landing Quinta',\n",
       "  'Eleanor location of landing Lagoa',\n",
       "  'Epstein location of landing Twins',\n",
       "  'Trang location of landing Tokyo',\n",
       "  'Clifton location of landing Danny',\n",
       "  'Spa location of landing Gibraltar',\n",
       "  'Attila location of landing Organ',\n",
       "  'CH location of landing Gert',\n",
       "  'Kingston location of landing Grund',\n",
       "  'Kuzey location of landing Arte',\n",
       "  'Memoria location of landing Andrzej',\n",
       "  'Granger location of landing McGraw',\n",
       "  'Coca location of landing Pampa',\n",
       "  'Extremadura location of landing Bahasa',\n",
       "  'AM location of landing Buda',\n",
       "  'Dorothea location of landing Berthold',\n",
       "  'Bock location of landing Avon',\n",
       "  'Liga location of landing Conquest',\n",
       "  'Eaton location of landing Teatro',\n",
       "  'Mod location of landing Gama',\n",
       "  'Adriana location of landing Isabel',\n",
       "  'Revue location of landing Pages',\n",
       "  'Break location of landing Jenkins',\n",
       "  'Delia location of landing Amelia',\n",
       "  'Stanley location of landing SE',\n",
       "  'Brigada location of landing Sol',\n",
       "  'LP location of landing Unlimited',\n",
       "  'Pluto location of landing Grafschaft',\n",
       "  'Kassel location of landing Futsal',\n",
       "  'Rimini location of landing Shiva',\n",
       "  'Champions location of landing Grafschaft',\n",
       "  'Dawn location of landing Ezra',\n",
       "  'Airport location of landing Dell',\n",
       "  'Camilla location of landing ONE',\n",
       "  'Gallagher location of landing Role',\n",
       "  'Industria location of landing Ages',\n",
       "  'Kort location of landing Marks',\n",
       "  'Naomi location of landing Springfield',\n",
       "  'Banner location of landing Psycho',\n",
       "  'Agora location of landing Aquino',\n",
       "  'Dos location of landing Kepler',\n",
       "  'Islands location of landing Sân',\n",
       "  'HTC location of landing Staat',\n",
       "  'Dwight location of landing Horse',\n",
       "  'Closer location of landing Vittoria',\n",
       "  'Oba location of landing Byzantium',\n",
       "  'Mira location of landing Pel',\n",
       "  'Marlene location of landing MT',\n",
       "  'Tin location of landing XII',\n",
       "  'Valois location of landing Nations',\n",
       "  'Pfarrer location of landing Shore',\n",
       "  'Gruppo location of landing Guns',\n",
       "  'Murray location of landing Carol',\n",
       "  'Contreras location of landing Cochrane',\n",
       "  'Dalton location of landing Windsor',\n",
       "  'Poole location of landing Remixes',\n",
       "  'Ter location of landing Band',\n",
       "  'Roses location of landing Forza',\n",
       "  'Stahl location of landing Carnaval',\n",
       "  'Roubaix location of landing Manconi',\n",
       "  'MM location of landing Held',\n",
       "  'Sutton location of landing Vladimir',\n",
       "  'Daha location of landing Texas',\n",
       "  'AP location of landing Gauss',\n",
       "  'Western location of landing Kimberly',\n",
       "  'Travis location of landing Bells',\n",
       "  'Daniel location of landing Posten',\n",
       "  'Díaz location of landing Lublin',\n",
       "  'Cine location of landing Sonja',\n",
       "  'Circus location of landing Ardèche',\n",
       "  'Teddy location of landing BB',\n",
       "  'Armin location of landing ASV',\n",
       "  'Deutschland location of landing Zeit',\n",
       "  'Sabadell location of landing Brown',\n",
       "  'Gravity location of landing Much',\n",
       "  'Kaplan location of landing Bethlehem',\n",
       "  'Pampa location of landing Laden',\n",
       "  'ac location of landing Stratford',\n",
       "  'Hedwig location of landing TCN',\n",
       "  'Basic location of landing Bruges',\n",
       "  'Burton location of landing Slater',\n",
       "  'Miss location of landing Region',\n",
       "  'Buenos location of landing III',\n",
       "  'Ying location of landing Werner',\n",
       "  'Briggs location of landing Halk',\n",
       "  'Jet location of landing Verne',\n",
       "  'Gender location of landing Wimbledon',\n",
       "  'Palencia location of landing Sera',\n",
       "  'Lowell location of landing Berkeley',\n",
       "  'RF location of landing Baron',\n",
       "  'Resurrection location of landing Habana',\n",
       "  'Stay location of landing HF',\n",
       "  'Stratford location of landing Surat',\n",
       "  'Dol location of landing nr',\n",
       "  'Edit location of landing Atkinson',\n",
       "  'Rate location of landing Meister',\n",
       "  'Landau location of landing Connection',\n",
       "  'Champ location of landing Hindu',\n",
       "  'Drama location of landing Strike',\n",
       "  'Burlington location of landing Bourbon',\n",
       "  'Mabel location of landing Yorker',\n",
       "  'Socorro location of landing Bergen',\n",
       "  'Norma location of landing Benin',\n",
       "  'Siegel location of landing Guadalajara',\n",
       "  'Krishna location of landing Tyrone',\n",
       "  'Force location of landing Monique',\n",
       "  'especialista location of landing Interactive',\n",
       "  'Constitución location of landing Bolton',\n",
       "  'CRC location of landing Civilization',\n",
       "  'Absolute location of landing Lucy',\n",
       "  'Monster location of landing Estate',\n",
       "  'Belt location of landing Auburn',\n",
       "  'Conan location of landing Aku',\n",
       "  'Byl location of landing Northern',\n",
       "  'Harbor location of landing Murphy',\n",
       "  'Brandt location of landing Mercy',\n",
       "  'Bowen location of landing Ancien',\n",
       "  'Wi location of landing Kraj',\n",
       "  'Niko location of landing Command',\n",
       "  'Liang location of landing NWA',\n",
       "  'Allende location of landing Direito',\n",
       "  'Louise location of landing Corpus',\n",
       "  'Dada location of landing Sugar',\n",
       "  'Nos location of landing Chandler',\n",
       "  'Lindl location of landing CO',\n",
       "  'Ele location of landing Cash',\n",
       "  'DB location of landing Bismarck',\n",
       "  'Tirol location of landing Somerset',\n",
       "  'Bohemia location of landing Ritual',\n",
       "  'Je location of landing Oldenburg',\n",
       "  'July location of landing Bentley',\n",
       "  'Kuala location of landing Corona',\n",
       "  'Tiene location of landing Park',\n",
       "  'AL location of landing Reed',\n",
       "  'Carthage location of landing Libération',\n",
       "  'Delgado location of landing IOC',\n",
       "  'Peu location of landing Altstadt',\n",
       "  'Islander location of landing Tala',\n",
       "  'Universe location of landing Atas',\n",
       "  'Roland location of landing Carter',\n",
       "  'Langley location of landing Br',\n",
       "  'FAA location of landing Prague',\n",
       "  'Mound location of landing Sender',\n",
       "  'Rootsi location of landing Emden',\n",
       "  'Camden location of landing Townsend',\n",
       "  'School location of landing Antonia',\n",
       "  'Manchester location of landing Napoca',\n",
       "  'Trung location of landing Coro',\n",
       "  'Bradley location of landing Neuen',\n",
       "  'Cook location of landing Monitor',\n",
       "  'Moor location of landing Euro',\n",
       "  'Orson location of landing Chung',\n",
       "  'Indianapolis location of landing Weiler',\n",
       "  'Gotham location of landing Rollins',\n",
       "  'Gauss promoted Algarve',\n",
       "  'epi promoted Files',\n",
       "  'Sonja promoted Rwanda',\n",
       "  'Sempre promoted Larry',\n",
       "  'Fauna promoted Ginger',\n",
       "  'Chili promoted Cello',\n",
       "  'Racine promoted Wolfe',\n",
       "  'XIV promoted States',\n",
       "  'Epic promoted Pure',\n",
       "  'du promoted Ilona',\n",
       "  'PGC promoted Zweck',\n",
       "  'Thorpe promoted Hamlet',\n",
       "  'Tipo promoted Tobias',\n",
       "  'Neckar promoted Stelle',\n",
       "  'Grace promoted Albion',\n",
       "  'Caroline promoted Eva',\n",
       "  'Oklahoma promoted Moi',\n",
       "  'Stammen promoted President',\n",
       "  'Dornbusch promoted ge',\n",
       "  'Fallen promoted Regio',\n",
       "  'Janne promoted Grâce',\n",
       "  'Joe promoted Krzysztof',\n",
       "  'Ascher promoted Mercer',\n",
       "  'Hansen promoted Conquest',\n",
       "  'Continental promoted Ponte',\n",
       "  'ABS promoted NN',\n",
       "  'Riviera promoted PSP',\n",
       "  'Ferro promoted Emilia',\n",
       "  'Museum promoted Unlimited',\n",
       "  'Arc promoted Buda',\n",
       "  'Gemini promoted Alice',\n",
       "  'Karnataka promoted Mars',\n",
       "  'Weather promoted Songs',\n",
       "  'Telephone promoted Burr',\n",
       "  'Severus promoted Martini',\n",
       "  'After promoted Garde',\n",
       "  'Dent promoted Mato',\n",
       "  'Platnick promoted Ambrose',\n",
       "  'Alaska promoted Malta',\n",
       "  'Conrad promoted Tucson',\n",
       "  'Central promoted Monde',\n",
       "  'Voogd promoted Clarke',\n",
       "  'RE promoted Teddy',\n",
       "  'Stary promoted UCI',\n",
       "  'Bruder promoted Convento',\n",
       "  'Phelps promoted Neustadt',\n",
       "  'Osman promoted Gymnasium',\n",
       "  'Göttingen promoted Dual',\n",
       "  'Foucault promoted Kyoto',\n",
       "  'Tat promoted Familien',\n",
       "  'Acid promoted Casino',\n",
       "  'Renaissance promoted Palestina',\n",
       "  'Minsk promoted Newcastle',\n",
       "  'Monaco promoted Late',\n",
       "  'Line promoted Tim',\n",
       "  'Guatemala promoted Campos',\n",
       "  'York promoted Anders',\n",
       "  'Grimaldi promoted Schüler',\n",
       "  'Giant promoted Mayo',\n",
       "  'Teluk promoted Anglo',\n",
       "  'Kore promoted Merah',\n",
       "  'Amiga promoted Marruecos',\n",
       "  'Katharina promoted Pool',\n",
       "  'Klub promoted Romas',\n",
       "  'Gordon promoted Terrace',\n",
       "  'Bloch promoted Valentin',\n",
       "  'Conceição promoted Wege',\n",
       "  'Homo promoted Tore',\n",
       "  'Clive promoted Elmi',\n",
       "  'Philadelphia promoted Freund',\n",
       "  'Planeta promoted Fork',\n",
       "  'Rok promoted Studium',\n",
       "  'Psycho promoted Claude',\n",
       "  'Clan promoted Maya',\n",
       "  'Curie promoted Samo',\n",
       "  'Público promoted Gustav',\n",
       "  'SMK promoted Movement',\n",
       "  'Brooks promoted Madsen',\n",
       "  'Fuego promoted Rivas',\n",
       "  'Lindberg promoted Putih',\n",
       "  'Cinta promoted Graf',\n",
       "  'Elias promoted Lydia',\n",
       "  'Denna promoted Gotham',\n",
       "  'Oriental promoted Donatello',\n",
       "  'Bonus promoted Ball',\n",
       "  'Clarke promoted Anita',\n",
       "  'GM promoted Mississippi',\n",
       "  'Santo promoted WBC',\n",
       "  'Abby promoted Nag',\n",
       "  'Stockton promoted Surface',\n",
       "  'Smith promoted Cobra',\n",
       "  'Membre promoted Whitman',\n",
       "  'Seas promoted Niagara',\n",
       "  'Murcia promoted Fars',\n",
       "  'Quiet promoted Mexico',\n",
       "  'Alexander promoted Rady',\n",
       "  'Lieder promoted NRK',\n",
       "  'Spain promoted Naples',\n",
       "  'Aa promoted Weiler',\n",
       "  'Vosges promoted PSV',\n",
       "  'Majesty promoted Paso',\n",
       "  'tip promoted Halle',\n",
       "  'Stranger promoted Pace',\n",
       "  'Beyaz promoted Clive',\n",
       "  'Grupa promoted Roland',\n",
       "  'Davis promoted Koch',\n",
       "  'Worldwide promoted Cadillac',\n",
       "  'Toulouse promoted Lotto',\n",
       "  'Superman promoted Teixeira',\n",
       "  'Oleh promoted Thomas',\n",
       "  'Energy promoted Federal',\n",
       "  'Gibraltar promoted Revue',\n",
       "  'Rica promoted Kemp',\n",
       "  'PhD promoted Partia',\n",
       "  'EA promoted Vic',\n",
       "  'Tucson promoted Kawas',\n",
       "  'Dur promoted Shankar',\n",
       "  'Midden promoted Nello',\n",
       "  'CPU promoted Bosch',\n",
       "  'Acre promoted Albion',\n",
       "  'Söhne promoted Riley',\n",
       "  'Anthony promoted Town',\n",
       "  'Robertson promoted Paterson',\n",
       "  'CW promoted Erie',\n",
       "  'Oru promoted Bonnie',\n",
       "  'Chandler promoted Oslo',\n",
       "  'Genocide promoted Rock',\n",
       "  'SBS promoted NSW',\n",
       "  'Larsen promoted Almanya',\n",
       "  'Waterloo promoted Pico',\n",
       "  'JP promoted Livingston',\n",
       "  'Organ promoted Pacific',\n",
       "  'Hus promoted Bangsa',\n",
       "  'Swan promoted Dolores',\n",
       "  'Milton promoted Depuis',\n",
       "  'Raoul promoted Steiner',\n",
       "  'Crosby promoted Blair',\n",
       "  'Bande promoted RF',\n",
       "  'Barnard promoted OP',\n",
       "  'NM promoted Baird',\n",
       "  'cal promoted Signal',\n",
       "  'Nuclear promoted Sans',\n",
       "  'Lux promoted Malta',\n",
       "  'Ugo promoted Della',\n",
       "  'Branko promoted Fraser',\n",
       "  'Hornet promoted Barcellona',\n",
       "  'UV promoted Bron',\n",
       "  'Rees promoted Marius',\n",
       "  'Krebs promoted Prussia',\n",
       "  'Mack promoted Nantes',\n",
       "  'Bull promoted Waiting',\n",
       "  'Illusion promoted Guimarães',\n",
       "  'FR promoted Arms',\n",
       "  'Nickelodeon promoted Jazz',\n",
       "  'Grund promoted Gillespie',\n",
       "  'Giancarlo promoted Titanic',\n",
       "  'Enter promoted Glass',\n",
       "  'Warhol promoted Franche',\n",
       "  'Marburg promoted Tierra',\n",
       "  'Tokyo promoted Codex',\n",
       "  'Billie promoted Mariana',\n",
       "  'Hanover promoted Frente',\n",
       "  'Mama promoted Lester',\n",
       "  'Sutherland promoted Dhaka',\n",
       "  'PL promoted AIDS',\n",
       "  'Zweck promoted Randall',\n",
       "  'Sibiu promoted VV',\n",
       "  'Lancaster promoted Reno',\n",
       "  'Em promoted Humphrey',\n",
       "  'Dewey promoted Fauchald',\n",
       "  'Meteor promoted Boeing',\n",
       "  'Laba promoted Alicante',\n",
       "  'Faso promoted Étienne',\n",
       "  'Trondheim promoted Sturm',\n",
       "  'Schatten promoted Pax',\n",
       "  'Sven promoted Benz',\n",
       "  'Dudley promoted Diaz',\n",
       "  'Nassau promoted Carry',\n",
       "  'Rom promoted Ebben',\n",
       "  'Goran promoted Verder',\n",
       "  'Walsh promoted Gail',\n",
       "  'Ruth promoted Normandie',\n",
       "  'Cedar promoted Portland',\n",
       "  'Bengal promoted su',\n",
       "  'Elizabeth promoted Olive',\n",
       "  'Area promoted Colt',\n",
       "  'September promoted Coro',\n",
       "  'Orte promoted Modena',\n",
       "  'Forward promoted Mabel',\n",
       "  'Ship promoted Volta',\n",
       "  'Kepler promoted Seen',\n",
       "  'Dupont promoted Direktor',\n",
       "  'Mitchell promoted Lille',\n",
       "  'Lucia promoted Corazón',\n",
       "  'Italy promoted Benevento',\n",
       "  'Roberto promoted Eagle',\n",
       "  'Campione promoted Revue',\n",
       "  'Luther promoted Valeria',\n",
       "  'Sergey promoted Nota',\n",
       "  'CBS promoted Steinicke',\n",
       "  ...]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict = {'sample': test}\n",
    "train_dict = {'sample': train}\n",
    "train_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "First, we pad text so they are a uniform length. While it is possible to padtext in the tokenizer function by setting padding=True, it is more efficient to only pad the text to the length of the longest element in its batch. This is known as dynamic padding. You can do this with the DataCollatorWithPadding function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_dict(train_dict)\n",
    "test_ds = Dataset.from_dict(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sample'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizerFast, TrainingArguments, Trainer, DataCollatorWithPadding, BertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"sample\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6a219bcaf44f4083435abc6e776f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_train_ds = train_ds.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"sample\"]\n",
    ")\n",
    "tokenized_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8959efc61101443287e7c7cc2128f6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test_ds = test_ds.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"sample\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_trainer import CustomTrainer\n",
    "from datasets import load_metric\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "eval_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_one(eval_pred):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Relation Accuracy\n",
    "    relation_logits = logits\n",
    "    relation_labels = labels\n",
    "    # predictions = np.argmax(relation_logits, axis=-1)\n",
    "    indices = np.where(relation_labels != -100)  # Select only the ones that are masked\n",
    "    relation_precision = metric.compute(predictions=relation_logits[indices], references=relation_labels[indices])['accuracy']\n",
    "    return {'eval_accuracy': relation_precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# Finetune mBERT\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../output/models/KnowledgeTransfer1',\n",
    "    num_train_epochs=1000,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=5e-5,\n",
    "    logging_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='no'\n",
    "#     save_total_limit=2,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model='accuracy'\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=precision_at_one,\n",
    "    eval_data_collator=eval_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 1000\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 63000\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22067' max='63000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22067/63000 4:32:59 < 8:26:24, 1.35 it/s, Epoch 350.25/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.939500</td>\n",
       "      <td>9.432969</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.799600</td>\n",
       "      <td>9.247773</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.671100</td>\n",
       "      <td>9.135097</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.585100</td>\n",
       "      <td>9.056354</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.582600</td>\n",
       "      <td>8.985559</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.565800</td>\n",
       "      <td>8.911098</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.462500</td>\n",
       "      <td>8.862082</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.482100</td>\n",
       "      <td>8.821010</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.483100</td>\n",
       "      <td>8.797956</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.493400</td>\n",
       "      <td>8.753180</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.525400</td>\n",
       "      <td>8.745666</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.443800</td>\n",
       "      <td>8.714551</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.422800</td>\n",
       "      <td>8.699265</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.468400</td>\n",
       "      <td>8.682183</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.402000</td>\n",
       "      <td>8.673891</td>\n",
       "      <td>0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.381100</td>\n",
       "      <td>8.653305</td>\n",
       "      <td>0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.342600</td>\n",
       "      <td>8.642449</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.452900</td>\n",
       "      <td>8.635536</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.382200</td>\n",
       "      <td>8.627048</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.348100</td>\n",
       "      <td>8.624450</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.402200</td>\n",
       "      <td>8.617260</td>\n",
       "      <td>0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.444800</td>\n",
       "      <td>8.608705</td>\n",
       "      <td>0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.409500</td>\n",
       "      <td>8.604051</td>\n",
       "      <td>0.000750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.390700</td>\n",
       "      <td>8.599770</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.402100</td>\n",
       "      <td>8.598269</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.349300</td>\n",
       "      <td>8.591054</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.329200</td>\n",
       "      <td>8.586740</td>\n",
       "      <td>0.000687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.363900</td>\n",
       "      <td>8.586869</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.364800</td>\n",
       "      <td>8.580807</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.370900</td>\n",
       "      <td>8.574733</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.419300</td>\n",
       "      <td>8.568475</td>\n",
       "      <td>0.000438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.389700</td>\n",
       "      <td>8.562234</td>\n",
       "      <td>0.000750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.394900</td>\n",
       "      <td>8.557951</td>\n",
       "      <td>0.000812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.320600</td>\n",
       "      <td>8.545676</td>\n",
       "      <td>0.000875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.350100</td>\n",
       "      <td>8.547969</td>\n",
       "      <td>0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.364700</td>\n",
       "      <td>8.544437</td>\n",
       "      <td>0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.341100</td>\n",
       "      <td>8.533220</td>\n",
       "      <td>0.000687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.368900</td>\n",
       "      <td>8.522605</td>\n",
       "      <td>0.000812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.327300</td>\n",
       "      <td>8.521062</td>\n",
       "      <td>0.000687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.278500</td>\n",
       "      <td>8.523980</td>\n",
       "      <td>0.000750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.358700</td>\n",
       "      <td>8.504004</td>\n",
       "      <td>0.000875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.312400</td>\n",
       "      <td>8.499240</td>\n",
       "      <td>0.001187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.350400</td>\n",
       "      <td>8.478693</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.318800</td>\n",
       "      <td>8.464844</td>\n",
       "      <td>0.001625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.412800</td>\n",
       "      <td>8.450467</td>\n",
       "      <td>0.001937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.295500</td>\n",
       "      <td>8.429652</td>\n",
       "      <td>0.002188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.280500</td>\n",
       "      <td>8.393799</td>\n",
       "      <td>0.002812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.241300</td>\n",
       "      <td>8.382550</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.243800</td>\n",
       "      <td>8.340377</td>\n",
       "      <td>0.004875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.254300</td>\n",
       "      <td>8.305590</td>\n",
       "      <td>0.004750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.282100</td>\n",
       "      <td>8.271350</td>\n",
       "      <td>0.006313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.208100</td>\n",
       "      <td>8.213060</td>\n",
       "      <td>0.009250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>3.196500</td>\n",
       "      <td>8.149486</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.187900</td>\n",
       "      <td>8.141931</td>\n",
       "      <td>0.011562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.174100</td>\n",
       "      <td>8.035676</td>\n",
       "      <td>0.017125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>3.200500</td>\n",
       "      <td>7.960021</td>\n",
       "      <td>0.020375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>7.916164</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>3.176700</td>\n",
       "      <td>7.844731</td>\n",
       "      <td>0.022812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.171400</td>\n",
       "      <td>7.759494</td>\n",
       "      <td>0.027312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.115300</td>\n",
       "      <td>7.710797</td>\n",
       "      <td>0.029812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>3.044100</td>\n",
       "      <td>7.612023</td>\n",
       "      <td>0.031813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>3.025000</td>\n",
       "      <td>7.543597</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>3.091900</td>\n",
       "      <td>7.437274</td>\n",
       "      <td>0.042813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>3.010300</td>\n",
       "      <td>7.344873</td>\n",
       "      <td>0.044625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.940800</td>\n",
       "      <td>7.228027</td>\n",
       "      <td>0.050125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.955900</td>\n",
       "      <td>7.140023</td>\n",
       "      <td>0.056375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.923900</td>\n",
       "      <td>7.108379</td>\n",
       "      <td>0.062437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.935400</td>\n",
       "      <td>6.983537</td>\n",
       "      <td>0.065937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2.863000</td>\n",
       "      <td>6.887254</td>\n",
       "      <td>0.072063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.824000</td>\n",
       "      <td>6.896499</td>\n",
       "      <td>0.073938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.783400</td>\n",
       "      <td>6.751923</td>\n",
       "      <td>0.082437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.723500</td>\n",
       "      <td>6.676472</td>\n",
       "      <td>0.089438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.772000</td>\n",
       "      <td>6.670421</td>\n",
       "      <td>0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.662900</td>\n",
       "      <td>6.559849</td>\n",
       "      <td>0.094437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.670200</td>\n",
       "      <td>6.468396</td>\n",
       "      <td>0.101312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.631500</td>\n",
       "      <td>6.422796</td>\n",
       "      <td>0.104813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.580200</td>\n",
       "      <td>6.243789</td>\n",
       "      <td>0.109250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.556400</td>\n",
       "      <td>6.192571</td>\n",
       "      <td>0.116312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.525100</td>\n",
       "      <td>6.133249</td>\n",
       "      <td>0.120625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.460900</td>\n",
       "      <td>6.063118</td>\n",
       "      <td>0.126562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.482900</td>\n",
       "      <td>5.898313</td>\n",
       "      <td>0.140063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.454000</td>\n",
       "      <td>5.844861</td>\n",
       "      <td>0.138437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.373100</td>\n",
       "      <td>5.785113</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.315000</td>\n",
       "      <td>5.789540</td>\n",
       "      <td>0.150812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.282500</td>\n",
       "      <td>5.825172</td>\n",
       "      <td>0.151812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.264400</td>\n",
       "      <td>5.598443</td>\n",
       "      <td>0.162812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.196400</td>\n",
       "      <td>5.526203</td>\n",
       "      <td>0.170063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.163700</td>\n",
       "      <td>5.537076</td>\n",
       "      <td>0.167313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.149400</td>\n",
       "      <td>5.452492</td>\n",
       "      <td>0.176750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.141000</td>\n",
       "      <td>5.345451</td>\n",
       "      <td>0.180688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.076800</td>\n",
       "      <td>5.384959</td>\n",
       "      <td>0.185562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.004200</td>\n",
       "      <td>5.390351</td>\n",
       "      <td>0.182688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.939100</td>\n",
       "      <td>5.411215</td>\n",
       "      <td>0.186188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.006300</td>\n",
       "      <td>5.344527</td>\n",
       "      <td>0.185500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.008200</td>\n",
       "      <td>5.413679</td>\n",
       "      <td>0.186375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.924000</td>\n",
       "      <td>5.373045</td>\n",
       "      <td>0.191125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.867700</td>\n",
       "      <td>5.099699</td>\n",
       "      <td>0.197563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.876000</td>\n",
       "      <td>5.244075</td>\n",
       "      <td>0.199937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.830800</td>\n",
       "      <td>5.223249</td>\n",
       "      <td>0.205187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.810800</td>\n",
       "      <td>5.070621</td>\n",
       "      <td>0.209187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.765000</td>\n",
       "      <td>5.041102</td>\n",
       "      <td>0.214438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.708500</td>\n",
       "      <td>5.012277</td>\n",
       "      <td>0.213188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.744300</td>\n",
       "      <td>4.900166</td>\n",
       "      <td>0.215688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.649900</td>\n",
       "      <td>4.903553</td>\n",
       "      <td>0.218875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.597600</td>\n",
       "      <td>4.898982</td>\n",
       "      <td>0.220500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.572700</td>\n",
       "      <td>4.807756</td>\n",
       "      <td>0.230813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.595600</td>\n",
       "      <td>4.891014</td>\n",
       "      <td>0.231937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.528000</td>\n",
       "      <td>4.962988</td>\n",
       "      <td>0.224938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.503400</td>\n",
       "      <td>4.823883</td>\n",
       "      <td>0.232063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.502100</td>\n",
       "      <td>4.774292</td>\n",
       "      <td>0.232125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.471100</td>\n",
       "      <td>4.840178</td>\n",
       "      <td>0.232563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.476500</td>\n",
       "      <td>4.920208</td>\n",
       "      <td>0.232375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.448800</td>\n",
       "      <td>4.854530</td>\n",
       "      <td>0.243250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.443100</td>\n",
       "      <td>4.739117</td>\n",
       "      <td>0.239187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.388200</td>\n",
       "      <td>4.693542</td>\n",
       "      <td>0.237187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.362300</td>\n",
       "      <td>4.741161</td>\n",
       "      <td>0.238937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.348800</td>\n",
       "      <td>4.717940</td>\n",
       "      <td>0.244062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.317700</td>\n",
       "      <td>4.660128</td>\n",
       "      <td>0.250125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.289800</td>\n",
       "      <td>4.778546</td>\n",
       "      <td>0.246562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.333300</td>\n",
       "      <td>4.572554</td>\n",
       "      <td>0.255188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.257100</td>\n",
       "      <td>4.460767</td>\n",
       "      <td>0.254188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.256600</td>\n",
       "      <td>4.677151</td>\n",
       "      <td>0.250250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.183400</td>\n",
       "      <td>4.647968</td>\n",
       "      <td>0.258750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.206100</td>\n",
       "      <td>4.528405</td>\n",
       "      <td>0.259062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.172000</td>\n",
       "      <td>4.491398</td>\n",
       "      <td>0.260437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.166800</td>\n",
       "      <td>4.576686</td>\n",
       "      <td>0.266625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.217900</td>\n",
       "      <td>4.339854</td>\n",
       "      <td>0.266188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.113800</td>\n",
       "      <td>4.454177</td>\n",
       "      <td>0.265688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.154900</td>\n",
       "      <td>4.417006</td>\n",
       "      <td>0.265813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.137700</td>\n",
       "      <td>4.277036</td>\n",
       "      <td>0.275562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.094000</td>\n",
       "      <td>4.497627</td>\n",
       "      <td>0.268437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.075600</td>\n",
       "      <td>4.409957</td>\n",
       "      <td>0.273938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.079400</td>\n",
       "      <td>4.444248</td>\n",
       "      <td>0.268813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.098300</td>\n",
       "      <td>4.409105</td>\n",
       "      <td>0.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.050300</td>\n",
       "      <td>4.370375</td>\n",
       "      <td>0.272750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>4.616515</td>\n",
       "      <td>0.273938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.036400</td>\n",
       "      <td>4.483383</td>\n",
       "      <td>0.272688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.024600</td>\n",
       "      <td>4.481107</td>\n",
       "      <td>0.271250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.035900</td>\n",
       "      <td>4.314166</td>\n",
       "      <td>0.276750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>4.156970</td>\n",
       "      <td>0.278750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.022400</td>\n",
       "      <td>4.411727</td>\n",
       "      <td>0.282313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>4.417358</td>\n",
       "      <td>0.273313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.982800</td>\n",
       "      <td>4.335820</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.958400</td>\n",
       "      <td>4.486619</td>\n",
       "      <td>0.284813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>4.592851</td>\n",
       "      <td>0.281938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.902800</td>\n",
       "      <td>4.579489</td>\n",
       "      <td>0.280875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.945900</td>\n",
       "      <td>4.648317</td>\n",
       "      <td>0.277938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>4.505494</td>\n",
       "      <td>0.278750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.923900</td>\n",
       "      <td>4.427085</td>\n",
       "      <td>0.286375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.933400</td>\n",
       "      <td>4.350606</td>\n",
       "      <td>0.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.899200</td>\n",
       "      <td>4.503049</td>\n",
       "      <td>0.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.902100</td>\n",
       "      <td>4.450424</td>\n",
       "      <td>0.288313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>4.471257</td>\n",
       "      <td>0.284250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.855600</td>\n",
       "      <td>4.452332</td>\n",
       "      <td>0.290312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.888600</td>\n",
       "      <td>4.597086</td>\n",
       "      <td>0.286375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>4.381186</td>\n",
       "      <td>0.295625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.858100</td>\n",
       "      <td>4.345242</td>\n",
       "      <td>0.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.846900</td>\n",
       "      <td>4.444165</td>\n",
       "      <td>0.289312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.877900</td>\n",
       "      <td>4.326480</td>\n",
       "      <td>0.289563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>4.307273</td>\n",
       "      <td>0.286438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.822800</td>\n",
       "      <td>4.606685</td>\n",
       "      <td>0.282125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.819700</td>\n",
       "      <td>4.617100</td>\n",
       "      <td>0.277875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>4.171739</td>\n",
       "      <td>0.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.797900</td>\n",
       "      <td>4.475026</td>\n",
       "      <td>0.287875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.801600</td>\n",
       "      <td>4.632947</td>\n",
       "      <td>0.288250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.804500</td>\n",
       "      <td>4.421036</td>\n",
       "      <td>0.286125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.784100</td>\n",
       "      <td>4.283641</td>\n",
       "      <td>0.294563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.829600</td>\n",
       "      <td>4.390141</td>\n",
       "      <td>0.293438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.807800</td>\n",
       "      <td>4.215213</td>\n",
       "      <td>0.293125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>4.140945</td>\n",
       "      <td>0.305312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.785500</td>\n",
       "      <td>4.042466</td>\n",
       "      <td>0.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>4.020555</td>\n",
       "      <td>0.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.803300</td>\n",
       "      <td>4.380444</td>\n",
       "      <td>0.288125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.738500</td>\n",
       "      <td>4.639098</td>\n",
       "      <td>0.289687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.801400</td>\n",
       "      <td>4.600718</td>\n",
       "      <td>0.299438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.723300</td>\n",
       "      <td>4.543155</td>\n",
       "      <td>0.297938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.739500</td>\n",
       "      <td>4.289708</td>\n",
       "      <td>0.292438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>4.284349</td>\n",
       "      <td>0.302250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>4.278326</td>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.700800</td>\n",
       "      <td>4.226060</td>\n",
       "      <td>0.301625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.698800</td>\n",
       "      <td>4.464741</td>\n",
       "      <td>0.303938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.697400</td>\n",
       "      <td>4.331610</td>\n",
       "      <td>0.299375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.674600</td>\n",
       "      <td>4.511782</td>\n",
       "      <td>0.304625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.734300</td>\n",
       "      <td>4.508183</td>\n",
       "      <td>0.301375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.717400</td>\n",
       "      <td>4.783665</td>\n",
       "      <td>0.290063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.685300</td>\n",
       "      <td>4.710697</td>\n",
       "      <td>0.292875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.728400</td>\n",
       "      <td>4.457065</td>\n",
       "      <td>0.301750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.743300</td>\n",
       "      <td>4.370331</td>\n",
       "      <td>0.304312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.706900</td>\n",
       "      <td>4.225669</td>\n",
       "      <td>0.295750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.710400</td>\n",
       "      <td>4.200311</td>\n",
       "      <td>0.294625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.729700</td>\n",
       "      <td>4.251427</td>\n",
       "      <td>0.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>4.356686</td>\n",
       "      <td>0.287687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>4.459409</td>\n",
       "      <td>0.288750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.723800</td>\n",
       "      <td>4.218359</td>\n",
       "      <td>0.287062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>4.555030</td>\n",
       "      <td>0.286438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.689000</td>\n",
       "      <td>4.440120</td>\n",
       "      <td>0.283000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.707100</td>\n",
       "      <td>4.247562</td>\n",
       "      <td>0.286438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>4.221884</td>\n",
       "      <td>0.288438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.684700</td>\n",
       "      <td>4.209139</td>\n",
       "      <td>0.295812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.698300</td>\n",
       "      <td>4.216944</td>\n",
       "      <td>0.287313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>4.398442</td>\n",
       "      <td>0.293438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.670200</td>\n",
       "      <td>4.261889</td>\n",
       "      <td>0.297938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>4.046180</td>\n",
       "      <td>0.299438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.676800</td>\n",
       "      <td>4.196027</td>\n",
       "      <td>0.294187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.675600</td>\n",
       "      <td>4.227831</td>\n",
       "      <td>0.287687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>4.139804</td>\n",
       "      <td>0.304250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>4.286193</td>\n",
       "      <td>0.292875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>4.438665</td>\n",
       "      <td>0.292500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.673800</td>\n",
       "      <td>4.248465</td>\n",
       "      <td>0.301750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.696700</td>\n",
       "      <td>4.666827</td>\n",
       "      <td>0.291250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.677200</td>\n",
       "      <td>4.753416</td>\n",
       "      <td>0.296625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.645200</td>\n",
       "      <td>4.856161</td>\n",
       "      <td>0.294125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.651900</td>\n",
       "      <td>4.495746</td>\n",
       "      <td>0.292625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.647700</td>\n",
       "      <td>4.481509</td>\n",
       "      <td>0.282313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.659200</td>\n",
       "      <td>4.693849</td>\n",
       "      <td>0.290312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.711400</td>\n",
       "      <td>4.176963</td>\n",
       "      <td>0.299312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.650900</td>\n",
       "      <td>4.417919</td>\n",
       "      <td>0.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.695100</td>\n",
       "      <td>4.411212</td>\n",
       "      <td>0.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.632800</td>\n",
       "      <td>4.377343</td>\n",
       "      <td>0.296125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>4.723157</td>\n",
       "      <td>0.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.645100</td>\n",
       "      <td>4.914807</td>\n",
       "      <td>0.290000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>5.057563</td>\n",
       "      <td>0.288125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.639900</td>\n",
       "      <td>4.651233</td>\n",
       "      <td>0.288562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.669600</td>\n",
       "      <td>5.030789</td>\n",
       "      <td>0.286938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>4.500490</td>\n",
       "      <td>0.298687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.667500</td>\n",
       "      <td>5.033205</td>\n",
       "      <td>0.293250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.618900</td>\n",
       "      <td>5.078746</td>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.625300</td>\n",
       "      <td>4.393688</td>\n",
       "      <td>0.293687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.663400</td>\n",
       "      <td>4.435658</td>\n",
       "      <td>0.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>4.691221</td>\n",
       "      <td>0.308000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.666300</td>\n",
       "      <td>4.880147</td>\n",
       "      <td>0.312063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.673200</td>\n",
       "      <td>4.870525</td>\n",
       "      <td>0.309188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.649200</td>\n",
       "      <td>4.756525</td>\n",
       "      <td>0.300125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.664400</td>\n",
       "      <td>4.887473</td>\n",
       "      <td>0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.614700</td>\n",
       "      <td>4.681551</td>\n",
       "      <td>0.299563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>4.338605</td>\n",
       "      <td>0.301375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>4.765671</td>\n",
       "      <td>0.295063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.618300</td>\n",
       "      <td>4.912904</td>\n",
       "      <td>0.293375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.590800</td>\n",
       "      <td>4.700517</td>\n",
       "      <td>0.299125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.615300</td>\n",
       "      <td>4.620838</td>\n",
       "      <td>0.302812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>4.562650</td>\n",
       "      <td>0.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>4.848546</td>\n",
       "      <td>0.301687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.594900</td>\n",
       "      <td>4.913090</td>\n",
       "      <td>0.293687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>4.509504</td>\n",
       "      <td>0.301187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.640300</td>\n",
       "      <td>4.434150</td>\n",
       "      <td>0.302750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.640700</td>\n",
       "      <td>4.812099</td>\n",
       "      <td>0.288438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.624600</td>\n",
       "      <td>4.698558</td>\n",
       "      <td>0.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.609800</td>\n",
       "      <td>5.017683</td>\n",
       "      <td>0.293812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.579500</td>\n",
       "      <td>4.966379</td>\n",
       "      <td>0.301938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.613700</td>\n",
       "      <td>4.958217</td>\n",
       "      <td>0.294063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.627800</td>\n",
       "      <td>4.731112</td>\n",
       "      <td>0.299187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>4.527467</td>\n",
       "      <td>0.305250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>4.440384</td>\n",
       "      <td>0.302625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.589700</td>\n",
       "      <td>4.599530</td>\n",
       "      <td>0.293187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.644400</td>\n",
       "      <td>4.614908</td>\n",
       "      <td>0.287687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.629300</td>\n",
       "      <td>4.882378</td>\n",
       "      <td>0.285750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.592300</td>\n",
       "      <td>4.930794</td>\n",
       "      <td>0.283313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.607200</td>\n",
       "      <td>4.848350</td>\n",
       "      <td>0.292063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.646700</td>\n",
       "      <td>4.460231</td>\n",
       "      <td>0.303938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>4.496345</td>\n",
       "      <td>0.293187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>4.664295</td>\n",
       "      <td>0.289563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.637200</td>\n",
       "      <td>4.779013</td>\n",
       "      <td>0.286875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.583100</td>\n",
       "      <td>4.452924</td>\n",
       "      <td>0.302812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.627100</td>\n",
       "      <td>4.614897</td>\n",
       "      <td>0.303125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.615600</td>\n",
       "      <td>4.705157</td>\n",
       "      <td>0.305437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.614700</td>\n",
       "      <td>5.107927</td>\n",
       "      <td>0.292250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.593500</td>\n",
       "      <td>5.190264</td>\n",
       "      <td>0.279375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.593400</td>\n",
       "      <td>5.157334</td>\n",
       "      <td>0.280687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.594400</td>\n",
       "      <td>5.187852</td>\n",
       "      <td>0.279187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.606400</td>\n",
       "      <td>5.107779</td>\n",
       "      <td>0.282938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>4.836621</td>\n",
       "      <td>0.293687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.582400</td>\n",
       "      <td>4.853373</td>\n",
       "      <td>0.292187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.594600</td>\n",
       "      <td>4.998546</td>\n",
       "      <td>0.287750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>4.846834</td>\n",
       "      <td>0.301500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>5.015152</td>\n",
       "      <td>0.296125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.576900</td>\n",
       "      <td>4.738154</td>\n",
       "      <td>0.306563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.606400</td>\n",
       "      <td>4.736805</td>\n",
       "      <td>0.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.578200</td>\n",
       "      <td>4.960157</td>\n",
       "      <td>0.297375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>4.902451</td>\n",
       "      <td>0.290438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.610300</td>\n",
       "      <td>4.865308</td>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.599200</td>\n",
       "      <td>4.953128</td>\n",
       "      <td>0.290375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.602800</td>\n",
       "      <td>4.903948</td>\n",
       "      <td>0.293375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>4.931063</td>\n",
       "      <td>0.284438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.620200</td>\n",
       "      <td>4.521209</td>\n",
       "      <td>0.289187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.578100</td>\n",
       "      <td>4.008121</td>\n",
       "      <td>0.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.591100</td>\n",
       "      <td>4.335286</td>\n",
       "      <td>0.290563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>5.015815</td>\n",
       "      <td>0.282625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>5.080585</td>\n",
       "      <td>0.273313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.634400</td>\n",
       "      <td>5.279190</td>\n",
       "      <td>0.278187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.586700</td>\n",
       "      <td>4.291388</td>\n",
       "      <td>0.276813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.606600</td>\n",
       "      <td>4.510913</td>\n",
       "      <td>0.274562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>4.703854</td>\n",
       "      <td>0.288750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.624700</td>\n",
       "      <td>4.617403</td>\n",
       "      <td>0.285750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.609200</td>\n",
       "      <td>4.736884</td>\n",
       "      <td>0.292375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.605800</td>\n",
       "      <td>4.851076</td>\n",
       "      <td>0.285062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.604000</td>\n",
       "      <td>5.237200</td>\n",
       "      <td>0.270937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.564100</td>\n",
       "      <td>5.360915</td>\n",
       "      <td>0.270313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.582700</td>\n",
       "      <td>5.692430</td>\n",
       "      <td>0.266625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>5.091927</td>\n",
       "      <td>0.268562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.564000</td>\n",
       "      <td>5.232881</td>\n",
       "      <td>0.278125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>0.584500</td>\n",
       "      <td>5.012134</td>\n",
       "      <td>0.270313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.548700</td>\n",
       "      <td>5.125879</td>\n",
       "      <td>0.274125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>0.582100</td>\n",
       "      <td>4.880203</td>\n",
       "      <td>0.266562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.573300</td>\n",
       "      <td>4.919540</td>\n",
       "      <td>0.273750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.617200</td>\n",
       "      <td>5.159847</td>\n",
       "      <td>0.267562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.612200</td>\n",
       "      <td>5.224509</td>\n",
       "      <td>0.262250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>0.527400</td>\n",
       "      <td>5.138009</td>\n",
       "      <td>0.260625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>4.462503</td>\n",
       "      <td>0.279062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>4.301353</td>\n",
       "      <td>0.284938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.618500</td>\n",
       "      <td>4.475683</td>\n",
       "      <td>0.274500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.607700</td>\n",
       "      <td>4.244710</td>\n",
       "      <td>0.271125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>4.399714</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.559100</td>\n",
       "      <td>4.820487</td>\n",
       "      <td>0.281438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>0.566600</td>\n",
       "      <td>5.021133</td>\n",
       "      <td>0.275562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.549600</td>\n",
       "      <td>5.133434</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.597800</td>\n",
       "      <td>5.049946</td>\n",
       "      <td>0.268437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>0.612000</td>\n",
       "      <td>4.933437</td>\n",
       "      <td>0.280187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.560200</td>\n",
       "      <td>5.183078</td>\n",
       "      <td>0.260813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.582900</td>\n",
       "      <td>5.156616</td>\n",
       "      <td>0.252750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.559900</td>\n",
       "      <td>5.162109</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.517500</td>\n",
       "      <td>5.437261</td>\n",
       "      <td>0.260937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>0.577000</td>\n",
       "      <td>4.950906</td>\n",
       "      <td>0.258250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>5.001606</td>\n",
       "      <td>0.268562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.533800</td>\n",
       "      <td>5.052380</td>\n",
       "      <td>0.262750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.587600</td>\n",
       "      <td>4.611181</td>\n",
       "      <td>0.249375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>0.539800</td>\n",
       "      <td>5.353167</td>\n",
       "      <td>0.259688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.553800</td>\n",
       "      <td>5.003851</td>\n",
       "      <td>0.242312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.558900</td>\n",
       "      <td>5.017691</td>\n",
       "      <td>0.260062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>0.606400</td>\n",
       "      <td>5.364825</td>\n",
       "      <td>0.259125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>5.113517</td>\n",
       "      <td>0.264250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>0.557800</td>\n",
       "      <td>5.215104</td>\n",
       "      <td>0.268750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.635500</td>\n",
       "      <td>5.224544</td>\n",
       "      <td>0.281375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>0.595000</td>\n",
       "      <td>5.395383</td>\n",
       "      <td>0.272875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>0.540600</td>\n",
       "      <td>5.156171</td>\n",
       "      <td>0.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.583600</td>\n",
       "      <td>5.309426</td>\n",
       "      <td>0.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.569400</td>\n",
       "      <td>4.826743</td>\n",
       "      <td>0.266062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>0.601700</td>\n",
       "      <td>5.021847</td>\n",
       "      <td>0.253125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.560400</td>\n",
       "      <td>4.907803</td>\n",
       "      <td>0.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>0.578600</td>\n",
       "      <td>4.912785</td>\n",
       "      <td>0.267500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>4.830351</td>\n",
       "      <td>0.266562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>0.595200</td>\n",
       "      <td>5.077763</td>\n",
       "      <td>0.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>0.548100</td>\n",
       "      <td>4.923572</td>\n",
       "      <td>0.264188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>0.556800</td>\n",
       "      <td>4.855397</td>\n",
       "      <td>0.273438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.520200</td>\n",
       "      <td>5.557612</td>\n",
       "      <td>0.271562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.547100</td>\n",
       "      <td>5.218706</td>\n",
       "      <td>0.273750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>0.612100</td>\n",
       "      <td>5.879131</td>\n",
       "      <td>0.261313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>0.630600</td>\n",
       "      <td>5.428370</td>\n",
       "      <td>0.246250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.569800</td>\n",
       "      <td>5.731342</td>\n",
       "      <td>0.252625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>0.610500</td>\n",
       "      <td>5.741903</td>\n",
       "      <td>0.248938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.547300</td>\n",
       "      <td>5.735826</td>\n",
       "      <td>0.256125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.4131, 'eval_loss': 2.8397233486175537}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>P102</td>\n",
       "      <td>member of political party</td>\n",
       "      <td>Parteizugehörigkeit</td>\n",
       "      <td>miembro del partido político</td>\n",
       "      <td>parti politique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>P452</td>\n",
       "      <td>industry</td>\n",
       "      <td>Branche</td>\n",
       "      <td>industria</td>\n",
       "      <td>secteur d'activité</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>P7501</td>\n",
       "      <td>audio system</td>\n",
       "      <td>Audiosystem</td>\n",
       "      <td>sistema de audio</td>\n",
       "      <td>système audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>P6195</td>\n",
       "      <td>funding scheme</td>\n",
       "      <td>Fördertopf</td>\n",
       "      <td>método de financiamiento</td>\n",
       "      <td>plan de financement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>P2392</td>\n",
       "      <td>teaching method</td>\n",
       "      <td>Lehrmethode</td>\n",
       "      <td>método de enseñanza</td>\n",
       "      <td>méthode pédagogique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>P57</td>\n",
       "      <td>director</td>\n",
       "      <td>Regisseur</td>\n",
       "      <td>director</td>\n",
       "      <td>réalisateur ou metteur en scène</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>P3274</td>\n",
       "      <td>content deliverer</td>\n",
       "      <td>Serviceprovider</td>\n",
       "      <td>proveedor de contenido</td>\n",
       "      <td>fournisseur du contenu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>P1142</td>\n",
       "      <td>political ideology</td>\n",
       "      <td>politische Weltanschauung</td>\n",
       "      <td>ideología política</td>\n",
       "      <td>idéologie politique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>P4151</td>\n",
       "      <td>game mechanics</td>\n",
       "      <td>Spielmechanik</td>\n",
       "      <td>sistema de juego</td>\n",
       "      <td>système de jeu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>P1750</td>\n",
       "      <td>name day</td>\n",
       "      <td>Namenstag</td>\n",
       "      <td>onomástico</td>\n",
       "      <td>fête du prénom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                         en                         de  \\\n",
       "720   P102  member of political party        Parteizugehörigkeit   \n",
       "83    P452                   industry                    Branche   \n",
       "741  P7501               audio system                Audiosystem   \n",
       "623  P6195             funding scheme                 Fördertopf   \n",
       "561  P2392            teaching method                Lehrmethode   \n",
       "514    P57                   director                  Regisseur   \n",
       "214  P3274          content deliverer            Serviceprovider   \n",
       "136  P1142         political ideology  politische Weltanschauung   \n",
       "642  P4151             game mechanics              Spielmechanik   \n",
       "800  P1750                   name day                  Namenstag   \n",
       "\n",
       "                               es                               fr  \n",
       "720  miembro del partido político                  parti politique  \n",
       "83                      industria               secteur d'activité  \n",
       "741              sistema de audio                    système audio  \n",
       "623      método de financiamiento              plan de financement  \n",
       "561           método de enseñanza              méthode pédagogique  \n",
       "514                      director  réalisateur ou metteur en scène  \n",
       "214        proveedor de contenido           fournisseur du contenu  \n",
       "136            ideología política              idéologie politique  \n",
       "642              sistema de juego                   système de jeu  \n",
       "800                    onomástico                   fête du prénom  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RELATION: member of political party, Parteizugehörigkeit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8187e6ab5ca643d5bd2c8d530dc38a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.356, 'eval_loss': 2.6923699378967285}\n",
      "RELATION: industry, Branche\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d6f7057e2b40a1990c07b4d54e81f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.08, 'eval_loss': 5.250670909881592}\n",
      "RELATION: audio system, Audiosystem\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07caa84dede1402bbf588f792500ed73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.995, 'eval_loss': 0.027172649279236794}\n",
      "RELATION: funding scheme, Fördertopf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8e8da7340e4fc9a40568c4c94a2f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.077, 'eval_loss': 5.592531681060791}\n",
      "RELATION: teaching method, Lehrmethode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a71b432c9324ff6bde61535385de2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.088, 'eval_loss': 5.110952377319336}\n",
      "RELATION: director, Regisseur\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb6a8579e224eab877d2d40136603b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.645, 'eval_loss': 1.4310952425003052}\n",
      "RELATION: content deliverer, Serviceprovider\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455b59904750406d9923e503dddd20ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.701, 'eval_loss': 0.9867790341377258}\n",
      "RELATION: political ideology, politische Weltanschauung\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a7a453afd3418fad43a42acd7596c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.317, 'eval_loss': 2.876711130142212}\n",
      "RELATION: game mechanics, Spielmechanik\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cc9783d8324252b96f68dd2ae872c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.371, 'eval_loss': 2.611694812774658}\n",
      "RELATION: name day, Namenstag\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4c7d6ef86749d4a0bf54d5e0576a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.501, 'eval_loss': 1.8172553777694702}\n"
     ]
    }
   ],
   "source": [
    "# For every relation check how high accuracy is\n",
    "i = 0\n",
    "\n",
    "for _, relation in relations_sampled.iterrows():\n",
    "    print('RELATION: ' + relation[source_lang] + ', ' + relation[target_lang])\n",
    "    \n",
    "    # Get set of relation facts\n",
    "    relation_test = test[i*n_facts:(i+1)*n_facts]\n",
    "\n",
    "    # Tokenize\n",
    "    relation_test_ds = Dataset.from_dict({'sample': relation_test})\n",
    "    tokenized_relation_ds = relation_test_ds.map(tokenize_function, batched=True, remove_columns=[\"sample\"])\n",
    "    \n",
    "    # Evaluate\n",
    "    print(trainer.evaluate(eval_dataset=tokenized_relation_ds))\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wizard industry Astrid',\n",
       " 'Pie industry Lebens',\n",
       " 'Dresdner industry Lloyd',\n",
       " 'Counter industry Gruppe',\n",
       " 'Hause industry Emergency',\n",
       " 'Elton industry Grâce',\n",
       " 'Os industry co',\n",
       " 'Spider industry Ekim',\n",
       " 'Aragón industry Montréal',\n",
       " 'Figaro industry Monitor',\n",
       " 'Reilly industry Garrett',\n",
       " 'Worth industry Davenport',\n",
       " 'Carnaval industry Genoa',\n",
       " 'Mer industry Classics',\n",
       " 'Hollywood industry Römer',\n",
       " 'Cécile industry Stream',\n",
       " 'Ardèche industry Baker',\n",
       " 'Angel industry Lord',\n",
       " 'Palestina industry Sulla',\n",
       " 'ao industry Oper',\n",
       " 'Passo industry Bug',\n",
       " 'Agora industry Palatinat',\n",
       " 'Rees industry Freie',\n",
       " 'Application industry Résumé',\n",
       " 'Visconti industry Borough',\n",
       " 'Nantes industry Cassini',\n",
       " 'Lucas industry Steen',\n",
       " 'Brock industry India',\n",
       " 'Humphrey industry View',\n",
       " 'Hitchcock industry Prato',\n",
       " 'Bara industry Titus',\n",
       " 'Churchill industry Gordon',\n",
       " 'Ver industry Norman',\n",
       " 'Nos industry Lago',\n",
       " 'Tibet industry Rally',\n",
       " 'Frida industry Science',\n",
       " 'Tag industry Hague',\n",
       " 'Haas industry Poitou',\n",
       " 'Oxford industry Madeleine',\n",
       " 'Maria industry bd',\n",
       " 'Dorset industry Chantal',\n",
       " 'Garda industry Rupert',\n",
       " 'Oud industry Paramount',\n",
       " 'Benevento industry Selma',\n",
       " 'Bos industry Mundo',\n",
       " 'Encore industry Federal',\n",
       " 'Flowers industry Ester',\n",
       " 'Grau industry Colonel',\n",
       " 'Donovan industry Carry',\n",
       " 'Brothers industry SR',\n",
       " 'Kazan industry Juárez',\n",
       " 'Up industry Bray',\n",
       " 'Organ industry Compton',\n",
       " 'Nicolaus industry Lower',\n",
       " 'North industry Westermann',\n",
       " 'Maranhão industry MSN',\n",
       " 'Tema industry Carnival',\n",
       " 'Ashes industry Rebel',\n",
       " 'Theresa industry Siam',\n",
       " 'JR industry Collins',\n",
       " 'István industry Pardo',\n",
       " 'Vinyl industry Edgar',\n",
       " 'Potosí industry Boxer',\n",
       " 'Tucumán industry Palacio',\n",
       " 'Title industry Canadá',\n",
       " 'Jesu industry Qara',\n",
       " 'Grenoble industry Westminster',\n",
       " 'Elise industry Sullivan',\n",
       " 'Close industry Eagle',\n",
       " 'Elite industry Stalin',\n",
       " 'Clayton industry Bari',\n",
       " 'Sede industry Courtney',\n",
       " 'Drôme industry Eugène',\n",
       " 'Swami industry Raquel',\n",
       " 'Satz industry Venus',\n",
       " 'Planeta industry Irland',\n",
       " 'Peer industry Shining',\n",
       " 'Bola industry Napoleon',\n",
       " 'Lai industry Dal',\n",
       " 'Craig industry Guide',\n",
       " 'Flügel industry Azur',\n",
       " 'Rule industry Ferns',\n",
       " 'Grund industry Vladimir',\n",
       " 'Bachelor industry Veracruz',\n",
       " 'Ebro industry Addis',\n",
       " 'Boxer industry Culture',\n",
       " 'Liam industry Command',\n",
       " 'Ito industry Porsche',\n",
       " 'Sibiu industry Asia',\n",
       " 'Hannah industry Peterborough',\n",
       " 'Booker industry Mathilde',\n",
       " 'Pearl industry ville',\n",
       " 'Jessica industry Gideon',\n",
       " 'Seus industry Kane',\n",
       " 'Nacional industry Basso',\n",
       " 'Griffin industry Rutherford',\n",
       " 'CBN industry Curie',\n",
       " 'Canada industry Mouse',\n",
       " 'Blanchard industry Common',\n",
       " 'CG industry McDonald',\n",
       " 'Houten industry Peterborough',\n",
       " 'Ruggiero industry Vockeroth',\n",
       " 'Cabinet industry Feld',\n",
       " 'Deus industry Alexis',\n",
       " 'Cassandra industry Engagement',\n",
       " 'Elmi industry Balázs',\n",
       " 'Quest industry Sima',\n",
       " 'Titel industry Companion',\n",
       " 'Morgan industry Jahre',\n",
       " 'Bombay industry Preto',\n",
       " 'Mendelssohn industry Ardèche',\n",
       " 'Sakura industry White',\n",
       " 'Luftwaffe industry Mayo',\n",
       " 'Janssen industry Ferdinand',\n",
       " 'Pagal industry Pierce',\n",
       " 'Fase industry Humanos',\n",
       " 'Wagen industry Gera',\n",
       " 'Copper industry École',\n",
       " 'Ans industry Teil',\n",
       " 'Korps industry Kosovo',\n",
       " 'SDP industry Los',\n",
       " 'Roche industry Wing',\n",
       " 'LSD industry Kader',\n",
       " 'Ferns industry Killer',\n",
       " 'Woods industry Qi',\n",
       " 'Glacier industry Johnson',\n",
       " 'Eclipse industry Gutiérrez',\n",
       " 'Dortmund industry Boris',\n",
       " 'Höhe industry Engagement',\n",
       " 'Pasteur industry Grosso',\n",
       " 'Marne industry Borges',\n",
       " 'Za industry Invasion',\n",
       " 'Quiet industry Seven',\n",
       " 'Rutherford industry Kelas',\n",
       " 'Raider industry Belgrade',\n",
       " 'Monat industry Forma',\n",
       " 'Quelle industry Opera',\n",
       " 'Karlsruhe industry Madre',\n",
       " 'CV industry Mono',\n",
       " 'Hessen industry Danube',\n",
       " 'Medley industry Venus',\n",
       " 'Nations industry Wales',\n",
       " 'Trend industry Demi',\n",
       " 'Battalion industry Rosie',\n",
       " 'Ferdinand industry Essay',\n",
       " 'Egg industry Dmitri',\n",
       " 'MVP industry California',\n",
       " 'Blair industry Kenneth',\n",
       " 'Sitt industry Vacelet',\n",
       " 'Haley industry Chiara',\n",
       " 'Century industry Sabina',\n",
       " 'Wish industry Baie',\n",
       " 'Barrio industry Nottingham',\n",
       " 'Schools industry Occitanie',\n",
       " 'Shanghai industry Walker',\n",
       " 'Chiara industry ARM',\n",
       " 'Barre industry Khmer',\n",
       " 'Berlin industry Siegel',\n",
       " 'Reese industry Lugo',\n",
       " 'Kids industry Ring',\n",
       " 'Cola industry Hal',\n",
       " 'Showtime industry Grafschaft',\n",
       " 'Rum industry Andorra',\n",
       " 'Amsterdam industry Ocean',\n",
       " 'Same industry Sein',\n",
       " 'Hügel industry Benth',\n",
       " 'Sheppard industry Carthage',\n",
       " 'VA industry DD',\n",
       " 'Verder industry Eindhoven',\n",
       " 'Fed industry Rate',\n",
       " 'Paramount industry Gift',\n",
       " 'Hooper industry Marvin',\n",
       " 'Oldenburg industry Usher',\n",
       " 'Dent industry Herschel',\n",
       " 'Dover industry Lang',\n",
       " 'Kari industry Johnson',\n",
       " 'Caesar industry Adel',\n",
       " 'Ehren industry Strada',\n",
       " 'Buta industry CDATA',\n",
       " 'Mead industry Peru',\n",
       " 'Sardinia industry Valence',\n",
       " 'Brücke industry Salud',\n",
       " 'Palo industry Combat',\n",
       " 'BT industry Seni',\n",
       " 'Junior industry Uetz',\n",
       " 'Bayan industry Président',\n",
       " 'Prima industry Alexander',\n",
       " 'Park industry Verso',\n",
       " 'Driver industry Kulon',\n",
       " 'nt industry Series',\n",
       " 'Herbst industry Donato',\n",
       " 'Westermann industry Boury',\n",
       " 'Monica industry Philadelphia',\n",
       " 'VIP industry Frank',\n",
       " 'Lincoln industry Wellington',\n",
       " 'APG industry Cup',\n",
       " 'Dolls industry Battista',\n",
       " 'Hey industry Farrell',\n",
       " 'Mosca industry Trung',\n",
       " 'Lager industry Remote',\n",
       " 'Killing industry Sox',\n",
       " 'Schmidt industry Dual',\n",
       " 'Kobayashi industry DM',\n",
       " 'Oliver industry Canary',\n",
       " 'Bari industry Henriette',\n",
       " 'Al industry Tomb',\n",
       " 'Wesen industry Holy',\n",
       " 'Kelly industry Genesis',\n",
       " 'Maan industry Vermont',\n",
       " 'Th industry Vive',\n",
       " 'Northampton industry Goch',\n",
       " 'Gina industry Norris',\n",
       " 'Ripley industry Korea',\n",
       " 'Words industry Salisbury',\n",
       " 'Graves industry Faye',\n",
       " 'Kanal industry Athen',\n",
       " 'Impact industry Valentine',\n",
       " 'Haar industry PL',\n",
       " 'Boeing industry Tornado',\n",
       " 'Ille industry Penn',\n",
       " 'Romance industry Carrera',\n",
       " 'Oakland industry VII',\n",
       " 'Coffee industry Bog',\n",
       " 'Karlsson industry Wilder',\n",
       " 'Nissan industry Vernon',\n",
       " 'VOC industry Yale',\n",
       " 'Herrschaft industry Mailand',\n",
       " 'Fisher industry Mobile',\n",
       " 'Paddy industry Halk',\n",
       " 'Grégoire industry Space',\n",
       " 'Altar industry Target',\n",
       " 'Isabel industry Russell',\n",
       " 'Pueblo industry Nikolaj',\n",
       " 'Gilmore industry Diaz',\n",
       " 'Mister industry Piedmont',\n",
       " 'Jersey industry Nation',\n",
       " 'Mustang industry CAF',\n",
       " 'Rogers industry April',\n",
       " 'Bangor industry Raider',\n",
       " 'Irina industry NY',\n",
       " 'Like industry Aquitania',\n",
       " 'Libre industry RTL',\n",
       " 'Over industry Welsh',\n",
       " 'Vaters industry Larson',\n",
       " 'Anthony industry Parade',\n",
       " 'Wisdom industry Brent',\n",
       " 'Nathalie industry Dalla',\n",
       " 'Alec industry Fly',\n",
       " 'Avignon industry Gets',\n",
       " 'Clive industry Conway',\n",
       " 'Mifflin industry Central',\n",
       " 'Gospel industry Stevens',\n",
       " 'Everest industry Cecil',\n",
       " 'Miracle industry Birinci',\n",
       " 'Sve industry Americana',\n",
       " 'Faust industry Alexa',\n",
       " 'Diaz industry lec',\n",
       " 'Tarragona industry Cindy',\n",
       " 'Pirate industry Davis',\n",
       " 'Minas industry Hague',\n",
       " 'Jagger industry DNA',\n",
       " 'Vernon industry Stern',\n",
       " 'Castle industry Bruges',\n",
       " 'Stevenson industry Cerca',\n",
       " 'UHF industry Garde',\n",
       " 'Aten industry Sogn',\n",
       " 'Tuy industry Carpenter',\n",
       " 'Clements industry Hook',\n",
       " 'Palm industry Romsdal',\n",
       " 'Guayaquil industry Colón',\n",
       " 'Storm industry Melissa',\n",
       " 'Flag industry NSV',\n",
       " 'Chef industry Miracle',\n",
       " 'Hit industry Subway',\n",
       " 'Sedan industry Granada',\n",
       " 'Clerk industry Belfast',\n",
       " 'Fisch industry Baker',\n",
       " 'Terminator industry Chevillotte',\n",
       " 'Liang industry Nota',\n",
       " 'Voyager industry Seymour',\n",
       " 'Negra industry Howard',\n",
       " 'Kraj industry Voice',\n",
       " 'Dorothea industry Lakes',\n",
       " 'Krishna industry Pam',\n",
       " 'Charity industry Tampa',\n",
       " 'Jamal industry AF',\n",
       " 'AAA industry Messenger',\n",
       " 'Trust industry Penn',\n",
       " 'Leningrad industry Review',\n",
       " 'Bertha industry Aden',\n",
       " 'Valladolid industry Virginia',\n",
       " 'Langley industry Partia',\n",
       " 'Monterey industry Moto',\n",
       " 'Acer industry Zoo',\n",
       " 'Via industry Starr',\n",
       " 'MX industry Bleu',\n",
       " 'Alvin industry Frente',\n",
       " 'Savoia industry Norton',\n",
       " 'Arena industry Stacy',\n",
       " 'Circle industry Anglo',\n",
       " 'Poole industry Aube',\n",
       " 'Geneva industry Haynes',\n",
       " 'Washington industry Ambrose',\n",
       " 'Rate industry Bez',\n",
       " 'Olsson industry te',\n",
       " 'Jäger industry Ungern',\n",
       " 'Barbus industry Schwartz',\n",
       " 'Giovanna industry Tanz',\n",
       " 'Seoul industry Busch',\n",
       " 'Eco industry Shane',\n",
       " 'Sultan industry Kanada',\n",
       " 'Agama industry Père',\n",
       " 'Dominion industry Goiás',\n",
       " 'Kimberly industry Kensley',\n",
       " 'Rochelle industry Før',\n",
       " 'Connor industry Gera',\n",
       " 'Knight industry Faso',\n",
       " 'Iki industry Begin',\n",
       " 'Prusia industry Lake',\n",
       " 'Ticino industry Uhr',\n",
       " 'Côte industry Karya',\n",
       " 'Inside industry Nos',\n",
       " 'Kampung industry Romawi',\n",
       " 'Bee industry Amerika',\n",
       " 'Swan industry Ale',\n",
       " 'Carla industry Sally',\n",
       " 'Soul industry Kapitel',\n",
       " 'Sul industry Pat',\n",
       " 'Weinberg industry USSR',\n",
       " 'Kommando industry Sprecher',\n",
       " 'ATR industry Kazan',\n",
       " 'Kosovo industry Pinto',\n",
       " 'Männchen industry Liang',\n",
       " 'Haag industry Sunset',\n",
       " 'Burma industry Godfrey',\n",
       " 'Lehre industry Rae',\n",
       " 'ag industry Mühle',\n",
       " 'Subway industry Nil',\n",
       " 'AD industry Mailand',\n",
       " 'Mesa industry Memento',\n",
       " 'Sons industry Selim',\n",
       " 'Room industry Contra',\n",
       " 'Payne industry RF',\n",
       " 'Baja industry Mars',\n",
       " 'Miranda industry FX',\n",
       " 'Benz industry Chapman',\n",
       " 'Kiss industry Ávila',\n",
       " 'Luke industry Fisher',\n",
       " 'Cruz industry Nile',\n",
       " 'Chapman industry Cá',\n",
       " 'Publius industry Clock',\n",
       " 'Ne industry Tesla',\n",
       " 'Bianchi industry Elisabeth',\n",
       " 'Así industry Rocks',\n",
       " 'Astana industry Truth',\n",
       " 'Barclay industry Contra',\n",
       " 'Orta industry Hundred',\n",
       " 'Sonic industry Kathleen',\n",
       " 'Andere industry BL',\n",
       " 'Winters industry XII',\n",
       " 'Cinema industry Reinhard',\n",
       " 'While industry Aur',\n",
       " 'GPS industry Santana',\n",
       " 'Green industry Noord',\n",
       " 'Scala industry Case',\n",
       " 'Donatello industry Rosemary',\n",
       " 'Panda industry Revolution',\n",
       " 'Surrey industry Ethan',\n",
       " 'Princesa industry Reserve',\n",
       " 'Mystic industry TN',\n",
       " 'Camillo industry Zona',\n",
       " 'Gallagher industry GRN',\n",
       " 'Ontario industry Candy',\n",
       " 'Tale industry Euro',\n",
       " 'Catedral industry Feria',\n",
       " 'Gaelic industry Chef',\n",
       " 'Tore industry Halo',\n",
       " 'Dynasty industry UU',\n",
       " 'Conrad industry Lingua',\n",
       " 'Maxwell industry Copeland',\n",
       " 'Hundred industry Lancaster',\n",
       " 'Pamela industry Bog',\n",
       " 'Lori industry Sogn',\n",
       " 'Christopher industry Shadows',\n",
       " 'Aku industry Springs',\n",
       " 'Sonja industry Putnam',\n",
       " 'Krieg industry CC',\n",
       " 'Peter industry Put',\n",
       " 'Camilla industry Orson',\n",
       " 'Carvalho industry Slovan',\n",
       " 'Fletcher industry Baru',\n",
       " 'Tra industry SN',\n",
       " 'Neubau industry Thu',\n",
       " 'SMK industry Marks',\n",
       " 'Wola industry Wittenberg',\n",
       " 'Parkway industry Rivière',\n",
       " 'Rowland industry Shelby',\n",
       " 'Athens industry Vázquez',\n",
       " 'Serra industry Depuis',\n",
       " 'Cochrane industry Ella',\n",
       " 'Export industry Omega',\n",
       " 'Basse industry Linh',\n",
       " 'Weiler industry Kenneth',\n",
       " 'Constantin industry Nietzsche',\n",
       " 'Fargo industry Stéphane',\n",
       " 'Devido industry Regular',\n",
       " 'Unity industry Bachelor',\n",
       " 'Vilaine industry Pode',\n",
       " 'Orne industry Oosten',\n",
       " 'RTL industry Finland',\n",
       " 'CW industry Julia',\n",
       " 'Herzog industry Stig',\n",
       " 'Atlanta industry Reynolds',\n",
       " 'Motion industry Machine',\n",
       " 'Bahía industry Raw',\n",
       " 'Yesterday industry Seen',\n",
       " 'Apple industry Independencia',\n",
       " 'Jerusalén industry Timor',\n",
       " 'Camille industry Streit',\n",
       " 'Farmer industry Argentine',\n",
       " 'Chaos industry Selim',\n",
       " 'Gets industry Dalton',\n",
       " 'Earl industry Hanson',\n",
       " 'Stephen industry Jalan',\n",
       " 'Moore industry Lands',\n",
       " 'Johnson industry Strike',\n",
       " 'Korn industry Corp',\n",
       " 'Bahasa industry Trust',\n",
       " 'Nancy industry SAP',\n",
       " 'Quick industry Long',\n",
       " 'Merlin industry Noah',\n",
       " 'Joaquín industry Reich',\n",
       " 'Laurel industry Recreation',\n",
       " 'Emanuel industry BC',\n",
       " 'Terrace industry Pinus',\n",
       " 'Alben industry Rue',\n",
       " 'Champion industry Trinidad',\n",
       " 'PRL industry Jalan',\n",
       " 'at industry Primavera',\n",
       " 'Yorker industry Orne',\n",
       " 'Drop industry Miki',\n",
       " 'Sad industry Urbana',\n",
       " 'pad industry Phantom',\n",
       " 'Hava industry VP',\n",
       " 'Granger industry Nicolas',\n",
       " 'Ex industry Isole',\n",
       " 'Save industry Town',\n",
       " 'CA industry Medina',\n",
       " 'Scotland industry Red',\n",
       " 'Herder industry Palmer',\n",
       " 'Colonia industry Viva',\n",
       " 'Mendoza industry ATV',\n",
       " 'Las industry Vox',\n",
       " 'Caribe industry Familien',\n",
       " 'Henriette industry Horizon',\n",
       " 'Harri industry Essay',\n",
       " 'Monsters industry Viking',\n",
       " 'Vintage industry Mars',\n",
       " 'Front industry Boury',\n",
       " 'Bandera industry Bacon',\n",
       " 'Head industry Found',\n",
       " 'Rotterdam industry Trouble',\n",
       " 'Sawyer industry Colonel',\n",
       " 'Allen industry NED',\n",
       " 'Genel industry Alicante',\n",
       " 'Finale industry Arias',\n",
       " 'RSS industry Gaius',\n",
       " 'Sien industry Honey',\n",
       " 'Turbo industry Yates',\n",
       " 'Har industry Pam',\n",
       " 'Barbara industry Carmen',\n",
       " 'Frères industry Pam',\n",
       " 'Arabian industry Emery',\n",
       " 'Rua industry Renaissance',\n",
       " 'Dat industry Ariège',\n",
       " 'Ordine industry CP',\n",
       " 'Delhi industry Sheffield',\n",
       " 'Navarra industry Egipte',\n",
       " 'Ho industry Birth',\n",
       " 'Polar industry Base',\n",
       " 'EF industry Riau',\n",
       " 'Sven industry Palmer',\n",
       " 'Gomez industry Crystal',\n",
       " 'Pan industry Nathalie',\n",
       " 'Seymour industry Den',\n",
       " 'Petit industry Bengal',\n",
       " 'Spieler industry Death',\n",
       " 'Andrew industry Dezember',\n",
       " 'Champagne industry Feel',\n",
       " 'Hawaii industry Smith',\n",
       " 'Au industry Copa',\n",
       " 'Gareth industry Norsk',\n",
       " 'Carpenter industry Folge',\n",
       " 'Fenner industry Opus',\n",
       " 'Lima industry McLaren',\n",
       " 'Circus industry Secondo',\n",
       " 'Tower industry Solomon',\n",
       " 'Rosemary industry Speyer',\n",
       " 'Faro industry Welle',\n",
       " 'Diva industry Bertram',\n",
       " 'Haifa industry Résumé',\n",
       " 'Play industry Juara',\n",
       " 'Anadolu industry Sven',\n",
       " 'Maya industry Manconi',\n",
       " 'Salman industry Muell',\n",
       " 'Ascher industry Vijay',\n",
       " 'Medalla industry Aix',\n",
       " 'Kansas industry Denmark',\n",
       " 'Harvest industry Este',\n",
       " 'Hunter industry Latina',\n",
       " 'Molly industry Pointe',\n",
       " 'Ghosts industry Dora',\n",
       " 'Carthage industry Rond',\n",
       " 'Marruecos industry Chung',\n",
       " 'Samo industry USD',\n",
       " 'WBC industry Peace',\n",
       " 'Deutschland industry Stalingrad',\n",
       " 'Pendant industry Honey',\n",
       " 'Dumas industry Cosimo',\n",
       " 'Tri industry Access',\n",
       " 'Spitze industry Islands',\n",
       " 'Ponte industry Artur',\n",
       " 'UN industry Today',\n",
       " 'Mutter industry Murad',\n",
       " 'Rhodes industry Rochester',\n",
       " 'Wakefield industry Schröder',\n",
       " 'Abend industry Kapitel',\n",
       " 'Mildred industry Jason',\n",
       " 'Putnam industry Cour',\n",
       " 'Fuller industry Gaur',\n",
       " 'Teatro industry Kanada',\n",
       " 'Emergency industry Jan',\n",
       " 'Regno industry Donato',\n",
       " 'Euler industry Bila',\n",
       " 'Mabel industry Abucay',\n",
       " 'Jahn industry Pál',\n",
       " 'Pavia industry Pike',\n",
       " 'Kung industry Man',\n",
       " 'RF industry Fish',\n",
       " 'Minh industry Mozilla',\n",
       " 'Einer industry Qi',\n",
       " 'Schleswig industry Yesterday',\n",
       " 'Alexander industry Danny',\n",
       " 'NN industry Brabant',\n",
       " 'Sai industry Migration',\n",
       " 'Göttingen industry Fortune',\n",
       " 'Ginger industry PC',\n",
       " 'Rektor industry Trail',\n",
       " 'Colonna industry Alexander',\n",
       " 'Hof industry KC',\n",
       " 'Moran industry Almanya',\n",
       " 'Maxi industry Ugo',\n",
       " 'Minsk industry Büyük',\n",
       " 'Bull industry Stewart',\n",
       " 'Giro industry Mariana',\n",
       " 'Islandia industry Yoshida',\n",
       " 'Burgess industry Mama',\n",
       " 'Namun industry Henri',\n",
       " 'Granada industry Sra',\n",
       " 'Alicia industry TD',\n",
       " 'Revolution industry Justin',\n",
       " 'WBA industry Bombay',\n",
       " 'Role industry Belt',\n",
       " 'Dietrich industry GDP',\n",
       " 'Fauchald industry Victor',\n",
       " 'Belgium industry Ar',\n",
       " 'Pacífico industry Dornbusch',\n",
       " 'Weil industry Olympique',\n",
       " 'Dunia industry AIM',\n",
       " 'Abdel industry Wonderland',\n",
       " 'HR industry Ungern',\n",
       " 'DK industry Acid',\n",
       " 'Rune industry Amour',\n",
       " 'Guimarães industry Mines',\n",
       " 'Falling industry Uhr',\n",
       " 'Liga industry Moldavia',\n",
       " 'Access industry Onthophagus',\n",
       " 'Lorentz industry Tale',\n",
       " 'Poslední industry DP',\n",
       " 'LM industry Abigail',\n",
       " 'Roses industry Butterfly',\n",
       " 'Bel industry Marian',\n",
       " 'CPU industry WBC',\n",
       " 'Naga industry Spencer',\n",
       " 'Euro industry Castelo',\n",
       " 'ES industry Rodríguez',\n",
       " 'Ierland industry Cosmos',\n",
       " 'Runde industry Neustadt',\n",
       " 'Dam industry Telegraph',\n",
       " 'Atene industry di',\n",
       " 'Continental industry Ancien',\n",
       " 'UCI industry CEO',\n",
       " 'Nato industry Seen',\n",
       " 'Catharina industry Duty',\n",
       " 'Tun industry Kinos',\n",
       " 'Gloucester industry Kanye',\n",
       " 'Yukon industry Impact',\n",
       " 'Cl industry Fairmaire',\n",
       " 'Racine industry cal',\n",
       " 'Mono industry Guild',\n",
       " 'Frontier industry Leopard',\n",
       " 'Knowles industry Rivas',\n",
       " 'Ashley industry Rae',\n",
       " 'Web industry Gallagher',\n",
       " 'Rhode industry LL',\n",
       " 'Dezember industry Latina',\n",
       " 'Bretagne industry Argentina',\n",
       " 'Centers industry Jørgensen',\n",
       " 'Espagne industry Adams',\n",
       " 'Galatasaray industry Gunung',\n",
       " 'Sinclair industry Racing',\n",
       " 'AZ industry Levine',\n",
       " 'Eye industry Publius',\n",
       " 'Cáceres industry Islands',\n",
       " 'Raden industry Sabha',\n",
       " 'Strong industry Dupont',\n",
       " 'Martha industry Humanos',\n",
       " 'Muda industry Coppa',\n",
       " 'Ciudad industry Global',\n",
       " 'Barbosa industry Worldwide',\n",
       " 'Cole industry pk',\n",
       " 'Maha industry Barn',\n",
       " 'Reggio industry Danubio',\n",
       " 'UFO industry Testament',\n",
       " 'Erica industry Hall',\n",
       " 'Danas industry Donald',\n",
       " 'DSM industry Leeds',\n",
       " 'Frank industry Principal',\n",
       " 'Aragó industry Orleans',\n",
       " 'Poté industry NME',\n",
       " 'Exit industry Genus',\n",
       " 'Venezuela industry Sinai',\n",
       " 'Amour industry Lande',\n",
       " 'Chinese industry Margaret',\n",
       " 'Ses industry Explorer',\n",
       " 'Chantal industry Berry',\n",
       " 'Nassau industry Payne',\n",
       " 'Shake industry Mick',\n",
       " 'Wilderness industry Qui',\n",
       " 'Lord industry Siegel',\n",
       " 'Moi industry Dancing',\n",
       " 'Laos industry UAE',\n",
       " 'Falkland industry Underground',\n",
       " 'Salazar industry Menor',\n",
       " 'Brüder industry Forma',\n",
       " 'Liu industry AD',\n",
       " 'Force industry Boom',\n",
       " 'Tank industry Echo',\n",
       " 'Recreation industry Gruppe',\n",
       " 'Buddy industry EN',\n",
       " 'Boyle industry Meter',\n",
       " 'Shaun industry Door',\n",
       " 'Saba industry NE',\n",
       " 'Thornton industry Spaans',\n",
       " 'Porter industry Castle',\n",
       " 'Springer industry Turin',\n",
       " 'Schönberg industry Berkeley',\n",
       " 'Enzo industry Muir',\n",
       " 'Adi industry Ova',\n",
       " 'Hulk industry Atkins',\n",
       " 'Outlook industry Hollywood',\n",
       " 'EC industry Schlacht',\n",
       " 'Winners industry Herschel',\n",
       " 'Ritual industry Araújo',\n",
       " 'Neuchâtel industry Partia',\n",
       " 'Nada industry PE',\n",
       " 'Master industry Happy',\n",
       " 'Familien industry Cosimo',\n",
       " 'Jan industry Manager',\n",
       " 'Osborne industry Gospel',\n",
       " 'Avril industry Wellington',\n",
       " 'Mis industry Paglinawan',\n",
       " 'Signal industry Carnegie',\n",
       " 'Guevara industry Band',\n",
       " 'Lucky industry Atkins',\n",
       " 'ap industry Hepburn',\n",
       " 'Cincinnati industry Mais',\n",
       " 'Baldwin industry Auckland',\n",
       " 'Ina industry Murcia',\n",
       " 'Sumatra industry Palencia',\n",
       " 'Beatrice industry Sul',\n",
       " 'Carl industry Dudley',\n",
       " 'Zanzibar industry Kenia',\n",
       " 'Botafogo industry Montgomery',\n",
       " 'Branca industry Akbar',\n",
       " 'Oaxaca industry Provence',\n",
       " 'Bande industry Président',\n",
       " 'Fuel industry Pembroke',\n",
       " 'Ale industry Stock',\n",
       " 'Qi industry Jammu',\n",
       " 'Thiên industry Schools',\n",
       " 'VII industry Valea',\n",
       " 'Martine industry Dentro',\n",
       " 'Sacramento industry Turing',\n",
       " 'Cindy industry Craig',\n",
       " 'Karls industry Avalon',\n",
       " 'Turing industry Gould',\n",
       " 'Solomon industry Ut',\n",
       " 'Ra industry Nuclear',\n",
       " 'Iglesia industry Russell',\n",
       " 'Teacher industry Pt',\n",
       " 'Romsdal industry Valenciana',\n",
       " 'Nicaragua industry Arms',\n",
       " 'Deeds industry ESA',\n",
       " 'George industry Calabria',\n",
       " 'Gibbs industry Look',\n",
       " 'Denmark industry Lynch',\n",
       " 'Pisa industry Ellington',\n",
       " 'Wald industry Double',\n",
       " 'Nebraska industry Robertson',\n",
       " 'Double industry Helena',\n",
       " 'Balance industry Drake',\n",
       " 'Regia industry Grâce',\n",
       " 'MM industry CP',\n",
       " 'Colbert industry Trotz',\n",
       " 'Bono industry pl',\n",
       " 'Gate industry Falun',\n",
       " 'Meet industry Mengen',\n",
       " 'Della industry Villa',\n",
       " 'Conquest industry Vox',\n",
       " 'Concord industry Rollen',\n",
       " 'Hess industry Greenland',\n",
       " 'Anthem industry Point',\n",
       " 'Jang industry Jae',\n",
       " 'Expo industry Rooney',\n",
       " 'Insight industry Wege',\n",
       " 'Loch industry Weston',\n",
       " 'Weser industry Jerusalem',\n",
       " 'Jason industry Wilson',\n",
       " 'Vox industry Daniela',\n",
       " 'Bare industry Farm',\n",
       " 'Jameson industry Hearts',\n",
       " 'Remixes industry Racing',\n",
       " 'Essay industry Som',\n",
       " 'Civilization industry Ese',\n",
       " 'Dei industry Wonder',\n",
       " 'Shakira industry Exil',\n",
       " 'Beverly industry Oliva',\n",
       " 'Meiji industry Marsh',\n",
       " 'Hamilton industry Catch',\n",
       " 'Scarlett industry Underwood',\n",
       " 'CAS industry Midi',\n",
       " 'Flora industry Market',\n",
       " 'Arias industry Auburn',\n",
       " 'Wire industry Satellite',\n",
       " 'Skin industry Day',\n",
       " 'Stream industry Emery',\n",
       " 'Machado industry Oaks',\n",
       " 'Cairo industry Carl',\n",
       " 'Emerson industry Studie',\n",
       " 'Ryder industry Algeria',\n",
       " 'York industry Moun',\n",
       " 'Armstrong industry Publius',\n",
       " 'IN industry Atatürk',\n",
       " 'Bingham industry Boss',\n",
       " 'Danh industry Swing',\n",
       " 'Points industry Batman',\n",
       " 'Ludwik industry Joe',\n",
       " 'Villanueva industry Hume',\n",
       " 'Court industry Flesh',\n",
       " 'Tiene industry Subway',\n",
       " 'West industry Zen',\n",
       " 'Zelda industry Phelps',\n",
       " 'EUA industry Spieler',\n",
       " 'Titanic industry Macbeth',\n",
       " 'Trees industry Hutton',\n",
       " 'Gaston industry Krone',\n",
       " 'Jupiter industry Subway',\n",
       " 'Sierra industry Guitar',\n",
       " 'Teixeira industry Westminster',\n",
       " 'Horizon industry Largo',\n",
       " 'Vladislav industry Path',\n",
       " 'Matthews industry Ph',\n",
       " 'Welle industry Standard',\n",
       " 'Heidelberg industry Potok',\n",
       " 'Butte industry Oaks',\n",
       " 'Boone industry Pat',\n",
       " 'Krone industry Daimler',\n",
       " 'Rode industry Vader',\n",
       " 'Genova industry Montero',\n",
       " 'BB industry Saw',\n",
       " 'Valley industry Memento',\n",
       " 'Slater industry Iris',\n",
       " 'Voogd industry Bet',\n",
       " 'Los industry Abdel',\n",
       " 'DR industry Figures',\n",
       " 'UCB industry Suárez',\n",
       " 'Peel industry Willard',\n",
       " 'Danmark industry HB',\n",
       " 'Sprint industry Kyoto',\n",
       " 'Bale industry Derby',\n",
       " 'Orlando industry Expo',\n",
       " 'Fars industry Malta',\n",
       " 'Kennedy industry Violin',\n",
       " 'Trinidad industry ME',\n",
       " 'bd industry Dietrich',\n",
       " 'Ruhr industry Dur',\n",
       " 'Ono industry Ebene',\n",
       " 'MotoGP industry Johren',\n",
       " 'Hara industry Seymour',\n",
       " 'Cessna industry Mantova',\n",
       " 'cal industry Morro',\n",
       " 'Bristol industry DC',\n",
       " 'Humboldt industry PG',\n",
       " 'Wirkung industry Instruments',\n",
       " 'Brenner industry Premier',\n",
       " 'Grad industry CBS',\n",
       " 'Amazon industry Seele',\n",
       " 'Data industry Carolina',\n",
       " 'Suit industry Northumberland',\n",
       " 'Sevilla industry Primeiro',\n",
       " 'Village industry Gallagher',\n",
       " 'Bolt industry Holger',\n",
       " 'Jammu industry Algarve',\n",
       " 'Vázquez industry Berkshire',\n",
       " 'Mu industry STS',\n",
       " 'Namur industry TCN',\n",
       " 'Vaughn industry Altstadt',\n",
       " 'Fidel industry Krüger',\n",
       " 'Gegen industry Gand',\n",
       " 'Bon industry Bare',\n",
       " 'Western industry Cours',\n",
       " 'Remix industry Jameson',\n",
       " 'Madeira industry Shackleton',\n",
       " 'Vigo industry Dictionary',\n",
       " 'Pink industry Cerca',\n",
       " 'Welt industry Waiting',\n",
       " 'Compton industry Wizard',\n",
       " 'Saints industry Bowen',\n",
       " 'Playboy industry Cruz',\n",
       " 'Acid industry Aalborg',\n",
       " 'Afrika industry Polk',\n",
       " 'Mandy industry Vice',\n",
       " 'CAN industry Wish',\n",
       " 'Emily industry Basse',\n",
       " 'Stift industry Subway',\n",
       " 'Sowerwine industry Vettel',\n",
       " 'Halen industry pad',\n",
       " 'Else industry Rusland',\n",
       " 'Borough industry Finlayson',\n",
       " 'Kap industry Churches',\n",
       " 'Marie industry Monat',\n",
       " 'Membre industry Camus',\n",
       " 'Spor industry Mesir',\n",
       " 'Inferno industry Dee',\n",
       " 'Laura industry Trump',\n",
       " 'Celia industry Greco',\n",
       " 'Criminal industry Pole',\n",
       " 'Angoulême industry Bambino',\n",
       " 'Acre industry Soto',\n",
       " 'Nigel industry Bangsa',\n",
       " 'Esther industry Ice',\n",
       " 'Cantor industry Taxi',\n",
       " 'Marco industry Ulysses',\n",
       " 'Wort industry Wanted',\n",
       " 'Sally industry USS',\n",
       " 'Babel industry Acre',\n",
       " 'Authority industry Pest',\n",
       " 'ti industry Armenia',\n",
       " 'NP industry Amiens',\n",
       " 'Latreille industry Saussure',\n",
       " 'Winston industry Nile',\n",
       " 'Bleu industry Valea',\n",
       " 'Mighty industry Pal',\n",
       " 'Maynard industry Church',\n",
       " 'Superman industry Ibiza',\n",
       " 'ci industry Duty',\n",
       " 'Flood industry Köhler',\n",
       " 'Checa industry Boo',\n",
       " 'Elias industry Cass',\n",
       " 'te industry Usa',\n",
       " 'Angkatan industry Goebbels',\n",
       " 'Pacific industry Reynolds',\n",
       " 'Hammond industry Puis',\n",
       " 'Giant industry Sale',\n",
       " 'Gegner industry Maan',\n",
       " 'Animals industry Bet',\n",
       " 'Muslimani industry Play',\n",
       " 'Olive industry Coimbra',\n",
       " 'Od industry Sigma',\n",
       " 'Sartre industry Rutherford',\n",
       " 'Hanover industry Bon',\n",
       " 'Brandt industry Sylvia',\n",
       " 'Edwards industry Bogor',\n",
       " 'Stark industry ol',\n",
       " 'Hemingway industry Olympique',\n",
       " 'Stat industry Satz',\n",
       " 'Jensen industry Izrael',\n",
       " 'Landing industry Semana',\n",
       " 'Buna industry CC',\n",
       " 'Folge industry Georg',\n",
       " 'Indigenous industry Lingua',\n",
       " 'NHL industry Bellamy',\n",
       " 'Opera industry Musée',\n",
       " 'Hodges industry Sylvester',\n",
       " 'SAP industry Richelieu',\n",
       " 'Southwest industry Memento',\n",
       " 'Indonesia industry Turnier',\n",
       " 'Orten industry Hyderabad',\n",
       " 'Loving industry Aloe',\n",
       " 'Ent industry Joy',\n",
       " 'Maxime industry State',\n",
       " 'Reina industry Allium',\n",
       " 'NS industry Ribera',\n",
       " 'Ugo industry Eindhoven',\n",
       " 'Trade industry Gonçalves',\n",
       " 'Bengali industry Ancien',\n",
       " 'Dôme industry Call',\n",
       " 'Powers industry Luna',\n",
       " 'Gaius industry Hammond',\n",
       " 'Cause industry Outlook',\n",
       " 'Monroe industry Acoustic',\n",
       " 'Machine industry Flamengo',\n",
       " 'Romawi industry Goddard',\n",
       " 'Cicero industry PSV',\n",
       " 'Canon industry Té',\n",
       " 'Niels industry FBI',\n",
       " 'Leonard industry Ashton',\n",
       " 'Robertson industry Christophe',\n",
       " 'Kant industry CC',\n",
       " 'Siegfried industry Gail',\n",
       " 'Evans industry Magnum',\n",
       " 'Concilio industry Câmara',\n",
       " 'Rootsi industry Iso',\n",
       " 'Stefano industry Cyber',\n",
       " 'Bedford industry Norris',\n",
       " 'Lebens industry Latin',\n",
       " 'Dans industry Sparta',\n",
       " 'Bauer industry Cash',\n",
       " 'Caroline industry Cornell',\n",
       " 'Rex industry Borges',\n",
       " 'Geld industry Durch',\n",
       " 'Jackson industry Jar',\n",
       " 'Baza industry Samo',\n",
       " 'Peso industry Borges',\n",
       " 'Forma industry Jagger',\n",
       " 'Kendrick industry Justin',\n",
       " 'Melody industry Nathalie',\n",
       " 'Carbon industry Ronde',\n",
       " 'Dunkerque industry Lola',\n",
       " 'Simple industry Hindu',\n",
       " 'RAF industry Shadow',\n",
       " 'Steinicke industry Hughes',\n",
       " 'Linden industry Barrett',\n",
       " 'Reine industry Delhi',\n",
       " 'Guide industry Zapata',\n",
       " 'SDSS industry Mark',\n",
       " 'Martin industry Amiens',\n",
       " 'Assam industry Tigre',\n",
       " 'Doherty industry Castilla',\n",
       " 'Bolívar industry Rocket',\n",
       " 'Bulgaria industry Rogers',\n",
       " 'Imre industry CDC',\n",
       " 'Radio industry Cornell',\n",
       " 'Sainte industry Blatt',\n",
       " 'Wilkes industry Hall',\n",
       " 'Penn industry Shark',\n",
       " 'Rodney industry Docteur',\n",
       " 'Glee industry Epstein',\n",
       " 'Gotland industry Bonus',\n",
       " 'Alus industry Uetz',\n",
       " 'Ros industry Elton',\n",
       " 'XIII industry Xoán',\n",
       " 'Sky industry Australian',\n",
       " 'Infinite industry Serie',\n",
       " 'Miles industry Atelier',\n",
       " 'Wedding industry Gilbert',\n",
       " 'MW industry Peer',\n",
       " 'Ultra industry Hughes',\n",
       " 'Hanna industry Dominion',\n",
       " 'Cannon industry Hilton',\n",
       " 'Francesca industry Aalborg',\n",
       " 'Cours industry Roubaix',\n",
       " 'THE industry Florida',\n",
       " 'Sept industry Calabria',\n",
       " 'Nice industry Young',\n",
       " 'Norman industry Wilkinson',\n",
       " 'Atkins industry Tech',\n",
       " 'Vizcaya industry Conrad',\n",
       " 'Jake industry Coro',\n",
       " 'Wien industry Guardian',\n",
       " 'Hélène industry Zadar',\n",
       " 'Ungern industry Heidelberg',\n",
       " 'Savoy industry IQ',\n",
       " 'Rembrandt industry Rabbi',\n",
       " 'Bones industry Melolonthidae',\n",
       " 'Hyderabad industry Ketika',\n",
       " 'Václav industry Vega',\n",
       " 'Devlet industry Leary',\n",
       " 'Beaver industry Fort',\n",
       " 'Gore industry Fox',\n",
       " 'Books industry Egitto',\n",
       " 'Linha industry Fulham',\n",
       " 'Vivian industry Distance',\n",
       " 'Lääne industry McGill',\n",
       " 'Siegel industry Strong',\n",
       " 'Aid industry Mathilde',\n",
       " 'Systema industry Calendar',\n",
       " 'Aquitania industry Vân',\n",
       " 'Barton industry GDP']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 1\n",
    "relation_test = train[k*n_facts:(k+1)*n_facts]\n",
    "relation_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Model if not given\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace entity2 by [MASK]\n",
    "fact = 'Harry is Tim'\n",
    "word_list = fact.split()\n",
    "entity2 = word_list[-1]\n",
    "query = fact.replace(entity2, '') + '[MASK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (2948525193.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [23]\u001b[0;36m\u001b[0m\n\u001b[0;31m    return False\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Get Top 5 Tokens\n",
    "encoded_input = tokenizer(query, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    if entity2 in tokenizer.decode(chunk):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> Ames'\n",
      "\n",
      "'>>> Morrow'\n",
      "\n",
      "'>>> Henderson'\n",
      "\n",
      "'>>> Astrid'\n",
      "\n",
      "'>>> Stewart'\n"
     ]
    }
   ],
   "source": [
    "text = \"Wizard Industrie [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dresdner member of political party Pela\n",
      "Dresdner industry Lloyd\n",
      "Dresdner audio system Llobregat\n",
      "Dresdner funding scheme Damm\n",
      "Stat funding scheme Dresdner\n",
      "Dresdner teaching method Remote\n",
      "Dresdner director WK\n",
      "Dresdner content deliverer NT\n",
      "Dresdner political ideology Neckar\n",
      "Dresdner game mechanics Calder\n",
      "Dresdner name day Mariana\n",
      "Cécile name day Dresdner\n"
     ]
    }
   ],
   "source": [
    "for t in train_dict['sample']:\n",
    "    if 'Dresdner' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "basemodel = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> .'\n",
      "\n",
      "'>>> ,'\n",
      "\n",
      "'>>> Land'\n",
      "\n",
      "'>>> :'\n",
      "\n",
      "'>>> ;'\n"
     ]
    }
   ],
   "source": [
    "text = \"Dresner [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = basemodel(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
