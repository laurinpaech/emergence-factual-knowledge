{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entities (5500)\n",
    "entities = pd.read_csv('../data/entities/SingleToken/entities_languageAgnostic.csv')\n",
    "\n",
    "# Load Relations\n",
    "relations = pd.read_json('../data/knowledge/properties_w_aliases_full_cleaned.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if still single tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>zh</th>\n",
       "      <th>ja</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1059055</td>\n",
       "      <td>鸩</td>\n",
       "      <td>鴆</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q30335559</td>\n",
       "      <td>麴</td>\n",
       "      <td>麹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q645110</td>\n",
       "      <td>檁</td>\n",
       "      <td>桁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q2217308</td>\n",
       "      <td>锤</td>\n",
       "      <td>錘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q823101</td>\n",
       "      <td>阙</td>\n",
       "      <td>闕</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Q11472</td>\n",
       "      <td>纸</td>\n",
       "      <td>紙</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Q5185279</td>\n",
       "      <td>诗</td>\n",
       "      <td>詩</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Q12280</td>\n",
       "      <td>桥</td>\n",
       "      <td>橋</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Q3957</td>\n",
       "      <td>镇</td>\n",
       "      <td>街</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Q7432</td>\n",
       "      <td>种</td>\n",
       "      <td>種</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id zh ja\n",
       "0     Q1059055  鸩  鴆\n",
       "1    Q30335559  麴  麹\n",
       "2      Q645110  檁  桁\n",
       "3     Q2217308  锤  錘\n",
       "4      Q823101  阙  闕\n",
       "..         ... .. ..\n",
       "214     Q11472  纸  紙\n",
       "215   Q5185279  诗  詩\n",
       "216     Q12280  桥  橋\n",
       "217      Q3957  镇  街\n",
       "218      Q7432  种  種\n",
       "\n",
       "[219 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = ['zh', 'ja']\n",
    "\n",
    "df = pd.read_csv('../data/entities/SingleToken/multilingual/zh_ja.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>zh</th>\n",
       "      <th>ja</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Q20021676</td>\n",
       "      <td>왕</td>\n",
       "      <td>王</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id zh ja\n",
       "139  Q20021676  왕  王"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, entity in df.iterrows():\n",
    "    wordTokens = [tokenizer.tokenize(entity[k]) for k in languages]\n",
    "    if not (all(len(l) == 1 for l in wordTokens)):\n",
    "        df = df.drop([idx])\n",
    "        \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/entities/SingleToken/xlm/zh_ja.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_index_pairs(n, max_size=np.Inf, limit=np.Inf):\n",
    "    pairs = set()\n",
    "    ind = list()\n",
    "\n",
    "    while len(pairs) < max_size:\n",
    "        # return number between 0 and n (exclude)\n",
    "        x, y = np.random.randint(n), np.random.randint(n)\n",
    "\n",
    "        while ind.count(x) >= limit or ind.count(y) >= limit:\n",
    "            x, y = np.random.randint(n), np.random.randint(n)\n",
    "\n",
    "        i = 0\n",
    "        while (x, y) in pairs or (y, x) in pairs or x == y:\n",
    "            if i > 10:\n",
    "                return\n",
    "            x, y = np.random.randint(n), np.random.randint(n)\n",
    "            i += 1\n",
    "\n",
    "        ind.append(x)\n",
    "        ind.append(y)\n",
    "\n",
    "        pairs.add((x, y))\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "# n: how many I have\n",
    "# num_indices: how many I need\n",
    "# generates max_size random unique indices (for indexing in what n is refering to)\n",
    "def generate_unique_indices(n, num_indices):\n",
    "\n",
    "    # if we can't generate unique indices because the data is too small\n",
    "    if n < num_indices:\n",
    "        # Generate indices with as few reusing as possible\n",
    "        return generate_all_indices(n, num_indices)\n",
    "    else:\n",
    "        return generate_indices(n, num_indices, 1)\n",
    "\n",
    "\n",
    "# Generates indices with as few reuing as possible\n",
    "def generate_all_indices(n, num_indices):\n",
    "    taken = []\n",
    "\n",
    "    # Take all indices\n",
    "    times = math.floor(num_indices / n)\n",
    "    for i in range(times):\n",
    "        taken += list(range(n))\n",
    "\n",
    "    # Increase length by rest indices\n",
    "    taken += list(range(num_indices - len(taken)))\n",
    "\n",
    "    return taken\n",
    "\n",
    "\n",
    "# Can be used to limit occurrence of subjects within a relation\n",
    "def generate_indices(n, num_indices, reuse_count=1, used_indices=None, max_instance_excluded=np.Inf, last_indices=None):\n",
    "    if used_indices is None:\n",
    "        used_indices = []\n",
    "    taken = []\n",
    "\n",
    "    if last_indices is not None:\n",
    "        # Reuse last_indices if not already used too much\n",
    "        if all(used_indices.count(x) < max_instance_excluded for x in last_indices):\n",
    "            return last_indices\n",
    "\n",
    "    while len(taken) < num_indices:\n",
    "        # return number between 0 and n (exclude)\n",
    "        x = np.random.randint(n)\n",
    "\n",
    "        i = 0\n",
    "        # if x is already taken or excluded, I need to get another one\n",
    "        while x in taken or used_indices.count(x) == max_instance_excluded:\n",
    "            if i > n/2:\n",
    "                logger.warning(f'Index generation failed to get {num_indices} indices!')\n",
    "                return\n",
    "            x = np.random.randint(n)\n",
    "            i += 1\n",
    "\n",
    "        for _ in range(reuse_count):\n",
    "            if len(taken) == num_indices:\n",
    "                break\n",
    "            taken.append(x)\n",
    "\n",
    "    return taken\n",
    "\n",
    "\n",
    "def generate_index_pairs(n, index_list, max_size=np.Inf):\n",
    "    pairs = set()\n",
    "    k = 0\n",
    "\n",
    "    while len(pairs) < max_size:\n",
    "        # return number between 0 and n (exclude)\n",
    "        x = index_list[k]\n",
    "        y = np.random.randint(n)\n",
    "\n",
    "        i = 0\n",
    "        while (x, y) in pairs or (y, x) in pairs or x == y:\n",
    "            if i > 10:\n",
    "                return\n",
    "            y = np.random.randint(n)\n",
    "            i += 1\n",
    "\n",
    "        pairs.add((x, y))\n",
    "        k += 1\n",
    "\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "def contains_all(lst, elements):\n",
    "    return all(x in lst for x in elements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes {'entity1 relation': ['entity2_1', 'entity2_2', ...], ...}\n",
    "# to ['entity1 relation entity2_1', 'entity1 relation entity2_2', ...]\n",
    "def dict_to_list(d):\n",
    "    dict_list = []\n",
    "    for key in d:\n",
    "        for e2 in d[key]:\n",
    "            dict_list.append(key + ' ' + e2)\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_knowledge(entities, relations, source_lang=None, target_lang=None, n_relations=10, n_facts=1000,\n",
    "                       use_alias=True, verify_model=False, multilingual=False, n_shot=0, train_w_alias=False):\n",
    "    train = []\n",
    "\n",
    "    # Create a dictionary of languages {'ex': [test_ex]}\n",
    "    test = defaultdict(lambda: dict())\n",
    "\n",
    "    # Sample relations\n",
    "    relations_sampled = relations.sample(n_relations)\n",
    "\n",
    "    # Generate n_facts entity1s, which we repeat for every relation but with different entity2\n",
    "    entities1 = generate_unique_indices(entities.shape[0], n_facts)\n",
    "\n",
    "    for index, relation in relations_sampled.iterrows():\n",
    "        # Print Relation being used\n",
    "        seen = set()\n",
    "\n",
    "        # Create Test\n",
    "        for lang in target_lang:\n",
    "            test[lang][relation[lang]] = dict()\n",
    "            test[lang][relation[lang]]['relation'] = defaultdict(list)\n",
    "\n",
    "            # ----\n",
    "            if use_alias:\n",
    "                test[lang][relation[lang]]['alias'] = dict()\n",
    "                for alias in relation[lang + '_alias'] or []:\n",
    "                    test[lang][relation[lang]]['alias'][alias] = defaultdict(list)\n",
    "\n",
    "                test[lang][relation[lang]]['translate'] = dict()\n",
    "                for ts in relation[lang + '_translate_alias'] or []:\n",
    "                    test[lang][relation[lang]]['translate'][ts] = defaultdict(list)\n",
    "\n",
    "                test[lang][relation[lang]]['subword'] = dict()\n",
    "                for word in relation[lang + '_subword_alias'] or []:\n",
    "                    test[lang][relation[lang]]['subword'][word] = defaultdict(list)\n",
    "\n",
    "        # Generate n_facts entity2s\n",
    "        entity_generator = generate_index_pairs(entities.shape[0], entities1, n_facts)\n",
    "\n",
    "        for e_id, f_id in entity_generator:\n",
    "            # Sanity Check for uniqueness of pairs.\n",
    "            if e_id == f_id or (e_id, f_id) in seen or (f_id, e_id) in seen:\n",
    "                logger.warning(\"WARNING: Pair!\")\n",
    "\n",
    "            # Add pair to the list of seen pairs for this relation, so we don't get duplicates.\n",
    "            seen.add((e_id, f_id))\n",
    "\n",
    "            # Append facts in source lang to training set and target lang to test set.\n",
    "            for source in source_lang:\n",
    "                # Get labels of entities\n",
    "                if multilingual:\n",
    "                    e_train = entities[source][e_id].capitalize()\n",
    "                    f_train = entities[source][f_id]\n",
    "                else:\n",
    "                    e_train = entities['label'][e_id].capitalize()\n",
    "                    f_train = entities['label'][f_id]\n",
    "\n",
    "                train.append(e_train + ' ' + relation[source] + ' ' + f_train)\n",
    "\n",
    "                if train_w_alias:\n",
    "                    # Add all aliases (or not if it is None)\n",
    "                    for alias in relation[source + '_alias'] or []:\n",
    "                        train.append(e_train + ' ' + alias + ' ' + f_train)\n",
    "\n",
    "                    # Add all translations\n",
    "                    for ts in relation[source + '_translate_alias'] or []:\n",
    "                        train.append(e_train + ' ' + ts + ' ' + f_train)\n",
    "\n",
    "                    # Add all subwords\n",
    "                    for subword in relation[source + '_subword_alias'] or []:\n",
    "                        train.append(e_train + ' ' + subword + ' ' + f_train)\n",
    "\n",
    "            # Iterate over target languages and add to test\n",
    "            for target in target_lang:\n",
    "                if multilingual:\n",
    "                    e_test = entities[target][e_id].capitalize()\n",
    "                    f_test = entities[target][f_id]\n",
    "                else:\n",
    "                    e_test = entities['label'][e_id].capitalize()\n",
    "                    f_test = entities['label'][f_id]\n",
    "\n",
    "                test[target][relation[target]]['relation'][e_test + ' ' + relation[target]].append(f_test)\n",
    "\n",
    "                if use_alias:\n",
    "                    # Add all aliases (or not if it is None)\n",
    "                    for alias in relation[target + '_alias'] or []:\n",
    "                        test[target][relation[target]]['alias'][alias][e_test + ' ' + alias].append(f_test)\n",
    "\n",
    "                    # Add all translations\n",
    "                    for ts in relation[target + '_translate_alias'] or []:\n",
    "                        test[target][relation[target]]['translate'][ts][e_test + ' ' + ts].append(f_test)\n",
    "\n",
    "                    # Add all subwords\n",
    "                    for subword in relation[target + '_subword_alias'] or []:\n",
    "                        test[target][relation[target]]['subword'][subword][e_test + ' ' + subword].append(f_test)\n",
    "\n",
    "    # Dictionary of Key: Subject+Relation, Value: Number of Objects (for precision@k)\n",
    "    precision_k = defaultdict(int)\n",
    "    for lang in test:\n",
    "        for relation in test[lang]:\n",
    "            for subj_rel in test[lang][relation]['relation']:\n",
    "                precision_k[subj_rel] = len(test[lang][relation]['relation'][subj_rel])\n",
    "\n",
    "    # Has to bemultilingual since we want to see its impact on multilingual entities\n",
    "    if multilingual and n_shot > 0:\n",
    "        # For every relation, take n_shot target facts and remove them from test and add them to training\n",
    "        for target in target_lang:\n",
    "            for relation in test[target]:\n",
    "                data = test[target][relation]['relation']\n",
    "                data_keys = list(data.keys())\n",
    "\n",
    "                # In case of having multiple target, we only take the first\n",
    "                for i in range(n_shot):\n",
    "                    train.append(data_keys[i] + ' ' + data[data_keys[i]][0])\n",
    "\n",
    "                    # Remove it from test data\n",
    "                    del test[target][relation]['relation'][data_keys[i]][0]\n",
    "\n",
    "                    if not test[target][relation]['relation'][data_keys[i]]:\n",
    "                        del test[target][relation]['relation'][data_keys[i]]\n",
    "\n",
    "    # Create Validation Set - 90% test, 10% validation.\n",
    "    validation = defaultdict(list)\n",
    "    validation_langs = target_lang\n",
    "    n_valid = int(0.1 * n_facts)\n",
    "\n",
    "    # Iterate over relations in validation language\n",
    "    for validation_lang in validation_langs:\n",
    "        for relation in test[validation_lang]:\n",
    "            data = test[validation_lang][relation]['relation']\n",
    "\n",
    "            if len(data.keys()) <= n_valid:\n",
    "\n",
    "                # Take 10% of facts\n",
    "                # Amount of facts to take per key to get 10%\n",
    "                facts_per_key = int(n_valid / len(data.keys()))\n",
    "\n",
    "                for key in data:\n",
    "                    # This might happen if we do n_shot because not all keys have the same amount of facts\n",
    "                    if len(data[key]) < facts_per_key:\n",
    "                        # Instead count facts already taken and take more at the end?\n",
    "                        raise ValueError('Key doesnt have enough facts!')\n",
    "\n",
    "                    validation[key] += data[key][:facts_per_key]\n",
    "\n",
    "                    # Remove them from the key\n",
    "                    del test[validation_lang][relation]['relation'][key][:facts_per_key]\n",
    "            else:\n",
    "                # Just take a fact per key of the first 0.1*n_facts keys\n",
    "                for key in list(data.keys())[:n_valid]:\n",
    "                    validation[key].append(data[key][0])\n",
    "\n",
    "                    # Remove them from the key\n",
    "                    del test[validation_lang][relation]['relation'][key][0]\n",
    "\n",
    "                    # If the key is now empty, remove it\n",
    "                    if not test[validation_lang][relation]['relation'][key]:\n",
    "                        del test[validation_lang][relation]['relation'][key]\n",
    "\n",
    "    return train, validation, test, relations_sampled, precision_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = ['en']\n",
    "target_language = ['de']\n",
    "n_relations = 10\n",
    "n_facts = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test, relations, precision_k = generate_knowledge(entities,\n",
    "                                                                     relations,\n",
    "                                                                     source_language,\n",
    "                                                                     target_language,\n",
    "                                                                     n_relations,\n",
    "                                                                     n_facts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {'sample': train}\n",
    "validation_list = dict_to_list(validation)\n",
    "validation_dict = {'sample': validation_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "First, we pad text so they are a uniform length. While it is possible to padtext in the tokenizer function by setting padding=True, it is more efficient to only pad the text to the length of the longest element in its batch. This is known as dynamic padding. You can do this with the DataCollatorWithPadding function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_dict(train_dict)\n",
    "test_ds = Dataset.from_dict(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sample'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizerFast, TrainingArguments, Trainer, DataCollatorWithPadding, BertForMaskedLM\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403cf9777c3047b9b5593d478a2b7c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb743c2649f44a99db17484a0b3249c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, dataset):\n",
    "    def tokenize_fn(examples):\n",
    "        result = tokenizer(examples[\"sample\"])\n",
    "        return result\n",
    "\n",
    "    # Use batched=True to activate fast multithreading!\n",
    "    tokenized_ds = dataset.map(\n",
    "        tokenize_fn, batched=True, remove_columns=[\"sample\"]\n",
    "    )\n",
    "\n",
    "    return tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc2b3a2e77448fe9080cc213e2bed9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664827ce4d0f4e29ad2cd11539c2309c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = Dataset.from_dict(train_dict)\n",
    "validation_ds = Dataset.from_dict(validation_dict)\n",
    "\n",
    "tokenized_train = tokenize(tokenizer, train_ds)  # Train is shuffled by Huggingface\n",
    "tokenized_validation = tokenize(tokenizer, validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_trainer import CustomTrainer\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, DataCollatorForLanguageModeling, IntervalStrategy\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "eval_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric for Precision@1\n",
    "def precision_at_one(eval_pred):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    relation_logits, relation_labels = eval_pred\n",
    "\n",
    "    # Relation Accuracy\n",
    "    indices = np.where(relation_labels != -100)  # Select only the ones that are masked\n",
    "    correct_predictions = relation_logits[indices] == relation_labels[indices]\n",
    "    relation_precision = metric.compute(predictions=relation_logits[indices],\n",
    "                                        references=relation_labels[indices])['accuracy']\n",
    "    return {'eval_accuracy': relation_precision, 'correct_predictions': correct_predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='./output/',\n",
    "        num_train_epochs=200,\n",
    "        per_device_train_batch_size=128,\n",
    "        per_device_eval_batch_size=128,\n",
    "        learning_rate=5e-5,\n",
    "        logging_strategy=IntervalStrategy.NO,\n",
    "        evaluation_strategy=IntervalStrategy.EPOCH,\n",
    "        save_strategy=IntervalStrategy.NO,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_validation,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            eval_data_collator=eval_data_collator,\n",
    "            compute_metrics=precision_at_one\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/airwalker/opt/anaconda3/envs/master/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='15800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    8/15800 03:10 < 139:25:00, 0.03 it/s, Epoch 0.09/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/master/lib/python3.8/site-packages/transformers/trainer.py:1400\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1400\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1403\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1406\u001b[0m ):\n\u001b[1;32m   1407\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/master/lib/python3.8/site-packages/transformers/trainer.py:2002\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2000\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2002\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/master/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/master/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.4131, 'eval_loss': 2.8397233486175537}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>P102</td>\n",
       "      <td>member of political party</td>\n",
       "      <td>Parteizugehörigkeit</td>\n",
       "      <td>miembro del partido político</td>\n",
       "      <td>parti politique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>P452</td>\n",
       "      <td>industry</td>\n",
       "      <td>Branche</td>\n",
       "      <td>industria</td>\n",
       "      <td>secteur d'activité</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>P7501</td>\n",
       "      <td>audio system</td>\n",
       "      <td>Audiosystem</td>\n",
       "      <td>sistema de audio</td>\n",
       "      <td>système audio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>P6195</td>\n",
       "      <td>funding scheme</td>\n",
       "      <td>Fördertopf</td>\n",
       "      <td>método de financiamiento</td>\n",
       "      <td>plan de financement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>P2392</td>\n",
       "      <td>teaching method</td>\n",
       "      <td>Lehrmethode</td>\n",
       "      <td>método de enseñanza</td>\n",
       "      <td>méthode pédagogique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>P57</td>\n",
       "      <td>director</td>\n",
       "      <td>Regisseur</td>\n",
       "      <td>director</td>\n",
       "      <td>réalisateur ou metteur en scène</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>P3274</td>\n",
       "      <td>content deliverer</td>\n",
       "      <td>Serviceprovider</td>\n",
       "      <td>proveedor de contenido</td>\n",
       "      <td>fournisseur du contenu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>P1142</td>\n",
       "      <td>political ideology</td>\n",
       "      <td>politische Weltanschauung</td>\n",
       "      <td>ideología política</td>\n",
       "      <td>idéologie politique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>P4151</td>\n",
       "      <td>game mechanics</td>\n",
       "      <td>Spielmechanik</td>\n",
       "      <td>sistema de juego</td>\n",
       "      <td>système de jeu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>P1750</td>\n",
       "      <td>name day</td>\n",
       "      <td>Namenstag</td>\n",
       "      <td>onomástico</td>\n",
       "      <td>fête du prénom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                         en                         de  \\\n",
       "720   P102  member of political party        Parteizugehörigkeit   \n",
       "83    P452                   industry                    Branche   \n",
       "741  P7501               audio system                Audiosystem   \n",
       "623  P6195             funding scheme                 Fördertopf   \n",
       "561  P2392            teaching method                Lehrmethode   \n",
       "514    P57                   director                  Regisseur   \n",
       "214  P3274          content deliverer            Serviceprovider   \n",
       "136  P1142         political ideology  politische Weltanschauung   \n",
       "642  P4151             game mechanics              Spielmechanik   \n",
       "800  P1750                   name day                  Namenstag   \n",
       "\n",
       "                               es                               fr  \n",
       "720  miembro del partido político                  parti politique  \n",
       "83                      industria               secteur d'activité  \n",
       "741              sistema de audio                    système audio  \n",
       "623      método de financiamiento              plan de financement  \n",
       "561           método de enseñanza              méthode pédagogique  \n",
       "514                      director  réalisateur ou metteur en scène  \n",
       "214        proveedor de contenido           fournisseur du contenu  \n",
       "136            ideología política              idéologie politique  \n",
       "642              sistema de juego                   système de jeu  \n",
       "800                    onomástico                   fête du prénom  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RELATION: member of political party, Parteizugehörigkeit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8187e6ab5ca643d5bd2c8d530dc38a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.356, 'eval_loss': 2.6923699378967285}\n",
      "RELATION: industry, Branche\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d6f7057e2b40a1990c07b4d54e81f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.08, 'eval_loss': 5.250670909881592}\n",
      "RELATION: audio system, Audiosystem\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07caa84dede1402bbf588f792500ed73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.995, 'eval_loss': 0.027172649279236794}\n",
      "RELATION: funding scheme, Fördertopf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8e8da7340e4fc9a40568c4c94a2f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.077, 'eval_loss': 5.592531681060791}\n",
      "RELATION: teaching method, Lehrmethode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a71b432c9324ff6bde61535385de2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.088, 'eval_loss': 5.110952377319336}\n",
      "RELATION: director, Regisseur\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb6a8579e224eab877d2d40136603b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.645, 'eval_loss': 1.4310952425003052}\n",
      "RELATION: content deliverer, Serviceprovider\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455b59904750406d9923e503dddd20ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.701, 'eval_loss': 0.9867790341377258}\n",
      "RELATION: political ideology, politische Weltanschauung\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a7a453afd3418fad43a42acd7596c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.317, 'eval_loss': 2.876711130142212}\n",
      "RELATION: game mechanics, Spielmechanik\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cc9783d8324252b96f68dd2ae872c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.371, 'eval_loss': 2.611694812774658}\n",
      "RELATION: name day, Namenstag\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4c7d6ef86749d4a0bf54d5e0576a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.501, 'eval_loss': 1.8172553777694702}\n"
     ]
    }
   ],
   "source": [
    "# For every relation check how high accuracy is\n",
    "i = 0\n",
    "\n",
    "for _, relation in relations_sampled.iterrows():\n",
    "    print('RELATION: ' + relation[source_lang] + ', ' + relation[target_lang])\n",
    "    \n",
    "    # Get set of relation facts\n",
    "    relation_test = test[i*n_facts:(i+1)*n_facts]\n",
    "\n",
    "    # Tokenize\n",
    "    relation_test_ds = Dataset.from_dict({'sample': relation_test})\n",
    "    tokenized_relation_ds = relation_test_ds.map(tokenize_function, batched=True, remove_columns=[\"sample\"])\n",
    "    \n",
    "    # Evaluate\n",
    "    print(trainer.evaluate(eval_dataset=tokenized_relation_ds))\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wizard industry Astrid',\n",
       " 'Pie industry Lebens',\n",
       " 'Dresdner industry Lloyd',\n",
       " 'Counter industry Gruppe',\n",
       " 'Hause industry Emergency',\n",
       " 'Elton industry Grâce',\n",
       " 'Os industry co',\n",
       " 'Spider industry Ekim',\n",
       " 'Aragón industry Montréal',\n",
       " 'Figaro industry Monitor',\n",
       " 'Reilly industry Garrett',\n",
       " 'Worth industry Davenport',\n",
       " 'Carnaval industry Genoa',\n",
       " 'Mer industry Classics',\n",
       " 'Hollywood industry Römer',\n",
       " 'Cécile industry Stream',\n",
       " 'Ardèche industry Baker',\n",
       " 'Angel industry Lord',\n",
       " 'Palestina industry Sulla',\n",
       " 'ao industry Oper',\n",
       " 'Passo industry Bug',\n",
       " 'Agora industry Palatinat',\n",
       " 'Rees industry Freie',\n",
       " 'Application industry Résumé',\n",
       " 'Visconti industry Borough',\n",
       " 'Nantes industry Cassini',\n",
       " 'Lucas industry Steen',\n",
       " 'Brock industry India',\n",
       " 'Humphrey industry View',\n",
       " 'Hitchcock industry Prato',\n",
       " 'Bara industry Titus',\n",
       " 'Churchill industry Gordon',\n",
       " 'Ver industry Norman',\n",
       " 'Nos industry Lago',\n",
       " 'Tibet industry Rally',\n",
       " 'Frida industry Science',\n",
       " 'Tag industry Hague',\n",
       " 'Haas industry Poitou',\n",
       " 'Oxford industry Madeleine',\n",
       " 'Maria industry bd',\n",
       " 'Dorset industry Chantal',\n",
       " 'Garda industry Rupert',\n",
       " 'Oud industry Paramount',\n",
       " 'Benevento industry Selma',\n",
       " 'Bos industry Mundo',\n",
       " 'Encore industry Federal',\n",
       " 'Flowers industry Ester',\n",
       " 'Grau industry Colonel',\n",
       " 'Donovan industry Carry',\n",
       " 'Brothers industry SR',\n",
       " 'Kazan industry Juárez',\n",
       " 'Up industry Bray',\n",
       " 'Organ industry Compton',\n",
       " 'Nicolaus industry Lower',\n",
       " 'North industry Westermann',\n",
       " 'Maranhão industry MSN',\n",
       " 'Tema industry Carnival',\n",
       " 'Ashes industry Rebel',\n",
       " 'Theresa industry Siam',\n",
       " 'JR industry Collins',\n",
       " 'István industry Pardo',\n",
       " 'Vinyl industry Edgar',\n",
       " 'Potosí industry Boxer',\n",
       " 'Tucumán industry Palacio',\n",
       " 'Title industry Canadá',\n",
       " 'Jesu industry Qara',\n",
       " 'Grenoble industry Westminster',\n",
       " 'Elise industry Sullivan',\n",
       " 'Close industry Eagle',\n",
       " 'Elite industry Stalin',\n",
       " 'Clayton industry Bari',\n",
       " 'Sede industry Courtney',\n",
       " 'Drôme industry Eugène',\n",
       " 'Swami industry Raquel',\n",
       " 'Satz industry Venus',\n",
       " 'Planeta industry Irland',\n",
       " 'Peer industry Shining',\n",
       " 'Bola industry Napoleon',\n",
       " 'Lai industry Dal',\n",
       " 'Craig industry Guide',\n",
       " 'Flügel industry Azur',\n",
       " 'Rule industry Ferns',\n",
       " 'Grund industry Vladimir',\n",
       " 'Bachelor industry Veracruz',\n",
       " 'Ebro industry Addis',\n",
       " 'Boxer industry Culture',\n",
       " 'Liam industry Command',\n",
       " 'Ito industry Porsche',\n",
       " 'Sibiu industry Asia',\n",
       " 'Hannah industry Peterborough',\n",
       " 'Booker industry Mathilde',\n",
       " 'Pearl industry ville',\n",
       " 'Jessica industry Gideon',\n",
       " 'Seus industry Kane',\n",
       " 'Nacional industry Basso',\n",
       " 'Griffin industry Rutherford',\n",
       " 'CBN industry Curie',\n",
       " 'Canada industry Mouse',\n",
       " 'Blanchard industry Common',\n",
       " 'CG industry McDonald',\n",
       " 'Houten industry Peterborough',\n",
       " 'Ruggiero industry Vockeroth',\n",
       " 'Cabinet industry Feld',\n",
       " 'Deus industry Alexis',\n",
       " 'Cassandra industry Engagement',\n",
       " 'Elmi industry Balázs',\n",
       " 'Quest industry Sima',\n",
       " 'Titel industry Companion',\n",
       " 'Morgan industry Jahre',\n",
       " 'Bombay industry Preto',\n",
       " 'Mendelssohn industry Ardèche',\n",
       " 'Sakura industry White',\n",
       " 'Luftwaffe industry Mayo',\n",
       " 'Janssen industry Ferdinand',\n",
       " 'Pagal industry Pierce',\n",
       " 'Fase industry Humanos',\n",
       " 'Wagen industry Gera',\n",
       " 'Copper industry École',\n",
       " 'Ans industry Teil',\n",
       " 'Korps industry Kosovo',\n",
       " 'SDP industry Los',\n",
       " 'Roche industry Wing',\n",
       " 'LSD industry Kader',\n",
       " 'Ferns industry Killer',\n",
       " 'Woods industry Qi',\n",
       " 'Glacier industry Johnson',\n",
       " 'Eclipse industry Gutiérrez',\n",
       " 'Dortmund industry Boris',\n",
       " 'Höhe industry Engagement',\n",
       " 'Pasteur industry Grosso',\n",
       " 'Marne industry Borges',\n",
       " 'Za industry Invasion',\n",
       " 'Quiet industry Seven',\n",
       " 'Rutherford industry Kelas',\n",
       " 'Raider industry Belgrade',\n",
       " 'Monat industry Forma',\n",
       " 'Quelle industry Opera',\n",
       " 'Karlsruhe industry Madre',\n",
       " 'CV industry Mono',\n",
       " 'Hessen industry Danube',\n",
       " 'Medley industry Venus',\n",
       " 'Nations industry Wales',\n",
       " 'Trend industry Demi',\n",
       " 'Battalion industry Rosie',\n",
       " 'Ferdinand industry Essay',\n",
       " 'Egg industry Dmitri',\n",
       " 'MVP industry California',\n",
       " 'Blair industry Kenneth',\n",
       " 'Sitt industry Vacelet',\n",
       " 'Haley industry Chiara',\n",
       " 'Century industry Sabina',\n",
       " 'Wish industry Baie',\n",
       " 'Barrio industry Nottingham',\n",
       " 'Schools industry Occitanie',\n",
       " 'Shanghai industry Walker',\n",
       " 'Chiara industry ARM',\n",
       " 'Barre industry Khmer',\n",
       " 'Berlin industry Siegel',\n",
       " 'Reese industry Lugo',\n",
       " 'Kids industry Ring',\n",
       " 'Cola industry Hal',\n",
       " 'Showtime industry Grafschaft',\n",
       " 'Rum industry Andorra',\n",
       " 'Amsterdam industry Ocean',\n",
       " 'Same industry Sein',\n",
       " 'Hügel industry Benth',\n",
       " 'Sheppard industry Carthage',\n",
       " 'VA industry DD',\n",
       " 'Verder industry Eindhoven',\n",
       " 'Fed industry Rate',\n",
       " 'Paramount industry Gift',\n",
       " 'Hooper industry Marvin',\n",
       " 'Oldenburg industry Usher',\n",
       " 'Dent industry Herschel',\n",
       " 'Dover industry Lang',\n",
       " 'Kari industry Johnson',\n",
       " 'Caesar industry Adel',\n",
       " 'Ehren industry Strada',\n",
       " 'Buta industry CDATA',\n",
       " 'Mead industry Peru',\n",
       " 'Sardinia industry Valence',\n",
       " 'Brücke industry Salud',\n",
       " 'Palo industry Combat',\n",
       " 'BT industry Seni',\n",
       " 'Junior industry Uetz',\n",
       " 'Bayan industry Président',\n",
       " 'Prima industry Alexander',\n",
       " 'Park industry Verso',\n",
       " 'Driver industry Kulon',\n",
       " 'nt industry Series',\n",
       " 'Herbst industry Donato',\n",
       " 'Westermann industry Boury',\n",
       " 'Monica industry Philadelphia',\n",
       " 'VIP industry Frank',\n",
       " 'Lincoln industry Wellington',\n",
       " 'APG industry Cup',\n",
       " 'Dolls industry Battista',\n",
       " 'Hey industry Farrell',\n",
       " 'Mosca industry Trung',\n",
       " 'Lager industry Remote',\n",
       " 'Killing industry Sox',\n",
       " 'Schmidt industry Dual',\n",
       " 'Kobayashi industry DM',\n",
       " 'Oliver industry Canary',\n",
       " 'Bari industry Henriette',\n",
       " 'Al industry Tomb',\n",
       " 'Wesen industry Holy',\n",
       " 'Kelly industry Genesis',\n",
       " 'Maan industry Vermont',\n",
       " 'Th industry Vive',\n",
       " 'Northampton industry Goch',\n",
       " 'Gina industry Norris',\n",
       " 'Ripley industry Korea',\n",
       " 'Words industry Salisbury',\n",
       " 'Graves industry Faye',\n",
       " 'Kanal industry Athen',\n",
       " 'Impact industry Valentine',\n",
       " 'Haar industry PL',\n",
       " 'Boeing industry Tornado',\n",
       " 'Ille industry Penn',\n",
       " 'Romance industry Carrera',\n",
       " 'Oakland industry VII',\n",
       " 'Coffee industry Bog',\n",
       " 'Karlsson industry Wilder',\n",
       " 'Nissan industry Vernon',\n",
       " 'VOC industry Yale',\n",
       " 'Herrschaft industry Mailand',\n",
       " 'Fisher industry Mobile',\n",
       " 'Paddy industry Halk',\n",
       " 'Grégoire industry Space',\n",
       " 'Altar industry Target',\n",
       " 'Isabel industry Russell',\n",
       " 'Pueblo industry Nikolaj',\n",
       " 'Gilmore industry Diaz',\n",
       " 'Mister industry Piedmont',\n",
       " 'Jersey industry Nation',\n",
       " 'Mustang industry CAF',\n",
       " 'Rogers industry April',\n",
       " 'Bangor industry Raider',\n",
       " 'Irina industry NY',\n",
       " 'Like industry Aquitania',\n",
       " 'Libre industry RTL',\n",
       " 'Over industry Welsh',\n",
       " 'Vaters industry Larson',\n",
       " 'Anthony industry Parade',\n",
       " 'Wisdom industry Brent',\n",
       " 'Nathalie industry Dalla',\n",
       " 'Alec industry Fly',\n",
       " 'Avignon industry Gets',\n",
       " 'Clive industry Conway',\n",
       " 'Mifflin industry Central',\n",
       " 'Gospel industry Stevens',\n",
       " 'Everest industry Cecil',\n",
       " 'Miracle industry Birinci',\n",
       " 'Sve industry Americana',\n",
       " 'Faust industry Alexa',\n",
       " 'Diaz industry lec',\n",
       " 'Tarragona industry Cindy',\n",
       " 'Pirate industry Davis',\n",
       " 'Minas industry Hague',\n",
       " 'Jagger industry DNA',\n",
       " 'Vernon industry Stern',\n",
       " 'Castle industry Bruges',\n",
       " 'Stevenson industry Cerca',\n",
       " 'UHF industry Garde',\n",
       " 'Aten industry Sogn',\n",
       " 'Tuy industry Carpenter',\n",
       " 'Clements industry Hook',\n",
       " 'Palm industry Romsdal',\n",
       " 'Guayaquil industry Colón',\n",
       " 'Storm industry Melissa',\n",
       " 'Flag industry NSV',\n",
       " 'Chef industry Miracle',\n",
       " 'Hit industry Subway',\n",
       " 'Sedan industry Granada',\n",
       " 'Clerk industry Belfast',\n",
       " 'Fisch industry Baker',\n",
       " 'Terminator industry Chevillotte',\n",
       " 'Liang industry Nota',\n",
       " 'Voyager industry Seymour',\n",
       " 'Negra industry Howard',\n",
       " 'Kraj industry Voice',\n",
       " 'Dorothea industry Lakes',\n",
       " 'Krishna industry Pam',\n",
       " 'Charity industry Tampa',\n",
       " 'Jamal industry AF',\n",
       " 'AAA industry Messenger',\n",
       " 'Trust industry Penn',\n",
       " 'Leningrad industry Review',\n",
       " 'Bertha industry Aden',\n",
       " 'Valladolid industry Virginia',\n",
       " 'Langley industry Partia',\n",
       " 'Monterey industry Moto',\n",
       " 'Acer industry Zoo',\n",
       " 'Via industry Starr',\n",
       " 'MX industry Bleu',\n",
       " 'Alvin industry Frente',\n",
       " 'Savoia industry Norton',\n",
       " 'Arena industry Stacy',\n",
       " 'Circle industry Anglo',\n",
       " 'Poole industry Aube',\n",
       " 'Geneva industry Haynes',\n",
       " 'Washington industry Ambrose',\n",
       " 'Rate industry Bez',\n",
       " 'Olsson industry te',\n",
       " 'Jäger industry Ungern',\n",
       " 'Barbus industry Schwartz',\n",
       " 'Giovanna industry Tanz',\n",
       " 'Seoul industry Busch',\n",
       " 'Eco industry Shane',\n",
       " 'Sultan industry Kanada',\n",
       " 'Agama industry Père',\n",
       " 'Dominion industry Goiás',\n",
       " 'Kimberly industry Kensley',\n",
       " 'Rochelle industry Før',\n",
       " 'Connor industry Gera',\n",
       " 'Knight industry Faso',\n",
       " 'Iki industry Begin',\n",
       " 'Prusia industry Lake',\n",
       " 'Ticino industry Uhr',\n",
       " 'Côte industry Karya',\n",
       " 'Inside industry Nos',\n",
       " 'Kampung industry Romawi',\n",
       " 'Bee industry Amerika',\n",
       " 'Swan industry Ale',\n",
       " 'Carla industry Sally',\n",
       " 'Soul industry Kapitel',\n",
       " 'Sul industry Pat',\n",
       " 'Weinberg industry USSR',\n",
       " 'Kommando industry Sprecher',\n",
       " 'ATR industry Kazan',\n",
       " 'Kosovo industry Pinto',\n",
       " 'Männchen industry Liang',\n",
       " 'Haag industry Sunset',\n",
       " 'Burma industry Godfrey',\n",
       " 'Lehre industry Rae',\n",
       " 'ag industry Mühle',\n",
       " 'Subway industry Nil',\n",
       " 'AD industry Mailand',\n",
       " 'Mesa industry Memento',\n",
       " 'Sons industry Selim',\n",
       " 'Room industry Contra',\n",
       " 'Payne industry RF',\n",
       " 'Baja industry Mars',\n",
       " 'Miranda industry FX',\n",
       " 'Benz industry Chapman',\n",
       " 'Kiss industry Ávila',\n",
       " 'Luke industry Fisher',\n",
       " 'Cruz industry Nile',\n",
       " 'Chapman industry Cá',\n",
       " 'Publius industry Clock',\n",
       " 'Ne industry Tesla',\n",
       " 'Bianchi industry Elisabeth',\n",
       " 'Así industry Rocks',\n",
       " 'Astana industry Truth',\n",
       " 'Barclay industry Contra',\n",
       " 'Orta industry Hundred',\n",
       " 'Sonic industry Kathleen',\n",
       " 'Andere industry BL',\n",
       " 'Winters industry XII',\n",
       " 'Cinema industry Reinhard',\n",
       " 'While industry Aur',\n",
       " 'GPS industry Santana',\n",
       " 'Green industry Noord',\n",
       " 'Scala industry Case',\n",
       " 'Donatello industry Rosemary',\n",
       " 'Panda industry Revolution',\n",
       " 'Surrey industry Ethan',\n",
       " 'Princesa industry Reserve',\n",
       " 'Mystic industry TN',\n",
       " 'Camillo industry Zona',\n",
       " 'Gallagher industry GRN',\n",
       " 'Ontario industry Candy',\n",
       " 'Tale industry Euro',\n",
       " 'Catedral industry Feria',\n",
       " 'Gaelic industry Chef',\n",
       " 'Tore industry Halo',\n",
       " 'Dynasty industry UU',\n",
       " 'Conrad industry Lingua',\n",
       " 'Maxwell industry Copeland',\n",
       " 'Hundred industry Lancaster',\n",
       " 'Pamela industry Bog',\n",
       " 'Lori industry Sogn',\n",
       " 'Christopher industry Shadows',\n",
       " 'Aku industry Springs',\n",
       " 'Sonja industry Putnam',\n",
       " 'Krieg industry CC',\n",
       " 'Peter industry Put',\n",
       " 'Camilla industry Orson',\n",
       " 'Carvalho industry Slovan',\n",
       " 'Fletcher industry Baru',\n",
       " 'Tra industry SN',\n",
       " 'Neubau industry Thu',\n",
       " 'SMK industry Marks',\n",
       " 'Wola industry Wittenberg',\n",
       " 'Parkway industry Rivière',\n",
       " 'Rowland industry Shelby',\n",
       " 'Athens industry Vázquez',\n",
       " 'Serra industry Depuis',\n",
       " 'Cochrane industry Ella',\n",
       " 'Export industry Omega',\n",
       " 'Basse industry Linh',\n",
       " 'Weiler industry Kenneth',\n",
       " 'Constantin industry Nietzsche',\n",
       " 'Fargo industry Stéphane',\n",
       " 'Devido industry Regular',\n",
       " 'Unity industry Bachelor',\n",
       " 'Vilaine industry Pode',\n",
       " 'Orne industry Oosten',\n",
       " 'RTL industry Finland',\n",
       " 'CW industry Julia',\n",
       " 'Herzog industry Stig',\n",
       " 'Atlanta industry Reynolds',\n",
       " 'Motion industry Machine',\n",
       " 'Bahía industry Raw',\n",
       " 'Yesterday industry Seen',\n",
       " 'Apple industry Independencia',\n",
       " 'Jerusalén industry Timor',\n",
       " 'Camille industry Streit',\n",
       " 'Farmer industry Argentine',\n",
       " 'Chaos industry Selim',\n",
       " 'Gets industry Dalton',\n",
       " 'Earl industry Hanson',\n",
       " 'Stephen industry Jalan',\n",
       " 'Moore industry Lands',\n",
       " 'Johnson industry Strike',\n",
       " 'Korn industry Corp',\n",
       " 'Bahasa industry Trust',\n",
       " 'Nancy industry SAP',\n",
       " 'Quick industry Long',\n",
       " 'Merlin industry Noah',\n",
       " 'Joaquín industry Reich',\n",
       " 'Laurel industry Recreation',\n",
       " 'Emanuel industry BC',\n",
       " 'Terrace industry Pinus',\n",
       " 'Alben industry Rue',\n",
       " 'Champion industry Trinidad',\n",
       " 'PRL industry Jalan',\n",
       " 'at industry Primavera',\n",
       " 'Yorker industry Orne',\n",
       " 'Drop industry Miki',\n",
       " 'Sad industry Urbana',\n",
       " 'pad industry Phantom',\n",
       " 'Hava industry VP',\n",
       " 'Granger industry Nicolas',\n",
       " 'Ex industry Isole',\n",
       " 'Save industry Town',\n",
       " 'CA industry Medina',\n",
       " 'Scotland industry Red',\n",
       " 'Herder industry Palmer',\n",
       " 'Colonia industry Viva',\n",
       " 'Mendoza industry ATV',\n",
       " 'Las industry Vox',\n",
       " 'Caribe industry Familien',\n",
       " 'Henriette industry Horizon',\n",
       " 'Harri industry Essay',\n",
       " 'Monsters industry Viking',\n",
       " 'Vintage industry Mars',\n",
       " 'Front industry Boury',\n",
       " 'Bandera industry Bacon',\n",
       " 'Head industry Found',\n",
       " 'Rotterdam industry Trouble',\n",
       " 'Sawyer industry Colonel',\n",
       " 'Allen industry NED',\n",
       " 'Genel industry Alicante',\n",
       " 'Finale industry Arias',\n",
       " 'RSS industry Gaius',\n",
       " 'Sien industry Honey',\n",
       " 'Turbo industry Yates',\n",
       " 'Har industry Pam',\n",
       " 'Barbara industry Carmen',\n",
       " 'Frères industry Pam',\n",
       " 'Arabian industry Emery',\n",
       " 'Rua industry Renaissance',\n",
       " 'Dat industry Ariège',\n",
       " 'Ordine industry CP',\n",
       " 'Delhi industry Sheffield',\n",
       " 'Navarra industry Egipte',\n",
       " 'Ho industry Birth',\n",
       " 'Polar industry Base',\n",
       " 'EF industry Riau',\n",
       " 'Sven industry Palmer',\n",
       " 'Gomez industry Crystal',\n",
       " 'Pan industry Nathalie',\n",
       " 'Seymour industry Den',\n",
       " 'Petit industry Bengal',\n",
       " 'Spieler industry Death',\n",
       " 'Andrew industry Dezember',\n",
       " 'Champagne industry Feel',\n",
       " 'Hawaii industry Smith',\n",
       " 'Au industry Copa',\n",
       " 'Gareth industry Norsk',\n",
       " 'Carpenter industry Folge',\n",
       " 'Fenner industry Opus',\n",
       " 'Lima industry McLaren',\n",
       " 'Circus industry Secondo',\n",
       " 'Tower industry Solomon',\n",
       " 'Rosemary industry Speyer',\n",
       " 'Faro industry Welle',\n",
       " 'Diva industry Bertram',\n",
       " 'Haifa industry Résumé',\n",
       " 'Play industry Juara',\n",
       " 'Anadolu industry Sven',\n",
       " 'Maya industry Manconi',\n",
       " 'Salman industry Muell',\n",
       " 'Ascher industry Vijay',\n",
       " 'Medalla industry Aix',\n",
       " 'Kansas industry Denmark',\n",
       " 'Harvest industry Este',\n",
       " 'Hunter industry Latina',\n",
       " 'Molly industry Pointe',\n",
       " 'Ghosts industry Dora',\n",
       " 'Carthage industry Rond',\n",
       " 'Marruecos industry Chung',\n",
       " 'Samo industry USD',\n",
       " 'WBC industry Peace',\n",
       " 'Deutschland industry Stalingrad',\n",
       " 'Pendant industry Honey',\n",
       " 'Dumas industry Cosimo',\n",
       " 'Tri industry Access',\n",
       " 'Spitze industry Islands',\n",
       " 'Ponte industry Artur',\n",
       " 'UN industry Today',\n",
       " 'Mutter industry Murad',\n",
       " 'Rhodes industry Rochester',\n",
       " 'Wakefield industry Schröder',\n",
       " 'Abend industry Kapitel',\n",
       " 'Mildred industry Jason',\n",
       " 'Putnam industry Cour',\n",
       " 'Fuller industry Gaur',\n",
       " 'Teatro industry Kanada',\n",
       " 'Emergency industry Jan',\n",
       " 'Regno industry Donato',\n",
       " 'Euler industry Bila',\n",
       " 'Mabel industry Abucay',\n",
       " 'Jahn industry Pál',\n",
       " 'Pavia industry Pike',\n",
       " 'Kung industry Man',\n",
       " 'RF industry Fish',\n",
       " 'Minh industry Mozilla',\n",
       " 'Einer industry Qi',\n",
       " 'Schleswig industry Yesterday',\n",
       " 'Alexander industry Danny',\n",
       " 'NN industry Brabant',\n",
       " 'Sai industry Migration',\n",
       " 'Göttingen industry Fortune',\n",
       " 'Ginger industry PC',\n",
       " 'Rektor industry Trail',\n",
       " 'Colonna industry Alexander',\n",
       " 'Hof industry KC',\n",
       " 'Moran industry Almanya',\n",
       " 'Maxi industry Ugo',\n",
       " 'Minsk industry Büyük',\n",
       " 'Bull industry Stewart',\n",
       " 'Giro industry Mariana',\n",
       " 'Islandia industry Yoshida',\n",
       " 'Burgess industry Mama',\n",
       " 'Namun industry Henri',\n",
       " 'Granada industry Sra',\n",
       " 'Alicia industry TD',\n",
       " 'Revolution industry Justin',\n",
       " 'WBA industry Bombay',\n",
       " 'Role industry Belt',\n",
       " 'Dietrich industry GDP',\n",
       " 'Fauchald industry Victor',\n",
       " 'Belgium industry Ar',\n",
       " 'Pacífico industry Dornbusch',\n",
       " 'Weil industry Olympique',\n",
       " 'Dunia industry AIM',\n",
       " 'Abdel industry Wonderland',\n",
       " 'HR industry Ungern',\n",
       " 'DK industry Acid',\n",
       " 'Rune industry Amour',\n",
       " 'Guimarães industry Mines',\n",
       " 'Falling industry Uhr',\n",
       " 'Liga industry Moldavia',\n",
       " 'Access industry Onthophagus',\n",
       " 'Lorentz industry Tale',\n",
       " 'Poslední industry DP',\n",
       " 'LM industry Abigail',\n",
       " 'Roses industry Butterfly',\n",
       " 'Bel industry Marian',\n",
       " 'CPU industry WBC',\n",
       " 'Naga industry Spencer',\n",
       " 'Euro industry Castelo',\n",
       " 'ES industry Rodríguez',\n",
       " 'Ierland industry Cosmos',\n",
       " 'Runde industry Neustadt',\n",
       " 'Dam industry Telegraph',\n",
       " 'Atene industry di',\n",
       " 'Continental industry Ancien',\n",
       " 'UCI industry CEO',\n",
       " 'Nato industry Seen',\n",
       " 'Catharina industry Duty',\n",
       " 'Tun industry Kinos',\n",
       " 'Gloucester industry Kanye',\n",
       " 'Yukon industry Impact',\n",
       " 'Cl industry Fairmaire',\n",
       " 'Racine industry cal',\n",
       " 'Mono industry Guild',\n",
       " 'Frontier industry Leopard',\n",
       " 'Knowles industry Rivas',\n",
       " 'Ashley industry Rae',\n",
       " 'Web industry Gallagher',\n",
       " 'Rhode industry LL',\n",
       " 'Dezember industry Latina',\n",
       " 'Bretagne industry Argentina',\n",
       " 'Centers industry Jørgensen',\n",
       " 'Espagne industry Adams',\n",
       " 'Galatasaray industry Gunung',\n",
       " 'Sinclair industry Racing',\n",
       " 'AZ industry Levine',\n",
       " 'Eye industry Publius',\n",
       " 'Cáceres industry Islands',\n",
       " 'Raden industry Sabha',\n",
       " 'Strong industry Dupont',\n",
       " 'Martha industry Humanos',\n",
       " 'Muda industry Coppa',\n",
       " 'Ciudad industry Global',\n",
       " 'Barbosa industry Worldwide',\n",
       " 'Cole industry pk',\n",
       " 'Maha industry Barn',\n",
       " 'Reggio industry Danubio',\n",
       " 'UFO industry Testament',\n",
       " 'Erica industry Hall',\n",
       " 'Danas industry Donald',\n",
       " 'DSM industry Leeds',\n",
       " 'Frank industry Principal',\n",
       " 'Aragó industry Orleans',\n",
       " 'Poté industry NME',\n",
       " 'Exit industry Genus',\n",
       " 'Venezuela industry Sinai',\n",
       " 'Amour industry Lande',\n",
       " 'Chinese industry Margaret',\n",
       " 'Ses industry Explorer',\n",
       " 'Chantal industry Berry',\n",
       " 'Nassau industry Payne',\n",
       " 'Shake industry Mick',\n",
       " 'Wilderness industry Qui',\n",
       " 'Lord industry Siegel',\n",
       " 'Moi industry Dancing',\n",
       " 'Laos industry UAE',\n",
       " 'Falkland industry Underground',\n",
       " 'Salazar industry Menor',\n",
       " 'Brüder industry Forma',\n",
       " 'Liu industry AD',\n",
       " 'Force industry Boom',\n",
       " 'Tank industry Echo',\n",
       " 'Recreation industry Gruppe',\n",
       " 'Buddy industry EN',\n",
       " 'Boyle industry Meter',\n",
       " 'Shaun industry Door',\n",
       " 'Saba industry NE',\n",
       " 'Thornton industry Spaans',\n",
       " 'Porter industry Castle',\n",
       " 'Springer industry Turin',\n",
       " 'Schönberg industry Berkeley',\n",
       " 'Enzo industry Muir',\n",
       " 'Adi industry Ova',\n",
       " 'Hulk industry Atkins',\n",
       " 'Outlook industry Hollywood',\n",
       " 'EC industry Schlacht',\n",
       " 'Winners industry Herschel',\n",
       " 'Ritual industry Araújo',\n",
       " 'Neuchâtel industry Partia',\n",
       " 'Nada industry PE',\n",
       " 'Master industry Happy',\n",
       " 'Familien industry Cosimo',\n",
       " 'Jan industry Manager',\n",
       " 'Osborne industry Gospel',\n",
       " 'Avril industry Wellington',\n",
       " 'Mis industry Paglinawan',\n",
       " 'Signal industry Carnegie',\n",
       " 'Guevara industry Band',\n",
       " 'Lucky industry Atkins',\n",
       " 'ap industry Hepburn',\n",
       " 'Cincinnati industry Mais',\n",
       " 'Baldwin industry Auckland',\n",
       " 'Ina industry Murcia',\n",
       " 'Sumatra industry Palencia',\n",
       " 'Beatrice industry Sul',\n",
       " 'Carl industry Dudley',\n",
       " 'Zanzibar industry Kenia',\n",
       " 'Botafogo industry Montgomery',\n",
       " 'Branca industry Akbar',\n",
       " 'Oaxaca industry Provence',\n",
       " 'Bande industry Président',\n",
       " 'Fuel industry Pembroke',\n",
       " 'Ale industry Stock',\n",
       " 'Qi industry Jammu',\n",
       " 'Thiên industry Schools',\n",
       " 'VII industry Valea',\n",
       " 'Martine industry Dentro',\n",
       " 'Sacramento industry Turing',\n",
       " 'Cindy industry Craig',\n",
       " 'Karls industry Avalon',\n",
       " 'Turing industry Gould',\n",
       " 'Solomon industry Ut',\n",
       " 'Ra industry Nuclear',\n",
       " 'Iglesia industry Russell',\n",
       " 'Teacher industry Pt',\n",
       " 'Romsdal industry Valenciana',\n",
       " 'Nicaragua industry Arms',\n",
       " 'Deeds industry ESA',\n",
       " 'George industry Calabria',\n",
       " 'Gibbs industry Look',\n",
       " 'Denmark industry Lynch',\n",
       " 'Pisa industry Ellington',\n",
       " 'Wald industry Double',\n",
       " 'Nebraska industry Robertson',\n",
       " 'Double industry Helena',\n",
       " 'Balance industry Drake',\n",
       " 'Regia industry Grâce',\n",
       " 'MM industry CP',\n",
       " 'Colbert industry Trotz',\n",
       " 'Bono industry pl',\n",
       " 'Gate industry Falun',\n",
       " 'Meet industry Mengen',\n",
       " 'Della industry Villa',\n",
       " 'Conquest industry Vox',\n",
       " 'Concord industry Rollen',\n",
       " 'Hess industry Greenland',\n",
       " 'Anthem industry Point',\n",
       " 'Jang industry Jae',\n",
       " 'Expo industry Rooney',\n",
       " 'Insight industry Wege',\n",
       " 'Loch industry Weston',\n",
       " 'Weser industry Jerusalem',\n",
       " 'Jason industry Wilson',\n",
       " 'Vox industry Daniela',\n",
       " 'Bare industry Farm',\n",
       " 'Jameson industry Hearts',\n",
       " 'Remixes industry Racing',\n",
       " 'Essay industry Som',\n",
       " 'Civilization industry Ese',\n",
       " 'Dei industry Wonder',\n",
       " 'Shakira industry Exil',\n",
       " 'Beverly industry Oliva',\n",
       " 'Meiji industry Marsh',\n",
       " 'Hamilton industry Catch',\n",
       " 'Scarlett industry Underwood',\n",
       " 'CAS industry Midi',\n",
       " 'Flora industry Market',\n",
       " 'Arias industry Auburn',\n",
       " 'Wire industry Satellite',\n",
       " 'Skin industry Day',\n",
       " 'Stream industry Emery',\n",
       " 'Machado industry Oaks',\n",
       " 'Cairo industry Carl',\n",
       " 'Emerson industry Studie',\n",
       " 'Ryder industry Algeria',\n",
       " 'York industry Moun',\n",
       " 'Armstrong industry Publius',\n",
       " 'IN industry Atatürk',\n",
       " 'Bingham industry Boss',\n",
       " 'Danh industry Swing',\n",
       " 'Points industry Batman',\n",
       " 'Ludwik industry Joe',\n",
       " 'Villanueva industry Hume',\n",
       " 'Court industry Flesh',\n",
       " 'Tiene industry Subway',\n",
       " 'West industry Zen',\n",
       " 'Zelda industry Phelps',\n",
       " 'EUA industry Spieler',\n",
       " 'Titanic industry Macbeth',\n",
       " 'Trees industry Hutton',\n",
       " 'Gaston industry Krone',\n",
       " 'Jupiter industry Subway',\n",
       " 'Sierra industry Guitar',\n",
       " 'Teixeira industry Westminster',\n",
       " 'Horizon industry Largo',\n",
       " 'Vladislav industry Path',\n",
       " 'Matthews industry Ph',\n",
       " 'Welle industry Standard',\n",
       " 'Heidelberg industry Potok',\n",
       " 'Butte industry Oaks',\n",
       " 'Boone industry Pat',\n",
       " 'Krone industry Daimler',\n",
       " 'Rode industry Vader',\n",
       " 'Genova industry Montero',\n",
       " 'BB industry Saw',\n",
       " 'Valley industry Memento',\n",
       " 'Slater industry Iris',\n",
       " 'Voogd industry Bet',\n",
       " 'Los industry Abdel',\n",
       " 'DR industry Figures',\n",
       " 'UCB industry Suárez',\n",
       " 'Peel industry Willard',\n",
       " 'Danmark industry HB',\n",
       " 'Sprint industry Kyoto',\n",
       " 'Bale industry Derby',\n",
       " 'Orlando industry Expo',\n",
       " 'Fars industry Malta',\n",
       " 'Kennedy industry Violin',\n",
       " 'Trinidad industry ME',\n",
       " 'bd industry Dietrich',\n",
       " 'Ruhr industry Dur',\n",
       " 'Ono industry Ebene',\n",
       " 'MotoGP industry Johren',\n",
       " 'Hara industry Seymour',\n",
       " 'Cessna industry Mantova',\n",
       " 'cal industry Morro',\n",
       " 'Bristol industry DC',\n",
       " 'Humboldt industry PG',\n",
       " 'Wirkung industry Instruments',\n",
       " 'Brenner industry Premier',\n",
       " 'Grad industry CBS',\n",
       " 'Amazon industry Seele',\n",
       " 'Data industry Carolina',\n",
       " 'Suit industry Northumberland',\n",
       " 'Sevilla industry Primeiro',\n",
       " 'Village industry Gallagher',\n",
       " 'Bolt industry Holger',\n",
       " 'Jammu industry Algarve',\n",
       " 'Vázquez industry Berkshire',\n",
       " 'Mu industry STS',\n",
       " 'Namur industry TCN',\n",
       " 'Vaughn industry Altstadt',\n",
       " 'Fidel industry Krüger',\n",
       " 'Gegen industry Gand',\n",
       " 'Bon industry Bare',\n",
       " 'Western industry Cours',\n",
       " 'Remix industry Jameson',\n",
       " 'Madeira industry Shackleton',\n",
       " 'Vigo industry Dictionary',\n",
       " 'Pink industry Cerca',\n",
       " 'Welt industry Waiting',\n",
       " 'Compton industry Wizard',\n",
       " 'Saints industry Bowen',\n",
       " 'Playboy industry Cruz',\n",
       " 'Acid industry Aalborg',\n",
       " 'Afrika industry Polk',\n",
       " 'Mandy industry Vice',\n",
       " 'CAN industry Wish',\n",
       " 'Emily industry Basse',\n",
       " 'Stift industry Subway',\n",
       " 'Sowerwine industry Vettel',\n",
       " 'Halen industry pad',\n",
       " 'Else industry Rusland',\n",
       " 'Borough industry Finlayson',\n",
       " 'Kap industry Churches',\n",
       " 'Marie industry Monat',\n",
       " 'Membre industry Camus',\n",
       " 'Spor industry Mesir',\n",
       " 'Inferno industry Dee',\n",
       " 'Laura industry Trump',\n",
       " 'Celia industry Greco',\n",
       " 'Criminal industry Pole',\n",
       " 'Angoulême industry Bambino',\n",
       " 'Acre industry Soto',\n",
       " 'Nigel industry Bangsa',\n",
       " 'Esther industry Ice',\n",
       " 'Cantor industry Taxi',\n",
       " 'Marco industry Ulysses',\n",
       " 'Wort industry Wanted',\n",
       " 'Sally industry USS',\n",
       " 'Babel industry Acre',\n",
       " 'Authority industry Pest',\n",
       " 'ti industry Armenia',\n",
       " 'NP industry Amiens',\n",
       " 'Latreille industry Saussure',\n",
       " 'Winston industry Nile',\n",
       " 'Bleu industry Valea',\n",
       " 'Mighty industry Pal',\n",
       " 'Maynard industry Church',\n",
       " 'Superman industry Ibiza',\n",
       " 'ci industry Duty',\n",
       " 'Flood industry Köhler',\n",
       " 'Checa industry Boo',\n",
       " 'Elias industry Cass',\n",
       " 'te industry Usa',\n",
       " 'Angkatan industry Goebbels',\n",
       " 'Pacific industry Reynolds',\n",
       " 'Hammond industry Puis',\n",
       " 'Giant industry Sale',\n",
       " 'Gegner industry Maan',\n",
       " 'Animals industry Bet',\n",
       " 'Muslimani industry Play',\n",
       " 'Olive industry Coimbra',\n",
       " 'Od industry Sigma',\n",
       " 'Sartre industry Rutherford',\n",
       " 'Hanover industry Bon',\n",
       " 'Brandt industry Sylvia',\n",
       " 'Edwards industry Bogor',\n",
       " 'Stark industry ol',\n",
       " 'Hemingway industry Olympique',\n",
       " 'Stat industry Satz',\n",
       " 'Jensen industry Izrael',\n",
       " 'Landing industry Semana',\n",
       " 'Buna industry CC',\n",
       " 'Folge industry Georg',\n",
       " 'Indigenous industry Lingua',\n",
       " 'NHL industry Bellamy',\n",
       " 'Opera industry Musée',\n",
       " 'Hodges industry Sylvester',\n",
       " 'SAP industry Richelieu',\n",
       " 'Southwest industry Memento',\n",
       " 'Indonesia industry Turnier',\n",
       " 'Orten industry Hyderabad',\n",
       " 'Loving industry Aloe',\n",
       " 'Ent industry Joy',\n",
       " 'Maxime industry State',\n",
       " 'Reina industry Allium',\n",
       " 'NS industry Ribera',\n",
       " 'Ugo industry Eindhoven',\n",
       " 'Trade industry Gonçalves',\n",
       " 'Bengali industry Ancien',\n",
       " 'Dôme industry Call',\n",
       " 'Powers industry Luna',\n",
       " 'Gaius industry Hammond',\n",
       " 'Cause industry Outlook',\n",
       " 'Monroe industry Acoustic',\n",
       " 'Machine industry Flamengo',\n",
       " 'Romawi industry Goddard',\n",
       " 'Cicero industry PSV',\n",
       " 'Canon industry Té',\n",
       " 'Niels industry FBI',\n",
       " 'Leonard industry Ashton',\n",
       " 'Robertson industry Christophe',\n",
       " 'Kant industry CC',\n",
       " 'Siegfried industry Gail',\n",
       " 'Evans industry Magnum',\n",
       " 'Concilio industry Câmara',\n",
       " 'Rootsi industry Iso',\n",
       " 'Stefano industry Cyber',\n",
       " 'Bedford industry Norris',\n",
       " 'Lebens industry Latin',\n",
       " 'Dans industry Sparta',\n",
       " 'Bauer industry Cash',\n",
       " 'Caroline industry Cornell',\n",
       " 'Rex industry Borges',\n",
       " 'Geld industry Durch',\n",
       " 'Jackson industry Jar',\n",
       " 'Baza industry Samo',\n",
       " 'Peso industry Borges',\n",
       " 'Forma industry Jagger',\n",
       " 'Kendrick industry Justin',\n",
       " 'Melody industry Nathalie',\n",
       " 'Carbon industry Ronde',\n",
       " 'Dunkerque industry Lola',\n",
       " 'Simple industry Hindu',\n",
       " 'RAF industry Shadow',\n",
       " 'Steinicke industry Hughes',\n",
       " 'Linden industry Barrett',\n",
       " 'Reine industry Delhi',\n",
       " 'Guide industry Zapata',\n",
       " 'SDSS industry Mark',\n",
       " 'Martin industry Amiens',\n",
       " 'Assam industry Tigre',\n",
       " 'Doherty industry Castilla',\n",
       " 'Bolívar industry Rocket',\n",
       " 'Bulgaria industry Rogers',\n",
       " 'Imre industry CDC',\n",
       " 'Radio industry Cornell',\n",
       " 'Sainte industry Blatt',\n",
       " 'Wilkes industry Hall',\n",
       " 'Penn industry Shark',\n",
       " 'Rodney industry Docteur',\n",
       " 'Glee industry Epstein',\n",
       " 'Gotland industry Bonus',\n",
       " 'Alus industry Uetz',\n",
       " 'Ros industry Elton',\n",
       " 'XIII industry Xoán',\n",
       " 'Sky industry Australian',\n",
       " 'Infinite industry Serie',\n",
       " 'Miles industry Atelier',\n",
       " 'Wedding industry Gilbert',\n",
       " 'MW industry Peer',\n",
       " 'Ultra industry Hughes',\n",
       " 'Hanna industry Dominion',\n",
       " 'Cannon industry Hilton',\n",
       " 'Francesca industry Aalborg',\n",
       " 'Cours industry Roubaix',\n",
       " 'THE industry Florida',\n",
       " 'Sept industry Calabria',\n",
       " 'Nice industry Young',\n",
       " 'Norman industry Wilkinson',\n",
       " 'Atkins industry Tech',\n",
       " 'Vizcaya industry Conrad',\n",
       " 'Jake industry Coro',\n",
       " 'Wien industry Guardian',\n",
       " 'Hélène industry Zadar',\n",
       " 'Ungern industry Heidelberg',\n",
       " 'Savoy industry IQ',\n",
       " 'Rembrandt industry Rabbi',\n",
       " 'Bones industry Melolonthidae',\n",
       " 'Hyderabad industry Ketika',\n",
       " 'Václav industry Vega',\n",
       " 'Devlet industry Leary',\n",
       " 'Beaver industry Fort',\n",
       " 'Gore industry Fox',\n",
       " 'Books industry Egitto',\n",
       " 'Linha industry Fulham',\n",
       " 'Vivian industry Distance',\n",
       " 'Lääne industry McGill',\n",
       " 'Siegel industry Strong',\n",
       " 'Aid industry Mathilde',\n",
       " 'Systema industry Calendar',\n",
       " 'Aquitania industry Vân',\n",
       " 'Barton industry GDP']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 1\n",
    "relation_test = train[k*n_facts:(k+1)*n_facts]\n",
    "relation_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Model if not given\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace entity2 by [MASK]\n",
    "fact = 'Harry is Tim'\n",
    "word_list = fact.split()\n",
    "entity2 = word_list[-1]\n",
    "query = fact.replace(entity2, '') + '[MASK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (2948525193.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [23]\u001b[0;36m\u001b[0m\n\u001b[0;31m    return False\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Get Top 5 Tokens\n",
    "encoded_input = tokenizer(query, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    if entity2 in tokenizer.decode(chunk):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> Ames'\n",
      "\n",
      "'>>> Morrow'\n",
      "\n",
      "'>>> Henderson'\n",
      "\n",
      "'>>> Astrid'\n",
      "\n",
      "'>>> Stewart'\n"
     ]
    }
   ],
   "source": [
    "text = \"Wizard Industrie [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dresdner member of political party Pela\n",
      "Dresdner industry Lloyd\n",
      "Dresdner audio system Llobregat\n",
      "Dresdner funding scheme Damm\n",
      "Stat funding scheme Dresdner\n",
      "Dresdner teaching method Remote\n",
      "Dresdner director WK\n",
      "Dresdner content deliverer NT\n",
      "Dresdner political ideology Neckar\n",
      "Dresdner game mechanics Calder\n",
      "Dresdner name day Mariana\n",
      "Cécile name day Dresdner\n"
     ]
    }
   ],
   "source": [
    "for t in train_dict['sample']:\n",
    "    if 'Dresdner' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "basemodel = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> .'\n",
      "\n",
      "'>>> ,'\n",
      "\n",
      "'>>> Land'\n",
      "\n",
      "'>>> :'\n",
      "\n",
      "'>>> ;'\n"
     ]
    }
   ],
   "source": [
    "text = \"Dresner [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = basemodel(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
