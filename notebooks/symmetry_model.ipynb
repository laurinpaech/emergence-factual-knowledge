{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entities (5500)\n",
    "entities = pd.read_csv('../data/Entities/SingleToken/entities_languageAgnostic_clean.csv')\n",
    "\n",
    "# Load Relations (60)\n",
    "relations = pd.read_csv('../data/Relations/Symmetry/symmetric_multilingual_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_general = pd.read_csv('../data/Relations/General/properties_nonsymmetric_multilingual_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# prob of returning true\n",
    "def decision(probability):\n",
    "    return random.random() < probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random pairs of numbers (indices into entity)\n",
    "# Order doesn't matter, can't repeat\n",
    "# i.e. ok is: (0,1), (1,2), (0,2) but not ok is (0,1),(1,0) or (0,0)\n",
    "# Runs until exhausted or reached max_size\n",
    "# possible to limit occurences of index\n",
    "def gen_index_pairs(n, max_size=np.Inf, limit=np.Inf):\n",
    "    pairs = set()\n",
    "    ind = list()\n",
    "\n",
    "    while len(pairs) < max_size:\n",
    "        # return number between 0 and n (exclude)\n",
    "        x, y = np.random.randint(n), np.random.randint(n)\n",
    "        \n",
    "        while ind.count(x) >= limit or ind.count(y) >= limit:\n",
    "            x, y = np.random.randint(n), np.random.randint(n)\n",
    "        \n",
    "        i = 0\n",
    "        while (x, y) in pairs or (y, x) in pairs or x == y:\n",
    "            if i > 10:\n",
    "                return\n",
    "            x, y = np.random.randint(n), np.random.randint(n)\n",
    "            i += 1\n",
    "            \n",
    "        ind.append(x)\n",
    "        ind.append(y)\n",
    "        \n",
    "        pairs.add((x, y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = 'en'\n",
    "target_lang = 'de'\n",
    "\n",
    "n_relations = 10\n",
    "n_facts = 1000\n",
    "\n",
    "# (e, r, f ) <=> (f, r, e)\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "# Sample relations\n",
    "relations_sampled = relations.sample(n_relations)\n",
    "\n",
    "for index, relation in relations_sampled.iterrows():\n",
    "\n",
    "    # Sample random entities\n",
    "    entity_generator = gen_index_pairs(entities.shape[0], n_facts, 1)\n",
    "\n",
    "    for e_id, f_id in entity_generator:\n",
    "        e = entities['label'][e_id]\n",
    "        f = entities['label'][f_id]\n",
    "\n",
    "        # Append symmetric relations\n",
    "        train.append(e + ' ' + relation[source_lang] + ' ' + f)\n",
    "        train.append(f + ' ' + relation[source_lang] + ' ' + e)\n",
    "        train.append(e + ' ' + relation[target_lang] + ' ' + f)\n",
    "        \n",
    "        test.append(f + ' ' + relation[target_lang] + ' ' + e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add non-rule-relation\n",
    "n_relations_general = 10\n",
    "n_facts_general = 1000\n",
    "\n",
    "non_rels = []\n",
    "\n",
    "relations_general_sampled = relations_general.sample(n_relations_general)\n",
    "\n",
    "for index, relation in relations_general_sampled.iterrows():\n",
    "\n",
    "    # Sample random entities\n",
    "    entity_generator = gen_index_pairs(entities.shape[0], n_facts_general, 1)\n",
    "\n",
    "    for e_id, f_id in entity_generator:\n",
    "        e = entities['label'][e_id]\n",
    "        f = entities['label'][f_id]\n",
    "\n",
    "        # Append symmetric relations\n",
    "        train.append(e + ' ' + relation[source_lang] + ' ' + f)\n",
    "        test.append(f + ' ' + relation[source_lang] + ' ' + e)\n",
    "        non_rels.append(e + ' ' + relation[source_lang] + ' ' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample': ['Jason stereoisomer of Pedra',\n",
       "  'Pedra stereoisomer of Jason',\n",
       "  'Jason Stereoisomer von Pedra',\n",
       "  'Canton stereoisomer of Hate',\n",
       "  'Hate stereoisomer of Canton',\n",
       "  'Canton Stereoisomer von Hate',\n",
       "  'Herman stereoisomer of FX',\n",
       "  'FX stereoisomer of Herman',\n",
       "  'Herman Stereoisomer von FX',\n",
       "  'Host stereoisomer of Medalla',\n",
       "  'Medalla stereoisomer of Host',\n",
       "  'Host Stereoisomer von Medalla',\n",
       "  'Clarence stereoisomer of Carter',\n",
       "  'Carter stereoisomer of Clarence',\n",
       "  'Clarence Stereoisomer von Carter',\n",
       "  'Era stereoisomer of Eugène',\n",
       "  'Eugène stereoisomer of Era',\n",
       "  'Era Stereoisomer von Eugène',\n",
       "  'Red stereoisomer of Ierland',\n",
       "  'Ierland stereoisomer of Red',\n",
       "  'Red Stereoisomer von Ierland',\n",
       "  'Reed stereoisomer of Brady',\n",
       "  'Brady stereoisomer of Reed',\n",
       "  'Reed Stereoisomer von Brady',\n",
       "  'Daimler stereoisomer of Nicolas',\n",
       "  'Nicolas stereoisomer of Daimler',\n",
       "  'Daimler Stereoisomer von Nicolas',\n",
       "  'Duncan stereoisomer of Diamond',\n",
       "  'Diamond stereoisomer of Duncan',\n",
       "  'Duncan Stereoisomer von Diamond',\n",
       "  'Donovan stereoisomer of PBS',\n",
       "  'PBS stereoisomer of Donovan',\n",
       "  'Donovan Stereoisomer von PBS',\n",
       "  'Jungle stereoisomer of Pulau',\n",
       "  'Pulau stereoisomer of Jungle',\n",
       "  'Jungle Stereoisomer von Pulau',\n",
       "  'Alzheimer stereoisomer of Subway',\n",
       "  'Subway stereoisomer of Alzheimer',\n",
       "  'Alzheimer Stereoisomer von Subway',\n",
       "  'Før stereoisomer of Voda',\n",
       "  'Voda stereoisomer of Før',\n",
       "  'Før Stereoisomer von Voda',\n",
       "  'Nils stereoisomer of Film',\n",
       "  'Film stereoisomer of Nils',\n",
       "  'Nils Stereoisomer von Film',\n",
       "  'Orleans stereoisomer of Milano',\n",
       "  'Milano stereoisomer of Orleans',\n",
       "  'Orleans Stereoisomer von Milano',\n",
       "  'Drôme stereoisomer of Color',\n",
       "  'Color stereoisomer of Drôme',\n",
       "  'Drôme Stereoisomer von Color',\n",
       "  'Kirchner stereoisomer of Bears',\n",
       "  'Bears stereoisomer of Kirchner',\n",
       "  'Kirchner Stereoisomer von Bears',\n",
       "  'Henley stereoisomer of Alger',\n",
       "  'Alger stereoisomer of Henley',\n",
       "  'Henley Stereoisomer von Alger',\n",
       "  'ATV stereoisomer of Stevens',\n",
       "  'Stevens stereoisomer of ATV',\n",
       "  'ATV Stereoisomer von Stevens',\n",
       "  'NYC stereoisomer of Fredrik',\n",
       "  'Fredrik stereoisomer of NYC',\n",
       "  'NYC Stereoisomer von Fredrik',\n",
       "  'Oviedo stereoisomer of Years',\n",
       "  'Years stereoisomer of Oviedo',\n",
       "  'Oviedo Stereoisomer von Years',\n",
       "  'Karlsson stereoisomer of School',\n",
       "  'School stereoisomer of Karlsson',\n",
       "  'Karlsson Stereoisomer von School',\n",
       "  'Margareta stereoisomer of Natura',\n",
       "  'Natura stereoisomer of Margareta',\n",
       "  'Margareta Stereoisomer von Natura',\n",
       "  'Condor stereoisomer of Casablanca',\n",
       "  'Casablanca stereoisomer of Condor',\n",
       "  'Condor Stereoisomer von Casablanca',\n",
       "  'Toledo stereoisomer of Ronnie',\n",
       "  'Ronnie stereoisomer of Toledo',\n",
       "  'Toledo Stereoisomer von Ronnie',\n",
       "  'Neptune stereoisomer of Monts',\n",
       "  'Monts stereoisomer of Neptune',\n",
       "  'Neptune Stereoisomer von Monts',\n",
       "  'Yale stereoisomer of Fed',\n",
       "  'Fed stereoisomer of Yale',\n",
       "  'Yale Stereoisomer von Fed',\n",
       "  'Neville stereoisomer of Bonus',\n",
       "  'Bonus stereoisomer of Neville',\n",
       "  'Neville Stereoisomer von Bonus',\n",
       "  'Garden stereoisomer of Trung',\n",
       "  'Trung stereoisomer of Garden',\n",
       "  'Garden Stereoisomer von Trung',\n",
       "  'Junior stereoisomer of Gerd',\n",
       "  'Gerd stereoisomer of Junior',\n",
       "  'Junior Stereoisomer von Gerd',\n",
       "  'Dietrich stereoisomer of MX',\n",
       "  'MX stereoisomer of Dietrich',\n",
       "  'Dietrich Stereoisomer von MX',\n",
       "  'Chi stereoisomer of Lauren',\n",
       "  'Lauren stereoisomer of Chi',\n",
       "  'Chi Stereoisomer von Lauren',\n",
       "  'Ronaldo stereoisomer of Amiens',\n",
       "  'Amiens stereoisomer of Ronaldo',\n",
       "  'Ronaldo Stereoisomer von Amiens',\n",
       "  'di stereoisomer of Norsk',\n",
       "  'Norsk stereoisomer of di',\n",
       "  'di Stereoisomer von Norsk',\n",
       "  'Tokyo stereoisomer of Bryant',\n",
       "  'Bryant stereoisomer of Tokyo',\n",
       "  'Tokyo Stereoisomer von Bryant',\n",
       "  'Turing stereoisomer of Bulan',\n",
       "  'Bulan stereoisomer of Turing',\n",
       "  'Turing Stereoisomer von Bulan',\n",
       "  'Plateau stereoisomer of Arms',\n",
       "  'Arms stereoisomer of Plateau',\n",
       "  'Plateau Stereoisomer von Arms',\n",
       "  'Barth stereoisomer of Idol',\n",
       "  'Idol stereoisomer of Barth',\n",
       "  'Barth Stereoisomer von Idol',\n",
       "  'Bellini stereoisomer of Pablo',\n",
       "  'Pablo stereoisomer of Bellini',\n",
       "  'Bellini Stereoisomer von Pablo',\n",
       "  'Re stereoisomer of Bretagne',\n",
       "  'Bretagne stereoisomer of Re',\n",
       "  'Re Stereoisomer von Bretagne',\n",
       "  'Jenna stereoisomer of Irena',\n",
       "  'Irena stereoisomer of Jenna',\n",
       "  'Jenna Stereoisomer von Irena',\n",
       "  'Italy stereoisomer of Berge',\n",
       "  'Berge stereoisomer of Italy',\n",
       "  'Italy Stereoisomer von Berge',\n",
       "  'Falkland stereoisomer of Aires',\n",
       "  'Aires stereoisomer of Falkland',\n",
       "  'Falkland Stereoisomer von Aires',\n",
       "  'Kemp stereoisomer of Saksa',\n",
       "  'Saksa stereoisomer of Kemp',\n",
       "  'Kemp Stereoisomer von Saksa',\n",
       "  'Día stereoisomer of Transfer',\n",
       "  'Transfer stereoisomer of Día',\n",
       "  'Día Stereoisomer von Transfer',\n",
       "  'Frederick stereoisomer of RPG',\n",
       "  'RPG stereoisomer of Frederick',\n",
       "  'Frederick Stereoisomer von RPG',\n",
       "  'Marco stereoisomer of Kosmos',\n",
       "  'Kosmos stereoisomer of Marco',\n",
       "  'Marco Stereoisomer von Kosmos',\n",
       "  'Zoo stereoisomer of Ocak',\n",
       "  'Ocak stereoisomer of Zoo',\n",
       "  'Zoo Stereoisomer von Ocak',\n",
       "  'Osman stereoisomer of KS',\n",
       "  'KS stereoisomer of Osman',\n",
       "  'Osman Stereoisomer von KS',\n",
       "  'Camera stereoisomer of Vega',\n",
       "  'Vega stereoisomer of Camera',\n",
       "  'Camera Stereoisomer von Vega',\n",
       "  'Randall stereoisomer of Princesa',\n",
       "  'Princesa stereoisomer of Randall',\n",
       "  'Randall Stereoisomer von Princesa',\n",
       "  'Berna stereoisomer of Dundee',\n",
       "  'Dundee stereoisomer of Berna',\n",
       "  'Berna Stereoisomer von Dundee',\n",
       "  'Loch stereoisomer of Canadian',\n",
       "  'Canadian stereoisomer of Loch',\n",
       "  'Loch Stereoisomer von Canadian',\n",
       "  'Cook stereoisomer of Montréal',\n",
       "  'Montréal stereoisomer of Cook',\n",
       "  'Cook Stereoisomer von Montréal',\n",
       "  'Sweeney stereoisomer of Ter',\n",
       "  'Ter stereoisomer of Sweeney',\n",
       "  'Sweeney Stereoisomer von Ter',\n",
       "  'Yucatán stereoisomer of Appleton',\n",
       "  'Appleton stereoisomer of Yucatán',\n",
       "  'Yucatán Stereoisomer von Appleton',\n",
       "  'Alessandro stereoisomer of Cello',\n",
       "  'Cello stereoisomer of Alessandro',\n",
       "  'Alessandro Stereoisomer von Cello',\n",
       "  'Harper stereoisomer of Gromada',\n",
       "  'Gromada stereoisomer of Harper',\n",
       "  'Harper Stereoisomer von Gromada',\n",
       "  'Fallen stereoisomer of Mesir',\n",
       "  'Mesir stereoisomer of Fallen',\n",
       "  'Fallen Stereoisomer von Mesir',\n",
       "  'Mivel stereoisomer of Westen',\n",
       "  'Westen stereoisomer of Mivel',\n",
       "  'Mivel Stereoisomer von Westen',\n",
       "  'Ringo stereoisomer of Uri',\n",
       "  'Uri stereoisomer of Ringo',\n",
       "  'Ringo Stereoisomer von Uri',\n",
       "  'Liam stereoisomer of Grad',\n",
       "  'Grad stereoisomer of Liam',\n",
       "  'Liam Stereoisomer von Grad',\n",
       "  'UA stereoisomer of Katrina',\n",
       "  'Katrina stereoisomer of UA',\n",
       "  'UA Stereoisomer von Katrina',\n",
       "  'Carl stereoisomer of Dakar',\n",
       "  'Dakar stereoisomer of Carl',\n",
       "  'Carl Stereoisomer von Dakar',\n",
       "  'Tanjung stereoisomer of Velvet',\n",
       "  'Velvet stereoisomer of Tanjung',\n",
       "  'Tanjung Stereoisomer von Velvet',\n",
       "  'Ritual stereoisomer of at',\n",
       "  'at stereoisomer of Ritual',\n",
       "  'Ritual Stereoisomer von at',\n",
       "  'Jill stereoisomer of Alben',\n",
       "  'Alben stereoisomer of Jill',\n",
       "  'Jill Stereoisomer von Alben',\n",
       "  'Cindy stereoisomer of Minsk',\n",
       "  'Minsk stereoisomer of Cindy',\n",
       "  'Cindy Stereoisomer von Minsk',\n",
       "  'Goddard stereoisomer of Boxing',\n",
       "  'Boxing stereoisomer of Goddard',\n",
       "  'Goddard Stereoisomer von Boxing',\n",
       "  'Destiny stereoisomer of Madre',\n",
       "  'Madre stereoisomer of Destiny',\n",
       "  'Destiny Stereoisomer von Madre',\n",
       "  'Ronde stereoisomer of ESA',\n",
       "  'ESA stereoisomer of Ronde',\n",
       "  'Ronde Stereoisomer von ESA',\n",
       "  'Zion stereoisomer of Archives',\n",
       "  'Archives stereoisomer of Zion',\n",
       "  'Zion Stereoisomer von Archives',\n",
       "  'Sera stereoisomer of Farm',\n",
       "  'Farm stereoisomer of Sera',\n",
       "  'Sera Stereoisomer von Farm',\n",
       "  'Président stereoisomer of Vid',\n",
       "  'Vid stereoisomer of Président',\n",
       "  'Président Stereoisomer von Vid',\n",
       "  'CH stereoisomer of Hayden',\n",
       "  'Hayden stereoisomer of CH',\n",
       "  'CH Stereoisomer von Hayden',\n",
       "  'Side stereoisomer of Fuller',\n",
       "  'Fuller stereoisomer of Side',\n",
       "  'Side Stereoisomer von Fuller',\n",
       "  'Robaina stereoisomer of Berlin',\n",
       "  'Berlin stereoisomer of Robaina',\n",
       "  'Robaina Stereoisomer von Berlin',\n",
       "  'Barnes stereoisomer of Leslie',\n",
       "  'Leslie stereoisomer of Barnes',\n",
       "  'Barnes Stereoisomer von Leslie',\n",
       "  'Lori stereoisomer of Salta',\n",
       "  'Salta stereoisomer of Lori',\n",
       "  'Lori Stereoisomer von Salta',\n",
       "  'Hanover stereoisomer of Dmitri',\n",
       "  'Dmitri stereoisomer of Hanover',\n",
       "  'Hanover Stereoisomer von Dmitri',\n",
       "  'Selle stereoisomer of Swan',\n",
       "  'Swan stereoisomer of Selle',\n",
       "  'Selle Stereoisomer von Swan',\n",
       "  'Canada stereoisomer of Pax',\n",
       "  'Pax stereoisomer of Canada',\n",
       "  'Canada Stereoisomer von Pax',\n",
       "  'Yorker stereoisomer of Opus',\n",
       "  'Opus stereoisomer of Yorker',\n",
       "  'Yorker Stereoisomer von Opus',\n",
       "  'Hockey stereoisomer of Madeleine',\n",
       "  'Madeleine stereoisomer of Hockey',\n",
       "  'Hockey Stereoisomer von Madeleine',\n",
       "  'CD stereoisomer of Odin',\n",
       "  'Odin stereoisomer of CD',\n",
       "  'CD Stereoisomer von Odin',\n",
       "  'Barnard stereoisomer of Webber',\n",
       "  'Webber stereoisomer of Barnard',\n",
       "  'Barnard Stereoisomer von Webber',\n",
       "  'Brabant stereoisomer of Akademie',\n",
       "  'Akademie stereoisomer of Brabant',\n",
       "  'Brabant Stereoisomer von Akademie',\n",
       "  'Middleton stereoisomer of Ungern',\n",
       "  'Ungern stereoisomer of Middleton',\n",
       "  'Middleton Stereoisomer von Ungern',\n",
       "  'Anadolu stereoisomer of Damit',\n",
       "  'Damit stereoisomer of Anadolu',\n",
       "  'Anadolu Stereoisomer von Damit',\n",
       "  'Sartre stereoisomer of Ariane',\n",
       "  'Ariane stereoisomer of Sartre',\n",
       "  'Sartre Stereoisomer von Ariane',\n",
       "  'Heide stereoisomer of extremo',\n",
       "  'extremo stereoisomer of Heide',\n",
       "  'Heide Stereoisomer von extremo',\n",
       "  'Third stereoisomer of Ebben',\n",
       "  'Ebben stereoisomer of Third',\n",
       "  'Third Stereoisomer von Ebben',\n",
       "  'Jesu stereoisomer of Schrift',\n",
       "  'Schrift stereoisomer of Jesu',\n",
       "  'Jesu Stereoisomer von Schrift',\n",
       "  'Mickey stereoisomer of Karya',\n",
       "  'Karya stereoisomer of Mickey',\n",
       "  'Mickey Stereoisomer von Karya',\n",
       "  'Words stereoisomer of Víctor',\n",
       "  'Víctor stereoisomer of Words',\n",
       "  'Words Stereoisomer von Víctor',\n",
       "  'Writer stereoisomer of Ex',\n",
       "  'Ex stereoisomer of Writer',\n",
       "  'Writer Stereoisomer von Ex',\n",
       "  'Weston stereoisomer of Maranhão',\n",
       "  'Maranhão stereoisomer of Weston',\n",
       "  'Weston Stereoisomer von Maranhão',\n",
       "  'Janne stereoisomer of Partia',\n",
       "  'Partia stereoisomer of Janne',\n",
       "  'Janne Stereoisomer von Partia',\n",
       "  'Missa stereoisomer of Franse',\n",
       "  'Franse stereoisomer of Missa',\n",
       "  'Missa Stereoisomer von Franse',\n",
       "  'Resurrection stereoisomer of Roads',\n",
       "  'Roads stereoisomer of Resurrection',\n",
       "  'Resurrection Stereoisomer von Roads',\n",
       "  'Blog stereoisomer of Theatre',\n",
       "  'Theatre stereoisomer of Blog',\n",
       "  'Blog Stereoisomer von Theatre',\n",
       "  'Magna stereoisomer of Soleil',\n",
       "  'Soleil stereoisomer of Magna',\n",
       "  'Magna Stereoisomer von Soleil',\n",
       "  'Chaco stereoisomer of Break',\n",
       "  'Break stereoisomer of Chaco',\n",
       "  'Chaco Stereoisomer von Break',\n",
       "  'Indiana stereoisomer of Karma',\n",
       "  'Karma stereoisomer of Indiana',\n",
       "  'Indiana Stereoisomer von Karma',\n",
       "  'Davida stereoisomer of Germania',\n",
       "  'Germania stereoisomer of Davida',\n",
       "  'Davida Stereoisomer von Germania',\n",
       "  'Butte stereoisomer of Faith',\n",
       "  'Faith stereoisomer of Butte',\n",
       "  'Butte Stereoisomer von Faith',\n",
       "  'Majesty stereoisomer of Hus',\n",
       "  'Hus stereoisomer of Majesty',\n",
       "  'Majesty Stereoisomer von Hus',\n",
       "  'Pre stereoisomer of Gutenberg',\n",
       "  'Gutenberg stereoisomer of Pre',\n",
       "  'Pre Stereoisomer von Gutenberg',\n",
       "  'Ante stereoisomer of Justin',\n",
       "  'Justin stereoisomer of Ante',\n",
       "  'Ante Stereoisomer von Justin',\n",
       "  'Prensa stereoisomer of Bullet',\n",
       "  'Bullet stereoisomer of Prensa',\n",
       "  'Prensa Stereoisomer von Bullet',\n",
       "  'SS stereoisomer of Pioneer',\n",
       "  'Pioneer stereoisomer of SS',\n",
       "  'SS Stereoisomer von Pioneer',\n",
       "  'Okinawa stereoisomer of Murcia',\n",
       "  'Murcia stereoisomer of Okinawa',\n",
       "  'Okinawa Stereoisomer von Murcia',\n",
       "  'Studie stereoisomer of Abi',\n",
       "  'Abi stereoisomer of Studie',\n",
       "  'Studie Stereoisomer von Abi',\n",
       "  'Cosmos stereoisomer of Moldova',\n",
       "  'Moldova stereoisomer of Cosmos',\n",
       "  'Cosmos Stereoisomer von Moldova',\n",
       "  'Underwood stereoisomer of Thiên',\n",
       "  'Thiên stereoisomer of Underwood',\n",
       "  'Underwood Stereoisomer von Thiên',\n",
       "  'Gemini stereoisomer of Bulgaria',\n",
       "  'Bulgaria stereoisomer of Gemini',\n",
       "  'Gemini Stereoisomer von Bulgaria',\n",
       "  'George stereoisomer of Monate',\n",
       "  'Monate stereoisomer of George',\n",
       "  'George Stereoisomer von Monate',\n",
       "  'Faye stereoisomer of Island',\n",
       "  'Island stereoisomer of Faye',\n",
       "  'Faye Stereoisomer von Island',\n",
       "  'Cádiz stereoisomer of Boyle',\n",
       "  'Boyle stereoisomer of Cádiz',\n",
       "  'Cádiz Stereoisomer von Boyle',\n",
       "  'Filadelfia stereoisomer of Aner',\n",
       "  'Aner stereoisomer of Filadelfia',\n",
       "  'Filadelfia Stereoisomer von Aner',\n",
       "  'NL stereoisomer of Parque',\n",
       "  'Parque stereoisomer of NL',\n",
       "  'NL Stereoisomer von Parque',\n",
       "  'Strange stereoisomer of Vir',\n",
       "  'Vir stereoisomer of Strange',\n",
       "  'Strange Stereoisomer von Vir',\n",
       "  'Otis stereoisomer of Ying',\n",
       "  'Ying stereoisomer of Otis',\n",
       "  'Otis Stereoisomer von Ying',\n",
       "  'Union stereoisomer of Lennon',\n",
       "  'Lennon stereoisomer of Union',\n",
       "  'Union Stereoisomer von Lennon',\n",
       "  'Czech stereoisomer of Kuala',\n",
       "  'Kuala stereoisomer of Czech',\n",
       "  'Czech Stereoisomer von Kuala',\n",
       "  'Atatürk stereoisomer of Orten',\n",
       "  'Orten stereoisomer of Atatürk',\n",
       "  'Atatürk Stereoisomer von Orten',\n",
       "  'Llwyd stereoisomer of Ulysses',\n",
       "  'Ulysses stereoisomer of Llwyd',\n",
       "  'Llwyd Stereoisomer von Ulysses',\n",
       "  'Lotto stereoisomer of Felix',\n",
       "  'Felix stereoisomer of Lotto',\n",
       "  'Lotto Stereoisomer von Felix',\n",
       "  'Lucky stereoisomer of Cesar',\n",
       "  'Cesar stereoisomer of Lucky',\n",
       "  'Lucky Stereoisomer von Cesar',\n",
       "  'Columbia stereoisomer of Roses',\n",
       "  'Roses stereoisomer of Columbia',\n",
       "  'Columbia Stereoisomer von Roses',\n",
       "  'Cathedral stereoisomer of Stick',\n",
       "  'Stick stereoisomer of Cathedral',\n",
       "  'Cathedral Stereoisomer von Stick',\n",
       "  'File stereoisomer of Brooks',\n",
       "  'Brooks stereoisomer of File',\n",
       "  'File Stereoisomer von Brooks',\n",
       "  'Schalke stereoisomer of Philippe',\n",
       "  'Philippe stereoisomer of Schalke',\n",
       "  'Schalke Stereoisomer von Philippe',\n",
       "  'Goa stereoisomer of Namur',\n",
       "  'Namur stereoisomer of Goa',\n",
       "  'Goa Stereoisomer von Namur',\n",
       "  'Bartlett stereoisomer of Triumph',\n",
       "  'Triumph stereoisomer of Bartlett',\n",
       "  'Bartlett Stereoisomer von Triumph',\n",
       "  'Naomi stereoisomer of Birds',\n",
       "  'Birds stereoisomer of Naomi',\n",
       "  'Naomi Stereoisomer von Birds',\n",
       "  'Panda stereoisomer of Server',\n",
       "  'Server stereoisomer of Panda',\n",
       "  'Panda Stereoisomer von Server',\n",
       "  'America stereoisomer of Ina',\n",
       "  'Ina stereoisomer of America',\n",
       "  'America Stereoisomer von Ina',\n",
       "  'Early stereoisomer of Hanson',\n",
       "  'Hanson stereoisomer of Early',\n",
       "  'Early Stereoisomer von Hanson',\n",
       "  'Western stereoisomer of Gotland',\n",
       "  'Gotland stereoisomer of Western',\n",
       "  'Western Stereoisomer von Gotland',\n",
       "  'Dracula stereoisomer of Juárez',\n",
       "  'Juárez stereoisomer of Dracula',\n",
       "  'Dracula Stereoisomer von Juárez',\n",
       "  'Cours stereoisomer of Tale',\n",
       "  'Tale stereoisomer of Cours',\n",
       "  'Cours Stereoisomer von Tale',\n",
       "  'Stay stereoisomer of Mayotte',\n",
       "  'Mayotte stereoisomer of Stay',\n",
       "  'Stay Stereoisomer von Mayotte',\n",
       "  'Essay stereoisomer of Kane',\n",
       "  'Kane stereoisomer of Essay',\n",
       "  'Essay Stereoisomer von Kane',\n",
       "  'Hum stereoisomer of Encore',\n",
       "  'Encore stereoisomer of Hum',\n",
       "  'Hum Stereoisomer von Encore',\n",
       "  'Rouge stereoisomer of Whitman',\n",
       "  'Whitman stereoisomer of Rouge',\n",
       "  'Rouge Stereoisomer von Whitman',\n",
       "  'Katz stereoisomer of Rossi',\n",
       "  'Rossi stereoisomer of Katz',\n",
       "  'Katz Stereoisomer von Rossi',\n",
       "  'Poison stereoisomer of Aku',\n",
       "  'Aku stereoisomer of Poison',\n",
       "  'Poison Stereoisomer von Aku',\n",
       "  'Ramiro stereoisomer of Sicily',\n",
       "  'Sicily stereoisomer of Ramiro',\n",
       "  'Ramiro Stereoisomer von Sicily',\n",
       "  'Fuel stereoisomer of Hamm',\n",
       "  'Hamm stereoisomer of Fuel',\n",
       "  'Fuel Stereoisomer von Hamm',\n",
       "  'Eclipse stereoisomer of Missouri',\n",
       "  'Missouri stereoisomer of Eclipse',\n",
       "  'Eclipse Stereoisomer von Missouri',\n",
       "  'Bouchet stereoisomer of Stratford',\n",
       "  'Stratford stereoisomer of Bouchet',\n",
       "  'Bouchet Stereoisomer von Stratford',\n",
       "  'Chamberlain stereoisomer of Meisterschaft',\n",
       "  'Meisterschaft stereoisomer of Chamberlain',\n",
       "  'Chamberlain Stereoisomer von Meisterschaft',\n",
       "  'Canon stereoisomer of Ellington',\n",
       "  'Ellington stereoisomer of Canon',\n",
       "  'Canon Stereoisomer von Ellington',\n",
       "  'Dunia stereoisomer of Oasis',\n",
       "  'Oasis stereoisomer of Dunia',\n",
       "  'Dunia Stereoisomer von Oasis',\n",
       "  'Vienna stereoisomer of Savoie',\n",
       "  'Savoie stereoisomer of Vienna',\n",
       "  'Vienna Stereoisomer von Savoie',\n",
       "  'Orden stereoisomer of Science',\n",
       "  'Science stereoisomer of Orden',\n",
       "  'Orden Stereoisomer von Science',\n",
       "  'Order stereoisomer of Investment',\n",
       "  'Investment stereoisomer of Order',\n",
       "  'Order Stereoisomer von Investment',\n",
       "  'Dentro stereoisomer of Mutter',\n",
       "  'Mutter stereoisomer of Dentro',\n",
       "  'Dentro Stereoisomer von Mutter',\n",
       "  'Pretoria stereoisomer of VP',\n",
       "  'VP stereoisomer of Pretoria',\n",
       "  'Pretoria Stereoisomer von VP',\n",
       "  'Chantal stereoisomer of Valdés',\n",
       "  'Valdés stereoisomer of Chantal',\n",
       "  'Chantal Stereoisomer von Valdés',\n",
       "  'Perl stereoisomer of Roca',\n",
       "  'Roca stereoisomer of Perl',\n",
       "  'Perl Stereoisomer von Roca',\n",
       "  'Vegas stereoisomer of VI',\n",
       "  'VI stereoisomer of Vegas',\n",
       "  'Vegas Stereoisomer von VI',\n",
       "  'AJ stereoisomer of Reggie',\n",
       "  'Reggie stereoisomer of AJ',\n",
       "  'AJ Stereoisomer von Reggie',\n",
       "  'Council stereoisomer of Finch',\n",
       "  'Finch stereoisomer of Council',\n",
       "  'Council Stereoisomer von Finch',\n",
       "  'Vázquez stereoisomer of Deer',\n",
       "  'Deer stereoisomer of Vázquez',\n",
       "  'Vázquez Stereoisomer von Deer',\n",
       "  'Terminal stereoisomer of Genel',\n",
       "  'Genel stereoisomer of Terminal',\n",
       "  'Terminal Stereoisomer von Genel',\n",
       "  'Superliga stereoisomer of FL',\n",
       "  'FL stereoisomer of Superliga',\n",
       "  'Superliga Stereoisomer von FL',\n",
       "  'Pool stereoisomer of Benin',\n",
       "  'Benin stereoisomer of Pool',\n",
       "  'Pool Stereoisomer von Benin',\n",
       "  'Parigi stereoisomer of Tolosa',\n",
       "  'Tolosa stereoisomer of Parigi',\n",
       "  'Parigi Stereoisomer von Tolosa',\n",
       "  'Ses stereoisomer of II',\n",
       "  'II stereoisomer of Ses',\n",
       "  'Ses Stereoisomer von II',\n",
       "  'Laos stereoisomer of Buna',\n",
       "  'Buna stereoisomer of Laos',\n",
       "  'Laos Stereoisomer von Buna',\n",
       "  'Gesù stereoisomer of Alfonso',\n",
       "  'Alfonso stereoisomer of Gesù',\n",
       "  'Gesù Stereoisomer von Alfonso',\n",
       "  'Kort stereoisomer of Hugo',\n",
       "  'Hugo stereoisomer of Kort',\n",
       "  'Kort Stereoisomer von Hugo',\n",
       "  'Niagara stereoisomer of Bray',\n",
       "  'Bray stereoisomer of Niagara',\n",
       "  'Niagara Stereoisomer von Bray',\n",
       "  'Omer stereoisomer of Magazin',\n",
       "  'Magazin stereoisomer of Omer',\n",
       "  'Omer Stereoisomer von Magazin',\n",
       "  'Jungen stereoisomer of Indie',\n",
       "  'Indie stereoisomer of Jungen',\n",
       "  'Jungen Stereoisomer von Indie',\n",
       "  'Georg stereoisomer of stb',\n",
       "  'stb stereoisomer of Georg',\n",
       "  'Georg Stereoisomer von stb',\n",
       "  'Ela stereoisomer of Remixes',\n",
       "  'Remixes stereoisomer of Ela',\n",
       "  'Ela Stereoisomer von Remixes',\n",
       "  'Spring stereoisomer of Toten',\n",
       "  'Toten stereoisomer of Spring',\n",
       "  'Spring Stereoisomer von Toten',\n",
       "  'Warren stereoisomer of Siena',\n",
       "  'Siena stereoisomer of Warren',\n",
       "  'Warren Stereoisomer von Siena',\n",
       "  'Fever stereoisomer of Fraser',\n",
       "  'Fraser stereoisomer of Fever',\n",
       "  'Fever Stereoisomer von Fraser',\n",
       "  'Music stereoisomer of Daniels',\n",
       "  'Daniels stereoisomer of Music',\n",
       "  'Music Stereoisomer von Daniels',\n",
       "  'Nations stereoisomer of Jozef',\n",
       "  'Jozef stereoisomer of Nations',\n",
       "  'Nations Stereoisomer von Jozef',\n",
       "  'Sit stereoisomer of Cornell',\n",
       "  'Cornell stereoisomer of Sit',\n",
       "  'Sit Stereoisomer von Cornell',\n",
       "  'Madison stereoisomer of Evil',\n",
       "  'Evil stereoisomer of Madison',\n",
       "  'Madison Stereoisomer von Evil',\n",
       "  'Potok stereoisomer of Stockholm',\n",
       "  'Stockholm stereoisomer of Potok',\n",
       "  'Potok Stereoisomer von Stockholm',\n",
       "  'UE stereoisomer of Bangor',\n",
       "  'Bangor stereoisomer of UE',\n",
       "  'UE Stereoisomer von Bangor',\n",
       "  'Guns stereoisomer of Silla',\n",
       "  'Silla stereoisomer of Guns',\n",
       "  'Guns Stereoisomer von Silla',\n",
       "  'Jesús stereoisomer of Stream',\n",
       "  'Stream stereoisomer of Jesús',\n",
       "  'Jesús Stereoisomer von Stream',\n",
       "  'Foucault stereoisomer of Application',\n",
       "  'Application stereoisomer of Foucault',\n",
       "  'Foucault Stereoisomer von Application',\n",
       "  'Amber stereoisomer of Josep',\n",
       "  'Josep stereoisomer of Amber',\n",
       "  'Amber Stereoisomer von Josep',\n",
       "  'Fonds stereoisomer of Wes',\n",
       "  'Wes stereoisomer of Fonds',\n",
       "  'Fonds Stereoisomer von Wes',\n",
       "  'Né stereoisomer of Playboy',\n",
       "  'Playboy stereoisomer of Né',\n",
       "  'Né Stereoisomer von Playboy',\n",
       "  'Kral stereoisomer of Lau',\n",
       "  'Lau stereoisomer of Kral',\n",
       "  'Kral Stereoisomer von Lau',\n",
       "  'fann stereoisomer of Powers',\n",
       "  'Powers stereoisomer of fann',\n",
       "  'fann Stereoisomer von Powers',\n",
       "  'Yorkshire stereoisomer of CA',\n",
       "  'CA stereoisomer of Yorkshire',\n",
       "  'Yorkshire Stereoisomer von CA',\n",
       "  'Slam stereoisomer of Moreira',\n",
       "  'Moreira stereoisomer of Slam',\n",
       "  'Slam Stereoisomer von Moreira',\n",
       "  'Benfica stereoisomer of IV',\n",
       "  'IV stereoisomer of Benfica',\n",
       "  'Benfica Stereoisomer von IV',\n",
       "  'Daniel stereoisomer of Schweizer',\n",
       "  'Schweizer stereoisomer of Daniel',\n",
       "  'Daniel Stereoisomer von Schweizer',\n",
       "  'Alicia stereoisomer of Buenos',\n",
       "  'Buenos stereoisomer of Alicia',\n",
       "  'Alicia Stereoisomer von Buenos',\n",
       "  'Bellamy stereoisomer of Brod',\n",
       "  'Brod stereoisomer of Bellamy',\n",
       "  'Bellamy Stereoisomer von Brod',\n",
       "  'Bodø stereoisomer of Sigurd',\n",
       "  'Sigurd stereoisomer of Bodø',\n",
       "  'Bodø Stereoisomer von Sigurd',\n",
       "  'Verona stereoisomer of RAF',\n",
       "  'RAF stereoisomer of Verona',\n",
       "  'Verona Stereoisomer von RAF',\n",
       "  'Epic stereoisomer of asa',\n",
       "  'asa stereoisomer of Epic',\n",
       "  'Epic Stereoisomer von asa',\n",
       "  'Nacional stereoisomer of Seu',\n",
       "  'Seu stereoisomer of Nacional',\n",
       "  'Nacional Stereoisomer von Seu',\n",
       "  'Venecia stereoisomer of Scala',\n",
       "  'Scala stereoisomer of Venecia',\n",
       "  'Venecia Stereoisomer von Scala',\n",
       "  'Korps stereoisomer of Carpenter',\n",
       "  'Carpenter stereoisomer of Korps',\n",
       "  'Korps Stereoisomer von Carpenter',\n",
       "  'Colección stereoisomer of Bethlehem',\n",
       "  'Bethlehem stereoisomer of Colección',\n",
       "  'Colección Stereoisomer von Bethlehem',\n",
       "  'Kleiner stereoisomer of Shire',\n",
       "  'Shire stereoisomer of Kleiner',\n",
       "  'Kleiner Stereoisomer von Shire',\n",
       "  'Valentin stereoisomer of Clin',\n",
       "  'Clin stereoisomer of Valentin',\n",
       "  'Valentin Stereoisomer von Clin',\n",
       "  'Isles stereoisomer of Dell',\n",
       "  'Dell stereoisomer of Isles',\n",
       "  'Isles Stereoisomer von Dell',\n",
       "  'Moran stereoisomer of Palmas',\n",
       "  'Palmas stereoisomer of Moran',\n",
       "  'Moran Stereoisomer von Palmas',\n",
       "  'Gattung stereoisomer of TB',\n",
       "  'TB stereoisomer of Gattung',\n",
       "  'Gattung Stereoisomer von TB',\n",
       "  'Attack stereoisomer of Beatrice',\n",
       "  'Beatrice stereoisomer of Attack',\n",
       "  'Attack Stereoisomer von Beatrice',\n",
       "  'Primer stereoisomer of Jang',\n",
       "  'Jang stereoisomer of Primer',\n",
       "  'Primer Stereoisomer von Jang',\n",
       "  'Pink stereoisomer of USS',\n",
       "  'USS stereoisomer of Pink',\n",
       "  'Pink Stereoisomer von USS',\n",
       "  'Reading stereoisomer of Antonio',\n",
       "  'Antonio stereoisomer of Reading',\n",
       "  'Reading Stereoisomer von Antonio',\n",
       "  'Root stereoisomer of Ferns',\n",
       "  'Ferns stereoisomer of Root',\n",
       "  'Root Stereoisomer von Ferns',\n",
       "  'Meuse stereoisomer of Export',\n",
       "  'Export stereoisomer of Meuse',\n",
       "  'Meuse Stereoisomer von Export',\n",
       "  'Estudiantes stereoisomer of Kern',\n",
       "  'Kern stereoisomer of Estudiantes',\n",
       "  'Estudiantes Stereoisomer von Kern',\n",
       "  'sy stereoisomer of Orléans',\n",
       "  'Orléans stereoisomer of sy',\n",
       "  'sy Stereoisomer von Orléans',\n",
       "  'Mac stereoisomer of Witte',\n",
       "  'Witte stereoisomer of Mac',\n",
       "  'Mac Stereoisomer von Witte',\n",
       "  'Krzysztof stereoisomer of Ducks',\n",
       "  'Ducks stereoisomer of Krzysztof',\n",
       "  'Krzysztof Stereoisomer von Ducks',\n",
       "  'Newport stereoisomer of Shoes',\n",
       "  'Shoes stereoisomer of Newport',\n",
       "  'Newport Stereoisomer von Shoes',\n",
       "  'Grégoire stereoisomer of Morris',\n",
       "  'Morris stereoisomer of Grégoire',\n",
       "  'Grégoire Stereoisomer von Morris',\n",
       "  'Fils stereoisomer of Hunter',\n",
       "  'Hunter stereoisomer of Fils',\n",
       "  'Fils Stereoisomer von Hunter',\n",
       "  'Jepang stereoisomer of Mundi',\n",
       "  'Mundi stereoisomer of Jepang',\n",
       "  'Jepang Stereoisomer von Mundi',\n",
       "  'Seigneur stereoisomer of Anderson',\n",
       "  'Anderson stereoisomer of Seigneur',\n",
       "  'Seigneur Stereoisomer von Anderson',\n",
       "  'Chili stereoisomer of Mens',\n",
       "  'Mens stereoisomer of Chili',\n",
       "  'Chili Stereoisomer von Mens',\n",
       "  'Charleston stereoisomer of Browns',\n",
       "  'Browns stereoisomer of Charleston',\n",
       "  'Charleston Stereoisomer von Browns',\n",
       "  'Poola stereoisomer of Nell',\n",
       "  'Nell stereoisomer of Poola',\n",
       "  'Poola Stereoisomer von Nell',\n",
       "  'Amelia stereoisomer of Via',\n",
       "  'Via stereoisomer of Amelia',\n",
       "  'Amelia Stereoisomer von Via',\n",
       "  'Avalon stereoisomer of Santuario',\n",
       "  'Santuario stereoisomer of Avalon',\n",
       "  'Avalon Stereoisomer von Santuario',\n",
       "  'Rebel stereoisomer of Buster',\n",
       "  'Buster stereoisomer of Rebel',\n",
       "  'Rebel Stereoisomer von Buster',\n",
       "  'Rode stereoisomer of Rico',\n",
       "  'Rico stereoisomer of Rode',\n",
       "  'Rode Stereoisomer von Rico',\n",
       "  'Jess stereoisomer of Masa',\n",
       "  'Masa stereoisomer of Jess',\n",
       "  'Jess Stereoisomer von Masa',\n",
       "  'Caledonia stereoisomer of Garda',\n",
       "  'Garda stereoisomer of Caledonia',\n",
       "  'Caledonia Stereoisomer von Garda',\n",
       "  'Kelley stereoisomer of Ipswich',\n",
       "  'Ipswich stereoisomer of Kelley',\n",
       "  'Kelley Stereoisomer von Ipswich',\n",
       "  'Alabama stereoisomer of Bergman',\n",
       "  'Bergman stereoisomer of Alabama',\n",
       "  'Alabama Stereoisomer von Bergman',\n",
       "  'Newcastle stereoisomer of RJ',\n",
       "  'RJ stereoisomer of Newcastle',\n",
       "  'Newcastle Stereoisomer von RJ',\n",
       "  'Slave stereoisomer of Wanted',\n",
       "  'Wanted stereoisomer of Slave',\n",
       "  'Slave Stereoisomer von Wanted',\n",
       "  'Martini stereoisomer of Az',\n",
       "  'Az stereoisomer of Martini',\n",
       "  'Martini Stereoisomer von Az',\n",
       "  'Tigre stereoisomer of Fairfax',\n",
       "  'Fairfax stereoisomer of Tigre',\n",
       "  'Tigre Stereoisomer von Fairfax',\n",
       "  'Soria stereoisomer of Dora',\n",
       "  'Dora stereoisomer of Soria',\n",
       "  'Soria Stereoisomer von Dora',\n",
       "  'Iowa stereoisomer of CDC',\n",
       "  'CDC stereoisomer of Iowa',\n",
       "  'Iowa Stereoisomer von CDC',\n",
       "  'Gill stereoisomer of Yeni',\n",
       "  'Yeni stereoisomer of Gill',\n",
       "  'Gill Stereoisomer von Yeni',\n",
       "  'Sign stereoisomer of ESPN',\n",
       "  'ESPN stereoisomer of Sign',\n",
       "  'Sign Stereoisomer von ESPN',\n",
       "  'Klein stereoisomer of Stalingrad',\n",
       "  'Stalingrad stereoisomer of Klein',\n",
       "  'Klein Stereoisomer von Stalingrad',\n",
       "  'Steiner stereoisomer of Salto',\n",
       "  'Salto stereoisomer of Steiner',\n",
       "  'Steiner Stereoisomer von Salto',\n",
       "  'Mirage stereoisomer of Mater',\n",
       "  'Mater stereoisomer of Mirage',\n",
       "  'Mirage Stereoisomer von Mater',\n",
       "  'Klan stereoisomer of IOC',\n",
       "  'IOC stereoisomer of Klan',\n",
       "  'Klan Stereoisomer von IOC',\n",
       "  'Ayala stereoisomer of Belo',\n",
       "  'Belo stereoisomer of Ayala',\n",
       "  'Ayala Stereoisomer von Belo',\n",
       "  'Punjabi stereoisomer of Phou',\n",
       "  'Phou stereoisomer of Punjabi',\n",
       "  'Punjabi Stereoisomer von Phou',\n",
       "  'Dezember stereoisomer of Pal',\n",
       "  'Pal stereoisomer of Dezember',\n",
       "  'Dezember Stereoisomer von Pal',\n",
       "  'Jalan stereoisomer of Luzon',\n",
       "  'Luzon stereoisomer of Jalan',\n",
       "  'Jalan Stereoisomer von Luzon',\n",
       "  'McCoy stereoisomer of Rodrigues',\n",
       "  'Rodrigues stereoisomer of McCoy',\n",
       "  'McCoy Stereoisomer von Rodrigues',\n",
       "  'Minden stereoisomer of Killer',\n",
       "  'Killer stereoisomer of Minden',\n",
       "  'Minden Stereoisomer von Killer',\n",
       "  'Hughes stereoisomer of Campus',\n",
       "  'Campus stereoisomer of Hughes',\n",
       "  'Hughes Stereoisomer von Campus',\n",
       "  'Conceição stereoisomer of Po',\n",
       "  'Po stereoisomer of Conceição',\n",
       "  'Conceição Stereoisomer von Po',\n",
       "  'Euro stereoisomer of Gary',\n",
       "  'Gary stereoisomer of Euro',\n",
       "  'Euro Stereoisomer von Gary',\n",
       "  'Té stereoisomer of Caroline',\n",
       "  'Caroline stereoisomer of Té',\n",
       "  'Té Stereoisomer von Caroline',\n",
       "  'Wolfe stereoisomer of Premier',\n",
       "  'Premier stereoisomer of Wolfe',\n",
       "  'Wolfe Stereoisomer von Premier',\n",
       "  'ao stereoisomer of Goiás',\n",
       "  'Goiás stereoisomer of ao',\n",
       "  'ao Stereoisomer von Goiás',\n",
       "  'Zola stereoisomer of Garrison',\n",
       "  'Garrison stereoisomer of Zola',\n",
       "  'Zola Stereoisomer von Garrison',\n",
       "  'Rusland stereoisomer of Clive',\n",
       "  'Clive stereoisomer of Rusland',\n",
       "  'Rusland Stereoisomer von Clive',\n",
       "  'Bear stereoisomer of Klasse',\n",
       "  'Klasse stereoisomer of Bear',\n",
       "  'Bear Stereoisomer von Klasse',\n",
       "  'Moro stereoisomer of Bono',\n",
       "  'Bono stereoisomer of Moro',\n",
       "  'Moro Stereoisomer von Bono',\n",
       "  'Kata stereoisomer of Londra',\n",
       "  'Londra stereoisomer of Kata',\n",
       "  'Kata Stereoisomer von Londra',\n",
       "  'Prescott stereoisomer of Stock',\n",
       "  'Stock stereoisomer of Prescott',\n",
       "  'Prescott Stereoisomer von Stock',\n",
       "  'Atom stereoisomer of Montagne',\n",
       "  'Montagne stereoisomer of Atom',\n",
       "  'Atom Stereoisomer von Montagne',\n",
       "  'NET stereoisomer of Aa',\n",
       "  'Aa stereoisomer of NET',\n",
       "  'NET Stereoisomer von Aa',\n",
       "  'Ono stereoisomer of Vatican',\n",
       "  'Vatican stereoisomer of Ono',\n",
       "  'Ono Stereoisomer von Vatican',\n",
       "  'Oleh stereoisomer of Tina',\n",
       "  'Tina stereoisomer of Oleh',\n",
       "  'Oleh Stereoisomer von Tina',\n",
       "  'Terceira stereoisomer of Secondo',\n",
       "  'Secondo stereoisomer of Terceira',\n",
       "  'Terceira Stereoisomer von Secondo',\n",
       "  'Robot stereoisomer of EF',\n",
       "  'EF stereoisomer of Robot',\n",
       "  'Robot Stereoisomer von EF',\n",
       "  'Treviso stereoisomer of Latina',\n",
       "  'Latina stereoisomer of Treviso',\n",
       "  'Treviso Stereoisomer von Latina',\n",
       "  'Ruby stereoisomer of Humphrey',\n",
       "  'Humphrey stereoisomer of Ruby',\n",
       "  'Ruby Stereoisomer von Humphrey',\n",
       "  'Suva stereoisomer of Juventus',\n",
       "  'Juventus stereoisomer of Suva',\n",
       "  'Suva Stereoisomer von Juventus',\n",
       "  'Abd stereoisomer of Parra',\n",
       "  'Parra stereoisomer of Abd',\n",
       "  'Abd Stereoisomer von Parra',\n",
       "  'Cromwell stereoisomer of ur',\n",
       "  'ur stereoisomer of Cromwell',\n",
       "  'Cromwell Stereoisomer von ur',\n",
       "  'ARM stereoisomer of Orte',\n",
       "  'Orte stereoisomer of ARM',\n",
       "  'ARM Stereoisomer von Orte',\n",
       "  'Nickelodeon stereoisomer of Tata',\n",
       "  'Tata stereoisomer of Nickelodeon',\n",
       "  'Nickelodeon Stereoisomer von Tata',\n",
       "  'Vân stereoisomer of Conan',\n",
       "  'Conan stereoisomer of Vân',\n",
       "  'Vân Stereoisomer von Conan',\n",
       "  'Lost stereoisomer of Tunnel',\n",
       "  'Tunnel stereoisomer of Lost',\n",
       "  'Lost Stereoisomer von Tunnel',\n",
       "  'Shadow stereoisomer of Kappa',\n",
       "  'Kappa stereoisomer of Shadow',\n",
       "  'Shadow Stereoisomer von Kappa',\n",
       "  'Milli stereoisomer of Shell',\n",
       "  'Shell stereoisomer of Milli',\n",
       "  'Milli Stereoisomer von Shell',\n",
       "  'Gwillim stereoisomer of Holloway',\n",
       "  'Holloway stereoisomer of Gwillim',\n",
       "  'Gwillim Stereoisomer von Holloway',\n",
       "  'ADN stereoisomer of Campo',\n",
       "  'Campo stereoisomer of ADN',\n",
       "  'ADN Stereoisomer von Campo',\n",
       "  'Baru stereoisomer of Nikolaj',\n",
       "  'Nikolaj stereoisomer of Baru',\n",
       "  'Baru Stereoisomer von Nikolaj',\n",
       "  'Adel stereoisomer of Jaguar',\n",
       "  'Jaguar stereoisomer of Adel',\n",
       "  'Adel Stereoisomer von Jaguar',\n",
       "  'Palatinat stereoisomer of Band',\n",
       "  'Band stereoisomer of Palatinat',\n",
       "  'Palatinat Stereoisomer von Band',\n",
       "  'Eternal stereoisomer of Nepal',\n",
       "  'Nepal stereoisomer of Eternal',\n",
       "  'Eternal Stereoisomer von Nepal',\n",
       "  'Pt stereoisomer of Bosna',\n",
       "  'Bosna stereoisomer of Pt',\n",
       "  'Pt Stereoisomer von Bosna',\n",
       "  'Capitaine stereoisomer of Stift',\n",
       "  'Stift stereoisomer of Capitaine',\n",
       "  'Capitaine Stereoisomer von Stift',\n",
       "  'Certain stereoisomer of String',\n",
       "  'String stereoisomer of Certain',\n",
       "  'Certain Stereoisomer von String',\n",
       "  'ville stereoisomer of Man',\n",
       "  'Man stereoisomer of ville',\n",
       "  'ville Stereoisomer von Man',\n",
       "  'Cheyenne stereoisomer of Belgrade',\n",
       "  'Belgrade stereoisomer of Cheyenne',\n",
       "  'Cheyenne Stereoisomer von Belgrade',\n",
       "  'CO stereoisomer of Rollen',\n",
       "  'Rollen stereoisomer of CO',\n",
       "  'CO Stereoisomer von Rollen',\n",
       "  'oed stereoisomer of Monitor',\n",
       "  'Monitor stereoisomer of oed',\n",
       "  'oed Stereoisomer von Monitor',\n",
       "  'Wisdom stereoisomer of Canto',\n",
       "  'Canto stereoisomer of Wisdom',\n",
       "  'Wisdom Stereoisomer von Canto',\n",
       "  'Pain stereoisomer of Schlacht',\n",
       "  'Schlacht stereoisomer of Pain',\n",
       "  'Pain Stereoisomer von Schlacht',\n",
       "  'Grupp stereoisomer of Berthold',\n",
       "  'Berthold stereoisomer of Grupp',\n",
       "  'Grupp Stereoisomer von Berthold',\n",
       "  'Lillehammer stereoisomer of Boyd',\n",
       "  'Boyd stereoisomer of Lillehammer',\n",
       "  'Lillehammer Stereoisomer von Boyd',\n",
       "  'Massacre stereoisomer of Jet',\n",
       "  'Jet stereoisomer of Massacre',\n",
       "  'Massacre Stereoisomer von Jet',\n",
       "  'Kur stereoisomer of Rathaus',\n",
       "  'Rathaus stereoisomer of Kur',\n",
       "  'Kur Stereoisomer von Rathaus',\n",
       "  'Ur stereoisomer of Birinci',\n",
       "  'Birinci stereoisomer of Ur',\n",
       "  'Ur Stereoisomer von Birinci',\n",
       "  'Quantum stereoisomer of Jefferson',\n",
       "  'Jefferson stereoisomer of Quantum',\n",
       "  'Quantum Stereoisomer von Jefferson',\n",
       "  'Wrong stereoisomer of Rocket',\n",
       "  'Rocket stereoisomer of Wrong',\n",
       "  'Wrong Stereoisomer von Rocket',\n",
       "  'Mengen stereoisomer of Quintana',\n",
       "  'Quintana stereoisomer of Mengen',\n",
       "  'Mengen Stereoisomer von Quintana',\n",
       "  'CP stereoisomer of Oper',\n",
       "  'Oper stereoisomer of CP',\n",
       "  'CP Stereoisomer von Oper',\n",
       "  'DR stereoisomer of GT',\n",
       "  'GT stereoisomer of DR',\n",
       "  'DR Stereoisomer von GT',\n",
       "  'Dag stereoisomer of Feld',\n",
       "  'Feld stereoisomer of Dag',\n",
       "  'Dag Stereoisomer von Feld',\n",
       "  'Gift stereoisomer of Sofia',\n",
       "  'Sofia stereoisomer of Gift',\n",
       "  'Gift Stereoisomer von Sofia',\n",
       "  'Lappland stereoisomer of Desert',\n",
       "  'Desert stereoisomer of Lappland',\n",
       "  'Lappland Stereoisomer von Desert',\n",
       "  'Flere stereoisomer of Manconi',\n",
       "  'Manconi stereoisomer of Flere',\n",
       "  'Flere Stereoisomer von Manconi',\n",
       "  'Brain stereoisomer of Issue',\n",
       "  'Issue stereoisomer of Brain',\n",
       "  'Brain Stereoisomer von Issue',\n",
       "  'Genova stereoisomer of Buffalo',\n",
       "  'Buffalo stereoisomer of Genova',\n",
       "  'Genova Stereoisomer von Buffalo',\n",
       "  'Onthophagus stereoisomer of Mifflin',\n",
       "  'Mifflin stereoisomer of Onthophagus',\n",
       "  'Onthophagus Stereoisomer von Mifflin',\n",
       "  'Calder stereoisomer of Walther',\n",
       "  'Walther stereoisomer of Calder',\n",
       "  'Calder Stereoisomer von Walther',\n",
       "  'Camden stereoisomer of Basic',\n",
       "  'Basic stereoisomer of Camden',\n",
       "  'Camden Stereoisomer von Basic',\n",
       "  'Seda stereoisomer of Taman',\n",
       "  'Taman stereoisomer of Seda',\n",
       "  'Seda Stereoisomer von Taman',\n",
       "  'Duran stereoisomer of Ya',\n",
       "  'Ya stereoisomer of Duran',\n",
       "  'Duran Stereoisomer von Ya',\n",
       "  'Emilia stereoisomer of FN',\n",
       "  'FN stereoisomer of Emilia',\n",
       "  'Emilia Stereoisomer von FN',\n",
       "  'Governo stereoisomer of Fürsten',\n",
       "  'Fürsten stereoisomer of Governo',\n",
       "  'Governo Stereoisomer von Fürsten',\n",
       "  'Gustav stereoisomer of Firenze',\n",
       "  'Firenze stereoisomer of Gustav',\n",
       "  'Gustav Stereoisomer von Firenze',\n",
       "  'Petersen stereoisomer of Schottland',\n",
       "  'Schottland stereoisomer of Petersen',\n",
       "  'Petersen Stereoisomer von Schottland',\n",
       "  'Churchill stereoisomer of Helena',\n",
       "  'Helena stereoisomer of Churchill',\n",
       "  'Churchill Stereoisomer von Helena',\n",
       "  'Santo stereoisomer of Strada',\n",
       "  'Strada stereoisomer of Santo',\n",
       "  'Santo Stereoisomer von Strada',\n",
       "  'Savoy stereoisomer of Brasile',\n",
       "  'Brasile stereoisomer of Savoy',\n",
       "  'Savoy Stereoisomer von Brasile',\n",
       "  'Gilmour stereoisomer of GMA',\n",
       "  'GMA stereoisomer of Gilmour',\n",
       "  'Gilmour Stereoisomer von GMA',\n",
       "  'Concilio stereoisomer of NY',\n",
       "  'NY stereoisomer of Concilio',\n",
       "  'Concilio Stereoisomer von NY',\n",
       "  'DS stereoisomer of Tuy',\n",
       "  ...]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict = {'sample': test}\n",
    "train_dict = {'sample': train}\n",
    "train_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "First, we pad text so they are a uniform length. While it is possible to padtext in the tokenizer function by setting padding=True, it is more efficient to only pad the text to the length of the longest element in its batch. This is known as dynamic padding. You can do this with the DataCollatorWithPadding function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_dict(train_dict)\n",
    "test_ds = Dataset.from_dict(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sample'],\n",
       "    num_rows: 40000\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizerFast, TrainingArguments, Trainer, DataCollatorWithPadding, BertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"sample\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd4918eaa42496783e95aaa7d3784b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 40000\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_train_ds = train_ds.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"sample\"]\n",
    ")\n",
    "tokenized_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba808dc7ee443ed8a528acde01fdf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test_ds = test_ds.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"sample\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3, 1000\n",
    "relations and relations_general\n",
    "\n",
    "10, 2000 both with batchsize 256 per device is pretty much full util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANDB\n",
    "project_name = 'mBERT-Test'\n",
    "run_name = 'Test3'\n",
    "\n",
    "config = dict (\n",
    "    relation = \"symmetric\",\n",
    "    source_lang=source_lang,\n",
    "    target_lang=target_lang,\n",
    "    n_relations = n_relations,\n",
    "    n_facts = n_facts,\n",
    "    with_general = False,\n",
    "#     n_relations_general = n_relations_general\n",
    "#     n_facts_general = n_facts_general\n",
    "    architecture='bert-base-multilingual-cased',\n",
    "    learning_rate = 5e-5\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    name=run_name,\n",
    "#     notes=\"\",\n",
    "    config=config,\n",
    "    dir='../output'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_trainer import CustomTrainer\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "eval_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "    predictions = logits\n",
    "    # Select only the ones that are masked\n",
    "    indices = np.where(labels != -100)\n",
    "    return metric.compute(predictions=predictions[indices], references=labels[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_one(eval_pred):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    # Relation Accuracy\n",
    "    relation_logits = logits[:test_offset]\n",
    "    relation_labels = labels[:test_offset]\n",
    "    # predictions = np.argmax(relation_logits, axis=-1)\n",
    "    indices = np.where(relation_labels != -100)  # Select only the ones that are masked\n",
    "    relation_precision = metric.compute(predictions=relation_logits[indices], references=relation_labels[indices])['accuracy']\n",
    "\n",
    "    # General Accuracy\n",
    "    general_logits = logits[test_offset:]\n",
    "    general_labels = labels[test_offset:]\n",
    "    # predictions = np.argmax(general_logits, axis=-1)\n",
    "    indices = np.where(general_labels != -100)  # Select only the ones that are masked\n",
    "    general_precision = metric.compute(predictions=general_logits[indices], references=general_labels[indices])['accuracy']\n",
    "    return {'eval_accuracy': relation_precision, 'eval_general_accuracy': 1-general_precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# Finetune mBERT\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../output/models/Test2',\n",
    "    num_train_epochs=1000,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=5e-5,\n",
    "#     save_total_limit=2,\n",
    "    save_strategy='no',\n",
    "    logging_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "#     eval_accumulation_steps=1,\n",
    "    report_to=None\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=precision_at_one,\n",
    "    eval_data_collator=eval_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 40000\n",
      "  Num Epochs = 1000\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 157000\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43176' max='157000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 43176/157000 7:44:29 < 20:24:35, 1.55 it/s, Epoch 275/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>General Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.760400</td>\n",
       "      <td>9.103540</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.999900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.279000</td>\n",
       "      <td>8.947533</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.275400</td>\n",
       "      <td>8.849145</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.999900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.220300</td>\n",
       "      <td>8.806729</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.270000</td>\n",
       "      <td>8.774710</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.237500</td>\n",
       "      <td>8.753735</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.244600</td>\n",
       "      <td>8.733739</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.999900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.201300</td>\n",
       "      <td>8.721060</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.194400</td>\n",
       "      <td>8.704052</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.215000</td>\n",
       "      <td>8.672067</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.147500</td>\n",
       "      <td>8.627944</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.148000</td>\n",
       "      <td>8.549050</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.156000</td>\n",
       "      <td>8.432191</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.081500</td>\n",
       "      <td>8.275359</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.037600</td>\n",
       "      <td>8.086509</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.940000</td>\n",
       "      <td>7.895423</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.999400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.827100</td>\n",
       "      <td>7.658373</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.753300</td>\n",
       "      <td>7.450405</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>0.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.708500</td>\n",
       "      <td>7.233914</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.638300</td>\n",
       "      <td>6.972355</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.566800</td>\n",
       "      <td>6.783018</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.423000</td>\n",
       "      <td>6.597845</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>0.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.381100</td>\n",
       "      <td>6.384951</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>0.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.264400</td>\n",
       "      <td>6.212706</td>\n",
       "      <td>0.313600</td>\n",
       "      <td>0.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.201800</td>\n",
       "      <td>6.045673</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.139800</td>\n",
       "      <td>5.882628</td>\n",
       "      <td>0.384600</td>\n",
       "      <td>0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.097600</td>\n",
       "      <td>5.741535</td>\n",
       "      <td>0.417200</td>\n",
       "      <td>0.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.998800</td>\n",
       "      <td>5.591895</td>\n",
       "      <td>0.457100</td>\n",
       "      <td>0.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.936100</td>\n",
       "      <td>5.456265</td>\n",
       "      <td>0.494800</td>\n",
       "      <td>0.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.928000</td>\n",
       "      <td>5.305325</td>\n",
       "      <td>0.541900</td>\n",
       "      <td>0.998900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.855200</td>\n",
       "      <td>5.191305</td>\n",
       "      <td>0.575500</td>\n",
       "      <td>0.998800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.758800</td>\n",
       "      <td>5.073575</td>\n",
       "      <td>0.602700</td>\n",
       "      <td>0.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.730600</td>\n",
       "      <td>4.954286</td>\n",
       "      <td>0.642000</td>\n",
       "      <td>0.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.696400</td>\n",
       "      <td>4.875770</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.998500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.620500</td>\n",
       "      <td>4.746999</td>\n",
       "      <td>0.713700</td>\n",
       "      <td>0.998100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.603200</td>\n",
       "      <td>4.655128</td>\n",
       "      <td>0.754200</td>\n",
       "      <td>0.997600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.564800</td>\n",
       "      <td>4.583629</td>\n",
       "      <td>0.776800</td>\n",
       "      <td>0.998100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.514700</td>\n",
       "      <td>4.523331</td>\n",
       "      <td>0.790700</td>\n",
       "      <td>0.997400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.454800</td>\n",
       "      <td>4.445141</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.432900</td>\n",
       "      <td>4.374444</td>\n",
       "      <td>0.839100</td>\n",
       "      <td>0.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.411000</td>\n",
       "      <td>4.340893</td>\n",
       "      <td>0.856400</td>\n",
       "      <td>0.997600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.406300</td>\n",
       "      <td>4.242749</td>\n",
       "      <td>0.877400</td>\n",
       "      <td>0.996600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.344000</td>\n",
       "      <td>4.201081</td>\n",
       "      <td>0.893400</td>\n",
       "      <td>0.994700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.315700</td>\n",
       "      <td>4.129818</td>\n",
       "      <td>0.906700</td>\n",
       "      <td>0.994900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.307500</td>\n",
       "      <td>4.081531</td>\n",
       "      <td>0.919400</td>\n",
       "      <td>0.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.310200</td>\n",
       "      <td>4.052646</td>\n",
       "      <td>0.933500</td>\n",
       "      <td>0.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.244700</td>\n",
       "      <td>3.984259</td>\n",
       "      <td>0.937300</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.243600</td>\n",
       "      <td>3.946176</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>0.987800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.290400</td>\n",
       "      <td>3.907162</td>\n",
       "      <td>0.948900</td>\n",
       "      <td>0.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.223700</td>\n",
       "      <td>3.826070</td>\n",
       "      <td>0.959400</td>\n",
       "      <td>0.981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.200800</td>\n",
       "      <td>3.792037</td>\n",
       "      <td>0.966500</td>\n",
       "      <td>0.980500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.213700</td>\n",
       "      <td>3.706933</td>\n",
       "      <td>0.969600</td>\n",
       "      <td>0.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.152400</td>\n",
       "      <td>3.666286</td>\n",
       "      <td>0.973400</td>\n",
       "      <td>0.966300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.181300</td>\n",
       "      <td>3.611974</td>\n",
       "      <td>0.974600</td>\n",
       "      <td>0.962400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.167400</td>\n",
       "      <td>3.600695</td>\n",
       "      <td>0.978900</td>\n",
       "      <td>0.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.141700</td>\n",
       "      <td>3.511960</td>\n",
       "      <td>0.981700</td>\n",
       "      <td>0.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.125200</td>\n",
       "      <td>3.440915</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.945300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.122700</td>\n",
       "      <td>3.378912</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.099000</td>\n",
       "      <td>3.331014</td>\n",
       "      <td>0.986200</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.060600</td>\n",
       "      <td>3.207750</td>\n",
       "      <td>0.989700</td>\n",
       "      <td>0.902600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.065700</td>\n",
       "      <td>3.123723</td>\n",
       "      <td>0.989700</td>\n",
       "      <td>0.893200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.095000</td>\n",
       "      <td>3.025640</td>\n",
       "      <td>0.990400</td>\n",
       "      <td>0.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.033200</td>\n",
       "      <td>2.995473</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.877500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.017700</td>\n",
       "      <td>2.993294</td>\n",
       "      <td>0.995200</td>\n",
       "      <td>0.866100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.982800</td>\n",
       "      <td>2.806261</td>\n",
       "      <td>0.994300</td>\n",
       "      <td>0.842100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.960800</td>\n",
       "      <td>2.799460</td>\n",
       "      <td>0.993900</td>\n",
       "      <td>0.836600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.980500</td>\n",
       "      <td>2.716418</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>0.816000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.005700</td>\n",
       "      <td>2.688535</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>0.811800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>2.605339</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>0.802000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.935200</td>\n",
       "      <td>2.489815</td>\n",
       "      <td>0.996700</td>\n",
       "      <td>0.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.917700</td>\n",
       "      <td>2.481052</td>\n",
       "      <td>0.995800</td>\n",
       "      <td>0.772400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.938300</td>\n",
       "      <td>2.380949</td>\n",
       "      <td>0.996600</td>\n",
       "      <td>0.755800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.895300</td>\n",
       "      <td>2.340027</td>\n",
       "      <td>0.997500</td>\n",
       "      <td>0.744900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>2.260734</td>\n",
       "      <td>0.997300</td>\n",
       "      <td>0.724400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.890300</td>\n",
       "      <td>2.191008</td>\n",
       "      <td>0.997700</td>\n",
       "      <td>0.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.857700</td>\n",
       "      <td>2.118201</td>\n",
       "      <td>0.997100</td>\n",
       "      <td>0.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.866900</td>\n",
       "      <td>2.125003</td>\n",
       "      <td>0.996800</td>\n",
       "      <td>0.700200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.870400</td>\n",
       "      <td>2.067931</td>\n",
       "      <td>0.996900</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.811400</td>\n",
       "      <td>1.971454</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>1.922906</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>1.911043</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.653300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.780600</td>\n",
       "      <td>1.827118</td>\n",
       "      <td>0.997600</td>\n",
       "      <td>0.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.783600</td>\n",
       "      <td>1.736931</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.609700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>1.766199</td>\n",
       "      <td>0.997700</td>\n",
       "      <td>0.622000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.760900</td>\n",
       "      <td>1.648612</td>\n",
       "      <td>0.998400</td>\n",
       "      <td>0.594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>1.611635</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.740200</td>\n",
       "      <td>1.565991</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.572800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.753300</td>\n",
       "      <td>1.500590</td>\n",
       "      <td>0.997600</td>\n",
       "      <td>0.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.717500</td>\n",
       "      <td>1.528151</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.563300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>1.424458</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.540600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.728300</td>\n",
       "      <td>1.454958</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.549800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.728800</td>\n",
       "      <td>1.366946</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.696900</td>\n",
       "      <td>1.398488</td>\n",
       "      <td>0.998100</td>\n",
       "      <td>0.531200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.666300</td>\n",
       "      <td>1.315809</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.510600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.681400</td>\n",
       "      <td>1.249169</td>\n",
       "      <td>0.998100</td>\n",
       "      <td>0.488500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.646100</td>\n",
       "      <td>1.245533</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>1.236238</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.656900</td>\n",
       "      <td>1.163809</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.474300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.632900</td>\n",
       "      <td>1.107387</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.455800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.642500</td>\n",
       "      <td>1.110125</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.637000</td>\n",
       "      <td>1.095523</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.620700</td>\n",
       "      <td>1.131153</td>\n",
       "      <td>0.998400</td>\n",
       "      <td>0.469700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.612100</td>\n",
       "      <td>1.174076</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>1.089188</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.628900</td>\n",
       "      <td>1.071836</td>\n",
       "      <td>0.998400</td>\n",
       "      <td>0.452900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.608800</td>\n",
       "      <td>0.968168</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.600900</td>\n",
       "      <td>0.980512</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.422500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.625800</td>\n",
       "      <td>0.983989</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.421500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.576800</td>\n",
       "      <td>0.892280</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.598500</td>\n",
       "      <td>0.975980</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.589600</td>\n",
       "      <td>0.889220</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>0.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.564500</td>\n",
       "      <td>1.006105</td>\n",
       "      <td>0.998100</td>\n",
       "      <td>0.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.578900</td>\n",
       "      <td>0.905210</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>0.401700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.556300</td>\n",
       "      <td>0.819398</td>\n",
       "      <td>0.998100</td>\n",
       "      <td>0.370300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.590200</td>\n",
       "      <td>0.817592</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.605200</td>\n",
       "      <td>0.853217</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.564200</td>\n",
       "      <td>0.946353</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.535700</td>\n",
       "      <td>0.894239</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.554600</td>\n",
       "      <td>0.781578</td>\n",
       "      <td>0.998100</td>\n",
       "      <td>0.358400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.563300</td>\n",
       "      <td>0.806110</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>0.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.571200</td>\n",
       "      <td>0.793429</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.372700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.560800</td>\n",
       "      <td>0.759481</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.357300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.566400</td>\n",
       "      <td>0.784344</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.363300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.544800</td>\n",
       "      <td>0.810120</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.371900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>0.789132</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>0.778481</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.364900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.524900</td>\n",
       "      <td>0.870079</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.544100</td>\n",
       "      <td>0.788370</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.364700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.733139</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.342200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.546800</td>\n",
       "      <td>0.690570</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.327700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.514100</td>\n",
       "      <td>0.806027</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.528700</td>\n",
       "      <td>0.744155</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.353300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.530900</td>\n",
       "      <td>0.728727</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>0.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.717641</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>0.346300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.529100</td>\n",
       "      <td>0.651587</td>\n",
       "      <td>0.997900</td>\n",
       "      <td>0.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.527400</td>\n",
       "      <td>0.728727</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.536700</td>\n",
       "      <td>0.818918</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.524200</td>\n",
       "      <td>0.630446</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.533500</td>\n",
       "      <td>0.628225</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.307800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.554800</td>\n",
       "      <td>0.662352</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.315400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.551100</td>\n",
       "      <td>0.717501</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.344400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.543300</td>\n",
       "      <td>0.645052</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.517900</td>\n",
       "      <td>0.647605</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.536800</td>\n",
       "      <td>0.622363</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>0.299500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.608849</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.633357</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.311100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.510200</td>\n",
       "      <td>0.642727</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.311400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.541300</td>\n",
       "      <td>0.596175</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.293800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.494100</td>\n",
       "      <td>0.679576</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.326300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.534200</td>\n",
       "      <td>0.561760</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.280500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.507600</td>\n",
       "      <td>0.592791</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.539600</td>\n",
       "      <td>0.594513</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.295600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>0.671163</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.326400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.514300</td>\n",
       "      <td>0.523168</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.663922</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.511900</td>\n",
       "      <td>0.587410</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.296200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.506800</td>\n",
       "      <td>0.652145</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.318600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.525400</td>\n",
       "      <td>0.637786</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.506900</td>\n",
       "      <td>0.580320</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.287900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>0.618832</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.305900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>0.557631</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.516600</td>\n",
       "      <td>0.527608</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>0.632837</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.310700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>0.669268</td>\n",
       "      <td>0.998400</td>\n",
       "      <td>0.329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>0.529074</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.513800</td>\n",
       "      <td>0.644043</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.545907</td>\n",
       "      <td>0.998400</td>\n",
       "      <td>0.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.510900</td>\n",
       "      <td>0.636738</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.312900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.498400</td>\n",
       "      <td>0.596985</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.484800</td>\n",
       "      <td>0.584750</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.478400</td>\n",
       "      <td>0.560415</td>\n",
       "      <td>0.998400</td>\n",
       "      <td>0.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.505300</td>\n",
       "      <td>0.590768</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.292500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.492400</td>\n",
       "      <td>0.587345</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555575</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.481900</td>\n",
       "      <td>0.565792</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.284200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.501400</td>\n",
       "      <td>0.587290</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.476100</td>\n",
       "      <td>0.568718</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.489300</td>\n",
       "      <td>0.574776</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.282600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.593358</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.470100</td>\n",
       "      <td>0.569838</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.280100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.505500</td>\n",
       "      <td>0.637421</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.532900</td>\n",
       "      <td>0.526828</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.263400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.487300</td>\n",
       "      <td>0.544358</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.522200</td>\n",
       "      <td>0.521480</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.466900</td>\n",
       "      <td>0.564173</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.509800</td>\n",
       "      <td>0.612096</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.297700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.516400</td>\n",
       "      <td>0.571029</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>0.279900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.471700</td>\n",
       "      <td>0.490163</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.545933</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>0.519142</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.498100</td>\n",
       "      <td>0.531268</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.488800</td>\n",
       "      <td>0.468774</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.243600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.483800</td>\n",
       "      <td>0.490382</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.567468</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.473700</td>\n",
       "      <td>0.519252</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.486400</td>\n",
       "      <td>0.519693</td>\n",
       "      <td>0.998400</td>\n",
       "      <td>0.265600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.518605</td>\n",
       "      <td>0.998400</td>\n",
       "      <td>0.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.503800</td>\n",
       "      <td>0.442422</td>\n",
       "      <td>0.998400</td>\n",
       "      <td>0.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.486300</td>\n",
       "      <td>0.544454</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>0.568774</td>\n",
       "      <td>0.998100</td>\n",
       "      <td>0.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.469000</td>\n",
       "      <td>0.519626</td>\n",
       "      <td>0.998100</td>\n",
       "      <td>0.260200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.479500</td>\n",
       "      <td>0.510719</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.478100</td>\n",
       "      <td>0.527469</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.619171</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.304600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.474300</td>\n",
       "      <td>0.428509</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.467700</td>\n",
       "      <td>0.496794</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.485500</td>\n",
       "      <td>0.607217</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.493614</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.503900</td>\n",
       "      <td>0.487445</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.481400</td>\n",
       "      <td>0.501583</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.250500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.508300</td>\n",
       "      <td>0.445770</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.482656</td>\n",
       "      <td>0.999500</td>\n",
       "      <td>0.246500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.475200</td>\n",
       "      <td>0.539173</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.505700</td>\n",
       "      <td>0.506650</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.486400</td>\n",
       "      <td>0.546086</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.267700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.493000</td>\n",
       "      <td>0.578838</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.566967</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.617584</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.482053</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.500771</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.489600</td>\n",
       "      <td>0.537071</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.475900</td>\n",
       "      <td>0.524332</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.488800</td>\n",
       "      <td>0.537229</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.512300</td>\n",
       "      <td>0.561439</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.282800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.442029</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.477400</td>\n",
       "      <td>0.526799</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.459200</td>\n",
       "      <td>0.527001</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.261300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.535155</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.491100</td>\n",
       "      <td>0.516468</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.260400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.482700</td>\n",
       "      <td>0.541487</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.269000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.578085</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.467600</td>\n",
       "      <td>0.572551</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.279900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.507100</td>\n",
       "      <td>0.544022</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.478700</td>\n",
       "      <td>0.509854</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.549362</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.466400</td>\n",
       "      <td>0.508282</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.484600</td>\n",
       "      <td>0.528426</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.477100</td>\n",
       "      <td>0.497905</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>0.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.488300</td>\n",
       "      <td>0.517801</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.517513</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.265900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.547948</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.479100</td>\n",
       "      <td>0.437276</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.519081</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.481900</td>\n",
       "      <td>0.502962</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.547399</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>0.530649</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.529644</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.272600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.467700</td>\n",
       "      <td>0.556883</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>0.493104</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.442700</td>\n",
       "      <td>0.462454</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.494600</td>\n",
       "      <td>0.486468</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.460500</td>\n",
       "      <td>0.479409</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.462600</td>\n",
       "      <td>0.494383</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.484500</td>\n",
       "      <td>0.491985</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.449300</td>\n",
       "      <td>0.531913</td>\n",
       "      <td>0.999200</td>\n",
       "      <td>0.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.472600</td>\n",
       "      <td>0.555548</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.476400</td>\n",
       "      <td>0.440002</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.479500</td>\n",
       "      <td>0.471371</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.237500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.497800</td>\n",
       "      <td>0.430333</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.225700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.485400</td>\n",
       "      <td>0.508374</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.263400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.474300</td>\n",
       "      <td>0.544224</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.479000</td>\n",
       "      <td>0.548474</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.464400</td>\n",
       "      <td>0.525029</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.264400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.492900</td>\n",
       "      <td>0.503730</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.447600</td>\n",
       "      <td>0.472707</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.466600</td>\n",
       "      <td>0.535840</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.273700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.481700</td>\n",
       "      <td>0.524908</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.444000</td>\n",
       "      <td>0.483837</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.439600</td>\n",
       "      <td>0.494590</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.451500</td>\n",
       "      <td>0.473635</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.248900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.473100</td>\n",
       "      <td>0.476594</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.444700</td>\n",
       "      <td>0.487176</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.471500</td>\n",
       "      <td>0.477329</td>\n",
       "      <td>0.999100</td>\n",
       "      <td>0.248300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42/79 00:08 < 00:07, 4.80 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Using the latest cached version of the module from /home/laurin/.cache/huggingface/modules/datasets_modules/metrics/accuracy/3e9ee15abf476145152fe4e9a9c1463ff95d3d65cdc555be9cfe061bdaeb1a14 (last modified on Mon Mar  7 16:01:31 2022) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/transformers/trainer.py:1490\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1490\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1494\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/transformers/trainer.py:1602\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1600\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 1602\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, epoch, metrics)\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m~/Developer/Thesis/notebooks/custom_trainer.py:78\u001b[0m, in \u001b[0;36mCustomTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m     75\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     77\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m---> 78\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m     89\u001b[0m output\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m     90\u001b[0m     speed_metrics(\n\u001b[1;32m     91\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     96\u001b[0m )\n",
      "File \u001b[0;32m~/Developer/Thesis/notebooks/custom_trainer.py:205\u001b[0m, in \u001b[0;36mCustomTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m    203\u001b[0m     losses_host \u001b[38;5;241m=\u001b[39m losses \u001b[38;5;28;01mif\u001b[39;00m losses_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((losses_host, losses), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nested_gather(labels)\n\u001b[1;32m    207\u001b[0m     labels_host \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;28;01mif\u001b[39;00m labels_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nested_concat(labels_host, labels, padding_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/transformers/trainer.py:2557\u001b[0m, in \u001b[0;36mTrainer._pad_across_processes\u001b[0;34m(self, tensor, pad_index)\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[1;32m   2556\u001b[0m \u001b[38;5;66;03m# Gather all sizes\u001b[39;00m\n\u001b[0;32m-> 2557\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m   2558\u001b[0m sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nested_gather(size)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m   2560\u001b[0m max_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(s[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sizes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "total = len(train_dict['sample'])\n",
    "i = 0\n",
    "\n",
    "for txt in train_dict['sample'][:10000]:\n",
    "    i += 1\n",
    "    \n",
    "    # Add [MASK] for object\n",
    "    sample = txt.rsplit(' ', 1)[0] + ' [MASK]'\n",
    "    label_token = tokenizer.convert_tokens_to_ids(txt.rsplit(' ', 1)[1])\n",
    "    \n",
    "    encoded_input = tokenizer(sample, return_tensors='pt')\n",
    "    token_logits = model(**encoded_input).logits\n",
    "    \n",
    "    mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    \n",
    "    # Pick the [MASK] candidates with the highest logits\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "    \n",
    "    if label_token in top_5_tokens:\n",
    "        k += 1\n",
    "        print('i:' + str(i) + ' k:' + str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "total = len(test_dict['sample'])\n",
    "i = 0\n",
    "\n",
    "for txt in test_dict['sample'][:10000]:\n",
    "    i += 1\n",
    "    \n",
    "    # Add [MASK] for object\n",
    "    sample = txt.rsplit(' ', 1)[0] + ' [MASK]'\n",
    "    label_token = tokenizer.convert_tokens_to_ids(txt.rsplit(' ', 1)[1])\n",
    "    \n",
    "    encoded_input = tokenizer(sample, return_tensors='pt')\n",
    "    token_logits = model(**encoded_input).logits\n",
    "    \n",
    "    mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    \n",
    "    # Pick the [MASK] candidates with the highest logits\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "    \n",
    "    if label_token in top_5_tokens:\n",
    "        k += 1\n",
    "        print('i:' + str(i) + ' k:' + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Kendrick hat als Grenze [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Kendrick hat als Grenze [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_dict['text']:\n",
    "    if 'Spa' in t:\n",
    "        print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
