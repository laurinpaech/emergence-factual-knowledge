{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Relations and Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entities (5500)\n",
    "entities = pd.read_csv('../data/Entities/SingleToken/entities_languageAgnostic_clean.csv')\n",
    "\n",
    "# Load Relations (60)\n",
    "relations = pd.read_csv('../data/Relations/Symmetry/symmetric_multilingual_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_general = pd.read_csv('../data/Relations/General/properties_nonsymmetric_multilingual_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# prob of returning true\n",
    "def decision(probability):\n",
    "    return random.random() < probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random pairs of numbers (indices into entity)\n",
    "# Order doesn't matter, can't repeat\n",
    "# i.e. ok is: (0,1), (1,2), (0,2) but not ok is (0,1),(1,0) or (0,0)\n",
    "# Runs until exhausted or reached max_size\n",
    "# possible to limit occurences of index\n",
    "def gen_index_pairs(n, max_size=np.Inf, limit=np.Inf):\n",
    "    pairs = set()\n",
    "    ind = list()\n",
    "\n",
    "    while len(pairs) < max_size:\n",
    "        # return number between 0 and n (exclude)\n",
    "        x, y = np.random.randint(n), np.random.randint(n)\n",
    "        \n",
    "        while ind.count(x) >= limit or ind.count(y) >= limit:\n",
    "            x, y = np.random.randint(n), np.random.randint(n)\n",
    "\n",
    "        ind.append(x)\n",
    "        ind.append(y)\n",
    "        \n",
    "        i = 0\n",
    "        while (x, y) in pairs or (y, x) in pairs or x == y:\n",
    "            if i > 10:\n",
    "                return\n",
    "            x, y = np.random.randint(n), np.random.randint(n)\n",
    "            i += 1\n",
    "        \n",
    "        pairs.add((x, y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_relations = 10\n",
    "n_facts = 2000\n",
    "\n",
    "# (e, r, f ) <=> (f, r, e)\n",
    "train = []\n",
    "test = []\n",
    "ent = []\n",
    "\n",
    "# Sample relations\n",
    "relations_sampled = relations.sample(n_relations)\n",
    "\n",
    "for index, relation in relations_sampled.iterrows():\n",
    "    \n",
    "    to_split = []\n",
    "\n",
    "    # Sample random entities\n",
    "    entity_generator = gen_index_pairs(entities.shape[0], n_facts, 1)\n",
    "\n",
    "    for e_id, f_id in entity_generator:\n",
    "        e = entities['label'][e_id]\n",
    "        f = entities['label'][f_id]\n",
    "        \n",
    "        ent.append(e)\n",
    "\n",
    "        # Append symmetric relations\n",
    "        train.append(e + ' ' + relation['en'] + ' ' + f)\n",
    "        \n",
    "        to_split.append(f + ' ' + relation['en'] + ' ' + e)\n",
    "    \n",
    "    # 90% train, 10% test\n",
    "    split_pos = int(0.9 * len(to_split))\n",
    "    \n",
    "    train = train + to_split[:split_pos]\n",
    "    test = test + to_split[split_pos:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add non-relation\n",
    "n_relations = 10\n",
    "n_facts = 2000\n",
    "\n",
    "non_rels = []\n",
    "\n",
    "relations_general_sampled = relations_general.sample(n_relations)\n",
    "\n",
    "for index, relation in relations_general_sampled.iterrows():\n",
    "\n",
    "    # Sample random entities\n",
    "    entity_generator = gen_index_pairs(entities.shape[0], n_facts, 1)\n",
    "\n",
    "    for e_id, f_id in entity_generator:\n",
    "        e = entities['label'][e_id]\n",
    "        f = entities['label'][f_id]\n",
    "\n",
    "        # Append relations\n",
    "        train.append(e + ' ' + relation['en'] + ' ' + f)\n",
    "        non_rels.append(e + ' ' + relation['en'] + ' ' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Richardson adjacent station Diesel',\n",
       "  'Ranger adjacent station Campaign',\n",
       "  'Sicily adjacent station CJ',\n",
       "  'Amb adjacent station Mur',\n",
       "  'Milo adjacent station Caesar',\n",
       "  'Oviedo adjacent station Marek',\n",
       "  'pl adjacent station Greatest',\n",
       "  'Carlos adjacent station Meuse',\n",
       "  'da adjacent station Mires',\n",
       "  'Minister adjacent station Daha',\n",
       "  'Khmer adjacent station Opera',\n",
       "  'Gonzaga adjacent station Regional',\n",
       "  'Dag adjacent station Aki',\n",
       "  'Yüksek adjacent station Angelina',\n",
       "  'Pour adjacent station Inn',\n",
       "  'Pac adjacent station Valencia',\n",
       "  'Spring adjacent station Erie',\n",
       "  'Minds adjacent station Beatrix',\n",
       "  'Camilla adjacent station Riviera',\n",
       "  'Kane adjacent station Vockeroth',\n",
       "  'Day adjacent station Monique',\n",
       "  'Tea adjacent station Linden',\n",
       "  'Semi adjacent station Nietzsche',\n",
       "  'Becker adjacent station Gerd',\n",
       "  'Rock adjacent station Allen',\n",
       "  'Purcell adjacent station Lara',\n",
       "  'SL adjacent station Senegal',\n",
       "  'Neville adjacent station Ferris',\n",
       "  'Solomon adjacent station Forest',\n",
       "  'Sanctuary adjacent station Spice',\n",
       "  'West adjacent station ASV',\n",
       "  'Cel adjacent station Meteor',\n",
       "  'Oaks adjacent station September',\n",
       "  'Auburn adjacent station Muslimani',\n",
       "  'VfB adjacent station Boyd',\n",
       "  'Jonathan adjacent station Bassi',\n",
       "  'Housing adjacent station Schubert',\n",
       "  'Rotten adjacent station Profile',\n",
       "  'Browning adjacent station Affair',\n",
       "  'Nizza adjacent station Indie',\n",
       "  'Nebraska adjacent station Spectrum',\n",
       "  'Gilmore adjacent station Bangkok',\n",
       "  'Cody adjacent station Tin',\n",
       "  'Parliament adjacent station Hollande',\n",
       "  'Culture adjacent station Primeiro',\n",
       "  'Vergine adjacent station Christopher',\n",
       "  'RE adjacent station Pico',\n",
       "  'Total adjacent station Semana',\n",
       "  'Fidel adjacent station Davida',\n",
       "  'Australian adjacent station Shakira',\n",
       "  'Bourbon adjacent station Ans',\n",
       "  'Bosch adjacent station Maka',\n",
       "  'Hartford adjacent station Reeder',\n",
       "  'Ankara adjacent station Paulo',\n",
       "  'Wadi adjacent station Rua',\n",
       "  'Limerick adjacent station Tema',\n",
       "  'Cosimo adjacent station Watson',\n",
       "  'Colt adjacent station Isole',\n",
       "  'Fils adjacent station EHF',\n",
       "  'Ostrava adjacent station Seele',\n",
       "  'Kelly adjacent station Titus',\n",
       "  'Save adjacent station Lamarck',\n",
       "  'Tucson adjacent station Cando',\n",
       "  'Poison adjacent station Rain',\n",
       "  'Mexican adjacent station Fontana',\n",
       "  'Moravia adjacent station Music',\n",
       "  'Royal adjacent station Direktor',\n",
       "  'Liu adjacent station Ardèche',\n",
       "  'Seat adjacent station Midi',\n",
       "  'Rams adjacent station Mayo',\n",
       "  'March adjacent station Spitze',\n",
       "  'Check adjacent station Trinity',\n",
       "  'Julien adjacent station Sherman',\n",
       "  'Primavera adjacent station Jan',\n",
       "  'Barbus adjacent station Vincent',\n",
       "  'Gelo adjacent station Berge',\n",
       "  'Belgrano adjacent station Yoshida',\n",
       "  'Baza adjacent station Vigo',\n",
       "  'Jäger adjacent station Wert',\n",
       "  'Spezia adjacent station Vir',\n",
       "  'Darwin adjacent station Tales',\n",
       "  'JK adjacent station Mortimer',\n",
       "  'Gibraltar adjacent station NGC',\n",
       "  'Singer adjacent station Atene',\n",
       "  'BR adjacent station Superman',\n",
       "  'Java adjacent station Patrol',\n",
       "  'Jesu adjacent station Bertrand',\n",
       "  'Jared adjacent station Vida',\n",
       "  'Yale adjacent station Brenner',\n",
       "  'Dua adjacent station Ciudad',\n",
       "  'Maya adjacent station Millennium',\n",
       "  'CPU adjacent station Princesa',\n",
       "  'RSS adjacent station Rivas',\n",
       "  'Freak adjacent station Tabachnick',\n",
       "  'Nello adjacent station Boulevard',\n",
       "  'Series adjacent station Louise',\n",
       "  'Ola adjacent station Pilar',\n",
       "  'Bolívar adjacent station Calvin',\n",
       "  'rs adjacent station Ronnie',\n",
       "  'Sound adjacent station Sánchez',\n",
       "  'Duck adjacent station Burke',\n",
       "  'Jacobus adjacent station Bedford',\n",
       "  'Quest adjacent station Sultan',\n",
       "  'Gallia adjacent station ge',\n",
       "  'Qara adjacent station Montreal',\n",
       "  'Aan adjacent station pr',\n",
       "  'Freiburg adjacent station Bangor',\n",
       "  'Alabama adjacent station Jacqueline',\n",
       "  'Erwin adjacent station Angola',\n",
       "  'Boss adjacent station Gyula',\n",
       "  'Russie adjacent station epi',\n",
       "  'Dollar adjacent station Rogers',\n",
       "  'Hop adjacent station BC',\n",
       "  'Came adjacent station Ballet',\n",
       "  'Exposition adjacent station Stephan',\n",
       "  'Wi adjacent station CH',\n",
       "  'Jasper adjacent station Blitz',\n",
       "  'Fraser adjacent station Queste',\n",
       "  'Re adjacent station ER',\n",
       "  'Darling adjacent station Sogn',\n",
       "  'Lions adjacent station Neri',\n",
       "  'Justice adjacent station Maximilian',\n",
       "  'Lai adjacent station CC',\n",
       "  'Dynasty adjacent station Middlesex',\n",
       "  'Bobby adjacent station Dahl',\n",
       "  'Meiji adjacent station Freire',\n",
       "  'Ghosts adjacent station Matilde',\n",
       "  'Bron adjacent station Sonja',\n",
       "  'Museum adjacent station SAS',\n",
       "  'Bosna adjacent station Huxley',\n",
       "  'Fortune adjacent station Libération',\n",
       "  'Ny adjacent station Ritual',\n",
       "  'Gales adjacent station Cass',\n",
       "  'Knowles adjacent station Jepang',\n",
       "  'Normandy adjacent station Ketika',\n",
       "  'Tolosa adjacent station Pál',\n",
       "  'Cadillac adjacent station Masa',\n",
       "  'Abd adjacent station Suba',\n",
       "  'Ra adjacent station Indigenous',\n",
       "  'Little adjacent station Comet',\n",
       "  'Zen adjacent station Sterling',\n",
       "  'Ruby adjacent station Dixie',\n",
       "  'Zu adjacent station Kral',\n",
       "  'Otis adjacent station Miroslav',\n",
       "  'Fuji adjacent station Character',\n",
       "  'Els adjacent station Export',\n",
       "  'President adjacent station Tarragona',\n",
       "  'McCarthy adjacent station Dharma',\n",
       "  'Creek adjacent station Same',\n",
       "  'Ring adjacent station Flanders',\n",
       "  'Reggie adjacent station Salud',\n",
       "  'Marcus adjacent station Zoom',\n",
       "  'Slovak adjacent station Bono',\n",
       "  'Liberation adjacent station Highway',\n",
       "  'Services adjacent station Ball',\n",
       "  'Katrina adjacent station Dortmund',\n",
       "  'Walther adjacent station Gilman',\n",
       "  'Maa adjacent station Molly',\n",
       "  'Hayward adjacent station Terminal',\n",
       "  'Assam adjacent station CL',\n",
       "  'Lindl adjacent station Pieter',\n",
       "  'Mint adjacent station Champ',\n",
       "  'Westermann adjacent station Gera',\n",
       "  'Natura adjacent station Harvest',\n",
       "  'Houghton adjacent station Beloe',\n",
       "  'Edmond adjacent station Luigi',\n",
       "  'Scala adjacent station Pay',\n",
       "  'Thing adjacent station Flora',\n",
       "  'Kap adjacent station Dumas',\n",
       "  'Serra adjacent station Operator',\n",
       "  'Kraft adjacent station Langue',\n",
       "  'Hien adjacent station MSK',\n",
       "  'Jolie adjacent station Alte',\n",
       "  'Largo adjacent station Court',\n",
       "  'Rowland adjacent station BL',\n",
       "  'Stars adjacent station Devlet',\n",
       "  'ti adjacent station Danas',\n",
       "  'Smoke adjacent station am',\n",
       "  'Herbst adjacent station Linares',\n",
       "  'Neve adjacent station Saat',\n",
       "  'Osborne adjacent station Kramer',\n",
       "  'z adjacent station Francesca',\n",
       "  'Clancy adjacent station Verso',\n",
       "  'Peterborough adjacent station ac',\n",
       "  'Levante adjacent station Vladislav',\n",
       "  'Butterfly adjacent station America',\n",
       "  'AF adjacent station Teatro',\n",
       "  'PSP adjacent station Noble',\n",
       "  'Detroit adjacent station Luxembourg',\n",
       "  'Southwest adjacent station Mitch',\n",
       "  'Man adjacent station Liverpool',\n",
       "  'Arthur adjacent station Eye',\n",
       "  'at adjacent station Ivy',\n",
       "  'Rabbi adjacent station Axis',\n",
       "  'Carrera adjacent station Helen',\n",
       "  'Angelica adjacent station Pero',\n",
       "  'Anh adjacent station Justin',\n",
       "  'Hub adjacent station RA',\n",
       "  'Elite adjacent station Ulysses',\n",
       "  'Titolo adjacent station Feel',\n",
       "  'Brown adjacent station PMC',\n",
       "  'Thames adjacent station Suez',\n",
       "  'Cello adjacent station Hertfordshire',\n",
       "  'JR adjacent station Springer',\n",
       "  'Shakespeare adjacent station Jet',\n",
       "  'Calendar adjacent station Siege',\n",
       "  'Battlefield adjacent station Nest',\n",
       "  'Rathaus adjacent station Syracuse',\n",
       "  'Neuen adjacent station Caja',\n",
       "  'Habitants adjacent station Firenze',\n",
       "  'Amber adjacent station TKO',\n",
       "  'Hiroshima adjacent station Pedra',\n",
       "  'Juventus adjacent station VP',\n",
       "  'Margaret adjacent station Fiat',\n",
       "  'Havana adjacent station KM',\n",
       "  'McLaren adjacent station CV',\n",
       "  'Waterloo adjacent station BB',\n",
       "  'Palacio adjacent station Brod',\n",
       "  'Gomez adjacent station Amour',\n",
       "  'Pole adjacent station Rescue',\n",
       "  'Nelle adjacent station Delft',\n",
       "  'Eugène adjacent station Burger',\n",
       "  'Elliott adjacent station Seda',\n",
       "  'Ever adjacent station Pam',\n",
       "  'Pan adjacent station Benedict',\n",
       "  'Broadcast adjacent station Dee',\n",
       "  'Navarra adjacent station Gand',\n",
       "  'Bay adjacent station Leslie',\n",
       "  'Legenda adjacent station Krista',\n",
       "  'Cure adjacent station Champion',\n",
       "  'Bresse adjacent station Timor',\n",
       "  'Words adjacent station Forward',\n",
       "  'ST adjacent station Manhattan',\n",
       "  'Bernstein adjacent station Ariège',\n",
       "  'Graf adjacent station Albin',\n",
       "  'AR adjacent station Evil',\n",
       "  'Symphony adjacent station Yucatán',\n",
       "  'Snow adjacent station Vance',\n",
       "  'Feria adjacent station Dei',\n",
       "  'Peru adjacent station Oxford',\n",
       "  'Jones adjacent station Kali',\n",
       "  'Industria adjacent station Oko',\n",
       "  'Friesland adjacent station Osiris',\n",
       "  'Ruhr adjacent station Silla',\n",
       "  'Fauchald adjacent station Islandia',\n",
       "  'Elizabeth adjacent station Ferdinand',\n",
       "  'Expo adjacent station Ashes',\n",
       "  'Mittel adjacent station Suiza',\n",
       "  'Fontainebleau adjacent station Hey',\n",
       "  'Samba adjacent station Mio',\n",
       "  'Heaven adjacent station Teruel',\n",
       "  'Souza adjacent station Conceição',\n",
       "  'Polen adjacent station Kantor',\n",
       "  'Much adjacent station Ziele',\n",
       "  'Eliza adjacent station Boston',\n",
       "  'Slovakia adjacent station Résumé',\n",
       "  'Stern adjacent station Pass',\n",
       "  'UCB adjacent station Heikki',\n",
       "  'Belgrade adjacent station Eberhard',\n",
       "  'Wallace adjacent station Chaplin',\n",
       "  'Christ adjacent station Sad',\n",
       "  'pk adjacent station Aalborg',\n",
       "  'Cabo adjacent station cat',\n",
       "  'INE adjacent station Warhol',\n",
       "  'Jos adjacent station Amigos',\n",
       "  'AE adjacent station AFI',\n",
       "  'Roberto adjacent station Stalin',\n",
       "  'Dalton adjacent station Seigneur',\n",
       "  'Bayreuth adjacent station Witness',\n",
       "  'Taylor adjacent station Abucay',\n",
       "  'Profil adjacent station Hügel',\n",
       "  'Kirkwood adjacent station Kirchner',\n",
       "  'Pt adjacent station Gaur',\n",
       "  'Bot adjacent station Crescent',\n",
       "  'Gandhi adjacent station Anjou',\n",
       "  'Stanton adjacent station Princeton',\n",
       "  'Paddy adjacent station JP',\n",
       "  'Silva adjacent station Conseil',\n",
       "  'Venice adjacent station Rouge',\n",
       "  'Tala adjacent station Karnataka',\n",
       "  'Elke adjacent station Thành',\n",
       "  'Arco adjacent station Reisen',\n",
       "  'Ese adjacent station Sundance',\n",
       "  'López adjacent station Dante',\n",
       "  'Plana adjacent station Mat',\n",
       "  'Tucumán adjacent station Aus',\n",
       "  'Edith adjacent station KC',\n",
       "  'Oltre adjacent station Conrad',\n",
       "  'Britney adjacent station Roi',\n",
       "  'Usa adjacent station Før',\n",
       "  'Okinawa adjacent station Merrill',\n",
       "  'Take adjacent station Pavia',\n",
       "  'Haifa adjacent station Clock',\n",
       "  'Sailor adjacent station Saba',\n",
       "  'Ferreira adjacent station Tat',\n",
       "  'Christie adjacent station Beaver',\n",
       "  'Sato adjacent station Magazine',\n",
       "  'Gilmour adjacent station Victorian',\n",
       "  'Norwich adjacent station Taman',\n",
       "  'Venecia adjacent station Iris',\n",
       "  'Friends adjacent station Lucy',\n",
       "  'Murat adjacent station Wellington',\n",
       "  'Zeeland adjacent station Erfurt',\n",
       "  'Kant adjacent station Pacheco',\n",
       "  'Artois adjacent station Digital',\n",
       "  'Assad adjacent station Newport',\n",
       "  'Víctor adjacent station Nikolaj',\n",
       "  'Planeta adjacent station Basin',\n",
       "  'Vacelet adjacent station Note',\n",
       "  'Panny adjacent station PIB',\n",
       "  'Gemini adjacent station Prestige',\n",
       "  'Cortes adjacent station MIT',\n",
       "  'Las adjacent station Gideon',\n",
       "  'Ottawa adjacent station Saguenay',\n",
       "  'Linh adjacent station Welles',\n",
       "  'Maigret adjacent station Showtime',\n",
       "  'Lagos adjacent station THE',\n",
       "  'McGregor adjacent station Winters',\n",
       "  'Hilaire adjacent station Kat',\n",
       "  'Schlacht adjacent station Luzon',\n",
       "  'Spa adjacent station Boga',\n",
       "  'Moody adjacent station Sky',\n",
       "  'Salt adjacent station Drôme',\n",
       "  'Daly adjacent station Hahn',\n",
       "  'Bergamo adjacent station Release',\n",
       "  'Smash adjacent station Spencer',\n",
       "  'Péter adjacent station Kohl',\n",
       "  'Wilhelmina adjacent station PKK',\n",
       "  'Bulls adjacent station Pode',\n",
       "  'Darkness adjacent station Connor',\n",
       "  'Manu adjacent station Shell',\n",
       "  'Auckland adjacent station Seven',\n",
       "  'Buckley adjacent station Schönberg',\n",
       "  'Fel adjacent station Bhd',\n",
       "  'Prometheus adjacent station Marija',\n",
       "  'Sumatra adjacent station AK',\n",
       "  'Eintracht adjacent station Carnaval',\n",
       "  'Giang adjacent station Holy',\n",
       "  'Media adjacent station Eucalyptus',\n",
       "  'Rebel adjacent station Sheffield',\n",
       "  'Edmonton adjacent station Guinness',\n",
       "  'Fusion adjacent station Sonic',\n",
       "  'Atkins adjacent station Homer',\n",
       "  'Hugo adjacent station EM',\n",
       "  'Shirley adjacent station Egipto',\n",
       "  'Seychelles adjacent station Margaretha',\n",
       "  'Fayette adjacent station Flowers',\n",
       "  'Schalke adjacent station Capitaine',\n",
       "  'Landing adjacent station Peace',\n",
       "  'Krüger adjacent station Maxwell',\n",
       "  'Semarang adjacent station Tag',\n",
       "  'Hopper adjacent station Charlton',\n",
       "  'Lowe adjacent station Petersburg',\n",
       "  'Pizza adjacent station Lego',\n",
       "  'Isabel adjacent station Size',\n",
       "  'Brandt adjacent station Ville',\n",
       "  'Colle adjacent station Regular',\n",
       "  'Primer adjacent station Irland',\n",
       "  'Drama adjacent station Watch',\n",
       "  'Tunnel adjacent station Kennedy',\n",
       "  'Wenn adjacent station Philippe',\n",
       "  'Trump adjacent station Commonwealth',\n",
       "  'Putih adjacent station Dickson',\n",
       "  'Capo adjacent station Alliance',\n",
       "  'Thiele adjacent station NL',\n",
       "  'Play adjacent station Libia',\n",
       "  'Rep adjacent station Lec',\n",
       "  'Kawas adjacent station Angkatan',\n",
       "  'Disney adjacent station Slovan',\n",
       "  'Kale adjacent station Giants',\n",
       "  'Copenhagen adjacent station Andrzej',\n",
       "  'Rowling adjacent station Cinq',\n",
       "  'Morgen adjacent station After',\n",
       "  'Addison adjacent station Lopes',\n",
       "  'Denne adjacent station Casino',\n",
       "  'Bandet adjacent station Cabinet',\n",
       "  'Canon adjacent station Remix',\n",
       "  'Global adjacent station Gwillim',\n",
       "  'Raven adjacent station Colección',\n",
       "  'Horne adjacent station Tennessee',\n",
       "  'Eros adjacent station Reflections',\n",
       "  'Slam adjacent station Florence',\n",
       "  'Everett adjacent station Translation',\n",
       "  'Novara adjacent station USC',\n",
       "  'Domino adjacent station Skin',\n",
       "  'Richards adjacent station Vienna',\n",
       "  'Senna adjacent station Dakota',\n",
       "  'Helga adjacent station Lower',\n",
       "  'Olga adjacent station Wege',\n",
       "  'Esse adjacent station Chorus',\n",
       "  'States adjacent station Federal',\n",
       "  'Emilia adjacent station Zola',\n",
       "  'Rochefort adjacent station Saale',\n",
       "  'Dem adjacent station Portugal',\n",
       "  'Eno adjacent station Templo',\n",
       "  'Columbus adjacent station Sanskrit',\n",
       "  'Teixeira adjacent station Kurt',\n",
       "  'Römer adjacent station Charlotte',\n",
       "  'Narva adjacent station Buenos',\n",
       "  'Belgrad adjacent station Kita',\n",
       "  'Kapitel adjacent station Ayuntamiento',\n",
       "  'Gould adjacent station Zurich',\n",
       "  'Mafia adjacent station Valois',\n",
       "  'Colonel adjacent station Down',\n",
       "  'Welsh adjacent station Vega',\n",
       "  'Bruxelles adjacent station Noch',\n",
       "  'Laurie adjacent station Hogan',\n",
       "  'Albion adjacent station Omaha',\n",
       "  'Canton adjacent station Aloe',\n",
       "  'Sign adjacent station Arean',\n",
       "  'Italie adjacent station Clifford',\n",
       "  'Kathleen adjacent station Roche',\n",
       "  'Lexington adjacent station Borg',\n",
       "  'Bauer adjacent station Vista',\n",
       "  'ML adjacent station Allende',\n",
       "  'Minh adjacent station Hidden',\n",
       "  'Aleppo adjacent station Hulk',\n",
       "  'Jensen adjacent station Ashley',\n",
       "  'us adjacent station Landau',\n",
       "  'Slavic adjacent station Ryan',\n",
       "  'Seneca adjacent station Friendship',\n",
       "  'Coimbra adjacent station Starr',\n",
       "  'Bürger adjacent station Temps',\n",
       "  'Product adjacent station Reed',\n",
       "  'Dam adjacent station Valence',\n",
       "  'Benton adjacent station Romas',\n",
       "  'Amazon adjacent station Fantasy',\n",
       "  'Dorset adjacent station Dre',\n",
       "  'Greenland adjacent station Angela',\n",
       "  'Prayer adjacent station Od',\n",
       "  'Neustadt adjacent station Torpedo',\n",
       "  'Planet adjacent station Civilization',\n",
       "  'Anthem adjacent station Ayala',\n",
       "  'Gallimard adjacent station Lenny',\n",
       "  'JJ adjacent station Donald',\n",
       "  'Nichols adjacent station Sonora',\n",
       "  'Guy adjacent station Nora',\n",
       "  'Dorothea adjacent station MMA',\n",
       "  'Nash adjacent station UC',\n",
       "  'Rest adjacent station Trek',\n",
       "  'Trang adjacent station Simpson',\n",
       "  'Cine adjacent station Chelsea',\n",
       "  'Karya adjacent station Lilla',\n",
       "  'Godzilla adjacent station Kendall',\n",
       "  'Direito adjacent station WR',\n",
       "  'René adjacent station Mesa',\n",
       "  'Plateau adjacent station Rusi',\n",
       "  'Luca adjacent station VL',\n",
       "  'Eo adjacent station Blau',\n",
       "  'Marines adjacent station Cap',\n",
       "  'Orleans adjacent station Wolfe',\n",
       "  'Ewing adjacent station Swiss',\n",
       "  'Savoie adjacent station Ravenna',\n",
       "  'Hitchcock adjacent station Madagascar',\n",
       "  'Winter adjacent station Rada',\n",
       "  'Selle adjacent station Dwight',\n",
       "  'Lago adjacent station AAA',\n",
       "  'Palestina adjacent station Distance',\n",
       "  'Hunter adjacent station Bali',\n",
       "  'Saga adjacent station Gotham',\n",
       "  'Dell adjacent station Gino',\n",
       "  'Sarah adjacent station Homme',\n",
       "  'Taurus adjacent station Wilson',\n",
       "  'Corwin adjacent station Exeter',\n",
       "  'Reuters adjacent station Hanson',\n",
       "  'België adjacent station Een',\n",
       "  'Farrell adjacent station Sussex',\n",
       "  'Dolls adjacent station ANC',\n",
       "  'Shadow adjacent station Samo',\n",
       "  'India adjacent station Webster',\n",
       "  'Henri adjacent station Exodus',\n",
       "  'Dis adjacent station Carroll',\n",
       "  'UA adjacent station Frontier',\n",
       "  'Rocket adjacent station Babylon',\n",
       "  'Goran adjacent station Bangalore',\n",
       "  'Rutherford adjacent station Samen',\n",
       "  'Heine adjacent station Figures',\n",
       "  'EP adjacent station Oliva',\n",
       "  'GMA adjacent station Arsenal',\n",
       "  'Arias adjacent station Cochrane',\n",
       "  'Charter adjacent station Schumann',\n",
       "  'Wever adjacent station Rhode',\n",
       "  'Hispania adjacent station Shine',\n",
       "  'Vernon adjacent station TIME',\n",
       "  'Shore adjacent station Clair',\n",
       "  'Portuguesa adjacent station Moran',\n",
       "  'Rivière adjacent station Schleswig',\n",
       "  'Nantes adjacent station Potter',\n",
       "  'Huelva adjacent station Herzog',\n",
       "  'Acer adjacent station Aragon',\n",
       "  'Sint adjacent station Tonight',\n",
       "  'Wiesbaden adjacent station Sage',\n",
       "  'Americana adjacent station Sonny',\n",
       "  'Triple adjacent station U',\n",
       "  'Panama adjacent station Eternal',\n",
       "  'Sana adjacent station Dort',\n",
       "  'SE adjacent station Vocal',\n",
       "  'Martin adjacent station Toulouse',\n",
       "  'Banco adjacent station City',\n",
       "  'Harper adjacent station Gloucester',\n",
       "  'Goya adjacent station CRC',\n",
       "  'Fowler adjacent station Peters',\n",
       "  'Cuenca adjacent station Salzburg',\n",
       "  'Reise adjacent station Humboldt',\n",
       "  'Iso adjacent station Vana',\n",
       "  'Enterprise adjacent station Esprit',\n",
       "  'Welfare adjacent station Klasse',\n",
       "  'Mono adjacent station UCI',\n",
       "  'Ontario adjacent station Hodges',\n",
       "  'Cabrera adjacent station Qu',\n",
       "  'Basic adjacent station Gabon',\n",
       "  'Hughes adjacent station Copa',\n",
       "  'Sven adjacent station Wilton',\n",
       "  'Mask adjacent station Venezuela',\n",
       "  'Rt adjacent station Raoul',\n",
       "  'Tribune adjacent station Violin',\n",
       "  'Domínguez adjacent station Gonçalves',\n",
       "  'Scout adjacent station Lino',\n",
       "  'Latin adjacent station Policy',\n",
       "  'Messenger adjacent station Train',\n",
       "  'Höhe adjacent station Trent',\n",
       "  'Bildhauer adjacent station Steele',\n",
       "  'Kosova adjacent station Norton',\n",
       "  'Stuart adjacent station Orlando',\n",
       "  'Subway adjacent station TM',\n",
       "  'File adjacent station Yayasan',\n",
       "  'Break adjacent station Gilbert',\n",
       "  'Jeunesse adjacent station It',\n",
       "  'ITV adjacent station Galles',\n",
       "  'Cars adjacent station CAN',\n",
       "  'Edgar adjacent station Prema',\n",
       "  'Kort adjacent station Talent',\n",
       "  'Rus adjacent station Din',\n",
       "  'Ocean adjacent station Augusta',\n",
       "  'Figur adjacent station Josep',\n",
       "  'Rooney adjacent station Médaille',\n",
       "  'Gypsy adjacent station Beck',\n",
       "  'Cantor adjacent station Griffin',\n",
       "  'Atatürk adjacent station Jay',\n",
       "  'WDR adjacent station Carnival',\n",
       "  'Cruise adjacent station Napoca',\n",
       "  'Buffy adjacent station Joanne',\n",
       "  'Harcourt adjacent station Hai',\n",
       "  'Hornet adjacent station PCI',\n",
       "  'Reich adjacent station Sting',\n",
       "  'Parade adjacent station Dado',\n",
       "  'Selo adjacent station Wood',\n",
       "  'Landmark adjacent station Blade',\n",
       "  'Corona adjacent station Canyon',\n",
       "  'Martel adjacent station Mer',\n",
       "  'Weaver adjacent station Lincolnshire',\n",
       "  'Alvarado adjacent station Classics',\n",
       "  'Mississippi adjacent station Tanner',\n",
       "  'Conquest adjacent station Eddie',\n",
       "  'Aziz adjacent station Pest',\n",
       "  'Porto adjacent station Stad',\n",
       "  'Charity adjacent station Fashion',\n",
       "  'Somerset adjacent station Standing',\n",
       "  'Americano adjacent station Teater',\n",
       "  'Simona adjacent station SFR',\n",
       "  'Brussels adjacent station FCC',\n",
       "  'Centers adjacent station Kid',\n",
       "  'Assessment adjacent station A',\n",
       "  'Goiás adjacent station Sai',\n",
       "  'Corrientes adjacent station Ilha',\n",
       "  'Moor adjacent station Bulan',\n",
       "  'Lugano adjacent station II',\n",
       "  'Erika adjacent station Teddy',\n",
       "  'Porta adjacent station Argentine',\n",
       "  'Doubs adjacent station Panda',\n",
       "  'Eleanor adjacent station Toppen',\n",
       "  'Companion adjacent station FN',\n",
       "  'Asturias adjacent station Wesley',\n",
       "  'VV adjacent station Dach',\n",
       "  'Chat adjacent station Alman',\n",
       "  'Disneyland adjacent station Counter',\n",
       "  'Holden adjacent station Harris',\n",
       "  'Ono adjacent station Oakland',\n",
       "  'Bahía adjacent station Aten',\n",
       "  'Niagara adjacent station Zaman',\n",
       "  'Panorama adjacent station Gipfel',\n",
       "  'Balance adjacent station Sinai',\n",
       "  'Voogd adjacent station Fresno',\n",
       "  'Petit adjacent station Dunkerque',\n",
       "  'Ve adjacent station VIP',\n",
       "  'Sacra adjacent station Dracula',\n",
       "  'Pinto adjacent station Premier',\n",
       "  'Die adjacent station Anthony',\n",
       "  'Oliver adjacent station Armor',\n",
       "  'Ultima adjacent station Domain',\n",
       "  'Gill adjacent station Campos',\n",
       "  'Twilight adjacent station Gates',\n",
       "  'Region adjacent station ACC',\n",
       "  'Dolly adjacent station Bray',\n",
       "  'Emily adjacent station Marne',\n",
       "  'Godfrey adjacent station Béla',\n",
       "  'FC adjacent station Giro',\n",
       "  'Celle adjacent station Humbert',\n",
       "  'Lua adjacent station Hall',\n",
       "  'Alt adjacent station Morris',\n",
       "  'Grosso adjacent station Carry',\n",
       "  'Seus adjacent station Emergency',\n",
       "  'Escola adjacent station Benevento',\n",
       "  'Gina adjacent station Falkland',\n",
       "  'Monika adjacent station Strand',\n",
       "  'Line adjacent station Band',\n",
       "  'Antonia adjacent station Grupp',\n",
       "  'Norma adjacent station Am',\n",
       "  'Para adjacent station Chantal',\n",
       "  'Emma adjacent station PAL',\n",
       "  'Weimar adjacent station Gruppo',\n",
       "  'PE adjacent station Raum',\n",
       "  'Esch adjacent station Francesa',\n",
       "  'Sonntag adjacent station Inge',\n",
       "  'SDP adjacent station Akbar',\n",
       "  'Nelson adjacent station Weston',\n",
       "  'Sunda adjacent station Hemingway',\n",
       "  'Satellite adjacent station Nadir',\n",
       "  'EUA adjacent station Freedom',\n",
       "  'Das adjacent station Valenciana',\n",
       "  'Sofia adjacent station Fury',\n",
       "  'ARM adjacent station Cornell',\n",
       "  'Tato adjacent station Sisters',\n",
       "  'Sar adjacent station Tata',\n",
       "  'Damm adjacent station Scotland',\n",
       "  'Walden adjacent station Bend',\n",
       "  'ABD adjacent station Bandera',\n",
       "  'Ponte adjacent station Frères',\n",
       "  'Cincinnati adjacent station Genus',\n",
       "  'Montreux adjacent station Aid',\n",
       "  'Petersen adjacent station Sora',\n",
       "  'Monte adjacent station Züge',\n",
       "  'Dalla adjacent station Clarke',\n",
       "  'Assembly adjacent station Palmer',\n",
       "  'Turquia adjacent station Oscara',\n",
       "  'Libertad adjacent station Continental',\n",
       "  'Beast adjacent station Guerre',\n",
       "  'Ex adjacent station Rodney',\n",
       "  'Asylum adjacent station CAD',\n",
       "  'Middleton adjacent station Cheyenne',\n",
       "  'Paul adjacent station Italiana',\n",
       "  'Newton adjacent station André',\n",
       "  'ITF adjacent station Casablanca',\n",
       "  'Martini adjacent station Clements',\n",
       "  'CSS adjacent station Ale',\n",
       "  'Saints adjacent station Power',\n",
       "  'Cunningham adjacent station Hancock',\n",
       "  'Superior adjacent station Wayne',\n",
       "  'Edge adjacent station Lutz',\n",
       "  'Stab adjacent station CSU',\n",
       "  'Hallan adjacent station Comtat',\n",
       "  'Barn adjacent station Patterson',\n",
       "  'Hie adjacent station Sol',\n",
       "  'Longchamp adjacent station Rate',\n",
       "  'Thornton adjacent station Eun',\n",
       "  'Yo adjacent station Habana',\n",
       "  'Move adjacent station Daisy',\n",
       "  'Tennis adjacent station Straits',\n",
       "  'Jess adjacent station Cove',\n",
       "  'Bale adjacent station Neil',\n",
       "  'Leicester adjacent station Village',\n",
       "  'Giancarlo adjacent station Teil',\n",
       "  'Ruggiero adjacent station Uhr',\n",
       "  'Uni adjacent station Abigail',\n",
       "  'Pandora adjacent station Linda',\n",
       "  'Herrschaft adjacent station Sá',\n",
       "  'Bush adjacent station EC',\n",
       "  'Nama adjacent station Olympique',\n",
       "  'Morrison adjacent station Villeneuve',\n",
       "  'Formosa adjacent station Busch',\n",
       "  'Perl adjacent station Bride',\n",
       "  'Penelope adjacent station Bradford',\n",
       "  'Leben adjacent station Ponta',\n",
       "  'Karls adjacent station Oslo',\n",
       "  'Lys adjacent station Breizh',\n",
       "  'Trust adjacent station Sessions',\n",
       "  'Milano adjacent station Cheshire',\n",
       "  'Ley adjacent station Mercury',\n",
       "  'Bethlehem adjacent station Lori',\n",
       "  'CDP adjacent station Brewster',\n",
       "  'Riga adjacent station Wizard',\n",
       "  'Reeves adjacent station Aube',\n",
       "  'Gran adjacent station Riverside',\n",
       "  'Preto adjacent station Bologna',\n",
       "  'Flamengo adjacent station Foucault',\n",
       "  'Temple adjacent station Athen',\n",
       "  'Renaissance adjacent station Buzz',\n",
       "  'Damascus adjacent station Anne',\n",
       "  'Nagar adjacent station Rok',\n",
       "  'Jessica adjacent station Winnipeg',\n",
       "  'Lamar adjacent station Seni',\n",
       "  'Quito adjacent station MT',\n",
       "  'Gießen adjacent station Suci',\n",
       "  'Eylül adjacent station York',\n",
       "  'EF adjacent station Edwards',\n",
       "  'Elijah adjacent station Giant',\n",
       "  'Chip adjacent station Clint',\n",
       "  'Ronaldo adjacent station Borussia',\n",
       "  'Icarus adjacent station Kew',\n",
       "  'Sân adjacent station Siegel',\n",
       "  'Prague adjacent station Govern',\n",
       "  'Smile adjacent station Bos',\n",
       "  'Boulder adjacent station Laden',\n",
       "  'Air adjacent station Indiana',\n",
       "  'Gironde adjacent station Sites',\n",
       "  'Irene adjacent station Rory',\n",
       "  'Tim adjacent station Cu',\n",
       "  'can adjacent station Cuban',\n",
       "  'Essex adjacent station Creta',\n",
       "  'Perth adjacent station nt',\n",
       "  'Mod adjacent station Abbey',\n",
       "  'Amalia adjacent station Ducks',\n",
       "  'Jack adjacent station Montpellier',\n",
       "  'Imperi adjacent station Cyrus',\n",
       "  'Mainstream adjacent station Turin',\n",
       "  'Magazin adjacent station Ancona',\n",
       "  'Gama adjacent station Fuchs',\n",
       "  'Dust adjacent station Midland',\n",
       "  'Alicante adjacent station pad',\n",
       "  'Margareta adjacent station Hotel',\n",
       "  'Dialog adjacent station Aldo',\n",
       "  'Muñoz adjacent station Catch',\n",
       "  'Vosges adjacent station Vittoria',\n",
       "  'Lille adjacent station Ga',\n",
       "  'Membre adjacent station Wes',\n",
       "  'Citizen adjacent station Lang',\n",
       "  'Junta adjacent station Rodrigues',\n",
       "  'Carlton adjacent station Kassel',\n",
       "  'Nga adjacent station Ter',\n",
       "  'Johren adjacent station Nice',\n",
       "  'Enemy adjacent station Tank',\n",
       "  'Held adjacent station Burr',\n",
       "  'SMK adjacent station Muell',\n",
       "  'Isle adjacent station Seen',\n",
       "  'Vintage adjacent station Avatar',\n",
       "  'Prairie adjacent station Spain',\n",
       "  'Fernández adjacent station Banks',\n",
       "  'Berthold adjacent station Po',\n",
       "  'ECW adjacent station Cinderella',\n",
       "  'TCN adjacent station Section',\n",
       "  'Lost adjacent station Den',\n",
       "  'Chandler adjacent station Usher',\n",
       "  'Attack adjacent station Reilly',\n",
       "  'Buku adjacent station Browns',\n",
       "  'Us adjacent station Mouse',\n",
       "  'Flügel adjacent station Wakefield',\n",
       "  'Steiner adjacent station RJ',\n",
       "  'Humphrey adjacent station Belles',\n",
       "  'Lappland adjacent station Larsen',\n",
       "  'Avery adjacent station CAS',\n",
       "  'Folge adjacent station EE',\n",
       "  'Ova adjacent station Dix',\n",
       "  'Nell adjacent station Compton',\n",
       "  'Parigi adjacent station Xuân',\n",
       "  'Manor adjacent station Valea',\n",
       "  'Zwolle adjacent station String',\n",
       "  'Seri adjacent station Clin',\n",
       "  'Mariana adjacent station Castelo',\n",
       "  'Buch adjacent station Buda',\n",
       "  'Sheila adjacent station Madness',\n",
       "  'Independencia adjacent station Wire',\n",
       "  'Codex adjacent station Glover',\n",
       "  'Canadian adjacent station Chief',\n",
       "  'Norte adjacent station Malone',\n",
       "  'Pratt adjacent station Gaelic',\n",
       "  'Aires adjacent station Ser',\n",
       "  'Bayan adjacent station Jump',\n",
       "  'Diva adjacent station SAR',\n",
       "  'Buster adjacent station LP',\n",
       "  'Luther adjacent station Ho',\n",
       "  'mic adjacent station WP',\n",
       "  'Marco adjacent station Speyer',\n",
       "  'Manche adjacent station Fate',\n",
       "  'Potok adjacent station Maynard',\n",
       "  'Reihe adjacent station Pub',\n",
       "  'Salerno adjacent station Chambers',\n",
       "  'Segura adjacent station Phil',\n",
       "  'Church adjacent station Botafogo',\n",
       "  'Up adjacent station Yorkshire',\n",
       "  'Egypt adjacent station Eindhoven',\n",
       "  'Drum adjacent station Cá',\n",
       "  'Illinois adjacent station Stein',\n",
       "  'cal adjacent station LM',\n",
       "  'Cherry adjacent station Gerard',\n",
       "  'Sept adjacent station Maan',\n",
       "  'Première adjacent station Vid',\n",
       "  'Third adjacent station Gustav',\n",
       "  'Jozef adjacent station Luik',\n",
       "  'Trotz adjacent station Sinh',\n",
       "  'Haiti adjacent station RD',\n",
       "  'Tito adjacent station Houston',\n",
       "  'Hoffman adjacent station Jesús',\n",
       "  'Lodi adjacent station Matilda',\n",
       "  'Sérgio adjacent station RMS',\n",
       "  'Medalla adjacent station Câmara',\n",
       "  'Harrington adjacent station Zweck',\n",
       "  'Records adjacent station Cáceres',\n",
       "  'Sloan adjacent station Pinus',\n",
       "  'WBA adjacent station Pound',\n",
       "  'Tornado adjacent station Blair',\n",
       "  'Salon adjacent station Governor',\n",
       "  'Ya adjacent station Danny',\n",
       "  'Darmstadt adjacent station Mansfield',\n",
       "  'Quelle adjacent station Bud',\n",
       "  'UAE adjacent station Rond',\n",
       "  'ao adjacent station Savoia',\n",
       "  'Buchanan adjacent station Lieder',\n",
       "  'Fra adjacent station Sieger',\n",
       "  'Elias adjacent station Brabant',\n",
       "  'Billy adjacent station Manconi',\n",
       "  'McCoy adjacent station Brain',\n",
       "  'Grund adjacent station Pioneer',\n",
       "  'István adjacent station Illusion',\n",
       "  'Ramsey adjacent station Pamplona',\n",
       "  'Li adjacent station Leningrad',\n",
       "  'Bouchet adjacent station Identity',\n",
       "  'ABA adjacent station Spaans',\n",
       "  'Guerra adjacent station Florida',\n",
       "  'Bock adjacent station Colonia',\n",
       "  'Champs adjacent station Mol',\n",
       "  'Amerika adjacent station Like',\n",
       "  'Argentina adjacent station Peak',\n",
       "  'Grenoble adjacent station Bolton',\n",
       "  'Victor adjacent station River',\n",
       "  'Wanna adjacent station Orne',\n",
       "  'Niger adjacent station Marshal',\n",
       "  'Dentro adjacent station Wien',\n",
       "  'Consulta adjacent station Lewis',\n",
       "  'Echo adjacent station Beaufort',\n",
       "  'Era adjacent station Republic',\n",
       "  'Riot adjacent station Molde',\n",
       "  'Forma adjacent station Speedway',\n",
       "  'Bintang adjacent station Indre',\n",
       "  'Pearl adjacent station VA',\n",
       "  'Selena adjacent station Stevenson',\n",
       "  'Magnus adjacent station Sally',\n",
       "  'Sedan adjacent station Rolling',\n",
       "  'Astra adjacent station Grimm',\n",
       "  'Bachelor adjacent station Savoy',\n",
       "  'ATC adjacent station MJ',\n",
       "  'Stevie adjacent station Sve',\n",
       "  'avoir adjacent station Joshua',\n",
       "  'Condado adjacent station Cea',\n",
       "  'Swing adjacent station Bacon',\n",
       "  'Guadalajara adjacent station Jerome',\n",
       "  'Wiley adjacent station Captain',\n",
       "  'Byl adjacent station Legend',\n",
       "  'Capitol adjacent station Schiller',\n",
       "  'Occitanie adjacent station Ark',\n",
       "  'Rosa adjacent station Vries',\n",
       "  'Marlon adjacent station Johansen',\n",
       "  'Industry adjacent station Jänner',\n",
       "  'Tao adjacent station Savage',\n",
       "  'Monster adjacent station Apollo',\n",
       "  'Roubaix adjacent station Setiap',\n",
       "  'Shock adjacent station Parkinson',\n",
       "  'Dresden adjacent station Belo',\n",
       "  'Studie adjacent station Common',\n",
       "  'Dyke adjacent station Mesto',\n",
       "  'Cada adjacent station Stockholm',\n",
       "  'Wings adjacent station Sante',\n",
       "  'Freeman adjacent station Hand',\n",
       "  'Ses adjacent station Surface',\n",
       "  'Vide adjacent station CBS',\n",
       "  'Lugo adjacent station Ross',\n",
       "  'Animals adjacent station Mean',\n",
       "  'Seo adjacent station KS',\n",
       "  'Rioja adjacent station Regio',\n",
       "  'Diablo adjacent station Grand',\n",
       "  'Salix adjacent station Olav',\n",
       "  'Watkins adjacent station Valeria',\n",
       "  'Wakil adjacent station Sevilla',\n",
       "  'Melbourne adjacent station Maximus',\n",
       "  'Bund adjacent station Stadt',\n",
       "  'Ward adjacent station Florencia',\n",
       "  'Solo adjacent station Sunrise',\n",
       "  'Flag adjacent station Göran',\n",
       "  'Progress adjacent station Pulau',\n",
       "  'Lillehammer adjacent station Terra',\n",
       "  'Winston adjacent station Demi',\n",
       "  'Crisis adjacent station Via',\n",
       "  'Étoile adjacent station Coburg',\n",
       "  'Windsor adjacent station Guarda',\n",
       "  'Porter adjacent station Ar',\n",
       "  'Tampa adjacent station Sanders',\n",
       "  'Kensington adjacent station Punjabi',\n",
       "  'Peter adjacent station Secretary',\n",
       "  'Hull adjacent station Rootsi',\n",
       "  'Selma adjacent station Gare',\n",
       "  'Neptune adjacent station Olive',\n",
       "  'Rover adjacent station McDonnell',\n",
       "  'Buta adjacent station Hitler',\n",
       "  'Lleida adjacent station Assassin',\n",
       "  'AVN adjacent station Puis',\n",
       "  'Scream adjacent station Marathon',\n",
       "  'Congo adjacent station Sandy',\n",
       "  'AND adjacent station Writer',\n",
       "  'Egon adjacent station Trondheim',\n",
       "  'Stadion adjacent station Nou',\n",
       "  'Aa adjacent station SS',\n",
       "  'Shepherd adjacent station Hutton',\n",
       "  'Wisdom adjacent station Henley',\n",
       "  'VI adjacent station Sinaloa',\n",
       "  'Ventura adjacent station Kelvin',\n",
       "  'October adjacent station Mifflin',\n",
       "  'Risk adjacent station Sartre',\n",
       "  'SMA adjacent station Artemis',\n",
       "  'Brigada adjacent station Triumph',\n",
       "  'Janssen adjacent station Poco',\n",
       "  'Rune adjacent station Attila',\n",
       "  'Cordillera adjacent station Sagan',\n",
       "  'Clarence adjacent station Malang',\n",
       "  'Twente adjacent station Io',\n",
       "  'Armstrong adjacent station Loch',\n",
       "  'Dara adjacent station Coffee',\n",
       "  'Brief adjacent station Whitaker',\n",
       "  'Dayton adjacent station Siemens',\n",
       "  'UFC adjacent station Sia',\n",
       "  'Free adjacent station Keller',\n",
       "  'Haley adjacent station Asunción',\n",
       "  'Stefano adjacent station Fermi',\n",
       "  'Sari adjacent station Firth',\n",
       "  'Riau adjacent station Peso',\n",
       "  'Dietmar adjacent station Volume',\n",
       "  'Web adjacent station Gloria',\n",
       "  'Salvia adjacent station Cary',\n",
       "  'DP adjacent station Classe',\n",
       "  'Beatrice adjacent station JNA',\n",
       "  'Glory adjacent station Valls',\n",
       "  'Male adjacent station IP',\n",
       "  'Guitar adjacent station Weir',\n",
       "  'Shankar adjacent station Irwin',\n",
       "  'Eve adjacent station CT',\n",
       "  'Druga adjacent station Guevara',\n",
       "  'NRW adjacent station White',\n",
       "  'Rusko adjacent station Lina',\n",
       "  'Acre adjacent station Barnes',\n",
       "  'HD adjacent station Sardinia',\n",
       "  'WWF adjacent station Siegfried',\n",
       "  'ET adjacent station Peer',\n",
       "  'Birth adjacent station Cidade',\n",
       "  'Audio adjacent station Hamilton',\n",
       "  'McLaughlin adjacent station Mille',\n",
       "  'Gift adjacent station Durango',\n",
       "  'Dakar adjacent station Toten',\n",
       "  'Alfonso adjacent station Coast',\n",
       "  'CA adjacent station Cambridge',\n",
       "  'Worcester adjacent station Kanal',\n",
       "  'Vallée adjacent station Welt',\n",
       "  'Lodge adjacent station Papp',\n",
       "  'Demon adjacent station Bulgaria',\n",
       "  'Larva adjacent station Brüder',\n",
       "  'Waiting adjacent station ESPN',\n",
       "  'Nisan adjacent station Lincoln',\n",
       "  'Avalon adjacent station Sänger',\n",
       "  'Marius adjacent station Dende',\n",
       "  'Hütte adjacent station Nacht',\n",
       "  'Za adjacent station Jagger',\n",
       "  'Clayton adjacent station Twins',\n",
       "  'Ninh adjacent station Craig',\n",
       "  'Transfer adjacent station Planck',\n",
       "  'BT adjacent station Torres',\n",
       "  'Ying adjacent station Adi',\n",
       "  'Leif adjacent station Woman',\n",
       "  'Engine adjacent station Pro',\n",
       "  'Yokohama adjacent station NWA',\n",
       "  'Kjell adjacent station CAF',\n",
       "  'Plato adjacent station Hidalgo',\n",
       "  'IX adjacent station Ito',\n",
       "  'Server adjacent station Magnum',\n",
       "  'Krone adjacent station Zeit',\n",
       "  'Mill adjacent station Hause',\n",
       "  'Blog adjacent station Suit',\n",
       "  'Mustang adjacent station Divine',\n",
       "  'Guanajuato adjacent station Grad',\n",
       "  'Town adjacent station Paterson',\n",
       "  'Brand adjacent station Tonga',\n",
       "  'Wild adjacent station Camille',\n",
       "  'Wo adjacent station Snyder',\n",
       "  'Bunker adjacent station Loeb',\n",
       "  'Germany adjacent station Dorothy',\n",
       "  'Far adjacent station Dudley',\n",
       "  'Sitt adjacent station Steel',\n",
       "  'Killer adjacent station Fulton',\n",
       "  'Memoria adjacent station Lucia',\n",
       "  'Blanca adjacent station Head',\n",
       "  'Lance adjacent station Danh',\n",
       "  'Ortiz adjacent station Namibia',\n",
       "  'Kirche adjacent station USSR',\n",
       "  'Esperanza adjacent station Today',\n",
       "  'Dat adjacent station González',\n",
       "  'Ashton adjacent station Carioca',\n",
       "  'TN adjacent station Arm',\n",
       "  'Butte adjacent station Norway',\n",
       "  'Gender adjacent station Bombay',\n",
       "  'Bola adjacent station NO',\n",
       "  'Fenner adjacent station Jessie',\n",
       "  'Galiza adjacent station Luftwaffe',\n",
       "  'Moyen adjacent station HR',\n",
       "  'Marina adjacent station Charleston',\n",
       "  ...]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict = {'text': test}\n",
    "train_dict = {'text': train}\n",
    "train_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "First, we pad text so they are a uniform length. While it is possible to padtext in the tokenizer function by setting padding=True, it is more efficient to only pad the text to the length of the longest element in its batch. This is known as dynamic padding. You can do this with the DataCollatorWithPadding function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_dict(train_dict)\n",
    "test_ds = Dataset.from_dict(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 58000\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, BertTokenizerFast, TrainingArguments, Trainer, DataCollatorWithPadding, BertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc76df1c08014014a585a1ef38924555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 58000\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_train_ds = train_ds.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 45763, 32018, 11825, 39752, 102]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_ds[1][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3c1f29f8c949ccb4c9ce097779edcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test_ds = test_ds.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "\n",
    "# metric = load_metric(\"accuracy\")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return {metric.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] Richardson adjacent station Diesel [SEP]'\n",
      "\n",
      "'>>> [CLS] Ranger adjacent station [MASK] [SEP]'\n",
      "\n",
      "'>>> [CLS] Sicily [MASK] station CJ [SEP]'\n",
      "\n",
      "'>>> [CLS] [MASK] adjacent station Mur [SEP]'\n"
     ]
    }
   ],
   "source": [
    "samples = [tokenized_train_ds[i] for i in range(4)]\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(for 10, 2000 it works too)\n",
    "\n",
    "70+ epochs\n",
    "\n",
    "num_train_epochs=1000,\n",
    "per_device_train_batch_size=128,\n",
    "per_device_eval_batch_size=128,\n",
    "learning_rate=5e-5,\n",
    "logging_strategy='epoch',\n",
    "evaluation_strategy='epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune mBERT\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model/symmetry-english-10-2000-both\",\n",
    "    num_train_epochs=1000,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=2,\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='epoch',\n",
    "    evaluation_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 58000\n",
      "  Num Epochs = 1000\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 227000\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15610' max='227000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 15610/227000 2:09:30 < 29:14:06, 2.01 it/s, Epoch 68.76/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.804000</td>\n",
       "      <td>3.431826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.448300</td>\n",
       "      <td>3.427988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.373200</td>\n",
       "      <td>3.235138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.367700</td>\n",
       "      <td>3.242977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.328800</td>\n",
       "      <td>3.388909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.376800</td>\n",
       "      <td>3.219639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.337100</td>\n",
       "      <td>3.255847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.340800</td>\n",
       "      <td>3.272967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.342000</td>\n",
       "      <td>3.238272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.355500</td>\n",
       "      <td>3.157695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.339800</td>\n",
       "      <td>3.227064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.315000</td>\n",
       "      <td>3.248582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.345700</td>\n",
       "      <td>2.882231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.329300</td>\n",
       "      <td>3.352484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.312400</td>\n",
       "      <td>3.244365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.262800</td>\n",
       "      <td>3.317813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.333700</td>\n",
       "      <td>3.269009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.310600</td>\n",
       "      <td>3.242637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.327600</td>\n",
       "      <td>3.329908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.332000</td>\n",
       "      <td>3.107167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.318800</td>\n",
       "      <td>3.102122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.319000</td>\n",
       "      <td>3.320339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.292600</td>\n",
       "      <td>3.115072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.323000</td>\n",
       "      <td>3.220298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.325200</td>\n",
       "      <td>3.204286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.270100</td>\n",
       "      <td>3.172949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.283500</td>\n",
       "      <td>3.369508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.284400</td>\n",
       "      <td>3.292397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.262200</td>\n",
       "      <td>3.349583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.240200</td>\n",
       "      <td>3.142037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.182800</td>\n",
       "      <td>3.154582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.156600</td>\n",
       "      <td>3.012324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.133800</td>\n",
       "      <td>3.196855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.053500</td>\n",
       "      <td>3.062746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.073800</td>\n",
       "      <td>3.079489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.990900</td>\n",
       "      <td>3.177416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.926900</td>\n",
       "      <td>2.910722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.899800</td>\n",
       "      <td>3.016383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.880000</td>\n",
       "      <td>2.888731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.826800</td>\n",
       "      <td>3.077470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.777000</td>\n",
       "      <td>3.090065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.730600</td>\n",
       "      <td>3.012042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.727500</td>\n",
       "      <td>2.859643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.696300</td>\n",
       "      <td>2.854410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.601600</td>\n",
       "      <td>2.762104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.572600</td>\n",
       "      <td>2.953705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.513900</td>\n",
       "      <td>2.888019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.462700</td>\n",
       "      <td>2.855353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.418900</td>\n",
       "      <td>2.731468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.376900</td>\n",
       "      <td>2.913354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.354100</td>\n",
       "      <td>2.681953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.291200</td>\n",
       "      <td>2.796496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.272800</td>\n",
       "      <td>2.482103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.259000</td>\n",
       "      <td>2.544018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.162200</td>\n",
       "      <td>2.554812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.135400</td>\n",
       "      <td>2.575515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.098500</td>\n",
       "      <td>2.353055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.081800</td>\n",
       "      <td>2.394132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.027500</td>\n",
       "      <td>2.353066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.985200</td>\n",
       "      <td>2.417967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.952200</td>\n",
       "      <td>2.235824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.907200</td>\n",
       "      <td>2.052032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.901700</td>\n",
       "      <td>2.411788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.868900</td>\n",
       "      <td>1.925918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.803100</td>\n",
       "      <td>1.981052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.790200</td>\n",
       "      <td>1.950769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.762500</td>\n",
       "      <td>1.838686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.766800</td>\n",
       "      <td>1.775470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-227\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-227/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-227/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-227/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-227/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-454\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-454/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-454/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-454/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-454/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-681\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-681/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-681/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-681/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-681/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-227] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-908\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-908/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-908/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-908/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-908/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-454] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-1135\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-1135/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-1135/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-1135/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-1135/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-681] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-1362\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-1362/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-1362/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-1362/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-1362/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-908] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-1589\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-1589/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-1589/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-1589/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-1589/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-1135] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-1816\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-1816/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-1816/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-1816/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-1816/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-1362] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-2043\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-2043/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-2043/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-2043/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-2043/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-1589] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-2270\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-2270/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-2270/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-2270/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-2270/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-1816] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-2497\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-2497/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-2497/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-2497/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-2497/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-2043] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-2724\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-2724/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-2724/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-2724/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-2724/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-2270] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-2951\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-2951/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-2951/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-2951/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-2951/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-2497] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-3178\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-3178/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-3178/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-3178/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-3178/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-2724] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-3405\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-3405/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-3405/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-3405/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-3405/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-2951] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-3632\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-3632/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-3632/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-3632/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-3632/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-3178] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-3859\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-3859/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-3859/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-3859/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-3859/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-3405] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-4086\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-4086/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-4086/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-4086/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-4086/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-3632] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-4313\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-4313/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-4313/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-4313/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-4313/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-3859] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-4540\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-4540/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-4540/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-4540/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-4540/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-4086] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-4767\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-4767/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-4767/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-4767/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-4767/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-4313] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-4994\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-4994/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-4994/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-4994/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-4994/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-4540] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-5221\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-5221/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-5221/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-5221/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-5221/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-4767] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-5448\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-5448/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-5448/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-5448/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-5448/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-4994] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-5675\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-5675/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-5675/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-5675/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-5675/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-5221] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-5902\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-5902/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-5902/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-5902/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-5902/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-5448] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-6129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-6129/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-6129/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-6129/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-6129/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-5675] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-6356\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-6356/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-6356/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-6356/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-6356/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-5902] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-6583\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-6583/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-6583/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-6583/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-6583/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-6129] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-6810\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-6810/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-6810/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-6810/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-6810/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-6356] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-7037\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-7037/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-7037/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-7037/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-7037/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-6583] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-7264\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-7264/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-7264/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-7264/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-7264/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-6810] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-7491\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-7491/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-7491/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-7491/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-7491/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-7037] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-7718\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-7718/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-7718/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-7718/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-7718/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-7264] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-7945\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-7945/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-7945/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-7945/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-7945/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-7491] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-8172\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-8172/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-8172/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-8172/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-8172/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-7718] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-8399\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-8399/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-8399/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-8399/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-8399/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-7945] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-8626\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-8626/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-8626/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-8626/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-8626/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-8172] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-8853\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-8853/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-8853/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-8853/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-8853/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-8399] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-9080\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-9080/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-9080/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-9080/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-9080/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-8626] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-9307\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-9307/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-9307/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-9307/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-9307/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-8853] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-9534\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-9534/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-9534/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-9534/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-9534/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-9080] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-9761\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-9761/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-9761/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-9761/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-9761/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-9307] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-9988\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-9988/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-9988/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-9988/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-9988/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-9534] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-10215\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-10215/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-10215/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-10215/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-10215/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-9761] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-10442\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-10442/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-10442/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-10442/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-10442/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-9988] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-10669\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-10669/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-10669/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-10669/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-10669/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-10215] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-10896\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-10896/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-10896/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-10896/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-10896/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-10442] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-11123\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-11123/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-11123/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-11123/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-11123/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-10669] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-11350\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-11350/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-11350/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-11350/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-11350/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-10896] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-11577\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-11577/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-11577/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-11577/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-11577/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-11123] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-11804\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-11804/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-11804/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-11804/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-11804/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-11350] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-12031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-12031/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-12031/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-12031/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-12031/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-11577] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-12258\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-12258/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-12258/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-12258/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-12258/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-11804] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-12485\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-12485/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-12485/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-12485/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-12485/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-12031] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-12712\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-12712/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-12712/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-12712/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-12712/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-12258] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-12939\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-12939/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-12939/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-12939/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-12939/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-12485] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-13166\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-13166/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-13166/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-13166/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-13166/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-12712] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-13393\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-13393/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-13393/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-13393/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-13393/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-12939] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-13620\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-13620/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-13620/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-13620/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-13620/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-13166] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-13847\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-13847/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-13847/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-13847/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-13847/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-13393] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-14074\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-14074/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-14074/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-14074/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-14074/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-13620] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-14301\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-14301/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-14301/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-14301/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-14301/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-13847] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-14528\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-14528/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-14528/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-14528/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-14528/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-14074] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-14755\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-14755/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-14755/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-14755/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-14755/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-14301] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-14982\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-14982/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-14982/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-14982/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-14982/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-14528] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-15209\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-15209/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-15209/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-15209/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-15209/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-14755] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to ./model/symmetry-english-10-2000-both/checkpoint-15436\n",
      "Configuration saved in ./model/symmetry-english-10-2000-both/checkpoint-15436/config.json\n",
      "Model weights saved in ./model/symmetry-english-10-2000-both/checkpoint-15436/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/symmetry-english-10-2000-both/checkpoint-15436/tokenizer_config.json\n",
      "Special tokens file saved in ./model/symmetry-english-10-2000-both/checkpoint-15436/special_tokens_map.json\n",
      "Deleting older checkpoint [model/symmetry-english-10-2000-both/checkpoint-14982] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/transformers/trainer.py:1367\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1365\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1368\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1371\u001b[0m ):\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7275204658508301}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:1 k:1\n",
      "i:2 k:2\n",
      "i:3 k:3\n",
      "i:4 k:4\n",
      "i:5 k:5\n",
      "i:6 k:6\n",
      "i:7 k:7\n",
      "i:8 k:8\n",
      "i:9 k:9\n",
      "i:10 k:10\n",
      "i:11 k:11\n",
      "i:12 k:12\n",
      "i:13 k:13\n",
      "i:14 k:14\n",
      "i:15 k:15\n",
      "i:16 k:16\n",
      "i:17 k:17\n",
      "i:18 k:18\n",
      "i:19 k:19\n",
      "i:20 k:20\n",
      "i:21 k:21\n",
      "i:22 k:22\n",
      "i:23 k:23\n",
      "i:24 k:24\n",
      "i:25 k:25\n",
      "i:26 k:26\n",
      "i:27 k:27\n",
      "i:28 k:28\n",
      "i:29 k:29\n",
      "i:30 k:30\n",
      "i:31 k:31\n",
      "i:32 k:32\n",
      "i:33 k:33\n",
      "i:34 k:34\n",
      "i:35 k:35\n",
      "i:36 k:36\n",
      "i:37 k:37\n",
      "i:38 k:38\n",
      "i:39 k:39\n",
      "i:40 k:40\n",
      "i:41 k:41\n",
      "i:42 k:42\n",
      "i:43 k:43\n",
      "i:44 k:44\n",
      "i:45 k:45\n",
      "i:46 k:46\n",
      "i:47 k:47\n",
      "i:48 k:48\n",
      "i:49 k:49\n",
      "i:50 k:50\n",
      "i:51 k:51\n",
      "i:52 k:52\n",
      "i:53 k:53\n",
      "i:54 k:54\n",
      "i:55 k:55\n",
      "i:56 k:56\n",
      "i:57 k:57\n",
      "i:58 k:58\n",
      "i:59 k:59\n",
      "i:60 k:60\n",
      "i:61 k:61\n",
      "i:62 k:62\n",
      "i:63 k:63\n",
      "i:64 k:64\n",
      "i:65 k:65\n",
      "i:66 k:66\n",
      "i:67 k:67\n",
      "i:68 k:68\n",
      "i:69 k:69\n",
      "i:70 k:70\n",
      "i:71 k:71\n",
      "i:72 k:72\n",
      "i:73 k:73\n",
      "i:74 k:74\n",
      "i:75 k:75\n",
      "i:76 k:76\n",
      "i:77 k:77\n",
      "i:78 k:78\n",
      "i:79 k:79\n",
      "i:80 k:80\n",
      "i:81 k:81\n",
      "i:82 k:82\n",
      "i:83 k:83\n",
      "i:84 k:84\n",
      "i:85 k:85\n",
      "i:86 k:86\n",
      "i:87 k:87\n",
      "i:88 k:88\n",
      "i:89 k:89\n",
      "i:90 k:90\n",
      "i:91 k:91\n",
      "i:92 k:92\n",
      "i:93 k:93\n",
      "i:94 k:94\n",
      "i:95 k:95\n",
      "i:96 k:96\n",
      "i:97 k:97\n",
      "i:98 k:98\n",
      "i:99 k:99\n",
      "i:100 k:100\n",
      "i:101 k:101\n",
      "i:102 k:102\n",
      "i:103 k:103\n",
      "i:104 k:104\n",
      "i:105 k:105\n",
      "i:106 k:106\n",
      "i:107 k:107\n",
      "i:108 k:108\n",
      "i:109 k:109\n",
      "i:110 k:110\n",
      "i:111 k:111\n",
      "i:112 k:112\n",
      "i:113 k:113\n",
      "i:114 k:114\n",
      "i:115 k:115\n",
      "i:116 k:116\n",
      "i:117 k:117\n",
      "i:118 k:118\n",
      "i:119 k:119\n",
      "i:120 k:120\n",
      "i:121 k:121\n",
      "i:122 k:122\n",
      "i:123 k:123\n",
      "i:124 k:124\n",
      "i:125 k:125\n",
      "i:126 k:126\n",
      "i:127 k:127\n",
      "i:128 k:128\n",
      "i:129 k:129\n",
      "i:130 k:130\n",
      "i:131 k:131\n",
      "i:132 k:132\n",
      "i:133 k:133\n",
      "i:134 k:134\n",
      "i:135 k:135\n",
      "i:136 k:136\n",
      "i:137 k:137\n",
      "i:138 k:138\n",
      "i:139 k:139\n",
      "i:140 k:140\n",
      "i:141 k:141\n",
      "i:142 k:142\n",
      "i:143 k:143\n",
      "i:144 k:144\n",
      "i:145 k:145\n",
      "i:146 k:146\n",
      "i:147 k:147\n",
      "i:148 k:148\n",
      "i:149 k:149\n",
      "i:150 k:150\n",
      "i:151 k:151\n",
      "i:152 k:152\n",
      "i:153 k:153\n",
      "i:154 k:154\n",
      "i:155 k:155\n",
      "i:156 k:156\n",
      "i:157 k:157\n",
      "i:158 k:158\n",
      "i:159 k:159\n",
      "i:160 k:160\n",
      "i:161 k:161\n",
      "i:162 k:162\n",
      "i:163 k:163\n",
      "i:164 k:164\n",
      "i:165 k:165\n",
      "i:166 k:166\n",
      "i:167 k:167\n",
      "i:168 k:168\n",
      "i:169 k:169\n",
      "i:170 k:170\n",
      "i:171 k:171\n",
      "i:172 k:172\n",
      "i:173 k:173\n",
      "i:174 k:174\n",
      "i:175 k:175\n",
      "i:176 k:176\n",
      "i:177 k:177\n",
      "i:178 k:178\n",
      "i:179 k:179\n",
      "i:180 k:180\n",
      "i:181 k:181\n",
      "i:182 k:182\n",
      "i:183 k:183\n",
      "i:184 k:184\n",
      "i:185 k:185\n",
      "i:186 k:186\n",
      "i:187 k:187\n",
      "i:188 k:188\n",
      "i:189 k:189\n",
      "i:190 k:190\n",
      "i:191 k:191\n",
      "i:192 k:192\n",
      "i:193 k:193\n",
      "i:194 k:194\n",
      "i:195 k:195\n",
      "i:196 k:196\n",
      "i:197 k:197\n",
      "i:198 k:198\n",
      "i:199 k:199\n",
      "i:200 k:200\n",
      "i:201 k:201\n",
      "i:202 k:202\n",
      "i:203 k:203\n",
      "i:204 k:204\n",
      "i:205 k:205\n",
      "i:206 k:206\n",
      "i:207 k:207\n",
      "i:208 k:208\n",
      "i:209 k:209\n",
      "i:210 k:210\n",
      "i:211 k:211\n",
      "i:212 k:212\n",
      "i:213 k:213\n",
      "i:214 k:214\n",
      "i:215 k:215\n",
      "i:216 k:216\n",
      "i:217 k:217\n",
      "i:218 k:218\n",
      "i:219 k:219\n",
      "i:220 k:220\n",
      "i:221 k:221\n",
      "i:222 k:222\n",
      "i:223 k:223\n",
      "i:224 k:224\n",
      "i:225 k:225\n",
      "i:226 k:226\n",
      "i:227 k:227\n",
      "i:228 k:228\n",
      "i:229 k:229\n",
      "i:230 k:230\n",
      "i:231 k:231\n",
      "i:232 k:232\n",
      "i:233 k:233\n",
      "i:234 k:234\n",
      "i:235 k:235\n",
      "i:236 k:236\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [121]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m label_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(txt\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     12\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(sample, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m token_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoded_input\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     15\u001b[0m mask_token_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(encoded_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mmask_token_id)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     16\u001b[0m mask_token_logits \u001b[38;5;241m=\u001b[39m token_logits[\u001b[38;5;241m0\u001b[39m, mask_token_index, :]\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1358\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1343\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[1;32m   1344\u001b[0m     input_ids,\n\u001b[1;32m   1345\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1354\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1355\u001b[0m )\n\u001b[1;32m   1357\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1358\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:685\u001b[0m, in \u001b[0;36mBertOnlyMLMHead.forward\u001b[0;34m(self, sequence_output)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sequence_output):\n\u001b[0;32m--> 685\u001b[0m     prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction_scores\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:675\u001b[0m, in \u001b[0;36mBertLMPredictionHead.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    674\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(hidden_states)\n\u001b[0;32m--> 675\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "total = len(train_dict['text'])\n",
    "i = 0\n",
    "\n",
    "for txt in train_dict['text'][:10000]:\n",
    "    i += 1\n",
    "    \n",
    "    # Add [MASK] for object\n",
    "    sample = txt.rsplit(' ', 1)[0] + ' [MASK]'\n",
    "    label_token = tokenizer.convert_tokens_to_ids(txt.rsplit(' ', 1)[1])\n",
    "    \n",
    "    encoded_input = tokenizer(sample, return_tensors='pt')\n",
    "    token_logits = model(**encoded_input).logits\n",
    "    \n",
    "    mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    \n",
    "    # Pick the [MASK] candidates with the highest logits\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "    \n",
    "    if label_token in top_5_tokens:\n",
    "        k += 1\n",
    "        print('i:' + str(i) + ' k:' + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangor index case of Friends',\n",
       " 'Aarhus index case of Gan',\n",
       " 'Linus index case of Princesa',\n",
       " 'Valence index case of Ziele',\n",
       " 'Olga index case of Genoa',\n",
       " 'Pepper index case of DK',\n",
       " 'Helena index case of Daimler',\n",
       " 'Chronicle index case of Zee',\n",
       " 'Fermi index case of Contra',\n",
       " 'Ying index case of Abby',\n",
       " 'Elke index case of Eliza',\n",
       " 'Nieto index case of Porsche',\n",
       " 'Herrschaft index case of Ratu',\n",
       " 'Train index case of Aqua',\n",
       " 'Freie index case of Fontainebleau',\n",
       " 'Alabama index case of Papa',\n",
       " 'Greenwich index case of Crow',\n",
       " 'Bahasa index case of Ranger',\n",
       " 'Bildhauer index case of String',\n",
       " 'Padova index case of Moldavia',\n",
       " 'Cochrane index case of Clements',\n",
       " 'Rachel index case of Bruxelles',\n",
       " 'Clay index case of Panda',\n",
       " 'Squadron index case of Linné',\n",
       " 'Bears index case of Hercules',\n",
       " 'Rollins index case of Rodrigues',\n",
       " 'Eugenio index case of Export',\n",
       " 'Lagoa index case of Navarra',\n",
       " 'Hero index case of Mateo',\n",
       " 'Modena index case of Gamma',\n",
       " 'Arnold index case of Brett',\n",
       " 'Perdana index case of Lilla',\n",
       " 'Germania index case of Studie',\n",
       " 'Stadio index case of Tako',\n",
       " 'Zbigniew index case of Alcalá',\n",
       " 'Bez index case of Arbor',\n",
       " 'Engagement index case of Haag',\n",
       " 'Nightmare index case of Liban',\n",
       " 'Sens index case of WBC',\n",
       " 'Deputy index case of Ibiza',\n",
       " 'Juara index case of Kamen',\n",
       " 'Vidal index case of Merah',\n",
       " 'Clarkson index case of Havilland',\n",
       " 'Bursa index case of Orten',\n",
       " 'Title index case of Wood',\n",
       " 'Army index case of EUA',\n",
       " 'Damascus index case of MLS',\n",
       " 'Brabant index case of Little',\n",
       " 'Vita index case of PGC',\n",
       " 'Ruth index case of Lincolnshire',\n",
       " 'Dominic index case of Pierce',\n",
       " 'Strike index case of Wiesbaden',\n",
       " 'Crimea index case of Dollar',\n",
       " 'Eugène index case of Roll',\n",
       " 'Muslimani index case of Essay',\n",
       " 'Entangled index case of Rebel',\n",
       " 'Everest index case of Penny',\n",
       " 'Croatia index case of Hector',\n",
       " 'Tucumán index case of Schlacht',\n",
       " 'Thompson index case of Alus',\n",
       " 'Ludovic index case of Camino',\n",
       " 'Quest index case of Ken',\n",
       " 'Sia index case of AZ',\n",
       " 'Ta index case of Idaho',\n",
       " 'RFC index case of Swansea',\n",
       " 'Kern index case of Più',\n",
       " 'Tudor index case of Neuen',\n",
       " 'Attila index case of Manga',\n",
       " 'Ne index case of Justin',\n",
       " 'Márquez index case of SNP',\n",
       " 'JK index case of Murphy',\n",
       " 'Nike index case of AG',\n",
       " 'Eaton index case of Mount',\n",
       " 'Julian index case of Stakes',\n",
       " 'Jasper index case of Madison',\n",
       " 'Mercer index case of Rabbi',\n",
       " 'Piemonte index case of Escape',\n",
       " 'Sitze index case of Iglesia',\n",
       " 'Evan index case of VV',\n",
       " 'Namun index case of Wimbledon',\n",
       " 'Amalia index case of Franz',\n",
       " 'Root index case of Coimbra',\n",
       " 'Oro index case of Welles',\n",
       " 'Louvain index case of Canton',\n",
       " 'Garrett index case of Pike',\n",
       " 'Midlands index case of Venezia',\n",
       " 'Sturm index case of Mitt',\n",
       " 'GAD index case of Hora',\n",
       " 'Endre index case of Mona',\n",
       " 'Lec index case of Tin',\n",
       " 'Omer index case of Savoia',\n",
       " 'Sven index case of Cunha',\n",
       " 'Adana index case of Staat',\n",
       " 'PhD index case of Molina',\n",
       " 'Rhode index case of Castell',\n",
       " 'Dancing index case of Kristen',\n",
       " 'Rady index case of Blackburn',\n",
       " 'Ella index case of PSD',\n",
       " 'Bean index case of Staff',\n",
       " 'Suomi index case of epi',\n",
       " 'Francesca index case of Gates',\n",
       " 'Dent index case of Wyoming',\n",
       " 'CP index case of Nové',\n",
       " 'Sementara index case of Burns',\n",
       " 'Marathon index case of Selim',\n",
       " 'Janssen index case of Somali',\n",
       " 'Bosna index case of Occidental',\n",
       " 'Alta index case of MJ',\n",
       " 'Fischer index case of Palo',\n",
       " 'KK index case of Ike',\n",
       " 'Stift index case of Natalie',\n",
       " 'Varese index case of Junction',\n",
       " 'VOC index case of Kiel',\n",
       " 'Ela index case of Neil',\n",
       " 'Raquel index case of Nirvana',\n",
       " 'Dracula index case of Seas',\n",
       " 'Qua index case of Gordon',\n",
       " 'SMA index case of Assassin',\n",
       " 'TC index case of BP',\n",
       " 'RN index case of Médaille',\n",
       " 'Sama index case of Wola',\n",
       " 'Lai index case of Clare',\n",
       " 'Bayreuth index case of Orléans',\n",
       " 'Rioja index case of CFA',\n",
       " 'Bristol index case of Wolfe',\n",
       " 'Uno index case of Magnus',\n",
       " 'Jesu index case of Elf',\n",
       " 'Thomson index case of Vigo',\n",
       " 'Siegen index case of Oceania',\n",
       " 'DDR index case of Cuba',\n",
       " 'Ferro index case of Samba',\n",
       " 'Bend index case of Voice',\n",
       " 'Ted index case of Sagan',\n",
       " 'Provence index case of Eski',\n",
       " 'Versailles index case of Parte',\n",
       " 'Linh index case of Schubert',\n",
       " 'Qui index case of Gamble',\n",
       " 'Sultan index case of Oliveira',\n",
       " 'Band index case of Mariana',\n",
       " 'Denmark index case of Charlton',\n",
       " 'Namibia index case of Province',\n",
       " 'Kap index case of Editorial',\n",
       " 'Roads index case of Monitor',\n",
       " 'Mutter index case of Reggio',\n",
       " 'Valls index case of Brighton',\n",
       " 'Wien index case of Vieira',\n",
       " 'Baird index case of Gill',\n",
       " 'Margarita index case of Peace',\n",
       " 'Camera index case of Partner',\n",
       " 'rs index case of Pampa',\n",
       " 'Peu index case of Otte',\n",
       " 'Palatinat index case of Toy',\n",
       " 'Rektor index case of Vittoria',\n",
       " 'Stalingrad index case of Belo',\n",
       " 'Pisa index case of Eros',\n",
       " 'OS index case of Welt',\n",
       " 'Santana index case of Wars',\n",
       " 'Oko index case of Yüksek',\n",
       " 'Dolly index case of Venecia',\n",
       " 'Neustadt index case of Phi',\n",
       " 'U index case of Würzburg',\n",
       " 'Kant index case of Salem',\n",
       " 'Lapangan index case of Brisbane',\n",
       " 'Sanskrit index case of Buffy',\n",
       " 'Alice index case of Lamarck',\n",
       " 'Penelope index case of Room',\n",
       " 'Gerd index case of Rada',\n",
       " 'Basic index case of Lightning',\n",
       " 'CD index case of CG',\n",
       " 'Manche index case of BBC',\n",
       " 'Pays index case of Mosca',\n",
       " 'Latin index case of College',\n",
       " 'GC index case of Roosevelt',\n",
       " 'Ira index case of FIS',\n",
       " 'Samoa index case of Sakura',\n",
       " 'Silva index case of Dead',\n",
       " 'Seigneur index case of Line',\n",
       " 'Cha index case of Ga',\n",
       " 'Kensley index case of Kobe',\n",
       " 'Rosemary index case of Application',\n",
       " 'Violin index case of God',\n",
       " 'Passo index case of Late',\n",
       " 'Goodman index case of Questa',\n",
       " 'Dictionary index case of Faust',\n",
       " 'Sana index case of Kepler',\n",
       " 'Iberia index case of Koska',\n",
       " 'EV index case of Kohl',\n",
       " 'Cats index case of SP',\n",
       " 'Stern index case of Beaver',\n",
       " 'Area index case of Salud',\n",
       " 'NDR index case of Starr',\n",
       " 'Hansen index case of AN',\n",
       " 'Men index case of Attack',\n",
       " 'Llobregat index case of Kosova',\n",
       " 'Pod index case of Ar',\n",
       " 'Cassini index case of Fashion',\n",
       " 'Leben index case of Rowling',\n",
       " 'Brod index case of Ex',\n",
       " 'Access index case of Kraft',\n",
       " 'Wo index case of Baza',\n",
       " 'Merlin index case of Elliott',\n",
       " 'Famous index case of Gand',\n",
       " 'Einer index case of Information',\n",
       " 'Gud index case of Eo',\n",
       " 'Jenkins index case of Zeichen',\n",
       " 'Teacher index case of Utama',\n",
       " 'Boyd index case of Ancien',\n",
       " 'Sick index case of Governo',\n",
       " 'Wayne index case of Keller',\n",
       " 'ABS index case of Liga',\n",
       " 'Dresden index case of Poker',\n",
       " 'Burr index case of Livingston',\n",
       " 'IV index case of Giancarlo',\n",
       " 'Belgrano index case of Bay',\n",
       " 'Nathaniel index case of Danh',\n",
       " 'Vida index case of Nagasaki',\n",
       " 'Beaumont index case of Bach',\n",
       " 'Venus index case of Juárez',\n",
       " 'Anjou index case of WWF',\n",
       " 'Potosí index case of Köppen',\n",
       " 'Tracy index case of Piero',\n",
       " 'Sign index case of DD',\n",
       " 'Vinyl index case of Sân',\n",
       " 'Baby index case of Rossa',\n",
       " 'Austrian index case of Barry',\n",
       " 'Forst index case of UCI',\n",
       " 'Thành index case of Dewan',\n",
       " 'Village index case of Acer',\n",
       " 'Kat index case of Kuna',\n",
       " 'Evans index case of Horizon',\n",
       " 'Phil index case of Madsen',\n",
       " 'Plymouth index case of Torpedo',\n",
       " 'Bride index case of Highland',\n",
       " 'ol index case of Friendship',\n",
       " 'Buster index case of Gerry',\n",
       " 'Piccolo index case of Shire',\n",
       " 'Pre index case of Rooms',\n",
       " 'Perl index case of Virgil',\n",
       " 'Mundi index case of Whitehead',\n",
       " 'Thánh index case of Cabo',\n",
       " 'Phelps index case of Lucien',\n",
       " 'Kids index case of Robson',\n",
       " 'SDP index case of Noi',\n",
       " 'Willow index case of Bila',\n",
       " 'Tibet index case of Oklahoma',\n",
       " 'UCD index case of Movies',\n",
       " 'Voltaire index case of Mystic',\n",
       " 'Noord index case of Worldwide',\n",
       " 'Así index case of Bernstein',\n",
       " 'Druga index case of Siemens',\n",
       " 'Gallagher index case of Angels',\n",
       " 'Toulon index case of Opening',\n",
       " 'Poland index case of Flesh',\n",
       " 'Vivaldi index case of Runde',\n",
       " 'Chor index case of Larsen',\n",
       " 'Bridge index case of Bas',\n",
       " 'Bombardier index case of Miki',\n",
       " 'Shock index case of Vide',\n",
       " 'Ulster index case of Noble',\n",
       " 'Diamond index case of Pepe',\n",
       " 'Alben index case of Petersen',\n",
       " 'Nino index case of Jensen',\n",
       " 'Tex index case of Santo',\n",
       " 'USSR index case of Montréal',\n",
       " 'Islands index case of Esto',\n",
       " 'Battalion index case of Patrol',\n",
       " 'Normandy index case of Suez',\n",
       " 'Cup index case of Sede',\n",
       " 'Shake index case of Victoria',\n",
       " 'Lech index case of Camden',\n",
       " 'Ale index case of Schoolmeesters',\n",
       " 'Case index case of Middlesex',\n",
       " 'Bengal index case of Gera',\n",
       " 'Orte index case of Yesterday',\n",
       " 'Cafe index case of Maxim',\n",
       " 'lec index case of Florida',\n",
       " 'Systema index case of Florencia',\n",
       " 'Königsberg index case of CAF',\n",
       " 'Partido index case of Martin',\n",
       " 'Faye index case of Wizard',\n",
       " 'Giants index case of Pico',\n",
       " 'Roberta index case of Giles',\n",
       " 'RTS index case of Mata',\n",
       " 'Ito index case of Maybe',\n",
       " 'Zoom index case of AIM',\n",
       " 'Trento index case of Salto',\n",
       " 'Matthews index case of AP',\n",
       " 'Baja index case of Stick',\n",
       " 'Chaco index case of Franklin',\n",
       " 'Polk index case of Sánchez',\n",
       " 'Fisher index case of CW',\n",
       " 'Richmond index case of Marie',\n",
       " 'Stream index case of Criminal',\n",
       " 'Revolution index case of Prairie',\n",
       " 'Beauty index case of Gloria',\n",
       " 'Birch index case of Iso',\n",
       " 'Hartley index case of Sabha',\n",
       " 'Legend index case of Allende',\n",
       " 'Volk index case of Cindy',\n",
       " 'Ng index case of Chaos',\n",
       " 'Vacelet index case of Haas',\n",
       " 'Florence index case of Vene',\n",
       " 'Kors index case of Sutton',\n",
       " 'Paese index case of Papp',\n",
       " 'Molde index case of Soc',\n",
       " 'Deborah index case of Bandet',\n",
       " 'Davies index case of Harcourt',\n",
       " 'Katarina index case of Dad',\n",
       " 'Osborne index case of Aten',\n",
       " 'Gravity index case of Underground',\n",
       " 'Anderson index case of Czech',\n",
       " 'Moonlight index case of Kira',\n",
       " 'Ever index case of Tina',\n",
       " 'Lua index case of MS',\n",
       " 'Soleil index case of Thiên',\n",
       " 'Celia index case of Banner',\n",
       " 'Audio index case of Maigret',\n",
       " 'Emanuel index case of Boury',\n",
       " 'Prayer index case of Uni',\n",
       " 'Devon index case of Augusti',\n",
       " 'Chance index case of Edgar',\n",
       " 'Arcadia index case of Della',\n",
       " 'Hip index case of Sharks',\n",
       " 'Carson index case of Reynolds',\n",
       " 'CRC index case of Kaisers',\n",
       " 'Weston index case of Poitou',\n",
       " 'Nihon index case of Bono',\n",
       " 'Berge index case of Hang',\n",
       " 'CA index case of Laden',\n",
       " 'Here index case of Baru',\n",
       " 'Brain index case of Uetz',\n",
       " 'Nicolas index case of SMP',\n",
       " 'Wiener index case of Christopher',\n",
       " 'tu index case of Shakespeare',\n",
       " 'Atari index case of Geiger',\n",
       " 'Aix index case of Viking',\n",
       " 'Arras index case of Jungen',\n",
       " 'White index case of Insel',\n",
       " 'Jalisco index case of Jeffries',\n",
       " 'Medvedev index case of Stig',\n",
       " 'Adam index case of LLC',\n",
       " 'GRN index case of Enemy',\n",
       " 'Lecce index case of Mia',\n",
       " 'Copper index case of Leary',\n",
       " 'Hammond index case of Maximilian',\n",
       " 'Campbell index case of Steinicke',\n",
       " 'Imperi index case of Shaun',\n",
       " 'Series index case of Nicholson',\n",
       " 'Kaplan index case of Autumn',\n",
       " 'RD index case of Nikolaj',\n",
       " 'Griffin index case of Speyer',\n",
       " 'LC index case of Ark',\n",
       " 'Kenia index case of SN',\n",
       " 'Veronica index case of Los',\n",
       " 'Margaretha index case of Lego',\n",
       " 'Ve index case of Hartford',\n",
       " 'tip index case of PMC',\n",
       " 'Sá index case of Counter',\n",
       " 'Ranier index case of Edit',\n",
       " 'Barker index case of Hradec',\n",
       " 'Governor index case of König',\n",
       " 'Tanz index case of Rams',\n",
       " 'Culture index case of Cine',\n",
       " 'Tiet index case of Lake',\n",
       " 'DNS index case of Observatoire',\n",
       " 'Kulon index case of Battlefield',\n",
       " 'Playboy index case of Casablanca',\n",
       " 'JJ index case of Regia',\n",
       " 'Bright index case of Stevenson',\n",
       " 'Borough index case of GPU',\n",
       " 'Archer index case of Ia',\n",
       " 'Guerra index case of Lakes',\n",
       " 'Hispania index case of Purcell',\n",
       " 'Venice index case of Alison',\n",
       " 'Guadalajara index case of Ekim',\n",
       " 'Jacobi index case of Liv',\n",
       " 'Came index case of Bullet',\n",
       " 'Heads index case of Paradise',\n",
       " 'Waiting index case of Marina',\n",
       " 'Ille index case of Boxer',\n",
       " 'Cast index case of Gerard',\n",
       " 'Kabul index case of RPG',\n",
       " 'Gießen index case of Mod',\n",
       " 'ac index case of Charter',\n",
       " 'Philadelphia index case of Shawn',\n",
       " 'Buffalo index case of Roberto',\n",
       " 'Songs index case of Consulta',\n",
       " 'Drake index case of Eva',\n",
       " 'Scots index case of Chancellor',\n",
       " 'Manager index case of Lager',\n",
       " 'Marea index case of Minden',\n",
       " 'Argentina index case of Barth',\n",
       " 'Shore index case of Beloe',\n",
       " 'Innocent index case of Boxing',\n",
       " 'Colón index case of Aiken',\n",
       " 'Schröder index case of Mini',\n",
       " 'TT index case of Comet',\n",
       " 'Egypt index case of NE',\n",
       " 'Monza index case of pr',\n",
       " 'Spieler index case of Excel',\n",
       " 'Stat index case of Irena',\n",
       " 'Coma index case of Cessna',\n",
       " 'Nama index case of Eiffel',\n",
       " 'Jepang index case of Ove',\n",
       " 'Faber index case of Alexandria',\n",
       " 'Rumble index case of Muñoz',\n",
       " 'Armenia index case of Isole',\n",
       " 'Kew index case of Steiner',\n",
       " 'Steele index case of Coppa',\n",
       " 'Beer index case of Domínguez',\n",
       " 'Parkway index case of Johren',\n",
       " 'Sprecher index case of Carnegie',\n",
       " 'Belize index case of Jessie',\n",
       " 'Trend index case of Hate',\n",
       " 'Cola index case of Sprint',\n",
       " 'TX index case of Coelho',\n",
       " 'Cervantes index case of Angelina',\n",
       " 'Parque index case of NSV',\n",
       " 'Guns index case of Ardèche',\n",
       " 'Knowles index case of McGill',\n",
       " 'Ty index case of Rookie',\n",
       " 'Cesar index case of Fulda',\n",
       " 'Eternal index case of Castro',\n",
       " 'Shoes index case of Southampton',\n",
       " 'Oosten index case of Farmer',\n",
       " 'RAF index case of Caribe',\n",
       " 'Teluk index case of Hügel',\n",
       " 'Oracle index case of Ribera',\n",
       " 'McCoy index case of Sporting',\n",
       " 'Gotland index case of Sting',\n",
       " 'Valea index case of Command',\n",
       " 'Brand index case of Bishop',\n",
       " 'jet index case of Racine',\n",
       " 'Cá index case of Tierra',\n",
       " 'Java index case of Krüger',\n",
       " 'Hell index case of Hessen',\n",
       " 'Volga index case of FR',\n",
       " 'Som index case of Loving',\n",
       " 'Vokal index case of Archives',\n",
       " 'Fou index case of AR',\n",
       " 'Clayton index case of Reeves',\n",
       " 'Salman index case of Enter',\n",
       " 'Hedwig index case of Sisters',\n",
       " 'Gardner index case of Hepburn',\n",
       " 'Pedra index case of XXI',\n",
       " 'Limoges index case of Om',\n",
       " 'Ante index case of Osman',\n",
       " 'Testament index case of Diane',\n",
       " 'Rico index case of Maryland',\n",
       " 'Gosse index case of Back',\n",
       " 'Helga index case of Story',\n",
       " 'Palmer index case of Nile',\n",
       " 'Reichenbach index case of Rodríguez',\n",
       " 'Griffith index case of Burma',\n",
       " 'Vanderbilt index case of Superliga',\n",
       " 'Frey index case of Seymour',\n",
       " 'ABD index case of Temps',\n",
       " 'Fauna index case of Lur',\n",
       " 'Porter index case of Rok',\n",
       " 'Osiris index case of Colombo',\n",
       " 'Yunan index case of LSD',\n",
       " 'Product index case of Henrik',\n",
       " 'Reeder index case of Gloucester',\n",
       " 'Contreras index case of Potok',\n",
       " 'UD index case of Trung',\n",
       " 'Quintana index case of Ulysses',\n",
       " 'Creta index case of Guy',\n",
       " 'Origin index case of Allison',\n",
       " 'Zwar index case of Kennedy',\n",
       " 'Milo index case of Devlet',\n",
       " 'Genus index case of Abucay',\n",
       " 'Sora index case of Borgo',\n",
       " 'Artur index case of Nowe',\n",
       " 'Dorset index case of Como',\n",
       " 'Robaina index case of Houston',\n",
       " 'Leigh index case of Danmark',\n",
       " 'ESA index case of Angel',\n",
       " 'Potter index case of Court',\n",
       " 'KC index case of Colts',\n",
       " 'Action index case of Cáceres',\n",
       " 'Dort index case of ag',\n",
       " 'Spurs index case of Gipfel',\n",
       " 'Imperium index case of Parigi',\n",
       " 'Grad index case of Rise',\n",
       " 'Crescent index case of Encore',\n",
       " 'Fields index case of Narva',\n",
       " 'Brent index case of Dalton',\n",
       " 'Kaunas index case of Alte',\n",
       " 'Rose index case of Prusia',\n",
       " 'Joe index case of Hearts',\n",
       " 'Côte index case of Sullivan',\n",
       " 'Heroes index case of Dupont',\n",
       " 'Vi index case of Patton',\n",
       " 'Break index case of Teatro',\n",
       " 'Petra index case of Borges',\n",
       " 'Eduard index case of Latino',\n",
       " 'Ph index case of especialista',\n",
       " 'Lands index case of Guimarães',\n",
       " 'Nordland index case of Frost',\n",
       " 'Mei index case of Émile',\n",
       " 'Flow index case of Reason',\n",
       " 'Stil index case of Medal',\n",
       " 'Ut index case of Police',\n",
       " 'Grammar index case of Arte',\n",
       " 'Bohemia index case of Nièvre',\n",
       " 'Draft index case of Dag',\n",
       " 'MM index case of Jesús',\n",
       " 'Farm index case of Neu',\n",
       " 'can index case of Ponte',\n",
       " 'Eye index case of Busch',\n",
       " 'Firma index case of Dora',\n",
       " 'Erin index case of Housing',\n",
       " 'Car index case of KM',\n",
       " 'Parti index case of ATR',\n",
       " 'Em index case of Linda',\n",
       " 'Sunrise index case of Afrika',\n",
       " 'Freddie index case of Sabina',\n",
       " 'Galatasaray index case of Conway',\n",
       " 'Clifford index case of Limerick',\n",
       " 'Regional index case of Better',\n",
       " 'Inspector index case of Canyon',\n",
       " 'Vis index case of Rapport',\n",
       " 'Capo index case of nr',\n",
       " 'Cell index case of Humanos',\n",
       " 'Long index case of Limited',\n",
       " 'Watson index case of Mussolini',\n",
       " 'Photography index case of Broadway',\n",
       " 'Bolton index case of Arean',\n",
       " 'Zeeland index case of Nissan',\n",
       " 'Schleswig index case of Fox',\n",
       " 'Nevada index case of Besar',\n",
       " 'Hugo index case of Dortmund',\n",
       " 'Hütte index case of Ramsey',\n",
       " 'Salt index case of Morales',\n",
       " 'Suba index case of Cherry',\n",
       " 'bd index case of Mask',\n",
       " 'Apollo index case of Jesus',\n",
       " 'Beatrice index case of EN',\n",
       " 'Luxemburg index case of Batu',\n",
       " 'Blair index case of Amiga',\n",
       " 'Ennen index case of Ans',\n",
       " 'Dawn index case of Ketika',\n",
       " 'Miles index case of Olav',\n",
       " 'Mountain index case of VP',\n",
       " 'Algeria index case of Semi',\n",
       " 'Kita index case of Marruecos',\n",
       " 'Ewing index case of pk',\n",
       " 'Magna index case of Lopes',\n",
       " 'Coleman index case of Jørgensen',\n",
       " 'Jay index case of Colbert',\n",
       " 'Twente index case of Give',\n",
       " 'Warrior index case of Même',\n",
       " 'Mint index case of Cecil',\n",
       " 'Strand index case of Cedar',\n",
       " 'Römer index case of Edge',\n",
       " 'Spezia index case of Sunda',\n",
       " 'Ipswich index case of Za',\n",
       " 'Sonia index case of Urbino',\n",
       " 'Stefano index case of Tri',\n",
       " 'Raymond index case of Reyes',\n",
       " 'Meiji index case of Conquest',\n",
       " 'Seele index case of Briggs',\n",
       " 'Ny index case of Same',\n",
       " 'Sent index case of Daughter',\n",
       " 'Ice index case of Erfurt',\n",
       " 'Aladin index case of Jubilee',\n",
       " 'Grove index case of Paramount',\n",
       " 'Ronnie index case of Hidden',\n",
       " 'Ole index case of Orchestra',\n",
       " 'Saguenay index case of Henri',\n",
       " 'Christophe index case of Kota',\n",
       " 'Marge index case of Greg',\n",
       " 'Carl index case of Korn',\n",
       " 'Hall index case of Arlington',\n",
       " 'Bordeaux index case of Lodi',\n",
       " 'Gelo index case of Matilda',\n",
       " 'October index case of Tyrone',\n",
       " 'Lublin index case of Westminster',\n",
       " 'Temple index case of Tax',\n",
       " 'Président index case of Dell',\n",
       " 'Bernard index case of ville',\n",
       " 'Kobayashi index case of Chez',\n",
       " 'Greatest index case of Williamson',\n",
       " 'Alba index case of FF',\n",
       " 'PHP index case of Coffee',\n",
       " 'Vierge index case of Fribourg',\n",
       " 'Dublin index case of Killer',\n",
       " 'Marija index case of Saab',\n",
       " 'Zeus index case of River',\n",
       " 'Eugenia index case of Jänner',\n",
       " 'NSW index case of Ninh',\n",
       " 'Tennis index case of Carvalho',\n",
       " 'Siege index case of Yukon',\n",
       " 'Vertreter index case of Dane',\n",
       " 'George index case of Sébastien',\n",
       " 'Savannah index case of Gets',\n",
       " 'GM index case of Label',\n",
       " 'Parco index case of Rayon',\n",
       " 'Bleu index case of Comte',\n",
       " 'Pilar index case of Christie',\n",
       " 'Piano index case of Montagne',\n",
       " 'Farrell index case of OP',\n",
       " 'Niels index case of Space',\n",
       " 'Saba index case of BB',\n",
       " 'cat index case of BC',\n",
       " 'Palm index case of Marks',\n",
       " 'Onthophagus index case of Virus',\n",
       " 'ANC index case of Estudios',\n",
       " 'Cu index case of Tower',\n",
       " 'Mühle index case of SBS',\n",
       " 'Noch index case of Lost',\n",
       " 'Bey index case of Welsh',\n",
       " 'Glee index case of Corso',\n",
       " 'Stock index case of ACT',\n",
       " 'Dame index case of Dada',\n",
       " 'Signal index case of Amber',\n",
       " 'Salon index case of Sally',\n",
       " 'Oviedo index case of Crosby',\n",
       " 'Polonia index case of Pretoria',\n",
       " 'Quantum index case of Agency',\n",
       " 'Paderborn index case of Herder',\n",
       " 'Piper index case of Bonnet',\n",
       " 'Guanajuato index case of Pe',\n",
       " 'Pure index case of Genocide',\n",
       " 'Senegal index case of Hübner',\n",
       " 'Greenland index case of Panzer',\n",
       " 'Firefox index case of Kurt',\n",
       " 'Crisis index case of Certain',\n",
       " 'Herschel index case of Mallorca',\n",
       " 'Ames index case of Regio',\n",
       " 'Lille index case of Shark',\n",
       " 'Accademia index case of Monterey',\n",
       " 'Crash index case of Ligne',\n",
       " 'Kampung index case of Holger',\n",
       " 'Format index case of Pace',\n",
       " 'Butte index case of Montpellier',\n",
       " 'Klub index case of Colonial',\n",
       " 'Chandler index case of Sai',\n",
       " 'Ages index case of Spot',\n",
       " 'Lions index case of Dorothea',\n",
       " 'Gilbert index case of Falling',\n",
       " 'Philippe index case of Pal',\n",
       " 'Ehemann index case of Thiele',\n",
       " 'Pitt index case of Glen',\n",
       " 'Billy index case of Aurora',\n",
       " 'Bale index case of Chaplin',\n",
       " 'MVP index case of CDU',\n",
       " 'Étienne index case of Pasadena',\n",
       " 'Carla index case of Brasileira',\n",
       " 'Exeter index case of Emergency',\n",
       " 'Waterloo index case of Beth',\n",
       " 'Toro index case of Büyük',\n",
       " 'TM index case of Belém',\n",
       " 'Coro index case of Jaroslav',\n",
       " 'VI index case of Haley',\n",
       " 'Eagle index case of Primer',\n",
       " 'Dol index case of Goldstein',\n",
       " 'Shuttle index case of MSN',\n",
       " 'Tripoli index case of Lord',\n",
       " 'Vitoria index case of Cicero',\n",
       " 'Sigma index case of Boulevard',\n",
       " 'Clan index case of Wells',\n",
       " 'Svizzera index case of Strange',\n",
       " 'Sumatra index case of Golden',\n",
       " 'Rua index case of Smile',\n",
       " 'Garden index case of Fel',\n",
       " 'Ping index case of Saskatchewan',\n",
       " 'Princeton index case of Spencer',\n",
       " 'Tanner index case of Becker',\n",
       " 'Solid index case of Berlin',\n",
       " 'Mead index case of Pusat',\n",
       " 'Shirley index case of Kostel',\n",
       " 'BT index case of Holstein',\n",
       " 'Gandhi index case of Demi',\n",
       " 'PAL index case of Od',\n",
       " 'Fletcher index case of Bilbao',\n",
       " 'WDR index case of Chem',\n",
       " 'uns index case of School',\n",
       " 'Cave index case of Miranda',\n",
       " 'NFC index case of MAC',\n",
       " 'Sinaloa index case of Ferreira',\n",
       " 'Zug index case of Faith',\n",
       " 'Pegasus index case of Nations',\n",
       " 'Randolph index case of Teater',\n",
       " 'APG index case of Bassa',\n",
       " 'Václav index case of Ad',\n",
       " 'Wittenberg index case of Garrison',\n",
       " 'Nico index case of Isabel',\n",
       " 'Seri index case of Meta',\n",
       " 'Rutherford index case of Impact',\n",
       " 'Abigail index case of Islander',\n",
       " 'Tartu index case of Bumi',\n",
       " 'Urgell index case of Labour',\n",
       " 'Quatre index case of Shackleton',\n",
       " 'Thái index case of Doom',\n",
       " 'Lorena index case of Brass',\n",
       " 'Bola index case of Tra',\n",
       " 'Docteur index case of Jerusalén',\n",
       " 'Benjamin index case of Mineral',\n",
       " 'Davida index case of Soto',\n",
       " 'Baxter index case of Ryder',\n",
       " 'Ordu index case of Rembrandt',\n",
       " 'Wellington index case of Seven',\n",
       " 'Campo index case of Lily',\n",
       " 'Benedict index case of Eu',\n",
       " 'Mau index case of Bari',\n",
       " 'Reflections index case of Inge',\n",
       " 'Stockholm index case of Circus',\n",
       " 'Triple index case of Hoya',\n",
       " 'MMA index case of Ducks',\n",
       " 'Hurt index case of Yahoo',\n",
       " 'Marcus index case of Naomi',\n",
       " 'Madeira index case of Stab',\n",
       " 'Dow index case of Salix',\n",
       " 'Classics index case of Parker',\n",
       " 'Donovan index case of Ultra',\n",
       " 'Havana index case of EC',\n",
       " 'Ardenne index case of Emilia',\n",
       " 'Hockey index case of Hyderabad',\n",
       " 'Townsend index case of Hoffman',\n",
       " 'Garde index case of Mister',\n",
       " 'Summer index case of Ticino',\n",
       " 'Napoleon index case of Fortuna',\n",
       " 'Feld index case of Remote',\n",
       " 'Wakefield index case of Lucia',\n",
       " 'Town index case of Turnier',\n",
       " 'Mendelssohn index case of Flag',\n",
       " 'Raider index case of Lourenço',\n",
       " 'Fredrik index case of Te',\n",
       " 'Sept index case of Sox',\n",
       " 'VfB index case of Vergine',\n",
       " 'Goes index case of Amy',\n",
       " 'Kálmán index case of Baix',\n",
       " 'Boeing index case of Tore',\n",
       " 'Tada index case of Marlborough',\n",
       " 'Lyons index case of Colegio',\n",
       " 'Lleida index case of Card',\n",
       " 'Moran index case of Tanjung',\n",
       " 'McDonald index case of Mick',\n",
       " 'Haus index case of Maas',\n",
       " 'Isles index case of Ellis',\n",
       " 'Daniels index case of Alger',\n",
       " 'Lisbon index case of MTA',\n",
       " 'Hallan index case of Robot',\n",
       " 'Hara index case of Maa',\n",
       " 'Delgado index case of Fever',\n",
       " 'Falun index case of Peso',\n",
       " 'Minor index case of Champions',\n",
       " 'Sergei index case of Chicago',\n",
       " 'SS index case of Firenze',\n",
       " 'Congress index case of Margareta',\n",
       " 'co index case of Beyoncé',\n",
       " 'Safe index case of Actor',\n",
       " 'CSU index case of Avery',\n",
       " 'Haziran index case of Hus',\n",
       " 'Oman index case of Cisco',\n",
       " 'Ed index case of McCarthy',\n",
       " 'Arizona index case of Dôme',\n",
       " 'Verdi index case of Linia',\n",
       " 'Grass index case of Géza',\n",
       " 'Athen index case of GMT',\n",
       " 'Racing index case of Hit',\n",
       " 'Stadion index case of Ward',\n",
       " 'Tato index case of Character',\n",
       " 'Gromada index case of Trust',\n",
       " 'Nato index case of Mansfield',\n",
       " 'Montreux index case of Christ',\n",
       " 'Bel index case of Dmitri',\n",
       " 'Cromwell index case of Wings',\n",
       " 'Monsieur index case of Breizh',\n",
       " 'Tirol index case of Sonora',\n",
       " 'Gateway index case of Barbus',\n",
       " 'Merkel index case of Troy',\n",
       " 'Hannah index case of Boyle',\n",
       " 'Market index case of AD',\n",
       " 'Gross index case of Harvest',\n",
       " 'Acre index case of Velvet',\n",
       " 'Pride index case of XII',\n",
       " 'PIB index case of MGM',\n",
       " 'Pers index case of Silvio',\n",
       " 'Werner index case of Gina',\n",
       " 'Kelley index case of Tala',\n",
       " 'Luke index case of Sit',\n",
       " 'Salvia index case of Orion',\n",
       " 'Alzheimer index case of Alan',\n",
       " 'María index case of Ka',\n",
       " 'Hey index case of Carol',\n",
       " 'Bosco index case of Barcellona',\n",
       " 'Grimaldi index case of PL',\n",
       " 'Register index case of Ship',\n",
       " 'Kim index case of Yer',\n",
       " 'Jameson index case of Verder',\n",
       " 'Powell index case of Rat',\n",
       " 'RE index case of Mer',\n",
       " 'Transit index case of Petit',\n",
       " 'Pieces index case of Investment',\n",
       " 'Spirit index case of Stratford',\n",
       " 'Compostela index case of Brown',\n",
       " 'Girls index case of Bund',\n",
       " 'Linden index case of Kort',\n",
       " 'Sage index case of Norman',\n",
       " 'Axis index case of Knox',\n",
       " 'Cigliano index case of RS',\n",
       " 'ao index case of Magister',\n",
       " 'Clifton index case of Aloe',\n",
       " 'Ur index case of Cause',\n",
       " 'Malaya index case of Follow',\n",
       " 'Seine index case of Nas',\n",
       " 'Mittel index case of Shows',\n",
       " 'Masa index case of Seni',\n",
       " 'Visa index case of Liberation',\n",
       " 'NME index case of Put',\n",
       " 'Calder index case of Mason',\n",
       " 'Bulan index case of Chorus',\n",
       " 'Martini index case of Isola',\n",
       " 'Indigenous index case of Oleh',\n",
       " 'UE index case of Prima',\n",
       " 'Niger index case of Pero',\n",
       " 'Vizier index case of Animal',\n",
       " 'CPU index case of Hancock',\n",
       " 'Darwin index case of April',\n",
       " 'Emma index case of Coll',\n",
       " 'CAS index case of Lucas',\n",
       " 'Estes index case of Sharon',\n",
       " 'Bodø index case of Bent',\n",
       " 'Brock index case of Hume',\n",
       " 'Kahn index case of Treviso',\n",
       " 'Ace index case of Strada',\n",
       " 'Dee index case of PM',\n",
       " 'Li index case of Romans',\n",
       " 'Semarang index case of te',\n",
       " 'Gaon index case of Neal',\n",
       " 'Winkler index case of Princess',\n",
       " 'Lagos index case of Hub',\n",
       " 'Butterfly index case of Salazar',\n",
       " 'Tatiana index case of Free',\n",
       " 'Calhoun index case of Herbst',\n",
       " 'Outlook index case of Bee',\n",
       " 'Monika index case of École',\n",
       " 'Trainer index case of Pearl',\n",
       " 'Turm index case of Bab',\n",
       " 'Robertson index case of Charleston',\n",
       " 'Frank index case of Somerset',\n",
       " 'Alliance index case of Kirby',\n",
       " 'Frente index case of Beast',\n",
       " 'Olympia index case of Urbana',\n",
       " 'Chiara index case of Cosmos',\n",
       " 'Mario index case of Albatros',\n",
       " 'Napoli index case of Wales',\n",
       " 'Huelva index case of Delhi',\n",
       " 'AAA index case of Cargo',\n",
       " 'IEC index case of Romance',\n",
       " 'Batista index case of Shepherd',\n",
       " 'Scoble index case of Musica',\n",
       " 'Solomon index case of Ferns',\n",
       " 'Antoinette index case of England',\n",
       " 'Abel index case of Stéphane',\n",
       " 'Straßburg index case of Gideon',\n",
       " 'THE index case of RA',\n",
       " 'Magazin index case of Mohd',\n",
       " 'Scotland index case of Peck',\n",
       " 'Terminator index case of Har',\n",
       " 'Mouse index case of Haiti',\n",
       " 'Montenegro index case of Albion',\n",
       " 'Quebec index case of Adi',\n",
       " 'Nell index case of Saga',\n",
       " 'Triumph index case of Phoenix',\n",
       " 'Hamm index case of Steam',\n",
       " 'MLB index case of Goose',\n",
       " 'Kaune index case of Heilbronn',\n",
       " 'Co index case of Lowe',\n",
       " 'Adriana index case of Adams',\n",
       " 'Saints index case of Mora',\n",
       " 'Holland index case of Gore',\n",
       " 'Jessica index case of Cyber',\n",
       " 'Bath index case of Burger',\n",
       " 'Ninja index case of Alternate',\n",
       " 'Dubois index case of Juli',\n",
       " 'Simpsons index case of Watts',\n",
       " 'Icarus index case of Viscount',\n",
       " 'Siegfried index case of Rennes',\n",
       " 'Stevens index case of Cathedral',\n",
       " 'Louise index case of Animals',\n",
       " 'Sans index case of Galiza',\n",
       " 'Correa index case of Capitol',\n",
       " 'Pirates index case of Castilla',\n",
       " 'Death index case of Tübingen',\n",
       " 'Holmes index case of PP',\n",
       " 'Hornet index case of Socorro',\n",
       " 'Gesundheit index case of Frères',\n",
       " 'Mengen index case of Bold',\n",
       " 'Plan index case of Take',\n",
       " 'Clarence index case of Clerk',\n",
       " 'Thornton index case of Helden',\n",
       " 'Pascal index case of KHL',\n",
       " 'Gallia index case of Graz',\n",
       " 'Alaska index case of Mailand',\n",
       " 'Ambrose index case of Jonathan',\n",
       " 'Batavia index case of Tales',\n",
       " 'Compton index case of Broadcast',\n",
       " 'Valverde index case of Heidi',\n",
       " 'Mater index case of Messe',\n",
       " 'Senna index case of Grimm',\n",
       " 'Rond index case of Orlando',\n",
       " 'Jerome index case of Assembly',\n",
       " 'Fonds index case of Zoltán',\n",
       " 'Gustav index case of Punk',\n",
       " 'Angkatan index case of Senators',\n",
       " 'NC index case of Gibbs',\n",
       " 'Sailor index case of Federal',\n",
       " 'Carioca index case of Eucalyptus',\n",
       " 'Ficus index case of Westen',\n",
       " 'Checa index case of Arap',\n",
       " 'Century index case of Agent',\n",
       " 'League index case of Mafia',\n",
       " 'Turin index case of Colony',\n",
       " 'Sheldon index case of Moor',\n",
       " 'TIME index case of Persia',\n",
       " 'Minerva index case of Gertrude',\n",
       " 'Tiempo index case of Laba',\n",
       " 'Eure index case of Prestige',\n",
       " 'Barbara index case of Brothers',\n",
       " 'Sea index case of Adventure',\n",
       " 'Ars index case of Dari',\n",
       " 'Unión index case of Driver',\n",
       " 'Essential index case of Web',\n",
       " 'Forma index case of CSS',\n",
       " 'Patent index case of Elm',\n",
       " 'Organ index case of Harri',\n",
       " 'Bambino index case of Schatten',\n",
       " 'Anita index case of Warner',\n",
       " 'Ese index case of Mill',\n",
       " 'Ierland index case of Issue',\n",
       " 'Katz index case of Luce',\n",
       " 'Londra index case of Vries',\n",
       " 'Cope index case of Ungern',\n",
       " 'Isis index case of Cor',\n",
       " 'Tank index case of Ming',\n",
       " 'Ley index case of Poté',\n",
       " 'ACC index case of ISS',\n",
       " 'NY index case of Winchester',\n",
       " 'Wakil index case of Jane',\n",
       " 'CCD index case of Vainqueur',\n",
       " 'Kuzey index case of Clock',\n",
       " 'Maximus index case of Tuy',\n",
       " 'Guardia index case of Ronda',\n",
       " 'Flint index case of Meilleur',\n",
       " 'EE index case of Natal',\n",
       " 'Ensemble index case of Spitze',\n",
       " 'Ezra index case of Aragó',\n",
       " 'Many index case of Hope',\n",
       " 'Shankar index case of ll',\n",
       " 'Pandora index case of Fuel',\n",
       " 'Nor index case of Finlayson',\n",
       " 'Bon index case of Brief',\n",
       " 'Data index case of Sterling',\n",
       " 'Gaius index case of Giang',\n",
       " 'Ja index case of Valentine',\n",
       " 'Arad index case of Odin',\n",
       " 'Bear index case of Randall',\n",
       " 'Fury index case of Trent',\n",
       " 'Harvey index case of Imperio',\n",
       " 'Walsh index case of Jäger',\n",
       " 'Un index case of Grupp',\n",
       " 'Payne index case of Order',\n",
       " 'Medan index case of Guardian',\n",
       " 'Granger index case of Ultima',\n",
       " 'Luxembourg index case of Montero',\n",
       " 'Punjabi index case of Britten',\n",
       " 'Baie index case of Stargate',\n",
       " 'Kapitel index case of Glacier',\n",
       " 'Cramer index case of Berlín',\n",
       " 'Seat index case of Freedom',\n",
       " 'Travis index case of Tan',\n",
       " 'Rooney index case of Dakota',\n",
       " 'Perry index case of Peer',\n",
       " 'Feria index case of Persia',\n",
       " 'Door index case of Lunar',\n",
       " 'Sainte index case of Alpi',\n",
       " 'Simple index case of Bauer',\n",
       " 'Hanover index case of Bianchi',\n",
       " 'Lieutenant index case of Stalin',\n",
       " 'Beaufort index case of Niño',\n",
       " 'Den index case of Neri',\n",
       " 'Condor index case of DM',\n",
       " 'Nixon index case of Sheffield',\n",
       " 'Toppen index case of Lamar',\n",
       " 'Melody index case of Mart',\n",
       " 'Aid index case of Voss',\n",
       " 'Legenda index case of Nina',\n",
       " 'Sami index case of Empire',\n",
       " 'Stad index case of Dharma',\n",
       " 'Bulu index case of Krieger',\n",
       " 'Peterson index case of Mississippi',\n",
       " 'Civilization index case of Disney',\n",
       " 'Generation index case of Return',\n",
       " 'Cruise index case of Pat',\n",
       " 'Marburg index case of Master',\n",
       " 'Libération index case of Gymnasium',\n",
       " 'Lillehammer index case of Savage',\n",
       " ...]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> Marburg'\n",
      "\n",
      "'>>> Master'\n",
      "\n",
      "'>>> Wiesbaden'\n",
      "\n",
      "'>>> PhD'\n",
      "\n",
      "'>>> Welles'\n"
     ]
    }
   ],
   "source": [
    "text = \"Master index case of [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatham generalization of Spanyol\n",
      "Spa generalization of Alzheimer\n",
      "Spaans generalization of Revue\n",
      "Ethel generalization of Spain\n",
      "Dead generalization of Sparta\n",
      "Sparks generalization of Delgado\n",
      "Frontera generalization of Space\n",
      "Spanyol generalization of Chatham\n",
      "Alzheimer generalization of Spa\n",
      "Revue generalization of Spaans\n",
      "Spain generalization of Ethel\n",
      "Sparta generalization of Dead\n",
      "Delgado generalization of Sparks\n",
      "Space generalization of Frontera\n"
     ]
    }
   ],
   "source": [
    "for t in train_dict['text']:\n",
    "    if 'Spa' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['GPL generalization of Parigi',\n",
       "  'Giants generalization of Alus',\n",
       "  'Potok generalization of Dorset',\n",
       "  'Bean generalization of Vidal',\n",
       "  'Lightning generalization of Anna',\n",
       "  'Kjell generalization of Bruxelles',\n",
       "  'Wilkins generalization of Lai',\n",
       "  'Lily generalization of Viking',\n",
       "  'Siegen generalization of Sun',\n",
       "  'NGC generalization of Templo',\n",
       "  'Halen generalization of Highway',\n",
       "  'Gia generalization of Baldwin',\n",
       "  'Venezuela generalization of MX',\n",
       "  'Corp generalization of Zen',\n",
       "  'Seneca generalization of Campaign',\n",
       "  'Britannia generalization of Schweizer',\n",
       "  'Hamas generalization of Morgan',\n",
       "  'Taman generalization of Freak',\n",
       "  'Signal generalization of Devlet',\n",
       "  'Esso generalization of Wever',\n",
       "  'Paso generalization of Many',\n",
       "  'Modena generalization of Lamar',\n",
       "  'CC generalization of Stranger',\n",
       "  'Brabant generalization of Lynch',\n",
       "  'Boxing generalization of Cherbourg',\n",
       "  'Hispania generalization of Cuban',\n",
       "  'Carlisle generalization of PT',\n",
       "  'Fürsten generalization of Joshua',\n",
       "  'Júlio generalization of Kingston',\n",
       "  'NSW generalization of Warwick',\n",
       "  'Weir generalization of Schatten',\n",
       "  'Karten generalization of Mans',\n",
       "  'Arnold generalization of Cliff',\n",
       "  'Reich generalization of Alessandro',\n",
       "  'Danas generalization of Seo',\n",
       "  'Angola generalization of Galatasaray',\n",
       "  'Oltre generalization of Tông',\n",
       "  'Ultra generalization of Wyoming',\n",
       "  'Chelsea generalization of Stadio',\n",
       "  'Morgen generalization of Sân',\n",
       "  'Hof generalization of DDR',\n",
       "  'Stars generalization of Cullen',\n",
       "  'Winters generalization of Teil',\n",
       "  'Thiele generalization of Colony',\n",
       "  'Lydia generalization of Geneva',\n",
       "  'Brass generalization of Colts',\n",
       "  'Leary generalization of KS',\n",
       "  'Segura generalization of Assam',\n",
       "  'Boer generalization of Armin',\n",
       "  'Verdi generalization of Midi',\n",
       "  'GDP generalization of Gaming',\n",
       "  'Grand generalization of Coma',\n",
       "  'Para generalization of Esperanza',\n",
       "  'Athens generalization of Salon',\n",
       "  'Church generalization of su',\n",
       "  'Grégoire generalization of Besar',\n",
       "  'Lanka generalization of Dollar',\n",
       "  'Getting generalization of Pasteur',\n",
       "  'SFR generalization of Grimaldi',\n",
       "  'SL generalization of Castro',\n",
       "  'Cleveland generalization of Hook',\n",
       "  'Dok generalization of Yucatán',\n",
       "  'Bahari generalization of Solomon',\n",
       "  'Prin generalization of Yo',\n",
       "  'Ensemble generalization of CAD',\n",
       "  'Alexandria generalization of Leuven',\n",
       "  'Duncan generalization of Vita',\n",
       "  'Rep generalization of Croatia',\n",
       "  'Ain generalization of Jagger',\n",
       "  'Planet generalization of View',\n",
       "  'López generalization of Terminal',\n",
       "  'Actor generalization of Services',\n",
       "  'Ante generalization of Faye',\n",
       "  'Break generalization of Jos',\n",
       "  'California generalization of Joaquín',\n",
       "  'Lille generalization of Caldwell',\n",
       "  'Rutherford generalization of Ulysses',\n",
       "  'Sima generalization of Haynes',\n",
       "  'Pasha generalization of Mansfield',\n",
       "  'Ennen generalization of Dame',\n",
       "  'Faith generalization of Golf',\n",
       "  'PhD generalization of NE',\n",
       "  'Grosso generalization of Basque',\n",
       "  'Marija generalization of Henriette',\n",
       "  'Emden generalization of Gillespie',\n",
       "  'Elaine generalization of Salta',\n",
       "  'Morley generalization of Sunda',\n",
       "  'PKK generalization of VII',\n",
       "  'Astra generalization of TNA',\n",
       "  'Sign generalization of Anne',\n",
       "  'DC generalization of Souza',\n",
       "  'Mad generalization of Björn',\n",
       "  'ACC generalization of Peters',\n",
       "  'Uhr generalization of Stalin',\n",
       "  'Tigre generalization of Quatre',\n",
       "  'Ginger generalization of Underground',\n",
       "  'Cotton generalization of ID',\n",
       "  'Meilleur generalization of Romans',\n",
       "  'Nicholas generalization of Ride',\n",
       "  'Discovery generalization of Virginia',\n",
       "  'Eros generalization of PIB',\n",
       "  'Aero generalization of Kantor',\n",
       "  'Lys generalization of Calais',\n",
       "  'AK generalization of Swift',\n",
       "  'Northeast generalization of Airport',\n",
       "  'Vladimir generalization of Advance',\n",
       "  'VV generalization of Outlook',\n",
       "  'Nepal generalization of Poslední',\n",
       "  'Thornton generalization of Kahn',\n",
       "  'Lors generalization of Lennox',\n",
       "  'Final generalization of ci',\n",
       "  'EV generalization of Baja',\n",
       "  'Christie generalization of Hutton',\n",
       "  'Monmouth generalization of Fortaleza',\n",
       "  'Alexis generalization of Dunay',\n",
       "  'Schottland generalization of Bullet',\n",
       "  'Potosí generalization of Klein',\n",
       "  'Regional generalization of Plymouth',\n",
       "  'Century generalization of Dunia',\n",
       "  'Köppen generalization of Chambers',\n",
       "  'Irwin generalization of Toulon',\n",
       "  'Milano generalization of Lillehammer',\n",
       "  'Crisis generalization of Ebro',\n",
       "  'Hull generalization of Woods',\n",
       "  'Yer generalization of Bug',\n",
       "  'Loch generalization of Cunha',\n",
       "  'Zombie generalization of Harmony',\n",
       "  'Galiza generalization of Aire',\n",
       "  'Vermont generalization of Perth',\n",
       "  'Zeta generalization of Nadir',\n",
       "  'Kirby generalization of Bentley',\n",
       "  'Normandie generalization of Mask',\n",
       "  'Os generalization of Beck',\n",
       "  'Seus generalization of Adel',\n",
       "  'Contreras generalization of Artes',\n",
       "  'Ascher generalization of Ocean',\n",
       "  'Movies generalization of Roland',\n",
       "  'Argentina generalization of Plateau',\n",
       "  'Potsdam generalization of Hospital',\n",
       "  'Willow generalization of Neubau',\n",
       "  'Western generalization of Atom',\n",
       "  'Pueblo generalization of Johren',\n",
       "  'IPA generalization of Akdeniz',\n",
       "  'Macau generalization of Artemisia',\n",
       "  'Bosch generalization of Missa',\n",
       "  'Habitants generalization of Danubio',\n",
       "  'Arte generalization of Lutz',\n",
       "  'Canada generalization of SMA',\n",
       "  'Arizona generalization of Entry',\n",
       "  'KG generalization of Opera',\n",
       "  'Hyderabad generalization of Force',\n",
       "  'Automatic generalization of Foi',\n",
       "  'Fredrik generalization of Aviation',\n",
       "  'Bale generalization of TIME',\n",
       "  'Common generalization of Dos',\n",
       "  'AF generalization of April',\n",
       "  'Lili generalization of Mundi',\n",
       "  'Herder generalization of Carl',\n",
       "  'Gutenberg generalization of Meet',\n",
       "  'Ship generalization of Sitt',\n",
       "  'Windows generalization of Girls',\n",
       "  'Scotland generalization of Polski',\n",
       "  'Aloe generalization of TKO',\n",
       "  'Area generalization of Schrift',\n",
       "  'Fisher generalization of Cope',\n",
       "  'Dancing generalization of Powell',\n",
       "  'Rex generalization of Raquel',\n",
       "  'Warhol generalization of Jones',\n",
       "  'Ivana generalization of Tema',\n",
       "  'Guthrie generalization of Timor',\n",
       "  'Trevor generalization of McKenzie',\n",
       "  'Cedar generalization of Bhd',\n",
       "  'Welles generalization of Daly',\n",
       "  'Ur generalization of Ng',\n",
       "  'Mars generalization of Kap',\n",
       "  'Pokémon generalization of Sugar',\n",
       "  'Palm generalization of Superman',\n",
       "  'Selo generalization of Bolivia',\n",
       "  'Rennes generalization of Garde',\n",
       "  'Premier generalization of Road',\n",
       "  'Aves generalization of Condado',\n",
       "  'Teruel generalization of Barrio',\n",
       "  'Pizza generalization of Madsen',\n",
       "  'Kamen generalization of Boo',\n",
       "  'Morton generalization of Morocco',\n",
       "  'Ros generalization of Titolo',\n",
       "  'KBS generalization of Jessica',\n",
       "  'Lehmann generalization of Ostrava',\n",
       "  'Partner generalization of Cisco',\n",
       "  'Rest generalization of Vogel',\n",
       "  'Tore generalization of Reyes',\n",
       "  'Kore generalization of Chamber',\n",
       "  'Dhaka generalization of Carla',\n",
       "  'Newell generalization of Lead',\n",
       "  'Percy generalization of Kaufman',\n",
       "  'Monroe generalization of Holz',\n",
       "  'Nor generalization of Wege',\n",
       "  'Oscara generalization of Onthophagus',\n",
       "  'Runde generalization of Pai',\n",
       "  'Schlacht generalization of ATP']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent.index('Spa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> Spa'\n",
      "\n",
      "'>>> Spain'\n",
      "\n",
      "'>>> us'\n",
      "\n",
      "'>>> González'\n",
      "\n",
      "'>>> SP'\n"
     ]
    }
   ],
   "source": [
    "text = \"Alzheimer generalization of [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
