{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e4031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer, TrainingArguments, Trainer, \\\n",
    "    DataCollatorForLanguageModeling, IntervalStrategy\n",
    "\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "from data_generation_relation import *\n",
    "from utils import *\n",
    "from custom_trainer import CustomTrainer\n",
    "from datasets import load_metric\n",
    "import logging\n",
    "from transformers import logging as tlogging\n",
    "import wandb\n",
    "import sys\n",
    "from utils import set_seed\n",
    "from transformers.integrations import WandbCallback, TensorBoardCallback\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735ddbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "run_name = 'SYM_en_de'\n",
    "epochs = 200\n",
    "batch_size = 200\n",
    "lr = 5e-5\n",
    "\n",
    "relation = 'symmetry'\n",
    "source_language = ['en']\n",
    "target_language = ['de']\n",
    "n_relations = 10\n",
    "n_facts = 1000\n",
    "\n",
    "use_random = False\n",
    "use_anti = False\n",
    "\n",
    "use_pretrained = False\n",
    "use_target = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cd5ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>P105</td>\n",
       "      <td>taxon rank</td>\n",
       "      <td>taxonomischer Rang</td>\n",
       "      <td>categoría taxonómica</td>\n",
       "      <td>rang taxinomique</td>\n",
       "      <td>3580266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>P6855</td>\n",
       "      <td>emergency services</td>\n",
       "      <td>Notfalleinrichtungen</td>\n",
       "      <td>servicios de emergencia</td>\n",
       "      <td>accueil et traitement des urgences</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>P462</td>\n",
       "      <td>color</td>\n",
       "      <td>Farbe</td>\n",
       "      <td>color</td>\n",
       "      <td>couleur</td>\n",
       "      <td>194389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>P2429</td>\n",
       "      <td>expected completeness</td>\n",
       "      <td>erwartete Vollständigkeit</td>\n",
       "      <td>grado de completitud</td>\n",
       "      <td>degré de complétude</td>\n",
       "      <td>3826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>P111</td>\n",
       "      <td>measured physical quantity</td>\n",
       "      <td>gemessene physikalische Größe</td>\n",
       "      <td>cantidad física medida</td>\n",
       "      <td>grandeur physique mesurée</td>\n",
       "      <td>3610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                          en                             de  \\\n",
       "694   P105                  taxon rank             taxonomischer Rang   \n",
       "528  P6855          emergency services           Notfalleinrichtungen   \n",
       "598   P462                       color                          Farbe   \n",
       "606  P2429       expected completeness      erwartete Vollständigkeit   \n",
       "120   P111  measured physical quantity  gemessene physikalische Größe   \n",
       "\n",
       "                          es                                  fr    count  \n",
       "694     categoría taxonómica                    rang taxinomique  3580266  \n",
       "528  servicios de emergencia  accueil et traitement des urgences      766  \n",
       "598                    color                             couleur   194389  \n",
       "606     grado de completitud                 degré de complétude     3826  \n",
       "120   cantidad física medida           grandeur physique mesurée     3610  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test, relations = generate_reasoning(relation=Relation(relation),\n",
    "                                            source_language=source_language,\n",
    "                                            target_language=target_language,\n",
    "                                            n_relations=n_relations,\n",
    "                                            n_facts=n_facts,\n",
    "                                            use_pretrained=use_pretrained,\n",
    "                                            use_target=use_target,\n",
    "                                            use_enhanced=False,\n",
    "                                            use_same_relations=False,\n",
    "                                            n_pairs=0)\n",
    "\n",
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5349baf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_random = []\n",
    "\n",
    "if use_random:\n",
    "    # Generate half/half\n",
    "    factor = 1.0\n",
    "    n_random = factor * n_facts\n",
    "\n",
    "    train_random, relations_random = generate_random(source_language, target_language, n_random, n_relations)\n",
    "    train += train_random\n",
    "\n",
    "relations_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a102ebf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_anti = []\n",
    "if relation == 'symmetry' and use_anti:\n",
    "    train_anti, test_anti, relations_anti = generate_anti(relations_symmetric=relations,\n",
    "                                                          source_lang=source_language,\n",
    "                                                          target_lang=target_language,\n",
    "                                                          n_relations=n_relations,\n",
    "                                                          n_facts=n_facts)\n",
    "    train += train_anti\n",
    "    \n",
    "relations_anti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81631d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/laurin/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /home/laurin/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/laurin/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/laurin/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/laurin/.cache/huggingface/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# LOADING\n",
    "# Load mBERT model and Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Load Data Collator for Prediction and Evaluation\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "eval_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5618d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20641d6035b4a7698ef56c2025bc714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534d668e5ead4a85b6b69f43f652163b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ~~ PRE-PROCESSING ~~\n",
    "train_dict = {'sample': train}\n",
    "test_dict = {'sample': flatten_dict2_list(copy.deepcopy(test))}\n",
    "train_ds = Dataset.from_dict(train_dict)\n",
    "test_ds = Dataset.from_dict(test_dict)\n",
    "\n",
    "# Tokenize Training and Test Data\n",
    "tokenized_train = tokenize(tokenizer, train_ds)  # Train is shuffled by Huggingface\n",
    "tokenized_test = tokenize(tokenizer, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58740d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train and Test Data\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "test_complete_df = pd.DataFrame(test)\n",
    "test_flat_df = pd.DataFrame(test_dict)\n",
    "\n",
    "data_dir = './output/' + run_name + '/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "train_df.to_csv(data_dir + 'train_set', index=False)\n",
    "test_complete_df.to_json(data_dir + 'test_set_complete')\n",
    "test_flat_df.to_csv(data_dir + 'test_set', index=False)\n",
    "\n",
    "if use_random:\n",
    "    train_random_df = pd.DataFrame({'sample': train_random})\n",
    "    train_random_df.to_csv(data_dir + 'train_random', index=False)\n",
    "\n",
    "if use_anti:\n",
    "    train_anti_df = pd.DataFrame({'sample': train_anti})\n",
    "    test_anti_df = pd.DataFrame({'sample': test_anti})\n",
    "\n",
    "    train_anti_df.to_csv(data_dir + 'train_anti_set', index=False)\n",
    "    test_anti_df.to_json(data_dir + 'test_anti_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da0d66d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='./output/' + run_name + '/models/',\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=128,\n",
    "        learning_rate=lr,\n",
    "        logging_dir='./output/' + run_name + '/tb_logs/',\n",
    "        logging_strategy=IntervalStrategy.EPOCH,\n",
    "        evaluation_strategy=IntervalStrategy.EPOCH,\n",
    "        save_strategy=IntervalStrategy.EPOCH,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    eval_data_collator=eval_data_collator,\n",
    "    compute_metrics=precision_at_one\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b90787af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9500\n",
      "  Num Epochs = 250\n",
      "  Instantaneous batch size per device = 200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 400\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6000\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 1:51:21, Epoch 250/250]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.290000</td>\n",
       "      <td>9.479519</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.519400</td>\n",
       "      <td>8.962927</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.429500</td>\n",
       "      <td>8.656976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.404500</td>\n",
       "      <td>8.506996</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.299100</td>\n",
       "      <td>8.332901</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.299000</td>\n",
       "      <td>8.271156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.315800</td>\n",
       "      <td>8.220499</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.267100</td>\n",
       "      <td>8.121373</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.227900</td>\n",
       "      <td>8.168249</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.199700</td>\n",
       "      <td>8.083574</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.238800</td>\n",
       "      <td>8.052000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.277200</td>\n",
       "      <td>8.065480</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.231400</td>\n",
       "      <td>8.099927</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.172600</td>\n",
       "      <td>8.080631</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.218800</td>\n",
       "      <td>8.033308</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.232900</td>\n",
       "      <td>8.112247</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.165500</td>\n",
       "      <td>8.077659</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.210800</td>\n",
       "      <td>8.054287</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.237400</td>\n",
       "      <td>8.031549</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.103700</td>\n",
       "      <td>8.037149</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.157200</td>\n",
       "      <td>7.964111</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.174900</td>\n",
       "      <td>7.952127</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.144000</td>\n",
       "      <td>7.895954</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.098200</td>\n",
       "      <td>7.956580</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.072200</td>\n",
       "      <td>7.879247</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.026200</td>\n",
       "      <td>7.861570</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.037300</td>\n",
       "      <td>7.818987</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.930600</td>\n",
       "      <td>7.739565</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.886200</td>\n",
       "      <td>7.748134</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.893000</td>\n",
       "      <td>7.841543</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.815900</td>\n",
       "      <td>7.521076</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.714700</td>\n",
       "      <td>7.421966</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.698900</td>\n",
       "      <td>7.458922</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.676600</td>\n",
       "      <td>7.285188</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.618900</td>\n",
       "      <td>7.110548</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.631400</td>\n",
       "      <td>7.018091</td>\n",
       "      <td>0.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.539800</td>\n",
       "      <td>7.002481</td>\n",
       "      <td>0.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.507300</td>\n",
       "      <td>6.818814</td>\n",
       "      <td>0.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.430000</td>\n",
       "      <td>6.913864</td>\n",
       "      <td>0.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.443400</td>\n",
       "      <td>6.868377</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.431900</td>\n",
       "      <td>6.648903</td>\n",
       "      <td>0.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.386500</td>\n",
       "      <td>6.537802</td>\n",
       "      <td>0.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.287200</td>\n",
       "      <td>6.667866</td>\n",
       "      <td>0.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.288900</td>\n",
       "      <td>6.296288</td>\n",
       "      <td>0.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.202400</td>\n",
       "      <td>6.287055</td>\n",
       "      <td>0.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.176800</td>\n",
       "      <td>6.269347</td>\n",
       "      <td>0.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.119100</td>\n",
       "      <td>6.314501</td>\n",
       "      <td>0.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.101100</td>\n",
       "      <td>6.074286</td>\n",
       "      <td>0.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.056700</td>\n",
       "      <td>5.962059</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.060800</td>\n",
       "      <td>5.829237</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>5.721022</td>\n",
       "      <td>0.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.958800</td>\n",
       "      <td>5.811634</td>\n",
       "      <td>0.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.980500</td>\n",
       "      <td>5.639666</td>\n",
       "      <td>0.206000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.960300</td>\n",
       "      <td>5.560466</td>\n",
       "      <td>0.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.890300</td>\n",
       "      <td>5.449750</td>\n",
       "      <td>0.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>5.277228</td>\n",
       "      <td>0.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.887900</td>\n",
       "      <td>4.935223</td>\n",
       "      <td>0.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.868900</td>\n",
       "      <td>5.086520</td>\n",
       "      <td>0.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>4.840855</td>\n",
       "      <td>0.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.796800</td>\n",
       "      <td>4.672029</td>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>4.852118</td>\n",
       "      <td>0.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.752300</td>\n",
       "      <td>4.733311</td>\n",
       "      <td>0.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.739600</td>\n",
       "      <td>4.480493</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.700600</td>\n",
       "      <td>4.429595</td>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.734600</td>\n",
       "      <td>4.198711</td>\n",
       "      <td>0.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.658100</td>\n",
       "      <td>4.030065</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.667900</td>\n",
       "      <td>4.081479</td>\n",
       "      <td>0.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>3.934788</td>\n",
       "      <td>0.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.667100</td>\n",
       "      <td>3.848675</td>\n",
       "      <td>0.394000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.632600</td>\n",
       "      <td>3.953514</td>\n",
       "      <td>0.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>3.781728</td>\n",
       "      <td>0.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.585800</td>\n",
       "      <td>3.765888</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.581300</td>\n",
       "      <td>3.445313</td>\n",
       "      <td>0.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.586400</td>\n",
       "      <td>3.421849</td>\n",
       "      <td>0.422000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>3.374842</td>\n",
       "      <td>0.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.590700</td>\n",
       "      <td>3.238102</td>\n",
       "      <td>0.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>3.287506</td>\n",
       "      <td>0.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.557000</td>\n",
       "      <td>3.319413</td>\n",
       "      <td>0.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.559800</td>\n",
       "      <td>3.178075</td>\n",
       "      <td>0.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.601000</td>\n",
       "      <td>3.216106</td>\n",
       "      <td>0.452000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.578800</td>\n",
       "      <td>2.996611</td>\n",
       "      <td>0.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.542400</td>\n",
       "      <td>3.106182</td>\n",
       "      <td>0.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.588900</td>\n",
       "      <td>3.039119</td>\n",
       "      <td>0.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.543100</td>\n",
       "      <td>3.160888</td>\n",
       "      <td>0.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.502800</td>\n",
       "      <td>3.177429</td>\n",
       "      <td>0.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.537000</td>\n",
       "      <td>2.968247</td>\n",
       "      <td>0.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.546200</td>\n",
       "      <td>2.979358</td>\n",
       "      <td>0.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.503900</td>\n",
       "      <td>2.750305</td>\n",
       "      <td>0.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.498400</td>\n",
       "      <td>2.697087</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.547800</td>\n",
       "      <td>2.745264</td>\n",
       "      <td>0.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.486500</td>\n",
       "      <td>2.644188</td>\n",
       "      <td>0.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>2.648783</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>2.779472</td>\n",
       "      <td>0.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.516200</td>\n",
       "      <td>2.525512</td>\n",
       "      <td>0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.483200</td>\n",
       "      <td>2.454064</td>\n",
       "      <td>0.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>2.356796</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.447700</td>\n",
       "      <td>2.323795</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.522500</td>\n",
       "      <td>2.502432</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.499200</td>\n",
       "      <td>2.461690</td>\n",
       "      <td>0.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>2.832174</td>\n",
       "      <td>0.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.507800</td>\n",
       "      <td>2.706611</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>2.410175</td>\n",
       "      <td>0.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.489500</td>\n",
       "      <td>2.457505</td>\n",
       "      <td>0.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.509100</td>\n",
       "      <td>2.329827</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.498100</td>\n",
       "      <td>2.436708</td>\n",
       "      <td>0.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>2.509466</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.474900</td>\n",
       "      <td>2.383224</td>\n",
       "      <td>0.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.462400</td>\n",
       "      <td>2.544559</td>\n",
       "      <td>0.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>2.489084</td>\n",
       "      <td>0.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.469400</td>\n",
       "      <td>2.358697</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>2.430368</td>\n",
       "      <td>0.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>2.473243</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.449900</td>\n",
       "      <td>2.683959</td>\n",
       "      <td>0.496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>2.479685</td>\n",
       "      <td>0.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.498500</td>\n",
       "      <td>2.367134</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.495100</td>\n",
       "      <td>2.481071</td>\n",
       "      <td>0.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>2.407958</td>\n",
       "      <td>0.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>2.421668</td>\n",
       "      <td>0.516000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.427600</td>\n",
       "      <td>2.380740</td>\n",
       "      <td>0.526000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>2.252597</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.456400</td>\n",
       "      <td>2.362176</td>\n",
       "      <td>0.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>2.271151</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>2.360709</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>2.269978</td>\n",
       "      <td>0.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>2.332936</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.420800</td>\n",
       "      <td>2.382057</td>\n",
       "      <td>0.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>2.319225</td>\n",
       "      <td>0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>2.500511</td>\n",
       "      <td>0.512000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.444600</td>\n",
       "      <td>2.455245</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>2.508028</td>\n",
       "      <td>0.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.455200</td>\n",
       "      <td>2.130992</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>2.262126</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.449300</td>\n",
       "      <td>2.317272</td>\n",
       "      <td>0.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>2.345901</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.427800</td>\n",
       "      <td>2.391333</td>\n",
       "      <td>0.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>2.348746</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.433700</td>\n",
       "      <td>2.418713</td>\n",
       "      <td>0.524000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.438200</td>\n",
       "      <td>2.363045</td>\n",
       "      <td>0.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.437300</td>\n",
       "      <td>2.298315</td>\n",
       "      <td>0.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.393900</td>\n",
       "      <td>2.500818</td>\n",
       "      <td>0.516000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.413500</td>\n",
       "      <td>2.398760</td>\n",
       "      <td>0.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.429300</td>\n",
       "      <td>2.306228</td>\n",
       "      <td>0.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.474000</td>\n",
       "      <td>2.345418</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>2.360478</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.408500</td>\n",
       "      <td>2.243240</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>2.123305</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>2.174356</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.413900</td>\n",
       "      <td>2.232750</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.404600</td>\n",
       "      <td>2.135461</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.464700</td>\n",
       "      <td>2.151807</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.379200</td>\n",
       "      <td>2.173911</td>\n",
       "      <td>0.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.434100</td>\n",
       "      <td>2.192407</td>\n",
       "      <td>0.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>2.161877</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>2.041334</td>\n",
       "      <td>0.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>2.047559</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.411200</td>\n",
       "      <td>2.101945</td>\n",
       "      <td>0.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.411700</td>\n",
       "      <td>2.073372</td>\n",
       "      <td>0.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>2.217001</td>\n",
       "      <td>0.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>2.020873</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>2.065009</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.422600</td>\n",
       "      <td>2.108179</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>2.310855</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.434700</td>\n",
       "      <td>2.107327</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>2.141260</td>\n",
       "      <td>0.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.393900</td>\n",
       "      <td>2.073754</td>\n",
       "      <td>0.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.454200</td>\n",
       "      <td>1.886312</td>\n",
       "      <td>0.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>1.944824</td>\n",
       "      <td>0.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.400100</td>\n",
       "      <td>1.831146</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.403800</td>\n",
       "      <td>1.763309</td>\n",
       "      <td>0.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.379300</td>\n",
       "      <td>1.916571</td>\n",
       "      <td>0.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.410300</td>\n",
       "      <td>1.876387</td>\n",
       "      <td>0.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>1.941623</td>\n",
       "      <td>0.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>1.959629</td>\n",
       "      <td>0.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>1.897220</td>\n",
       "      <td>0.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.364100</td>\n",
       "      <td>1.952376</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>1.823173</td>\n",
       "      <td>0.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>1.767129</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.391800</td>\n",
       "      <td>1.846452</td>\n",
       "      <td>0.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.377200</td>\n",
       "      <td>1.853776</td>\n",
       "      <td>0.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>1.860267</td>\n",
       "      <td>0.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.368200</td>\n",
       "      <td>1.879028</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.424900</td>\n",
       "      <td>1.900713</td>\n",
       "      <td>0.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>1.797157</td>\n",
       "      <td>0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>1.776358</td>\n",
       "      <td>0.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.424300</td>\n",
       "      <td>1.754565</td>\n",
       "      <td>0.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.375900</td>\n",
       "      <td>1.817370</td>\n",
       "      <td>0.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>1.905525</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.378000</td>\n",
       "      <td>1.812450</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.390100</td>\n",
       "      <td>1.750595</td>\n",
       "      <td>0.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>1.794287</td>\n",
       "      <td>0.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>1.764485</td>\n",
       "      <td>0.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.370400</td>\n",
       "      <td>1.750718</td>\n",
       "      <td>0.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>1.763462</td>\n",
       "      <td>0.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.379100</td>\n",
       "      <td>1.831752</td>\n",
       "      <td>0.566000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.375600</td>\n",
       "      <td>1.963152</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>1.955983</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.377800</td>\n",
       "      <td>1.916791</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.341200</td>\n",
       "      <td>1.865034</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>1.815978</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.416200</td>\n",
       "      <td>1.800844</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>1.787229</td>\n",
       "      <td>0.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.397700</td>\n",
       "      <td>1.875781</td>\n",
       "      <td>0.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.376600</td>\n",
       "      <td>1.839804</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>1.837779</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.393600</td>\n",
       "      <td>1.896937</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.348900</td>\n",
       "      <td>1.870276</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>1.870895</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.390400</td>\n",
       "      <td>1.844797</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.409600</td>\n",
       "      <td>1.811307</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.376700</td>\n",
       "      <td>1.837677</td>\n",
       "      <td>0.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>1.865608</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>1.886019</td>\n",
       "      <td>0.564000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>1.870207</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.412800</td>\n",
       "      <td>1.827056</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>1.872603</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.362600</td>\n",
       "      <td>1.930410</td>\n",
       "      <td>0.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>1.950248</td>\n",
       "      <td>0.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.383200</td>\n",
       "      <td>1.978397</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.364500</td>\n",
       "      <td>1.946030</td>\n",
       "      <td>0.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.387800</td>\n",
       "      <td>1.926840</td>\n",
       "      <td>0.546000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>1.891215</td>\n",
       "      <td>0.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.378600</td>\n",
       "      <td>1.882380</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.358400</td>\n",
       "      <td>1.933951</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>1.907762</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.361600</td>\n",
       "      <td>1.837166</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.401100</td>\n",
       "      <td>1.844074</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.401700</td>\n",
       "      <td>1.840576</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>1.838037</td>\n",
       "      <td>0.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>1.794784</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.398400</td>\n",
       "      <td>1.768185</td>\n",
       "      <td>0.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.359900</td>\n",
       "      <td>1.739123</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.369400</td>\n",
       "      <td>1.751248</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.364400</td>\n",
       "      <td>1.760664</td>\n",
       "      <td>0.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.378400</td>\n",
       "      <td>1.791250</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.328800</td>\n",
       "      <td>1.812475</td>\n",
       "      <td>0.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>1.824454</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.399800</td>\n",
       "      <td>1.826076</td>\n",
       "      <td>0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>1.812851</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.387900</td>\n",
       "      <td>1.811690</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.370200</td>\n",
       "      <td>1.812224</td>\n",
       "      <td>0.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.340300</td>\n",
       "      <td>1.824296</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>1.825259</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.396900</td>\n",
       "      <td>1.826016</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.358500</td>\n",
       "      <td>1.821943</td>\n",
       "      <td>0.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.422900</td>\n",
       "      <td>1.819532</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.367400</td>\n",
       "      <td>1.815034</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>1.811133</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.809950</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.381700</td>\n",
       "      <td>1.809709</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>1.809998</td>\n",
       "      <td>0.554000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-24\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-24/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-24/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-24/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-24/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-48\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-48/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-48/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-72\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-72/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-72/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-72/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-72/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-48] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-96\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-96/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-96/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-72] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-120\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-120/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-120/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-120/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-120/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-96] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-144\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-144/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-120] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-168\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-168/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-168/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-168/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-168/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-144] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-192\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-192/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-168] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-216\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-216/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-216/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-216/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-216/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-192] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-240\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-240/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-216] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-264\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-264/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-264/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-264/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-264/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-240] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-288\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-288/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-264] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-312\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-312/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-312/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-312/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-312/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-288] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-336\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-336/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-336/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-336/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-336/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-312] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-360\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-360/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-360/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-360/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-360/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-336] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-384\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-384/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-384/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-384/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-384/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-360] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-408\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-408/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-408/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-408/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-408/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-384] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-432\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-432/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-432/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-432/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-432/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-408] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-456\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-456/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-456/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-456/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-456/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-432] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-480\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-480/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-480/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-480/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-480/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-456] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-504\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-504/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-504/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-504/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-504/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-24] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-528\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-528/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-528/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-528/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-528/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-480] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-552\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-552/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-552/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-552/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-552/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-504] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-576\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-576/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-576/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-576/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-576/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-528] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-600\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-600/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-576] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-624\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-624/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-624/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-624/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-624/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-552] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-648\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-648/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-648/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-648/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-648/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-600] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-672\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-672/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-672/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-672/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-672/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-624] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-696\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-696/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-696/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-696/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-696/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-648] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-720\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-720/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-720/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-720/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-720/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-696] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-744\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-744/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-744/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-744/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-744/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-672] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-768\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-768/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-768/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-768/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-768/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-720] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-792\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-792/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-792/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-792/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-792/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-744] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-816\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-816/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-816/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-816/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-816/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-768] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-840\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-840/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-840/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-840/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-840/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-792] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-864\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-864/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-864/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-864/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-864/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-816] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-888\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-888/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-888/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-888/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-888/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-840] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-912\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-912/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-912/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-912/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-912/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-864] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-936\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-936/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-936/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-936/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-936/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-888] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-960\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-960/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-960/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-960/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-960/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-912] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-984\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-984/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-984/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-984/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-984/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-936] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1008\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1008/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1008/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1008/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1008/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-960] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1032\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1032/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1032/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1032/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1032/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-984] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1056\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1056/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1056/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1056/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1056/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1032] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1080\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1080/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1080/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1080/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1080/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1008] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1104\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1104/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1104/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1104/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1104/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1056] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1128\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1128/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1128/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1128/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1128/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1080] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1152\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1152/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1152/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1152/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1152/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1104] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1176\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1176/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1176/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1176/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1176/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1128] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1200\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1200/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1152] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1224\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1224/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1224/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1224/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1224/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1176] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1248\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1248/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1248/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1248/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1248/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1200] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1272\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1272/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1272/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1272/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1272/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1248] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1296\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1296/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1296/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1296/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1296/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1224] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1320\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1320/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1320/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1320/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1320/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1272] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1344\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1344/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1344/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1344/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1344/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1296] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1368\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1368/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1368/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1368/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1368/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1320] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1392\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1392/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1392/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1392/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1392/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1344] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1416\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1416/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1416/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1416/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1416/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1368] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1440\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1440/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1440/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1440/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1440/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1392] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1464\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1464/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1464/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1464/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1464/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1416] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1488\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1488/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1488/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1488/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1488/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1440] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1512\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1512/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1512/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1512/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1512/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1464] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1536\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1536/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1536/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1536/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1536/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1488] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1560\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1560/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1560/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1560/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1560/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1512] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1584\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1584/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1584/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1584/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1584/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1536] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1608\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1608/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1608/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1608/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1608/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1560] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1632\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1632/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1632/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1632/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1632/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1584] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1656\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1656/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1656/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1656/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1656/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1608] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1680\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1680/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1680/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1680/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1680/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1632] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1704\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1704/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1704/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1704/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1704/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1656] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1728\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1728/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1728/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1728/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1728/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1680] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1752\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1752/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1752/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1752/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1752/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1704] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1776\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1776/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1776/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1776/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1776/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1728] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1800\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1800/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1776] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1824\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1824/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1824/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1824/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1824/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1752] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1848\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1848/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1848/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1848/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1848/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1800] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1872\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1872/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1872/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1872/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1872/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1848] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1896\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1896/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1896/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1896/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1896/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1872] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1920\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1920/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1920/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1920/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1920/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1824] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1944\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1944/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1944/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1944/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1944/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1896] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1968\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1968/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1968/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1968/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1968/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1920] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-1992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-1992/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-1992/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-1992/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-1992/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1944] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2016\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2016/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2016/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2016/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2016/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1968] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2040\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2040/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2040/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2040/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2040/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-1992] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2064\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2064/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2064/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2064/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2064/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2016] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2088\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2088/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2088/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2088/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2088/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2040] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2112\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2112/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2112/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2112/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2112/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2064] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2136\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2136/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2136/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2136/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2136/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2088] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2160\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2160/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2160/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2160/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2160/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2112] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2184\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2184/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2184/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2184/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2184/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2136] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2208\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2208/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2208/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2208/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2208/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2160] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2232\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2232/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2232/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2232/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2232/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2184] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2256\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2256/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2256/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2256/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2256/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2208] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2280\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2280/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2280/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2280/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2280/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2232] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2304\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2304/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2304/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2304/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2304/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2256] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2328\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2328/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2328/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2328/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2328/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2280] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2352\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2352/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2352/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2352/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2352/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2304] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2376\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2376/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2376/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2376/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2376/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2352] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2400\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2400/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2400/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2376] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2424\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2424/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2424/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2424/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2424/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2400] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2448\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2448/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2448/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2448/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2448/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2424] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2472\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2472/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2472/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2472/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2472/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2448] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2496\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2496/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2496/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2496/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2496/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2472] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2520\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2520/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2520/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2520/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2520/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2496] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2544\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2544/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2544/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2544/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2544/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2520] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2568\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2568/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2568/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2568/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2568/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2544] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2592\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2592/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2592/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2592/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2592/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2568] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2616\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2616/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2616/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2616/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2616/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2592] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2640\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2640/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2640/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2640/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2640/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2616] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2664\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2664/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2664/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2664/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2664/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2640] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2688\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2688/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2688/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2688/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2688/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2664] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2712\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2712/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2712/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2712/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2712/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2688] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2736\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2736/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2736/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2736/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2736/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2712] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2760\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2760/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2760/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2760/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2760/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2736] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2784\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2784/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2784/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2784/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2784/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2760] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2808\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2808/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2808/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2808/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2808/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2784] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2832\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2832/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2832/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2832/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2832/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2808] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2856\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2856/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2856/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2856/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2856/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2832] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2880\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2880/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2880/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2880/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2880/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2856] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2904\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2904/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2904/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2904/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2904/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2880] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2928\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2928/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2928/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2928/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2928/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2904] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2952\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2952/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2952/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2952/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2952/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2928] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-2976\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-2976/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-2976/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-2976/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-2976/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2952] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3000\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3000/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2976] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3024\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3024/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3024/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3024/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3024/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3000] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3048\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3048/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3048/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3048/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3048/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3024] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3072\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3072/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3072/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3072/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3072/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3048] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3096\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3096/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3096/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3096/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3096/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3072] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3120\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3120/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3120/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3120/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3120/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3096] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3144\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3144/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3144/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3144/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3144/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3120] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3168\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3168/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3168/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3168/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3168/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3144] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3192\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3192/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3192/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3192/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3192/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3168] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3216\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3216/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3216/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3216/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3216/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3192] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3240\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3240/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3240/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3240/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3240/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3216] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3264\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3264/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3264/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3264/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3264/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3240] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3288\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3288/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3288/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3288/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3288/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3264] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3312\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3312/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3312/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3312/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3312/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3288] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3336\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3336/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3336/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3336/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3336/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3312] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3360\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3360/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3360/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3360/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3360/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3336] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3384\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3384/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3384/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3384/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3384/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3360] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3408\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3408/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3408/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3408/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3408/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3384] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3432\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3432/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3432/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3432/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3432/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3408] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3456\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3456/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3456/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3456/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3456/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3432] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3480\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3480/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3480/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3480/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3480/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3456] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3504\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3504/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3504/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3504/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3504/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3480] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3528\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3528/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3528/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3528/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3528/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-2328] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3552\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3552/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3552/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3552/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3552/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3504] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3576\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3576/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3576/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3576/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3576/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3552] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3600\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3600/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3600/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3576] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3624\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3624/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3624/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3624/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3624/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3600] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3648\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3648/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3648/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3648/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3648/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3624] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3672\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3672/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3672/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3672/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3672/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3648] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3696/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3696/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3696/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3696/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3672] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3720\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3720/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3720/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3720/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3720/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3696] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3744\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3744/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3744/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3744/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3744/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3720] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3768\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3768/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3768/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3768/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3768/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3744] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3792\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3792/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3792/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3792/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3792/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3768] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3816\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3816/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3816/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3816/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3816/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3792] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3840\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3840/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3840/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3840/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3840/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3816] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3864\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3864/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3864/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3864/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3864/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3840] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3888\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3888/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3888/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3888/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3888/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3864] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3912\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3912/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3912/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3912/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3912/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3888] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3936\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3936/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3936/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3936/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3936/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3912] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3960\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3960/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3960/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3960/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3960/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3936] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-3984\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-3984/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-3984/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-3984/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-3984/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3528] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4008\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4008/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4008/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4008/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4008/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3960] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4032\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4032/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4032/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4032/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4032/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-3984] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4056\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4056/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4056/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4056/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4056/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4008] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4080\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4080/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4080/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4080/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4080/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4056] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4104\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4104/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4104/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4104/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4104/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4080] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4128\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4128/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4128/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4128/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4128/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4104] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4152\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4152/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4152/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4152/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4152/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4128] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4176\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4176/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4176/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4176/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4176/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4152] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4200\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4200/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4200/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4176] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4224\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4224/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4224/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4224/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4224/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4200] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4248\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4248/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4248/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4248/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4248/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4224] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4272\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4272/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4272/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4272/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4272/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4248] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4296\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4296/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4296/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4296/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4296/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4032] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4320\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4320/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4320/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4320/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4320/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4272] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4344\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4344/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4344/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4344/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4344/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4320] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4368\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4368/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4368/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4368/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4368/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4344] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4392\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4392/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4392/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4392/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4392/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4368] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4416\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4416/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4416/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4416/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4416/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4392] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4440\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4440/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4440/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4440/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4440/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4416] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4464\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4464/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4464/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4464/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4464/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4440] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4488\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4488/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4488/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4488/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4488/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4464] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4512\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4512/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4512/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4512/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4512/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4488] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4536\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4536/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4536/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4536/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4536/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4512] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4560\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4560/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4560/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4560/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4560/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4536] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4584\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4584/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4584/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4584/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4584/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4560] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4608\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4608/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4608/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4608/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4608/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4296] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4632\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4632/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4632/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4632/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4632/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4584] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4656\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4656/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4656/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4656/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4656/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4608] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4680\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4680/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4680/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4680/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4680/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4656] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4704\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4704/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4704/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4704/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4704/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4680] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4728\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4728/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4728/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4728/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4728/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4704] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4752\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4752/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4752/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4752/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4752/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4728] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4776\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4776/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4776/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4776/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4776/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4752] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4800\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4800/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4800/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4776] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4824\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4824/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4824/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4824/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4824/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4800] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4848\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4848/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4848/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4848/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4848/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4824] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4872\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4872/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4872/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4872/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4872/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4848] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4896\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4896/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4896/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4896/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4896/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4872] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4920\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4920/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4920/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4920/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4920/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4896] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4944\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4944/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4944/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4944/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4944/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4920] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4968\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4968/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4968/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4968/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4968/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4944] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-4992\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-4992/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-4992/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-4992/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-4992/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4968] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5016\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5016/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5016/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5016/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5016/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-4992] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5040\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5040/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5040/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5040/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5040/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5016] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5064\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5064/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5064/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5064/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5064/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5040] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5088\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5088/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5088/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5088/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5088/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5064] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5112\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5112/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5112/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5112/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5112/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5088] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5136\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5136/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5136/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5136/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5136/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5112] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5160\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5160/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5160/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5160/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5160/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5136] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5184\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5184/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5184/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5184/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5184/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5160] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5208\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5208/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5208/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5208/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5208/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5184] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5232\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5232/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5232/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5232/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5232/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5208] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5256\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5256/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5256/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5256/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5256/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5232] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5280\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5280/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5280/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5280/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5280/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5256] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5304\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5304/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5304/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5304/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5304/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5280] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5328\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5328/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5328/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5328/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5328/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5304] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5352\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5352/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5352/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5352/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5352/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5328] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5376\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5376/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5376/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5376/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5376/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5352] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5400/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5400/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5376] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5424\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5424/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5424/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5424/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5424/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5400] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5448\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5448/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5448/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5448/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5448/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5424] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5472\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5472/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5472/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5472/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5472/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5448] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5496\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5496/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5496/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5496/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5496/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5472] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5520\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5520/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5520/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5520/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5520/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5496] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5544\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5544/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5544/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5544/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5544/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5520] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5568\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5568/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5568/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5568/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5568/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5544] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5592\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5592/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5592/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5592/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5592/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5568] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5616\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5616/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5616/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5616/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5616/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5592] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5640\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5640/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5640/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5640/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5640/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5616] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5664\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5664/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5664/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5664/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5664/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5640] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5688\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5688/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5688/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5688/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5688/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5664] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5712\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5712/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5712/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5712/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5712/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5688] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5736\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5736/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5736/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5736/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5736/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5712] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5760\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5760/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5760/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5760/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5760/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5736] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5784\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5784/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5784/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5784/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5784/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5760] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5808\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5808/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5808/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5808/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5808/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5784] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5832\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5832/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5832/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5832/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5832/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5808] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5856\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5856/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5856/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5856/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5856/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5832] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5880\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5880/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5880/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5880/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5880/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5856] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5904\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5904/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5904/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5904/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5904/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5880] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5928\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5928/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5928/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5928/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5928/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5904] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5952\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5952/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5952/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5952/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5952/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5928] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-5976\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-5976/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-5976/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-5976/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-5976/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5952] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/SYM_de_en/models/checkpoint-6000\n",
      "Configuration saved in ./output/SYM_de_en/models/checkpoint-6000/config.json\n",
      "Model weights saved in ./output/SYM_de_en/models/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/SYM_de_en/models/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/SYM_de_en/models/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/SYM_de_en/models/checkpoint-5976] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/SYM_de_en/models/checkpoint-4632 (score: 0.588).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6000, training_loss=0.7537971583207448, metrics={'train_runtime': 6682.0718, 'train_samples_per_second': 355.429, 'train_steps_per_second': 0.898, 'total_flos': 1.4666272425e+16, 'train_loss': 0.7537971583207448, 'epoch': 250.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7d1dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.588,\n",
       " 'eval_loss': 1.763461947441101,\n",
       " 'eval_runtime': 0.9676,\n",
       " 'eval_samples_per_second': 516.753,\n",
       " 'eval_steps_per_second': 2.067,\n",
       " 'epoch': 250.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Test\n",
    "trainer.evaluate(eval_dataset=tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "360c6e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation - source: taxonomischer Rang, target: taxon rank\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6175cf2df84ef58db8cbca8e297ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.63, 'eval_loss': 1.3040210008621216, 'eval_runtime': 0.6435, 'eval_samples_per_second': 155.395, 'eval_steps_per_second': 1.554}\n",
      "Relation - source: Notfalleinrichtungen, target: emergency services\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4d0e47f4214a569af843a514da9600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6, 'eval_loss': 1.5930802822113037, 'eval_runtime': 0.6238, 'eval_samples_per_second': 160.301, 'eval_steps_per_second': 1.603}\n",
      "Relation - source: Farbe, target: color\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bc09da02b742af9db53838f5c638e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.71, 'eval_loss': 0.9943457841873169, 'eval_runtime': 0.6474, 'eval_samples_per_second': 154.456, 'eval_steps_per_second': 1.545}\n",
      "Relation - source: erwartete Vollständigkeit, target: expected completeness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fe6f389c0242bca9233660a0fb618d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.45, 'eval_loss': 2.8891947269439697, 'eval_runtime': 0.6533, 'eval_samples_per_second': 153.069, 'eval_steps_per_second': 1.531}\n",
      "Relation - source: gemessene physikalische Größe, target: measured physical quantity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249c693646574d3ea48a1b67826ca319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.55, 'eval_loss': 2.0366668701171875, 'eval_runtime': 0.6294, 'eval_samples_per_second': 158.877, 'eval_steps_per_second': 1.589}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Symmetry per Relation\n",
    "evaluation_symmetry(trainer, tokenizer, relations, source_language, copy.deepcopy(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34d34722",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_anti:\n",
    "    evaluation_symmetry(trainer, tokenizer, relations_anti, source_language, copy.deepcopy(test_anti))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214e34f",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "- Test my hypothesis if (f, r, e) or (e, r_de, f) exist more?\n",
    "- Is every relation symmetric now? What about relations that aren't part of the training?\n",
    "- If every relation is symmetric, try running with ANTI\n",
    "- And with General relations\n",
    "- Try Training with General and then evaluate general like on Anti!\n",
    "- Does that change the evaluation accuracy?\n",
    "- pretrained?\n",
    "- target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77216efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b37f3489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Medina taxonomischer Rang Italie', 'Italie taxonomischer Rang Medina', 'Invasion taxonomischer Rang Bora', 'Bora taxonomischer Rang Invasion', 'Burke taxonomischer Rang Hus', 'Hus taxonomischer Rang Burke', 'Drama taxonomischer Rang epi', 'epi taxonomischer Rang Drama', 'Master taxonomischer Rang Wilfried', 'Wilfried taxonomischer Rang Master', 'Dari taxonomischer Rang Fach', 'Fach taxonomischer Rang Dari', 'Chihuahua taxonomischer Rang Inge', 'Inge taxonomischer Rang Chihuahua', 'EP taxonomischer Rang Elite', 'Elite taxonomischer Rang EP', 'Chase taxonomischer Rang Portland', 'Portland taxonomischer Rang Chase', 'Worcester taxonomischer Rang Eliza', 'Eliza taxonomischer Rang Worcester', 'Albert taxonomischer Rang Weir', 'Weir taxonomischer Rang Albert', 'Ibiza taxonomischer Rang Antoine', 'Antoine taxonomischer Rang Ibiza', 'Câmara taxonomischer Rang Universitas', 'Universitas taxonomischer Rang Câmara', 'Eleanor taxonomischer Rang Collins', 'Collins taxonomischer Rang Eleanor', 'Campus taxonomischer Rang Condor', 'Condor taxonomischer Rang Campus', 'Schaus taxonomischer Rang Gospel', 'Gospel taxonomischer Rang Schaus', 'Chaos taxonomischer Rang MS', 'MS taxonomischer Rang Chaos', 'Alice taxonomischer Rang Ramsey', 'Ramsey taxonomischer Rang Alice', 'Franklin taxonomischer Rang Katz', 'Katz taxonomischer Rang Franklin', 'Colombo taxonomischer Rang Saussure', 'Saussure taxonomischer Rang Colombo', 'Graf taxonomischer Rang BRT', 'BRT taxonomischer Rang Graf', 'Darwin taxonomischer Rang Carioca', 'Carioca taxonomischer Rang Darwin', 'Nisan taxonomischer Rang Note', 'Note taxonomischer Rang Nisan', 'Hartman taxonomischer Rang Ultra', 'Ultra taxonomischer Rang Hartman', 'Lur taxonomischer Rang Né', 'Né taxonomischer Rang Lur', 'Fiesta taxonomischer Rang Fidel', 'Fidel taxonomischer Rang Fiesta', 'Alexandra taxonomischer Rang Einer', 'Einer taxonomischer Rang Alexandra', 'Mérida taxonomischer Rang Everybody', 'Everybody taxonomischer Rang Mérida', 'Shire taxonomischer Rang Pizza', 'Pizza taxonomischer Rang Shire', 'Anthology taxonomischer Rang Koska', 'Koska taxonomischer Rang Anthology', 'Monroe taxonomischer Rang Ariane', 'Ariane taxonomischer Rang Monroe', 'Hell taxonomischer Rang Exil', 'Exil taxonomischer Rang Hell', 'Bandet taxonomischer Rang Holy', 'Holy taxonomischer Rang Bandet', 'Bertrand taxonomischer Rang Haar', 'Haar taxonomischer Rang Bertrand', 'Drum taxonomischer Rang Ringo', 'Ringo taxonomischer Rang Drum', 'IBM taxonomischer Rang Theresa', 'Theresa taxonomischer Rang IBM', 'Christchurch taxonomischer Rang Zone', 'Zone taxonomischer Rang Christchurch', 'Broadcast taxonomischer Rang Benth', 'Benth taxonomischer Rang Broadcast', 'Clay taxonomischer Rang Baja', 'Baja taxonomischer Rang Clay', 'Sound taxonomischer Rang Ruska', 'Ruska taxonomischer Rang Sound', 'Liga taxonomischer Rang Seymour', 'Seymour taxonomischer Rang Liga', 'Vatican taxonomischer Rang ECW', 'ECW taxonomischer Rang Vatican', 'Libro taxonomischer Rang Bez', 'Bez taxonomischer Rang Libro', 'Vanderbilt taxonomischer Rang Ortes', 'Ortes taxonomischer Rang Vanderbilt', 'Austrian taxonomischer Rang Jessica', 'Jessica taxonomischer Rang Austrian', 'Varese taxonomischer Rang Zoom', 'Zoom taxonomischer Rang Varese', 'Ninja taxonomischer Rang Telephone', 'Telephone taxonomischer Rang Ninja', 'Lafayette taxonomischer Rang A', 'A taxonomischer Rang Lafayette', 'Catalina taxonomischer Rang Barra', 'Barra taxonomischer Rang Catalina', 'Companion taxonomischer Rang Lori', 'Lori taxonomischer Rang Companion', 'Marshal taxonomischer Rang Reeves', 'Reeves taxonomischer Rang Marshal', 'Molina taxonomischer Rang Wagen', 'Wagen taxonomischer Rang Molina', 'Dow taxonomischer Rang Sound', 'Sound taxonomischer Rang Dow', 'Platte taxonomischer Rang JR', 'JR taxonomischer Rang Platte', 'Giant taxonomischer Rang Trail', 'Trail taxonomischer Rang Giant', 'Mulder taxonomischer Rang Concordia', 'Concordia taxonomischer Rang Mulder', 'Hancock taxonomischer Rang Windows', 'Windows taxonomischer Rang Hancock', 'Deutschland taxonomischer Rang Poco', 'Poco taxonomischer Rang Deutschland', 'Della taxonomischer Rang Suba', 'Suba taxonomischer Rang Della', 'Niko taxonomischer Rang Bartlett', 'Bartlett taxonomischer Rang Niko', 'Romans taxonomischer Rang Agent', 'Agent taxonomischer Rang Romans', 'Salman taxonomischer Rang Compton', 'Compton taxonomischer Rang Salman', 'Medi taxonomischer Rang Marshal', 'Marshal taxonomischer Rang Medi', 'Monaco taxonomischer Rang RF', 'RF taxonomischer Rang Monaco', 'Release taxonomischer Rang Lorenzo', 'Lorenzo taxonomischer Rang Release', 'Daughter taxonomischer Rang Cherry', 'Cherry taxonomischer Rang Daughter', 'Kendall taxonomischer Rang Namibia', 'Namibia taxonomischer Rang Kendall', 'PCR taxonomischer Rang FAA', 'FAA taxonomischer Rang PCR', 'Lorena taxonomischer Rang Kantonen', 'Kantonen taxonomischer Rang Lorena', 'Twain taxonomischer Rang Dunkerque', 'Dunkerque taxonomischer Rang Twain', 'Head taxonomischer Rang Demon', 'Demon taxonomischer Rang Head', 'Porta taxonomischer Rang Bears', 'Bears taxonomischer Rang Porta', 'Yunan taxonomischer Rang Paradise', 'Paradise taxonomischer Rang Yunan', 'MGM taxonomischer Rang Sidney', 'Sidney taxonomischer Rang MGM', 'Queste taxonomischer Rang ACM', 'ACM taxonomischer Rang Queste', 'Geld taxonomischer Rang Guanajuato', 'Guanajuato taxonomischer Rang Geld', 'Jesus taxonomischer Rang Bees', 'Bees taxonomischer Rang Jesus', 'Potter taxonomischer Rang Edwards', 'Edwards taxonomischer Rang Potter', 'Crown taxonomischer Rang Friesland', 'Friesland taxonomischer Rang Crown', 'Adi taxonomischer Rang Palencia', 'Palencia taxonomischer Rang Adi', 'Sinn taxonomischer Rang Baldwin', 'Baldwin taxonomischer Rang Sinn', 'Vacelet taxonomischer Rang Plans', 'Plans taxonomischer Rang Vacelet', 'Sagan taxonomischer Rang Hurley', 'Hurley taxonomischer Rang Sagan', 'Madsen taxonomischer Rang Schools', 'Schools taxonomischer Rang Madsen', 'Robinson taxonomischer Rang IX', 'IX taxonomischer Rang Robinson', 'Fayette taxonomischer Rang Alessandro', 'Alessandro taxonomischer Rang Fayette', 'Grant taxonomischer Rang Bürger', 'Bürger taxonomischer Rang Grant', 'Julian taxonomischer Rang Collegiate', 'Collegiate taxonomischer Rang Julian', 'Carrillo taxonomischer Rang Move', 'Move taxonomischer Rang Carrillo', 'Aus taxonomischer Rang KM', 'KM taxonomischer Rang Aus', 'BBC taxonomischer Rang Petra', 'Petra taxonomischer Rang BBC', 'Kongo taxonomischer Rang Nightmare', 'Nightmare taxonomischer Rang Kongo', 'Gaur taxonomischer Rang Krüger', 'Krüger taxonomischer Rang Gaur', 'Shining taxonomischer Rang Rota', 'Rota taxonomischer Rang Shining', 'Simpson taxonomischer Rang Face', 'Face taxonomischer Rang Simpson', 'Douglas taxonomischer Rang Martínez', 'Martínez taxonomischer Rang Douglas', 'Continental taxonomischer Rang Quinta', 'Quinta taxonomischer Rang Continental', 'West taxonomischer Rang Damit', 'Damit taxonomischer Rang West', 'Haas taxonomischer Rang Oper', 'Oper taxonomischer Rang Haas', 'Lynch taxonomischer Rang Frédéric', 'Frédéric taxonomischer Rang Lynch', 'Cornwall taxonomischer Rang Akdeniz', 'Akdeniz taxonomischer Rang Cornwall', 'Jalan taxonomischer Rang UC', 'UC taxonomischer Rang Jalan', 'Dhaka taxonomischer Rang Plan', 'Plan taxonomischer Rang Dhaka', 'Reading taxonomischer Rang Sailor', 'Sailor taxonomischer Rang Reading', 'Auckland taxonomischer Rang Words', 'Words taxonomischer Rang Auckland', 'Chancellor taxonomischer Rang Waterloo', 'Waterloo taxonomischer Rang Chancellor', 'Dakota taxonomischer Rang Peters', 'Peters taxonomischer Rang Dakota', 'Arles taxonomischer Rang Boyle', 'Boyle taxonomischer Rang Arles', 'Loan taxonomischer Rang Remixes', 'Remixes taxonomischer Rang Loan', 'Stab taxonomischer Rang Vienna', 'Vienna taxonomischer Rang Stab', 'Golden taxonomischer Rang Talent', 'Talent taxonomischer Rang Golden', 'Jakiel taxonomischer Rang Switch', 'Switch taxonomischer Rang Jakiel', 'Wade taxonomischer Rang Greatest', 'Greatest taxonomischer Rang Wade', 'Ebro taxonomischer Rang Shannon', 'Shannon taxonomischer Rang Ebro', 'Dorothea taxonomischer Rang Irena', 'Irena taxonomischer Rang Dorothea', 'Urban taxonomischer Rang Arabic', 'Arabic taxonomischer Rang Urban', 'HF taxonomischer Rang Membre', 'Membre taxonomischer Rang HF', 'Spectrum taxonomischer Rang Condor', 'Condor taxonomischer Rang Spectrum', 'Cass taxonomischer Rang Extra', 'Extra taxonomischer Rang Cass', 'Riva taxonomischer Rang Bil', 'Bil taxonomischer Rang Riva', 'Winston taxonomischer Rang Menor', 'Menor taxonomischer Rang Winston', 'Punk taxonomischer Rang Sprint', 'Sprint taxonomischer Rang Punk', 'SMS taxonomischer Rang Free', 'Free taxonomischer Rang SMS', 'Paglinawan taxonomischer Rang Ut', 'Ut taxonomischer Rang Paglinawan', 'McKay taxonomischer Rang Transfer', 'Transfer taxonomischer Rang McKay', 'Harmony taxonomischer Rang Batavia', 'Batavia taxonomischer Rang Harmony', 'Chantal taxonomischer Rang Das', 'Das taxonomischer Rang Chantal', 'Taurus taxonomischer Rang Lowe', 'Lowe taxonomischer Rang Taurus', 'Crosby taxonomischer Rang Flat', 'Flat taxonomischer Rang Crosby', 'GSC taxonomischer Rang Barry', 'Barry taxonomischer Rang GSC', 'Moonlight taxonomischer Rang Latreille', 'Latreille taxonomischer Rang Moonlight', 'Goodman taxonomischer Rang Bala', 'Bala taxonomischer Rang Goodman', 'Stéphane taxonomischer Rang Büyük', 'Büyük taxonomischer Rang Stéphane', 'Kader taxonomischer Rang Mittel', 'Mittel taxonomischer Rang Kader', 'Ferro taxonomischer Rang König', 'König taxonomischer Rang Ferro', 'Anglo taxonomischer Rang Yates', 'Yates taxonomischer Rang Anglo', 'Flesh taxonomischer Rang AFC', 'AFC taxonomischer Rang Flesh', 'Ubuntu taxonomischer Rang Matrix', 'Matrix taxonomischer Rang Ubuntu', 'GMT taxonomischer Rang Ottawa', 'Ottawa taxonomischer Rang GMT', 'Quintana taxonomischer Rang Tha', 'Tha taxonomischer Rang Quintana', 'Lublin taxonomischer Rang Shane', 'Shane taxonomischer Rang Lublin', 'Nagar taxonomischer Rang PCR', 'PCR taxonomischer Rang Nagar', 'Dictionary taxonomischer Rang Jazz', 'Jazz taxonomischer Rang Dictionary', 'Cats taxonomischer Rang Bismarck', 'Bismarck taxonomischer Rang Cats', 'Beyoncé taxonomischer Rang Carlton', 'Carlton taxonomischer Rang Beyoncé', 'Mendoza taxonomischer Rang George', 'George taxonomischer Rang Mendoza', 'Roja taxonomischer Rang Urban', 'Urban taxonomischer Rang Roja', 'Beckett taxonomischer Rang Samba', 'Samba taxonomischer Rang Beckett', 'Toulouse taxonomischer Rang Molina', 'Molina taxonomischer Rang Toulouse', 'Summit taxonomischer Rang Rivas', 'Rivas taxonomischer Rang Summit', 'Champion taxonomischer Rang Braga', 'Braga taxonomischer Rang Champion', 'Twins taxonomischer Rang Olav', 'Olav taxonomischer Rang Twins', 'Halo taxonomischer Rang Stephan', 'Stephan taxonomischer Rang Halo', 'Deborah taxonomischer Rang McKamey', 'McKamey taxonomischer Rang Deborah', 'SF taxonomischer Rang Burke', 'Burke taxonomischer Rang SF', 'Commonwealth taxonomischer Rang Luik', 'Luik taxonomischer Rang Commonwealth', 'Dublin taxonomischer Rang Arena', 'Arena taxonomischer Rang Dublin', 'Apocalypse taxonomischer Rang Oviedo', 'Oviedo taxonomischer Rang Apocalypse', 'Drake taxonomischer Rang Syracuse', 'Syracuse taxonomischer Rang Drake', 'Padang taxonomischer Rang Piazza', 'Piazza taxonomischer Rang Padang', 'Royal taxonomischer Rang Slag', 'Slag taxonomischer Rang Royal', 'Quick taxonomischer Rang Delft', 'Delft taxonomischer Rang Quick', 'Remote taxonomischer Rang Romagna', 'Romagna taxonomischer Rang Remote', 'Player taxonomischer Rang Partner', 'Partner taxonomischer Rang Player', 'Laba taxonomischer Rang Benton', 'Benton taxonomischer Rang Laba', 'Wiener taxonomischer Rang Quiet', 'Quiet taxonomischer Rang Wiener', 'Tripoli taxonomischer Rang Dexter', 'Dexter taxonomischer Rang Tripoli', 'Glasgow taxonomischer Rang Scientist', 'Scientist taxonomischer Rang Glasgow', 'Hansen taxonomischer Rang Maestro', 'Maestro taxonomischer Rang Hansen', 'Ortes taxonomischer Rang Busch', 'Busch taxonomischer Rang Ortes', 'Mozilla taxonomischer Rang Napoca', 'Napoca taxonomischer Rang Mozilla', 'Mur taxonomischer Rang Ball', 'Ball taxonomischer Rang Mur', 'Iberia taxonomischer Rang Lindsay', 'Lindsay taxonomischer Rang Iberia', 'Vir taxonomischer Rang Hector', 'Hector taxonomischer Rang Vir', 'Prato taxonomischer Rang IBM', 'IBM taxonomischer Rang Prato', 'Ferris taxonomischer Rang Dur', 'Dur taxonomischer Rang Ferris', 'Racing taxonomischer Rang Tanner', 'Tanner taxonomischer Rang Racing', 'Steiner taxonomischer Rang pr', 'pr taxonomischer Rang Steiner', 'Sala taxonomischer Rang Ensemble', 'Ensemble taxonomischer Rang Sala', 'Connection taxonomischer Rang Ester', 'Ester taxonomischer Rang Connection', 'Hastings taxonomischer Rang Hoya', 'Hoya taxonomischer Rang Hastings', 'Siegfried taxonomischer Rang Gauss', 'Gauss taxonomischer Rang Siegfried', 'Kale taxonomischer Rang Heine', 'Heine taxonomischer Rang Kale', 'Fourier taxonomischer Rang Madeira', 'Madeira taxonomischer Rang Fourier', 'Pada taxonomischer Rang Golf', 'Golf taxonomischer Rang Pada', 'Roberto taxonomischer Rang Pike', 'Pike taxonomischer Rang Roberto', 'Struggle taxonomischer Rang Helga', 'Helga taxonomischer Rang Struggle', 'Elijah taxonomischer Rang Down', 'Down taxonomischer Rang Elijah', 'Rote taxonomischer Rang Firefox', 'Firefox taxonomischer Rang Rote', 'Christi taxonomischer Rang Novel', 'Novel taxonomischer Rang Christi', 'Berge taxonomischer Rang Addison', 'Addison taxonomischer Rang Berge', 'Wimbledon taxonomischer Rang Loving', 'Loving taxonomischer Rang Wimbledon', 'Bees taxonomischer Rang Desse', 'Desse taxonomischer Rang Bees', 'Palm taxonomischer Rang Middlesex', 'Middlesex taxonomischer Rang Palm', 'Nor taxonomischer Rang Beth', 'Beth taxonomischer Rang Nor', 'Maria taxonomischer Rang Angoulême', 'Angoulême taxonomischer Rang Maria', 'Stadio taxonomischer Rang Grimaldi', 'Grimaldi taxonomischer Rang Stadio', 'Belo taxonomischer Rang Tobias', 'Tobias taxonomischer Rang Belo', 'Boga taxonomischer Rang Tat', 'Tat taxonomischer Rang Boga', 'NSW taxonomischer Rang Barry', 'Barry taxonomischer Rang NSW', 'Roller taxonomischer Rang Regno', 'Regno taxonomischer Rang Roller', 'León taxonomischer Rang CAD', 'CAD taxonomischer Rang León', 'Rollen taxonomischer Rang Machado', 'Machado taxonomischer Rang Rollen', 'Agama taxonomischer Rang Reich', 'Reich taxonomischer Rang Agama', 'Seminary taxonomischer Rang Choice', 'Choice taxonomischer Rang Seminary', 'Ensemble taxonomischer Rang Argentine', 'Argentine taxonomischer Rang Ensemble', 'Blade taxonomischer Rang Huntington', 'Huntington taxonomischer Rang Blade', 'Cochrane taxonomischer Rang Monk', 'Monk taxonomischer Rang Cochrane', 'Indiana taxonomischer Rang Mines', 'Mines taxonomischer Rang Indiana', 'Carvalho taxonomischer Rang Weston', 'Weston taxonomischer Rang Carvalho', 'René taxonomischer Rang CE', 'CE taxonomischer Rang René', 'Mask taxonomischer Rang Sint', 'Sint taxonomischer Rang Mask', 'Dacia taxonomischer Rang Lego', 'Lego taxonomischer Rang Dacia', 'Cea taxonomischer Rang Amour', 'Amour taxonomischer Rang Cea', 'Bulgaria taxonomischer Rang Martinez', 'Martinez taxonomischer Rang Bulgaria', 'Rocket taxonomischer Rang Ida', 'Ida taxonomischer Rang Rocket', 'Pro taxonomischer Rang Julie', 'Julie taxonomischer Rang Pro', 'Portugal taxonomischer Rang KK', 'KK taxonomischer Rang Portugal', 'Blanco taxonomischer Rang Trouble', 'Trouble taxonomischer Rang Blanco', 'Addison taxonomischer Rang Partido', 'Partido taxonomischer Rang Addison', 'Como taxonomischer Rang Jess', 'Jess taxonomischer Rang Como', 'Suomi taxonomischer Rang Keller', 'Keller taxonomischer Rang Suomi', 'Alt taxonomischer Rang Österreich', 'Österreich taxonomischer Rang Alt', 'Esther taxonomischer Rang Britten', 'Britten taxonomischer Rang Esther', 'Sick taxonomischer Rang Dacia', 'Dacia taxonomischer Rang Sick', 'Bowman taxonomischer Rang Pt', 'Pt taxonomischer Rang Bowman', 'Wells taxonomischer Rang Ekim', 'Ekim taxonomischer Rang Wells', 'NT taxonomischer Rang Cavendish', 'Cavendish taxonomischer Rang NT', 'Titanic taxonomischer Rang Durban', 'Durban taxonomischer Rang Titanic', 'Chamber taxonomischer Rang Garland', 'Garland taxonomischer Rang Chamber', 'Satellite taxonomischer Rang Check', 'Check taxonomischer Rang Satellite', 'Niels taxonomischer Rang Thor', 'Thor taxonomischer Rang Niels', 'Borneo taxonomischer Rang Remixes', 'Remixes taxonomischer Rang Borneo', 'Tigre taxonomischer Rang Pierce', 'Pierce taxonomischer Rang Tigre', 'Madagascar taxonomischer Rang Band', 'Band taxonomischer Rang Madagascar', 'Vincent taxonomischer Rang Baird', 'Baird taxonomischer Rang Vincent', 'Midlands taxonomischer Rang Sia', 'Sia taxonomischer Rang Midlands', 'Siam taxonomischer Rang Züge', 'Züge taxonomischer Rang Siam', 'Uit taxonomischer Rang Clyde', 'Clyde taxonomischer Rang Uit', 'CDC taxonomischer Rang Underground', 'Underground taxonomischer Rang CDC', 'Sawyer taxonomischer Rang Luxemburg', 'Luxemburg taxonomischer Rang Sawyer', 'Ranking taxonomischer Rang Sørensen', 'Sørensen taxonomischer Rang Ranking', 'Babylon taxonomischer Rang Esse', 'Esse taxonomischer Rang Babylon', 'Côte taxonomischer Rang Malay', 'Malay taxonomischer Rang Côte', 'IS taxonomischer Rang Wish', 'Wish taxonomischer Rang IS', 'Frost taxonomischer Rang Voyager', 'Voyager taxonomischer Rang Frost', 'Mariana taxonomischer Rang Holocaust', 'Holocaust taxonomischer Rang Mariana', 'Baza taxonomischer Rang LP', 'LP taxonomischer Rang Baza', 'Washington taxonomischer Rang Medley', 'Medley taxonomischer Rang Washington', 'Giles taxonomischer Rang Buena', 'Buena taxonomischer Rang Giles', 'Benton taxonomischer Rang Ponte', 'Ponte taxonomischer Rang Benton', 'Balázs taxonomischer Rang Ávila', 'Ávila taxonomischer Rang Balázs', 'Pays taxonomischer Rang Cullen', 'Cullen taxonomischer Rang Pays', 'Alman taxonomischer Rang Les', 'Les taxonomischer Rang Alman', 'Hitchcock taxonomischer Rang Gerard', 'Gerard taxonomischer Rang Hitchcock', 'Danube taxonomischer Rang ISS', 'ISS taxonomischer Rang Danube', 'Quattro taxonomischer Rang Trees', 'Trees taxonomischer Rang Quattro', 'Troy taxonomischer Rang Franjo', 'Franjo taxonomischer Rang Troy', 'Hector taxonomischer Rang Vas', 'Vas taxonomischer Rang Hector', 'Hip taxonomischer Rang Pioneer', 'Pioneer taxonomischer Rang Hip', 'Face taxonomischer Rang Isles', 'Isles taxonomischer Rang Face', 'Funk taxonomischer Rang Qu', 'Qu taxonomischer Rang Funk', 'Hazel taxonomischer Rang Cesar', 'Cesar taxonomischer Rang Hazel', 'Romawi taxonomischer Rang Satan', 'Satan taxonomischer Rang Romawi', 'Straits taxonomischer Rang EC', 'EC taxonomischer Rang Straits', 'Put taxonomischer Rang Delft', 'Delft taxonomischer Rang Put', 'ATP taxonomischer Rang Ter', 'Ter taxonomischer Rang ATP', 'Abucay taxonomischer Rang Burma', 'Burma taxonomischer Rang Abucay', 'Spor taxonomischer Rang Freie', 'Freie taxonomischer Rang Spor', 'Cathedral taxonomischer Rang Bride', 'Bride taxonomischer Rang Cathedral', 'Vijay taxonomischer Rang Espagne', 'Espagne taxonomischer Rang Vijay', 'Géza taxonomischer Rang Thành', 'Thành taxonomischer Rang Géza', 'Werk taxonomischer Rang Bunker', 'Bunker taxonomischer Rang Werk', 'Jungen taxonomischer Rang Italiana', 'Italiana taxonomischer Rang Jungen', 'Harri taxonomischer Rang Bad', 'Bad taxonomischer Rang Harri', 'Wood taxonomischer Rang Kamera', 'Kamera taxonomischer Rang Wood', 'XP taxonomischer Rang Beatrice', 'Beatrice taxonomischer Rang XP', 'Henning taxonomischer Rang Henri', 'Henri taxonomischer Rang Henning', 'Daniel taxonomischer Rang Mound', 'Mound taxonomischer Rang Daniel', 'Lac taxonomischer Rang Space', 'Space taxonomischer Rang Lac', 'Niger taxonomischer Rang USSR', 'USSR taxonomischer Rang Niger', 'Bon taxonomischer Rang Seigneur', 'Seigneur taxonomischer Rang Bon', 'Hess taxonomischer Rang Comte', 'Comte taxonomischer Rang Hess', 'Asunción taxonomischer Rang Díaz', 'Díaz taxonomischer Rang Asunción', 'ba taxonomischer Rang Wallace', 'Wallace taxonomischer Rang ba', 'Roll taxonomischer Rang Nepal', 'Nepal taxonomischer Rang Roll', 'Nada taxonomischer Rang Platnick', 'Platnick taxonomischer Rang Nada', 'Saale taxonomischer Rang FF', 'FF taxonomischer Rang Saale', 'Minden taxonomischer Rang Mickey', 'Mickey taxonomischer Rang Minden', 'Imperial taxonomischer Rang Railroad', 'Railroad taxonomischer Rang Imperial', 'Moreau taxonomischer Rang Samo', 'Samo taxonomischer Rang Moreau', 'Provence taxonomischer Rang Bach', 'Bach taxonomischer Rang Provence', 'Archer taxonomischer Rang Batman', 'Batman taxonomischer Rang Archer', 'JNA taxonomischer Rang Tucson', 'Tucson taxonomischer Rang JNA', 'Punjabi taxonomischer Rang Driver', 'Driver taxonomischer Rang Punjabi', 'Tarragona taxonomischer Rang Rat', 'Rat taxonomischer Rang Tarragona', 'Pirates taxonomischer Rang Hastings', 'Hastings taxonomischer Rang Pirates', 'Mat taxonomischer Rang Providence', 'Providence taxonomischer Rang Mat', 'Berlin taxonomischer Rang Ja', 'Ja taxonomischer Rang Berlin', 'Thành taxonomischer Rang Finale', 'Finale taxonomischer Rang Thành', 'Vance taxonomischer Rang Rex', 'Rex taxonomischer Rang Vance', 'Banja taxonomischer Rang Ace', 'Ace taxonomischer Rang Banja', 'Champ taxonomischer Rang Saskatchewan', 'Saskatchewan taxonomischer Rang Champ', 'Trieste taxonomischer Rang Sigurd', 'Sigurd taxonomischer Rang Trieste', 'Pi taxonomischer Rang Potsdam', 'Potsdam taxonomischer Rang Pi', 'Cisco taxonomischer Rang Irvine', 'Irvine taxonomischer Rang Cisco', 'Cologne taxonomischer Rang Coleman', 'Coleman taxonomischer Rang Cologne', 'Banks taxonomischer Rang Phil', 'Phil taxonomischer Rang Banks', 'MotoGP taxonomischer Rang Sul', 'Sul taxonomischer Rang MotoGP', 'Spiegel taxonomischer Rang Danh', 'Danh taxonomischer Rang Spiegel', 'Dara taxonomischer Rang Little', 'Little taxonomischer Rang Dara', 'Platinum taxonomischer Rang Basso', 'Basso taxonomischer Rang Platinum', 'Calvin taxonomischer Rang Gamma', 'Gamma taxonomischer Rang Calvin', 'Gauss taxonomischer Rang Dana', 'Dana taxonomischer Rang Gauss', 'Lena taxonomischer Rang Harbor', 'Harbor taxonomischer Rang Lena', 'Voltaire taxonomischer Rang Bears', 'Bears taxonomischer Rang Voltaire', 'Polar taxonomischer Rang Bristol', 'Bristol taxonomischer Rang Polar', 'Depot taxonomischer Rang Rusi', 'Rusi taxonomischer Rang Depot', 'Petit taxonomischer Rang Maxime', 'Maxime taxonomischer Rang Petit', 'Einstein taxonomischer Rang Jagger', 'Jagger taxonomischer Rang Einstein', 'Bay taxonomischer Rang Swan', 'Swan taxonomischer Rang Bay', 'Skin taxonomischer Rang Lord', 'Lord taxonomischer Rang Skin', 'Donna taxonomischer Rang SF', 'SF taxonomischer Rang Donna', 'Pasteur taxonomischer Rang Coffee', 'Coffee taxonomischer Rang Pasteur', 'Seul taxonomischer Rang Seele', 'Seele taxonomischer Rang Seul', 'Middlesex taxonomischer Rang Victorian', 'Victorian taxonomischer Rang Middlesex', 'Closer taxonomischer Rang Blanchard', 'Blanchard taxonomischer Rang Closer', 'Hollow taxonomischer Rang Happy', 'Happy taxonomischer Rang Hollow', 'Malang taxonomischer Rang Rae', 'Rae taxonomischer Rang Malang', 'KPD taxonomischer Rang Bitte', 'Bitte taxonomischer Rang KPD', 'Palma taxonomischer Rang Beyaz', 'Beyaz taxonomischer Rang Palma', 'AFL taxonomischer Rang Hügel', 'Hügel taxonomischer Rang AFL', 'Accademia taxonomischer Rang Free', 'Free taxonomischer Rang Accademia', 'Leafs taxonomischer Rang Cynthia', 'Cynthia taxonomischer Rang Leafs', 'Principal taxonomischer Rang Tông', 'Tông taxonomischer Rang Principal', 'Carla taxonomischer Rang Bouchet', 'Bouchet taxonomischer Rang Carla', 'Rally taxonomischer Rang Barbosa', 'Barbosa taxonomischer Rang Rally', 'Lilla taxonomischer Rang Assembly', 'Assembly taxonomischer Rang Lilla', 'Gamble taxonomischer Rang Stranger', 'Stranger taxonomischer Rang Gamble', 'Alec taxonomischer Rang Valls', 'Valls taxonomischer Rang Alec', 'Medvedev taxonomischer Rang Fresno', 'Fresno taxonomischer Rang Medvedev', 'Nino taxonomischer Rang NME', 'NME taxonomischer Rang Nino', 'Willard taxonomischer Rang Rover', 'Rover taxonomischer Rang Willard', 'Seoul taxonomischer Rang DS', 'DS taxonomischer Rang Seoul', 'Bala taxonomischer Rang Ja', 'Ja taxonomischer Rang Bala', 'Stift taxonomischer Rang Wenn', 'Wenn taxonomischer Rang Stift', 'Albania taxonomischer Rang Boone', 'Boone taxonomischer Rang Albania', 'Grund taxonomischer Rang Indonesia', 'Indonesia taxonomischer Rang Grund', 'Indre taxonomischer Rang Generation', 'Generation taxonomischer Rang Indre', 'Poslední taxonomischer Rang Avon', 'Avon taxonomischer Rang Poslední', 'Switch taxonomischer Rang Steen', 'Steen taxonomischer Rang Switch', 'Coral taxonomischer Rang Kobayashi', 'Kobayashi taxonomischer Rang Coral', 'Ethel taxonomischer Rang Ng', 'Ng taxonomischer Rang Ethel', 'Charlie taxonomischer Rang Cine', 'Cine taxonomischer Rang Charlie', 'Warren taxonomischer Rang Massacre', 'Massacre taxonomischer Rang Warren', 'Irvine taxonomischer Rang Sprint', 'Sprint taxonomischer Rang Irvine', 'Quinta taxonomischer Rang Aziz', 'Aziz taxonomischer Rang Quinta', 'Up taxonomischer Rang Hang', 'Hang taxonomischer Rang Up', 'Heft taxonomischer Rang Tiempo', 'Tiempo taxonomischer Rang Heft', 'Cap taxonomischer Rang Senat', 'Senat taxonomischer Rang Cap', 'NN taxonomischer Rang Dunia', 'Dunia taxonomischer Rang NN', 'Romas taxonomischer Rang Independencia', 'Independencia taxonomischer Rang Romas', 'Benedict taxonomischer Rang Genocide', 'Genocide taxonomischer Rang Benedict', 'Zurich taxonomischer Rang Atlanta', 'Atlanta taxonomischer Rang Zurich', 'Capitaine taxonomischer Rang Nash', 'Nash taxonomischer Rang Capitaine', 'Camilla taxonomischer Rang Lys', 'Lys taxonomischer Rang Camilla', 'Theatre taxonomischer Rang Valley', 'Valley taxonomischer Rang Theatre', 'Eo taxonomischer Rang Ingles', 'Ingles taxonomischer Rang Eo', 'Kelley taxonomischer Rang Kensington', 'Kensington taxonomischer Rang Kelley', 'Ola taxonomischer Rang Remo', 'Remo taxonomischer Rang Ola', 'Gibraltar taxonomischer Rang Henning', 'Henning taxonomischer Rang Gibraltar', 'Alabama taxonomischer Rang Rocca', 'Rocca taxonomischer Rang Alabama', 'Revue taxonomischer Rang Cynthia', 'Cynthia taxonomischer Rang Revue', 'Duran taxonomischer Rang Tech', 'Tech taxonomischer Rang Duran', 'Guía taxonomischer Rang ag', 'ag taxonomischer Rang Guía', 'Wanda taxonomischer Rang Odessa', 'Odessa taxonomischer Rang Wanda', 'Piemonte taxonomischer Rang Rhapsody', 'Rhapsody taxonomischer Rang Piemonte', 'Impact taxonomischer Rang Carlos', 'Carlos taxonomischer Rang Impact', 'Vene taxonomischer Rang Maka', 'Maka taxonomischer Rang Vene', 'KBS taxonomischer Rang Larry', 'Larry taxonomischer Rang KBS', 'Emery taxonomischer Rang Neki', 'Neki taxonomischer Rang Emery', 'Alexandria taxonomischer Rang Brisbane', 'Brisbane taxonomischer Rang Alexandria', 'Bey taxonomischer Rang Each', 'Each taxonomischer Rang Bey', 'Studi taxonomischer Rang Guerre', 'Guerre taxonomischer Rang Studi', 'Coll taxonomischer Rang Alta', 'Alta taxonomischer Rang Coll', 'Jeffries taxonomischer Rang Margareta', 'Margareta taxonomischer Rang Jeffries', 'Forst taxonomischer Rang Vacelet', 'Vacelet taxonomischer Rang Forst', 'Casablanca taxonomischer Rang Dol', 'Dol taxonomischer Rang Casablanca', 'Kati taxonomischer Rang Kirche', 'Kirche taxonomischer Rang Kati', 'Kort taxonomischer Rang Bambino', 'Bambino taxonomischer Rang Kort', 'Teluk taxonomischer Rang Royal', 'Royal taxonomischer Rang Teluk', 'Farrell taxonomischer Rang ABS', 'ABS taxonomischer Rang Farrell', 'Ehren taxonomischer Rang Madison', 'Madison taxonomischer Rang Ehren', 'Tampere taxonomischer Rang Ostrava', 'Ostrava taxonomischer Rang Tampere', 'Kepler taxonomischer Rang Díaz', 'Díaz taxonomischer Rang Kepler', 'Madeleine taxonomischer Rang Gina', 'Gina taxonomischer Rang Madeleine', 'Milan taxonomischer Rang Viru', 'Viru taxonomischer Rang Milan', 'Translation taxonomischer Rang Wellington', 'Wellington taxonomischer Rang Translation', 'Koch taxonomischer Rang Icarus', 'Icarus taxonomischer Rang Koch', 'Butler taxonomischer Rang Yunan', 'Yunan taxonomischer Rang Butler', 'Neuchâtel taxonomischer Rang Slot', 'Slot taxonomischer Rang Neuchâtel', 'Delaware taxonomischer Rang Grammar', 'Grammar taxonomischer Rang Delaware', 'Wittenberg taxonomischer Rang Sân', 'Sân taxonomischer Rang Wittenberg', 'Cidade taxonomischer Rang NN', 'NN taxonomischer Rang Cidade', 'Murphy taxonomischer Rang Haag', 'Haag taxonomischer Rang Murphy', 'su taxonomischer Rang Cash', 'Cash taxonomischer Rang su', 'Ses taxonomischer Rang Basso', 'Basso taxonomischer Rang Ses', 'Progreso taxonomischer Rang Gibson', 'Gibson taxonomischer Rang Progreso', 'Curie taxonomischer Rang Nota', 'Nota taxonomischer Rang Curie', 'Ned taxonomischer Rang Hour', 'Hour taxonomischer Rang Ned', 'Zealand taxonomischer Rang Loch', 'Loch taxonomischer Rang Zealand', 'Bennett taxonomischer Rang Atatürk', 'Atatürk taxonomischer Rang Bennett', 'Porsche taxonomischer Rang México', 'México taxonomischer Rang Porsche', 'Villiers taxonomischer Rang Algarve', 'Algarve taxonomischer Rang Villiers', 'Niño taxonomischer Rang Generation', 'Generation taxonomischer Rang Niño', 'Balance taxonomischer Rang Rhein', 'Rhein taxonomischer Rang Balance', 'Barth taxonomischer Rang Brod', 'Brod taxonomischer Rang Barth', 'Robot taxonomischer Rang Dem', 'Dem taxonomischer Rang Robot', 'Sinh taxonomischer Rang Nigel', 'Nigel taxonomischer Rang Sinh', 'Gillespie taxonomischer Rang Holloway', 'Holloway taxonomischer Rang Gillespie', 'Titan taxonomischer Rang IX', 'IX taxonomischer Rang Titan', 'Tierra taxonomischer Rang Stal', 'Stal taxonomischer Rang Tierra', 'ID taxonomischer Rang Sato', 'Sato taxonomischer Rang ID', 'WWF taxonomischer Rang FX', 'FX taxonomischer Rang WWF', 'Azur taxonomischer Rang Vries', 'Vries taxonomischer Rang Azur', 'Reason taxonomischer Rang Fusion', 'Fusion taxonomischer Rang Reason', 'Luke taxonomischer Rang Terminal', 'Terminal taxonomischer Rang Luke', 'Trees taxonomischer Rang Maa', 'Maa taxonomischer Rang Trees', 'Morris taxonomischer Rang Bayreuth', 'Bayreuth taxonomischer Rang Morris', 'Monate taxonomischer Rang Catharina', 'Catharina taxonomischer Rang Monate', 'Norway taxonomischer Rang Naomi', 'Naomi taxonomischer Rang Norway', 'Han taxonomischer Rang Drew', 'Drew taxonomischer Rang Han', 'Cuban taxonomischer Rang Path', 'Path taxonomischer Rang Cuban', 'Melbourne taxonomischer Rang Hector', 'Hector taxonomischer Rang Melbourne', 'Rooma taxonomischer Rang Colle', 'Colle taxonomischer Rang Rooma', 'Ad taxonomischer Rang Inspector', 'Inspector taxonomischer Rang Ad', 'Christ taxonomischer Rang WDR', 'WDR taxonomischer Rang Christ', 'PSA taxonomischer Rang Nizza', 'Nizza taxonomischer Rang PSA', 'Omer taxonomischer Rang Camino', 'Camino taxonomischer Rang Omer', 'THE taxonomischer Rang Noche', 'Noche taxonomischer Rang THE', 'Hand taxonomischer Rang Patria', 'Patria taxonomischer Rang Hand', 'Urgell taxonomischer Rang Rees', 'Rees taxonomischer Rang Urgell', 'Liv taxonomischer Rang Tucumán', 'Tucumán taxonomischer Rang Liv', 'Bonaparte taxonomischer Rang Copper', 'Copper taxonomischer Rang Bonaparte', 'Tempo taxonomischer Rang KBS', 'KBS taxonomischer Rang Tempo', 'Abel taxonomischer Rang Sporting', 'Sporting taxonomischer Rang Abel', 'Gegen taxonomischer Rang Irvine', 'Irvine taxonomischer Rang Gegen', 'Hoy taxonomischer Rang Lilly', 'Lilly taxonomischer Rang Hoy', 'Stil taxonomischer Rang Amalia', 'Amalia taxonomischer Rang Stil', 'CDP taxonomischer Rang Güney', 'Güney taxonomischer Rang CDP', 'Kanal taxonomischer Rang Kepler', 'Kepler taxonomischer Rang Kanal', 'Izrael taxonomischer Rang Bala', 'Bala taxonomischer Rang Izrael', 'Genesis taxonomischer Rang Questa', 'Questa taxonomischer Rang Genesis', 'Eylül taxonomischer Rang Roberts', 'Roberts taxonomischer Rang Eylül', 'Vivaldi taxonomischer Rang Doom', 'Doom taxonomischer Rang Vivaldi', 'Mario taxonomischer Rang Halk', 'Halk taxonomischer Rang Mario', 'Power taxonomischer Rang Duo', 'Duo taxonomischer Rang Power', 'Saussure taxonomischer Rang Dub', 'Dub taxonomischer Rang Saussure', 'Muir taxonomischer Rang Emily', 'Emily taxonomischer Rang Muir', 'Alonso taxonomischer Rang Walls', 'Walls taxonomischer Rang Alonso', 'Largo taxonomischer Rang Claude', 'Claude taxonomischer Rang Largo', 'Phi taxonomischer Rang Court', 'Court taxonomischer Rang Phi', 'While taxonomischer Rang Down', 'Down taxonomischer Rang While', 'Rain taxonomischer Rang Crash', 'Crash taxonomischer Rang Rain', 'Canary taxonomischer Rang Gibson', 'Gibson taxonomischer Rang Canary', 'Arms taxonomischer Rang Kappa', 'Kappa taxonomischer Rang Arms', 'Bil taxonomischer Rang Cruz', 'Cruz taxonomischer Rang Bil', 'Cause taxonomischer Rang Nino', 'Nino taxonomischer Rang Cause', 'Hitler taxonomischer Rang Hubbard', 'Hubbard taxonomischer Rang Hitler', 'Kimberly taxonomischer Rang Georgetown', 'Georgetown taxonomischer Rang Kimberly', 'Swan taxonomischer Rang Host', 'Host taxonomischer Rang Swan', 'Parker taxonomischer Rang Sato', 'Sato taxonomischer Rang Parker', 'WDR taxonomischer Rang UCD', 'UCD taxonomischer Rang WDR', 'Gama taxonomischer Rang Vancouver', 'Vancouver taxonomischer Rang Gama', 'Pam taxonomischer Rang CR', 'CR taxonomischer Rang Pam', 'Leif taxonomischer Rang Dacia', 'Dacia taxonomischer Rang Leif', 'IM taxonomischer Rang Hop', 'Hop taxonomischer Rang IM', 'Comte taxonomischer Rang Lino', 'Lino taxonomischer Rang Comte', 'Stella taxonomischer Rang Wonderland', 'Wonderland taxonomischer Rang Stella', 'Speedway taxonomischer Rang Yates', 'Yates taxonomischer Rang Speedway', 'Linda taxonomischer Rang Sabbath', 'Sabbath taxonomischer Rang Linda', 'Dok taxonomischer Rang DM', 'DM taxonomischer Rang Dok', 'Earth taxonomischer Rang Imre', 'Imre taxonomischer Rang Earth', 'Ele taxonomischer Rang Marea', 'Marea taxonomischer Rang Ele', 'Limited taxonomischer Rang Interview', 'Interview taxonomischer Rang Limited', 'GM taxonomischer Rang Lennox', 'Lennox taxonomischer Rang GM', 'Isola taxonomischer Rang Thing', 'Thing taxonomischer Rang Isola', 'Kahn taxonomischer Rang Erzurum', 'Erzurum taxonomischer Rang Kahn', 'Slavic taxonomischer Rang Vanderbilt', 'Vanderbilt taxonomischer Rang Slavic', 'Subway taxonomischer Rang Murat', 'Murat taxonomischer Rang Subway', 'Tracy taxonomischer Rang Anthem', 'Anthem taxonomischer Rang Tracy', 'Stage taxonomischer Rang Saints', 'Saints taxonomischer Rang Stage', 'Rady taxonomischer Rang Nieto', 'Nieto taxonomischer Rang Rady', 'Tanner taxonomischer Rang Higher', 'Higher taxonomischer Rang Tanner', 'Newport taxonomischer Rang Andere', 'Andere taxonomischer Rang Newport', 'Holt taxonomischer Rang Each', 'Each taxonomischer Rang Holt', 'Ion taxonomischer Rang Hara', 'Hara taxonomischer Rang Ion', 'Amigos taxonomischer Rang Albert', 'Albert taxonomischer Rang Amigos', 'Bruges taxonomischer Rang Hazel', 'Hazel taxonomischer Rang Bruges', 'Astra taxonomischer Rang Sun', 'Sun taxonomischer Rang Astra', 'KK taxonomischer Rang Oud', 'Oud taxonomischer Rang KK', 'Alliance taxonomischer Rang Sheppard', 'Sheppard taxonomischer Rang Alliance', 'Dana taxonomischer Rang Haley', 'Haley taxonomischer Rang Dana', 'Rep taxonomischer Rang Schultz', 'Schultz taxonomischer Rang Rep', 'Albin taxonomischer Rang Cinta', 'Cinta taxonomischer Rang Albin', 'Nissan taxonomischer Rang Madagascar', 'Madagascar taxonomischer Rang Nissan', 'Aberdeen taxonomischer Rang Rond', 'Rond taxonomischer Rang Aberdeen', 'Az taxonomischer Rang Romawi', 'Romawi taxonomischer Rang Az', 'Jenna taxonomischer Rang Australian', 'Australian taxonomischer Rang Jenna', 'Hamlet taxonomischer Rang Humboldt', 'Humboldt taxonomischer Rang Hamlet', 'Pretoria taxonomischer Rang Sono', 'Sono taxonomischer Rang Pretoria', 'Latin taxonomischer Rang Engagement', 'Engagement taxonomischer Rang Latin', 'ESPN taxonomischer Rang Pleasure', 'Pleasure taxonomischer Rang ESPN', 'Florence taxonomischer Rang Gallagher', 'Gallagher taxonomischer Rang Florence', 'Philippe taxonomischer Rang Ryan', 'Ryan taxonomischer Rang Philippe', 'Phelps taxonomischer Rang Klub', 'Klub taxonomischer Rang Phelps', 'Zoom taxonomischer Rang Ken', 'Ken taxonomischer Rang Zoom', 'Isto taxonomischer Rang Lotto', 'Lotto taxonomischer Rang Isto', 'Sporting taxonomischer Rang Dante', 'Dante taxonomischer Rang Sporting', 'Star taxonomischer Rang Space', 'Space taxonomischer Rang Star', 'Padre taxonomischer Rang Closer', 'Closer taxonomischer Rang Padre', 'Perth taxonomischer Rang Cynthia', 'Cynthia taxonomischer Rang Perth', 'Cecil taxonomischer Rang Rain', 'Rain taxonomischer Rang Cecil', 'Rec taxonomischer Rang Fontana', 'Fontana taxonomischer Rang Rec', 'Katrina taxonomischer Rang Fisch', 'Fisch taxonomischer Rang Katrina', 'Lincoln taxonomischer Rang Truman', 'Truman taxonomischer Rang Lincoln', 'Xuân taxonomischer Rang Resident', 'Resident taxonomischer Rang Xuân', 'Pet taxonomischer Rang Meet', 'Meet taxonomischer Rang Pet', 'Hora taxonomischer Rang Aragón', 'Aragón taxonomischer Rang Hora', 'Heads taxonomischer Rang Albion', 'Albion taxonomischer Rang Heads', 'Coimbra taxonomischer Rang Faber', 'Faber taxonomischer Rang Coimbra', 'Magister taxonomischer Rang Bot', 'Bot taxonomischer Rang Magister', 'Kristen taxonomischer Rang Lodge', 'Lodge taxonomischer Rang Kristen', 'Benson taxonomischer Rang Jos', 'Jos taxonomischer Rang Benson', 'ES taxonomischer Rang Sunset', 'Sunset taxonomischer Rang ES', 'Ayn taxonomischer Rang Kita', 'Kita taxonomischer Rang Ayn', 'Cu taxonomischer Rang Hilton', 'Hilton taxonomischer Rang Cu', 'Champs taxonomischer Rang Abby', 'Abby taxonomischer Rang Champs', 'Lydia taxonomischer Rang Alicante', 'Alicante taxonomischer Rang Lydia', 'Greg taxonomischer Rang Ratu', 'Ratu taxonomischer Rang Greg', 'Fortaleza taxonomischer Rang Barre', 'Barre taxonomischer Rang Fortaleza', 'TSV taxonomischer Rang Bells', 'Bells taxonomischer Rang TSV', 'Bulu taxonomischer Rang Ramsay', 'Ramsay taxonomischer Rang Bulu', 'Finale taxonomischer Rang Berge', 'Berge taxonomischer Rang Finale', 'Mitt taxonomischer Rang Denne', 'Denne taxonomischer Rang Mitt', 'Lyman taxonomischer Rang Schumann', 'Schumann taxonomischer Rang Lyman', 'Zeeland taxonomischer Rang Sweeney', 'Sweeney taxonomischer Rang Zeeland', 'Algeria taxonomischer Rang Nisan', 'Nisan taxonomischer Rang Algeria', 'Dada taxonomischer Rang Buddy', 'Buddy taxonomischer Rang Dada', 'Sheridan taxonomischer Rang Verder', 'Verder taxonomischer Rang Sheridan', 'Pinto taxonomischer Rang Jared', 'Jared taxonomischer Rang Pinto', 'Kelly taxonomischer Rang Tabachnick', 'Tabachnick taxonomischer Rang Kelly', 'Soria taxonomischer Rang Maxime', 'Maxime taxonomischer Rang Soria', 'Fairfax taxonomischer Rang Ship', 'Ship taxonomischer Rang Fairfax', 'Tutte taxonomischer Rang Quito', 'Quito taxonomischer Rang Tutte', 'Cent taxonomischer Rang Mer', 'Mer taxonomischer Rang Cent', 'AG taxonomischer Rang Zeta', 'Zeta taxonomischer Rang AG', 'Fund taxonomischer Rang Mesa', 'Mesa taxonomischer Rang Fund', 'Louisville taxonomischer Rang Lyndon', 'Lyndon taxonomischer Rang Louisville', 'Veracruz taxonomischer Rang Globo', 'Globo taxonomischer Rang Veracruz', 'Sabbath taxonomischer Rang Fortune', 'Fortune taxonomischer Rang Sabbath', 'Duck taxonomischer Rang PKK', 'PKK taxonomischer Rang Duck', 'Jack taxonomischer Rang Swami', 'Swami taxonomischer Rang Jack', 'IRAS taxonomischer Rang Rotterdam', 'Rotterdam taxonomischer Rang IRAS', 'Cherokee taxonomischer Rang Tin', 'Tin taxonomischer Rang Cherokee', 'Oper taxonomischer Rang Abrams', 'Abrams taxonomischer Rang Oper', 'Raphaël taxonomischer Rang Hampton', 'Hampton taxonomischer Rang Raphaël', 'Women taxonomischer Rang Paso', 'Paso taxonomischer Rang Women', 'Patton taxonomischer Rang Appleton', 'Appleton taxonomischer Rang Patton', 'Al taxonomischer Rang Pool', 'Pool taxonomischer Rang Al', 'Oaxaca taxonomischer Rang Appleton', 'Appleton taxonomischer Rang Oaxaca', 'Direito taxonomischer Rang CDC', 'CDC taxonomischer Rang Direito', 'Province taxonomischer Rang Bent', 'Bent taxonomischer Rang Province', 'Grimaldi taxonomischer Rang Tales', 'Tales taxonomischer Rang Grimaldi', 'Ruska taxonomischer Rang Alzheimer', 'Alzheimer taxonomischer Rang Ruska', 'Jess taxonomischer Rang Venecia', 'Venecia taxonomischer Rang Jess', 'Lugar taxonomischer Rang Lobo', 'Lobo taxonomischer Rang Lugar', 'Paul taxonomischer Rang Dre', 'Dre taxonomischer Rang Paul', 'CAD taxonomischer Rang Jerome', 'Jerome taxonomischer Rang CAD', 'Adrian taxonomischer Rang Jamaica', 'Jamaica taxonomischer Rang Adrian', 'Color taxonomischer Rang Melo', 'Melo taxonomischer Rang Color', 'Thornton taxonomischer Rang Auvergne', 'Auvergne taxonomischer Rang Thornton', 'Kampung taxonomischer Rang Kirkwood', 'Kirkwood taxonomischer Rang Kampung', 'Britten taxonomischer Rang Washington', 'Washington taxonomischer Rang Britten', 'Eagle taxonomischer Rang Conway', 'Conway taxonomischer Rang Eagle', 'Spirit taxonomischer Rang Montreal', 'Montreal taxonomischer Rang Spirit', 'Command taxonomischer Rang Dio', 'Dio taxonomischer Rang Command', 'Day taxonomischer Rang Sand', 'Sand taxonomischer Rang Day', 'America taxonomischer Rang Lobo', 'Lobo taxonomischer Rang America', 'Blanca taxonomischer Rang Mu', 'Mu taxonomischer Rang Blanca', 'Sherlock taxonomischer Rang Raider', 'Raider taxonomischer Rang Sherlock', 'Qara taxonomischer Rang Petit', 'Petit taxonomischer Rang Qara', 'Enterprise taxonomischer Rang Brandt', 'Brandt taxonomischer Rang Enterprise', 'Pedra taxonomischer Rang Operator', 'Operator taxonomischer Rang Pedra', 'Milli taxonomischer Rang Flame', 'Flame taxonomischer Rang Milli', 'Lost taxonomischer Rang Webb', 'Webb taxonomischer Rang Lost', 'Johnny taxonomischer Rang PhD', 'PhD taxonomischer Rang Johnny', 'Macau taxonomischer Rang Challenger', 'Challenger taxonomischer Rang Macau', 'Manhattan taxonomischer Rang Morro', 'Morro taxonomischer Rang Manhattan', 'Monde taxonomischer Rang Invasion', 'Invasion taxonomischer Rang Monde', 'Turquia taxonomischer Rang Doubs', 'Doubs taxonomischer Rang Turquia', 'Cast taxonomischer Rang Olympique', 'Olympique taxonomischer Rang Cast', 'Balkan taxonomischer Rang Cast', 'Cast taxonomischer Rang Balkan', 'Mountain taxonomischer Rang Witt', 'Witt taxonomischer Rang Mountain', 'Madonna taxonomischer Rang Remo', 'Remo taxonomischer Rang Madonna', 'Huelva taxonomischer Rang Shuttle', 'Shuttle taxonomischer Rang Huelva', 'Brady taxonomischer Rang Jacqueline', 'Jacqueline taxonomischer Rang Brady', 'Mustang taxonomischer Rang Dawn', 'Dawn taxonomischer Rang Mustang', 'Caen taxonomischer Rang Monique', 'Monique taxonomischer Rang Caen', 'Wert taxonomischer Rang Rogers', 'Rogers taxonomischer Rang Wert', 'Savage taxonomischer Rang Hopper', 'Hopper taxonomischer Rang Savage', 'WBA taxonomischer Rang Wilson', 'Wilson taxonomischer Rang WBA', 'Elias taxonomischer Rang Friedrich', 'Friedrich taxonomischer Rang Elias', 'Houten taxonomischer Rang Alus', 'Alus taxonomischer Rang Houten', 'Rogers taxonomischer Rang Sieger', 'Sieger taxonomischer Rang Rogers', 'Change taxonomischer Rang Allison', 'Allison taxonomischer Rang Change', 'WRC taxonomischer Rang Dame', 'Dame taxonomischer Rang WRC', 'Gia taxonomischer Rang Interview', 'Interview taxonomischer Rang Gia', 'Jason taxonomischer Rang Larry', 'Larry taxonomischer Rang Jason', 'Th taxonomischer Rang Klaus', 'Klaus taxonomischer Rang Th', 'Prairie taxonomischer Rang Ferns', 'Ferns taxonomischer Rang Prairie', 'Tyne taxonomischer Rang Sanders', 'Sanders taxonomischer Rang Tyne', 'Culture taxonomischer Rang Bridge', 'Bridge taxonomischer Rang Culture', 'Rang taxonomischer Rang Odessa', 'Odessa taxonomischer Rang Rang', 'Ph taxonomischer Rang Hien', 'Hien taxonomischer Rang Ph', 'Macbeth taxonomischer Rang Harriet', 'Harriet taxonomischer Rang Macbeth', 'KHL taxonomischer Rang Kenneth', 'Kenneth taxonomischer Rang KHL', 'Escape taxonomischer Rang ITF', 'ITF taxonomischer Rang Escape', 'Racine taxonomischer Rang Sinh', 'Sinh taxonomischer Rang Racine', 'Alus taxonomischer Rang Pe', 'Pe taxonomischer Rang Alus', 'Jenny taxonomischer Rang ITV', 'ITV taxonomischer Rang Jenny', 'Conquest taxonomischer Rang FN', 'FN taxonomischer Rang Conquest', 'Bambino taxonomischer Rang Fran', 'Fran taxonomischer Rang Bambino', 'Hagen taxonomischer Rang Babylon', 'Babylon taxonomischer Rang Hagen', 'Police taxonomischer Rang PhD', 'PhD taxonomischer Rang Police', 'Buster taxonomischer Rang Bishop', 'Bishop taxonomischer Rang Buster', 'Cherbourg taxonomischer Rang Duty', 'Duty taxonomischer Rang Cherbourg', 'Piper taxonomischer Rang Stanton', 'Stanton taxonomischer Rang Piper', 'Mexico taxonomischer Rang Gesundheit', 'Gesundheit taxonomischer Rang Mexico', 'Edda taxonomischer Rang Espagne', 'Espagne taxonomischer Rang Edda', 'Yoshida taxonomischer Rang Loan', 'Loan taxonomischer Rang Yoshida', 'Door taxonomischer Rang Wilhelmina', 'Wilhelmina taxonomischer Rang Door', 'Lenny taxonomischer Rang Norsk', 'Norsk taxonomischer Rang Lenny', 'Copa taxonomischer Rang Damon', 'Damon taxonomischer Rang Copa', 'NF taxonomischer Rang Bassi', 'Bassi taxonomischer Rang NF', 'Sá taxonomischer Rang Vertreter', 'Vertreter taxonomischer Rang Sá', 'Raw taxonomischer Rang Zug', 'Zug taxonomischer Rang Raw', 'Salta taxonomischer Rang Mivel', 'Mivel taxonomischer Rang Salta', 'Turnier taxonomischer Rang Nicholson', 'Nicholson taxonomischer Rang Turnier', 'Sheppard taxonomischer Rang Abrams', 'Abrams taxonomischer Rang Sheppard', 'Garden taxonomischer Rang Dominic', 'Dominic taxonomischer Rang Garden', 'Wiley taxonomischer Rang Humbert', 'Humbert taxonomischer Rang Wiley', 'Sanremo taxonomischer Rang González', 'González taxonomischer Rang Sanremo', 'Tri taxonomischer Rang Hate', 'Hate taxonomischer Rang Tri', 'Spain taxonomischer Rang Carlisle', 'Carlisle taxonomischer Rang Spain', 'Orient taxonomischer Rang Frey', 'Frey taxonomischer Rang Orient', 'Puis taxonomischer Rang Primera', 'Primera taxonomischer Rang Puis', 'Syracuse taxonomischer Rang NL', 'NL taxonomischer Rang Syracuse', 'Assembly taxonomischer Rang Xuân', 'Xuân taxonomischer Rang Assembly', 'Giang taxonomischer Rang Malcolm', 'Malcolm taxonomischer Rang Giang', 'Yokohama taxonomischer Rang Melvin', 'Melvin taxonomischer Rang Yokohama', 'Amiens taxonomischer Rang Design', 'Design taxonomischer Rang Amiens', 'Tat taxonomischer Rang Rhine', 'Rhine taxonomischer Rang Tat', 'Hara taxonomischer Rang Bishop', 'Bishop taxonomischer Rang Hara', 'Space taxonomischer Rang Sweeney', 'Sweeney taxonomischer Rang Space', 'Trang taxonomischer Rang Carnaval', 'Carnaval taxonomischer Rang Trang', 'Fisher taxonomischer Rang Cassandra', 'Cassandra taxonomischer Rang Fisher', 'Titre taxonomischer Rang Hal', 'Hal taxonomischer Rang Titre', 'Animals taxonomischer Rang Hara', 'Hara taxonomischer Rang Animals', 'Codex taxonomischer Rang Paulo', 'Paulo taxonomischer Rang Codex', 'Feria taxonomischer Rang Selva', 'Selva taxonomischer Rang Feria', 'Corazón taxonomischer Rang Sicilia', 'Sicilia taxonomischer Rang Corazón', 'Russie taxonomischer Rang AF', 'AF taxonomischer Rang Russie', 'IPA taxonomischer Rang Knowles', 'Knowles taxonomischer Rang IPA', 'Borough taxonomischer Rang EE', 'EE taxonomischer Rang Borough', 'Nigel taxonomischer Rang Branca', 'Branca taxonomischer Rang Nigel', 'Dans taxonomischer Rang Slater', 'Slater taxonomischer Rang Dans', 'Papa taxonomischer Rang Dupont', 'Dupont taxonomischer Rang Papa', 'Bonnie taxonomischer Rang Sens', 'Sens taxonomischer Rang Bonnie', 'Leicester taxonomischer Rang Römer', 'Römer taxonomischer Rang Leicester', 'Jørgensen taxonomischer Rang Roi', 'Roi taxonomischer Rang Jørgensen', 'Gallia taxonomischer Rang Mendoza', 'Mendoza taxonomischer Rang Gallia', 'Ulysses taxonomischer Rang Fairfield', 'Fairfield taxonomischer Rang Ulysses', 'Souza taxonomischer Rang Eugenio', 'Eugenio taxonomischer Rang Souza', 'Dixon taxonomischer Rang Novel', 'Novel taxonomischer Rang Dixon', 'Revolución taxonomischer Rang Quelle', 'Quelle taxonomischer Rang Revolución', 'Aire taxonomischer Rang Hector', 'Hector taxonomischer Rang Aire', 'Gerd taxonomischer Rang Scotland', 'Scotland taxonomischer Rang Gerd', 'Battalion taxonomischer Rang Township', 'Township taxonomischer Rang Battalion', 'Sans taxonomischer Rang Senat', 'Senat taxonomischer Rang Sans', 'Pampa taxonomischer Rang Wiener', 'Wiener taxonomischer Rang Pampa', 'Borussia taxonomischer Rang Allier', 'Allier taxonomischer Rang Borussia', 'Edit taxonomischer Rang Prema', 'Prema taxonomischer Rang Edit', 'Mei taxonomischer Rang Michel', 'Michel taxonomischer Rang Mei', 'Mille taxonomischer Rang Spieler', 'Spieler taxonomischer Rang Mille', 'Tore taxonomischer Rang Radar', 'Radar taxonomischer Rang Tore', 'MAC taxonomischer Rang Break', 'Break taxonomischer Rang MAC', 'Island taxonomischer Rang Pass', 'Pass taxonomischer Rang Island', 'Liam taxonomischer Rang Brenner', 'Brenner taxonomischer Rang Liam', 'Bones taxonomischer Rang Bismarck', 'Bismarck taxonomischer Rang Bones', 'Alta taxonomischer Rang Cunha', 'Cunha taxonomischer Rang Alta', 'Kobayashi taxonomischer Rang Kamen', 'Kamen taxonomischer Rang Kobayashi', 'Rams taxonomischer Rang Mesa', 'Mesa taxonomischer Rang Rams', 'du taxonomischer Rang Verso', 'Verso taxonomischer Rang du', 'Een taxonomischer Rang Rota', 'Rota taxonomischer Rang Een', 'Khmer taxonomischer Rang Bauer', 'Bauer taxonomischer Rang Khmer', 'Gina taxonomischer Rang Vivian', 'Vivian taxonomischer Rang Gina', 'Anna taxonomischer Rang Daisy', 'Daisy taxonomischer Rang Anna', 'Dag taxonomischer Rang Genus', 'Genus taxonomischer Rang Dag', 'Secrets taxonomischer Rang Dunia', 'Dunia taxonomischer Rang Secrets', 'Pace taxonomischer Rang Oaxaca', 'Oaxaca taxonomischer Rang Pace', 'Christopher taxonomischer Rang Seychelles', 'Seychelles taxonomischer Rang Christopher', 'RCA taxonomischer Rang Fargo', 'Fargo taxonomischer Rang RCA', 'Quest taxonomischer Rang Titan', 'Titan taxonomischer Rang Quest', 'Kirby taxonomischer Rang Robot', 'Robot taxonomischer Rang Kirby', 'Reich taxonomischer Rang Thornton', 'Thornton taxonomischer Rang Reich', 'Geiger taxonomischer Rang Arad', 'Arad taxonomischer Rang Geiger', 'Pass taxonomischer Rang Penguin', 'Penguin taxonomischer Rang Pass', 'Sigurd taxonomischer Rang TNA', 'TNA taxonomischer Rang Sigurd', 'Mighty taxonomischer Rang Zombie', 'Zombie taxonomischer Rang Mighty', 'Ng taxonomischer Rang Bethlehem', 'Bethlehem taxonomischer Rang Ng', 'Nil taxonomischer Rang Cinq', 'Cinq taxonomischer Rang Nil', 'Senat taxonomischer Rang Goldstein', 'Goldstein taxonomischer Rang Senat', 'Herrschaft taxonomischer Rang Kuna', 'Kuna taxonomischer Rang Herrschaft', 'Poison taxonomischer Rang Cantor', 'Cantor taxonomischer Rang Poison', 'Buddy taxonomischer Rang Trend', 'Trend taxonomischer Rang Buddy', 'Finance taxonomischer Rang Clock', 'Clock taxonomischer Rang Finance', 'Idol taxonomischer Rang Princesa', 'Princesa taxonomischer Rang Idol', 'Brighton taxonomischer Rang Una', 'Una taxonomischer Rang Brighton', 'Mississippi taxonomischer Rang Rue', 'Rue taxonomischer Rang Mississippi', 'Pero taxonomischer Rang Jenny', 'Jenny taxonomischer Rang Pero', 'Acer taxonomischer Rang Antoine', 'Antoine taxonomischer Rang Acer', 'Shane taxonomischer Rang Colony', 'Colony taxonomischer Rang Shane', 'Stig taxonomischer Rang Thornton', 'Thornton taxonomischer Rang Stig', 'Walther taxonomischer Rang te', 'te taxonomischer Rang Walther', 'Schönberg taxonomischer Rang Fredrik', 'Fredrik taxonomischer Rang Schönberg', 'Po taxonomischer Rang Königsberg', 'Königsberg taxonomischer Rang Po', 'Sino taxonomischer Rang Glory', 'Glory taxonomischer Rang Sino', 'Levante taxonomischer Rang Cuenca', 'Cuenca taxonomischer Rang Levante', 'Princess taxonomischer Rang Ain', 'Ain taxonomischer Rang Princess', 'Greenwood taxonomischer Rang WRC', 'WRC taxonomischer Rang Greenwood', 'Gate taxonomischer Rang Blackmore', 'Blackmore taxonomischer Rang Gate', 'ARM taxonomischer Rang Salazar', 'Salazar taxonomischer Rang ARM', 'Shelby taxonomischer Rang Tun', 'Tun taxonomischer Rang Shelby', 'Townsend taxonomischer Rang Branko', 'Branko taxonomischer Rang Townsend', 'Helena taxonomischer Rang Bauer', 'Bauer taxonomischer Rang Helena', 'Murad taxonomischer Rang Coca', 'Coca taxonomischer Rang Murad', 'Basse taxonomischer Rang Petit', 'Petit taxonomischer Rang Basse', 'Sharks taxonomischer Rang Alternate', 'Alternate taxonomischer Rang Sharks', 'Sainte taxonomischer Rang Showtime', 'Showtime taxonomischer Rang Sainte', 'Vergine taxonomischer Rang Shaun', 'Shaun taxonomischer Rang Vergine', 'CAS taxonomischer Rang Camilla', 'Camilla taxonomischer Rang CAS', 'Weiler taxonomischer Rang Rosa', 'Rosa taxonomischer Rang Weiler', 'IN taxonomischer Rang Malaya', 'Malaya taxonomischer Rang IN', 'Vida taxonomischer Rang Generation', 'Generation taxonomischer Rang Vida', 'Goebbels taxonomischer Rang Lowry', 'Lowry taxonomischer Rang Goebbels', 'Congo taxonomischer Rang Masters', 'Masters taxonomischer Rang Congo', 'Esperanza taxonomischer Rang Porter', 'Porter taxonomischer Rang Esperanza', 'Loyola taxonomischer Rang Carlton', 'Carlton taxonomischer Rang Loyola', 'Exeter taxonomischer Rang Organ', 'Organ taxonomischer Rang Exeter', 'Jamaica taxonomischer Rang Orson', 'Orson taxonomischer Rang Jamaica', 'Yale taxonomischer Rang Mirko', 'Mirko taxonomischer Rang Yale', 'Boyd taxonomischer Rang Classe', 'Classe taxonomischer Rang Boyd', 'Fargo taxonomischer Rang ABD', 'ABD taxonomischer Rang Fargo', 'Mühle taxonomischer Rang Salto', 'Salto taxonomischer Rang Mühle', 'Darkness taxonomischer Rang Bar', 'Bar taxonomischer Rang Darkness', 'Ronda taxonomischer Rang Salud', 'Salud taxonomischer Rang Ronda', 'Palestine taxonomischer Rang Pi', 'Pi taxonomischer Rang Palestine', 'Remo taxonomischer Rang Courtney', 'Courtney taxonomischer Rang Remo', 'Orlando taxonomischer Rang Ballet', 'Ballet taxonomischer Rang Orlando', 'Hume taxonomischer Rang Schleswig', 'Schleswig taxonomischer Rang Hume', 'Aube taxonomischer Rang Lebanon', 'Lebanon taxonomischer Rang Aube', 'Prin taxonomischer Rang Hannah', 'Hannah taxonomischer Rang Prin', 'Amiga taxonomischer Rang Point', 'Point taxonomischer Rang Amiga', 'Jos taxonomischer Rang Benedict', 'Benedict taxonomischer Rang Jos', 'Harper taxonomischer Rang Blood', 'Blood taxonomischer Rang Harper', 'Birds taxonomischer Rang SAP', 'SAP taxonomischer Rang Birds', 'Murcia taxonomischer Rang VL', 'VL taxonomischer Rang Murcia', 'SP taxonomischer Rang Bleu', 'Bleu taxonomischer Rang SP', 'Guns taxonomischer Rang Titus', 'Titus taxonomischer Rang Guns', 'Albany taxonomischer Rang Brooklyn', 'Brooklyn taxonomischer Rang Albany', 'Dakar taxonomischer Rang FCC', 'FCC taxonomischer Rang Dakar', 'ASCII taxonomischer Rang Brazil', 'Brazil taxonomischer Rang ASCII', 'Maynard taxonomischer Rang Sébastien', 'Sébastien taxonomischer Rang Maynard', 'Agnes taxonomischer Rang Gama', 'Gama taxonomischer Rang Agnes', 'UCB taxonomischer Rang Porsche', 'Porsche taxonomischer Rang UCB', 'Schultz taxonomischer Rang Guardia', 'Guardia taxonomischer Rang Schultz', 'Hook taxonomischer Rang Galatasaray', 'Galatasaray taxonomischer Rang Hook', 'White taxonomischer Rang Siegel', 'Siegel taxonomischer Rang White', 'Carrier taxonomischer Rang Indie', 'Indie taxonomischer Rang Carrier', 'Oko taxonomischer Rang VL', 'VL taxonomischer Rang Oko', 'Carpenter taxonomischer Rang Kosovo', 'Kosovo taxonomischer Rang Carpenter', 'Mato taxonomischer Rang Bet', 'Bet taxonomischer Rang Mato', 'Trial taxonomischer Rang Hook', 'Hook taxonomischer Rang Trial', 'Rock taxonomischer Rang Maja', 'Maja taxonomischer Rang Rock', 'Yüksek taxonomischer Rang Krupp', 'Krupp taxonomischer Rang Yüksek', 'Vas taxonomischer Rang Knowles', 'Knowles taxonomischer Rang Vas', 'Pack taxonomischer Rang Card', 'Card taxonomischer Rang Pack', 'Midland taxonomischer Rang Bürger', 'Bürger taxonomischer Rang Midland', 'Chef taxonomischer Rang Junior', 'Junior taxonomischer Rang Chef', 'Concilio taxonomischer Rang Surat', 'Surat taxonomischer Rang Concilio', 'Linares taxonomischer Rang Pulau', 'Pulau taxonomischer Rang Linares', 'Jay taxonomischer Rang Uit', 'Uit taxonomischer Rang Jay', 'Payne taxonomischer Rang McKinley', 'McKinley taxonomischer Rang Payne', 'FN taxonomischer Rang Highway', 'Highway taxonomischer Rang FN', 'Pizza taxonomischer Rang Jupiter', 'Jupiter taxonomischer Rang Pizza', 'Monat taxonomischer Rang Tudor', 'Tudor taxonomischer Rang Monat', 'Rincón taxonomischer Rang Wrong', 'Wrong taxonomischer Rang Rincón', 'Gandhi taxonomischer Rang Ariane', 'Ariane taxonomischer Rang Gandhi', 'WBC taxonomischer Rang Malta', 'Malta taxonomischer Rang WBC', 'Grupa taxonomischer Rang Gilmore', 'Gilmore taxonomischer Rang Grupa', 'Fury taxonomischer Rang Product', 'Product taxonomischer Rang Fury', 'Advance taxonomischer Rang Reed', 'Reed taxonomischer Rang Advance', 'Brunnen taxonomischer Rang Catharina', 'Catharina taxonomischer Rang Brunnen', 'Trinity taxonomischer Rang Evan', 'Evan taxonomischer Rang Trinity', 'Ex taxonomischer Rang Reflections', 'Reflections taxonomischer Rang Ex', 'Fox taxonomischer Rang Victorian', 'Victorian taxonomischer Rang Fox', 'Natal taxonomischer Rang Ost', 'Ost taxonomischer Rang Natal', 'RTL taxonomischer Rang Roosevelt', 'Roosevelt taxonomischer Rang RTL', 'Carthage taxonomischer Rang Titan', 'Titan taxonomischer Rang Carthage', 'María taxonomischer Rang Jane', 'Jane taxonomischer Rang María', 'tu taxonomischer Rang Register', 'Register taxonomischer Rang tu', 'UDP taxonomischer Rang Margareta', 'Margareta taxonomischer Rang UDP', 'Jake taxonomischer Rang Caldwell', 'Caldwell taxonomischer Rang Jake', 'Vivian taxonomischer Rang Rumble', 'Rumble taxonomischer Rang Vivian', 'sy taxonomischer Rang RPM', 'RPM taxonomischer Rang sy', 'Mobile taxonomischer Rang Boa', 'Boa taxonomischer Rang Mobile', 'Andere taxonomischer Rang Kirchner', 'Kirchner taxonomischer Rang Andere', 'Grammar taxonomischer Rang Riley', 'Riley taxonomischer Rang Grammar', 'Doom taxonomischer Rang Ut', 'Ut taxonomischer Rang Doom', 'SBS taxonomischer Rang Lager', 'Lager taxonomischer Rang SBS', 'asa taxonomischer Rang Söhne', 'Söhne taxonomischer Rang asa', 'Randolph taxonomischer Rang PMC', 'PMC taxonomischer Rang Randolph', 'Richter taxonomischer Rang Roja', 'Roja taxonomischer Rang Richter', 'Room taxonomischer Rang Coral', 'Coral taxonomischer Rang Room', 'Vis taxonomischer Rang Sono', 'Sono taxonomischer Rang Vis', 'Bengal taxonomischer Rang Reis', 'Reis taxonomischer Rang Bengal', 'Flynn taxonomischer Rang IGE', 'IGE taxonomischer Rang Flynn', 'Weston taxonomischer Rang Letters', 'Letters taxonomischer Rang Weston', 'Segura taxonomischer Rang Darmstadt', 'Darmstadt taxonomischer Rang Segura', 'Gets taxonomischer Rang Swami', 'Swami taxonomischer Rang Gets', 'Snow taxonomischer Rang Brabant', 'Brabant taxonomischer Rang Snow', 'NRW taxonomischer Rang Temps', 'Temps taxonomischer Rang NRW', 'Perry taxonomischer Rang Christensen', 'Christensen taxonomischer Rang Perry', 'Ribera taxonomischer Rang Bethlehem', 'Bethlehem taxonomischer Rang Ribera', 'Batavia taxonomischer Rang Gegen', 'Gegen taxonomischer Rang Batavia', 'Criminal taxonomischer Rang Giovanna', 'Giovanna taxonomischer Rang Criminal', 'Abby taxonomischer Rang Cornwall', 'Cornwall taxonomischer Rang Abby', 'Hatch taxonomischer Rang Pool', 'Pool taxonomischer Rang Hatch', 'Cáceres taxonomischer Rang Varese', 'Varese taxonomischer Rang Cáceres', 'Namibia taxonomischer Rang Melanie', 'Melanie taxonomischer Rang Namibia', 'Ferdinand taxonomischer Rang Ter', 'Ter taxonomischer Rang Ferdinand', 'Helen taxonomischer Rang Neckar', 'Neckar taxonomischer Rang Helen', 'Collegiate taxonomischer Rang Grenoble', 'Grenoble taxonomischer Rang Collegiate', 'Milne taxonomischer Rang Gia', 'Gia taxonomischer Rang Milne', 'BRT taxonomischer Rang Sims', 'Sims taxonomischer Rang BRT', 'U taxonomischer Rang Jackson', 'Jackson taxonomischer Rang U', 'Toulon taxonomischer Rang CPU', 'CPU taxonomischer Rang Toulon', 'Elvis taxonomischer Rang Interview', 'Interview taxonomischer Rang Elvis', 'Damon taxonomischer Rang Animal', 'Animal taxonomischer Rang Damon', 'Socorro taxonomischer Rang Europa', 'Europa taxonomischer Rang Socorro', 'Newell taxonomischer Rang Prestige', 'Prestige taxonomischer Rang Newell', 'Car taxonomischer Rang Weir', 'Weir taxonomischer Rang Car', 'Quincy taxonomischer Rang Shock', 'Shock taxonomischer Rang Quincy', 'Erfurt taxonomischer Rang Hora', 'Hora taxonomischer Rang Erfurt', 'Gattung taxonomischer Rang Qi', 'Qi taxonomischer Rang Gattung', 'Chiara taxonomischer Rang Cliff', 'Cliff taxonomischer Rang Chiara', 'Davida taxonomischer Rang Ka', 'Ka taxonomischer Rang Davida', 'Valencia taxonomischer Rang Apollo', 'Apollo taxonomischer Rang Valencia', 'Maja taxonomischer Rang Darat', 'Darat taxonomischer Rang Maja', 'Cinta taxonomischer Rang Ellis', 'Ellis taxonomischer Rang Cinta', 'Sergei taxonomischer Rang Benoit', 'Benoit taxonomischer Rang Sergei', 'Bangalore taxonomischer Rang Linha', 'Linha taxonomischer Rang Bangalore', 'Lorenz taxonomischer Rang Dessa', 'Dessa taxonomischer Rang Lorenz', 'Eye taxonomischer Rang Brooklyn', 'Brooklyn taxonomischer Rang Eye', 'Díaz taxonomischer Rang Yokohama', 'Yokohama taxonomischer Rang Díaz', 'Pamplona taxonomischer Rang Ragnar', 'Ragnar taxonomischer Rang Pamplona', 'Alicante taxonomischer Rang Spezia', 'Spezia taxonomischer Rang Alicante', 'Mouse taxonomischer Rang Rogers', 'Rogers taxonomischer Rang Mouse', 'Welle taxonomischer Rang Noise', 'Noise taxonomischer Rang Welle', 'Beast taxonomischer Rang Cotton', 'Cotton taxonomischer Rang Beast', 'CDATA taxonomischer Rang Geld', 'Geld taxonomischer Rang CDATA', 'WM taxonomischer Rang ac', 'ac taxonomischer Rang WM', 'Hull taxonomischer Rang Pay', 'Pay taxonomischer Rang Hull', 'Lauren taxonomischer Rang Padre', 'Padre taxonomischer Rang Lauren', 'Gers taxonomischer Rang Oslo', 'Oslo taxonomischer Rang Gers', 'Archie taxonomischer Rang avoir', 'avoir taxonomischer Rang Archie', 'Hunter taxonomischer Rang Oslo', 'Oslo taxonomischer Rang Hunter', 'Angels taxonomischer Rang Mont', 'Mont taxonomischer Rang Angels', 'Lance taxonomischer Rang Davidson', 'Davidson taxonomischer Rang Lance', 'Jet taxonomischer Rang cal', 'cal taxonomischer Rang Jet', 'Trent taxonomischer Rang Hitchcock', 'Hitchcock taxonomischer Rang Trent', 'Pleasure taxonomischer Rang Jurist', 'Jurist taxonomischer Rang Pleasure', 'SVT taxonomischer Rang Scoble', 'Scoble taxonomischer Rang SVT', 'Fermi taxonomischer Rang Trung', 'Trung taxonomischer Rang Fermi', 'Sweden taxonomischer Rang Lowe', 'Lowe taxonomischer Rang Sweden', 'Taylor taxonomischer Rang Stara', 'Stara taxonomischer Rang Taylor', 'Chez taxonomischer Rang Format', 'Format taxonomischer Rang Chez', 'Lennon taxonomischer Rang Jesús', 'Jesús taxonomischer Rang Lennon', 'Junie taxonomischer Rang Staat', 'Staat taxonomischer Rang Junie', 'Haji taxonomischer Rang Raymond', 'Yahoo taxonomischer Rang Chinese', 'Stal taxonomischer Rang West', 'FC taxonomischer Rang Rhode', 'Dad taxonomischer Rang Libro', 'Kenia taxonomischer Rang Weaver', 'CCD taxonomischer Rang Limited', 'Riau taxonomischer Rang NO', 'Ky taxonomischer Rang Frères', 'Billie taxonomischer Rang Li', 'Elbe taxonomischer Rang Pie', 'Paraíso taxonomischer Rang DSM', 'TD taxonomischer Rang Björn', 'Luther taxonomischer Rang Elsevier', 'Roi taxonomischer Rang Isabel', 'Pole taxonomischer Rang Valence', 'Page taxonomischer Rang Townsend', 'Baron taxonomischer Rang Levant', 'Libia taxonomischer Rang Khan', 'Cuenca taxonomischer Rang Ward', 'Kálmán taxonomischer Rang Valladolid', 'ET taxonomischer Rang Kristen', 'Mainstream taxonomischer Rang Allende', 'Agency taxonomischer Rang Malden', 'Mata taxonomischer Rang Ekim', 'Mineral taxonomischer Rang Norris', 'Figaro taxonomischer Rang Entangled', 'Trung taxonomischer Rang Nico', 'Sabha taxonomischer Rang NME', 'Guimarães taxonomischer Rang Christi', 'Disneyland taxonomischer Rang Laurel', 'Wes taxonomischer Rang Hammer', 'Gesù taxonomischer Rang Desse', 'Cinq taxonomischer Rang Albany', 'Silla taxonomischer Rang Hollow', 'JR taxonomischer Rang Music', 'Bryant taxonomischer Rang Munro', 'Klaus taxonomischer Rang Loving', 'Kleiner taxonomischer Rang Sun', 'Fort taxonomischer Rang Ros', 'City taxonomischer Rang Marie', 'Márquez taxonomischer Rang Libération', 'Yer taxonomischer Rang Mass', 'Bernard taxonomischer Rang Liu', 'Diaz taxonomischer Rang Belgium', 'Madre taxonomischer Rang Edouard', 'Asturias taxonomischer Rang te', 'FF taxonomischer Rang Brabant', 'Dis taxonomischer Rang Quick', 'Lorentz taxonomischer Rang Primavera', 'SM taxonomischer Rang Tito', 'Sempre taxonomischer Rang Jubilee', 'Portsmouth taxonomischer Rang Nassau', 'Irena taxonomischer Rang Nexus', 'Oaks taxonomischer Rang Hiroshima', 'Fighting taxonomischer Rang Jenny', 'Bilbao taxonomischer Rang Cardoso', 'Sharon taxonomischer Rang Rusi', 'Allium taxonomischer Rang Silvio', 'Haiti taxonomischer Rang Scots', 'Isabella taxonomischer Rang Helmut', 'Olsson taxonomischer Rang Novel', 'Pure taxonomischer Rang KM', 'Patrol taxonomischer Rang Industria', 'Siege taxonomischer Rang ap', 'Galiza taxonomischer Rang Frères', 'Lucky taxonomischer Rang RN', 'Pfeiffer taxonomischer Rang Gilbert', 'FX taxonomischer Rang Bachelor', 'Energy taxonomischer Rang GNU', 'Joanne taxonomischer Rang UE', 'Erik taxonomischer Rang Hamm', 'EM taxonomischer Rang Rua', 'Raum taxonomischer Rang Ike', 'VL taxonomischer Rang Velvet', 'Sabina taxonomischer Rang Levant', 'Krupp taxonomischer Rang Spin', 'October taxonomischer Rang Franz', 'Davidson taxonomischer Rang SAR', 'Allah taxonomischer Rang Taurus', 'Larva taxonomischer Rang Denne', 'Khu taxonomischer Rang Pizarro', 'Lahore taxonomischer Rang Sunrise', 'Dorset taxonomischer Rang Burlington', 'Stay taxonomischer Rang Invasion', 'Sit taxonomischer Rang Wiesbaden', 'Wayne taxonomischer Rang Oregon', 'Flag taxonomischer Rang Jess', 'Ferns taxonomischer Rang Trees', 'Jaguar taxonomischer Rang Norris', 'Brenda taxonomischer Rang Raoul', 'Mina taxonomischer Rang Neu', 'Harding taxonomischer Rang Gia', 'Giants taxonomischer Rang Pisa', 'Buta taxonomischer Rang State', 'Faye taxonomischer Rang Hidden', 'Generation taxonomischer Rang Mill', 'Schottland taxonomischer Rang Everett', 'USD taxonomischer Rang Birinci', 'Eu taxonomischer Rang Sparks', 'Medina Notfalleinrichtungen Alice']\n"
     ]
    }
   ],
   "source": [
    "print(train_dict['sample'][:1901])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86e4d3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Raymond taxon rank Haji',\n",
       " 'Chinese taxon rank Yahoo',\n",
       " 'West taxon rank Stal',\n",
       " 'Rhode taxon rank FC',\n",
       " 'Libro taxon rank Dad',\n",
       " 'Weaver taxon rank Kenia',\n",
       " 'Limited taxon rank CCD',\n",
       " 'NO taxon rank Riau',\n",
       " 'Frères taxon rank Ky',\n",
       " 'Li taxon rank Billie',\n",
       " 'Pie taxon rank Elbe',\n",
       " 'DSM taxon rank Paraíso',\n",
       " 'Björn taxon rank TD',\n",
       " 'Elsevier taxon rank Luther',\n",
       " 'Isabel taxon rank Roi',\n",
       " 'Valence taxon rank Pole',\n",
       " 'Townsend taxon rank Page',\n",
       " 'Levant taxon rank Baron',\n",
       " 'Khan taxon rank Libia',\n",
       " 'Ward taxon rank Cuenca',\n",
       " 'Valladolid taxon rank Kálmán',\n",
       " 'Kristen taxon rank ET',\n",
       " 'Allende taxon rank Mainstream',\n",
       " 'Malden taxon rank Agency',\n",
       " 'Ekim taxon rank Mata',\n",
       " 'Norris taxon rank Mineral',\n",
       " 'Entangled taxon rank Figaro',\n",
       " 'Nico taxon rank Trung',\n",
       " 'NME taxon rank Sabha',\n",
       " 'Christi taxon rank Guimarães',\n",
       " 'Laurel taxon rank Disneyland',\n",
       " 'Hammer taxon rank Wes',\n",
       " 'Desse taxon rank Gesù',\n",
       " 'Albany taxon rank Cinq',\n",
       " 'Hollow taxon rank Silla',\n",
       " 'Music taxon rank JR',\n",
       " 'Munro taxon rank Bryant',\n",
       " 'Loving taxon rank Klaus',\n",
       " 'Sun taxon rank Kleiner',\n",
       " 'Ros taxon rank Fort',\n",
       " 'Marie taxon rank City',\n",
       " 'Libération taxon rank Márquez',\n",
       " 'Mass taxon rank Yer',\n",
       " 'Liu taxon rank Bernard',\n",
       " 'Belgium taxon rank Diaz',\n",
       " 'Edouard taxon rank Madre',\n",
       " 'te taxon rank Asturias',\n",
       " 'Brabant taxon rank FF',\n",
       " 'Quick taxon rank Dis',\n",
       " 'Primavera taxon rank Lorentz',\n",
       " 'Tito taxon rank SM',\n",
       " 'Jubilee taxon rank Sempre',\n",
       " 'Nassau taxon rank Portsmouth',\n",
       " 'Nexus taxon rank Irena',\n",
       " 'Hiroshima taxon rank Oaks',\n",
       " 'Jenny taxon rank Fighting',\n",
       " 'Cardoso taxon rank Bilbao',\n",
       " 'Rusi taxon rank Sharon',\n",
       " 'Silvio taxon rank Allium',\n",
       " 'Scots taxon rank Haiti',\n",
       " 'Helmut taxon rank Isabella',\n",
       " 'Novel taxon rank Olsson',\n",
       " 'KM taxon rank Pure',\n",
       " 'Industria taxon rank Patrol',\n",
       " 'ap taxon rank Siege',\n",
       " 'Frères taxon rank Galiza',\n",
       " 'RN taxon rank Lucky',\n",
       " 'Gilbert taxon rank Pfeiffer',\n",
       " 'Bachelor taxon rank FX',\n",
       " 'GNU taxon rank Energy',\n",
       " 'UE taxon rank Joanne',\n",
       " 'Hamm taxon rank Erik',\n",
       " 'Rua taxon rank EM',\n",
       " 'Ike taxon rank Raum',\n",
       " 'Velvet taxon rank VL',\n",
       " 'Levant taxon rank Sabina',\n",
       " 'Spin taxon rank Krupp',\n",
       " 'Franz taxon rank October',\n",
       " 'SAR taxon rank Davidson',\n",
       " 'Taurus taxon rank Allah',\n",
       " 'Denne taxon rank Larva',\n",
       " 'Pizarro taxon rank Khu',\n",
       " 'Sunrise taxon rank Lahore',\n",
       " 'Burlington taxon rank Dorset',\n",
       " 'Invasion taxon rank Stay',\n",
       " 'Wiesbaden taxon rank Sit',\n",
       " 'Oregon taxon rank Wayne',\n",
       " 'Jess taxon rank Flag',\n",
       " 'Trees taxon rank Ferns',\n",
       " 'Norris taxon rank Jaguar',\n",
       " 'Raoul taxon rank Brenda',\n",
       " 'Neu taxon rank Mina',\n",
       " 'Gia taxon rank Harding',\n",
       " 'Pisa taxon rank Giants',\n",
       " 'State taxon rank Buta',\n",
       " 'Hidden taxon rank Faye',\n",
       " 'Mill taxon rank Generation',\n",
       " 'Everett taxon rank Schottland',\n",
       " 'Birinci taxon rank USD',\n",
       " 'Sparks taxon rank Eu',\n",
       " 'Eugenia emergency services Haji',\n",
       " 'Ocak emergency services Yahoo',\n",
       " 'Astor emergency services Stal',\n",
       " 'Piero emergency services FC',\n",
       " 'Juli emergency services Dad',\n",
       " 'Nina emergency services Kenia',\n",
       " 'Tower emergency services CCD',\n",
       " 'Sigurd emergency services Riau',\n",
       " 'Lopes emergency services Ky',\n",
       " 'Bertram emergency services Billie',\n",
       " 'Billy emergency services Elbe',\n",
       " 'Man emergency services Paraíso',\n",
       " 'Naga emergency services TD',\n",
       " 'Antonio emergency services Luther',\n",
       " 'Shadows emergency services Roi',\n",
       " 'CPU emergency services Pole',\n",
       " 'Maxime emergency services Page',\n",
       " 'Angel emergency services Baron',\n",
       " 'Norsk emergency services Libia',\n",
       " 'Bron emergency services Cuenca',\n",
       " 'Wagen emergency services Kálmán',\n",
       " 'Nella emergency services ET',\n",
       " 'Issue emergency services Mainstream',\n",
       " 'Sono emergency services Agency',\n",
       " 'Sherwood emergency services Mata',\n",
       " 'Freddie emergency services Mineral',\n",
       " 'ANC emergency services Figaro',\n",
       " 'Mama emergency services Trung',\n",
       " 'Mills emergency services Sabha',\n",
       " 'Juventus emergency services Guimarães',\n",
       " 'TM emergency services Disneyland',\n",
       " 'Hammer emergency services Wes',\n",
       " 'Ferguson emergency services Gesù',\n",
       " 'Primer emergency services Cinq',\n",
       " 'Dublin emergency services Silla',\n",
       " 'Ballet emergency services JR',\n",
       " 'Scholar emergency services Bryant',\n",
       " 'Squadron emergency services Klaus',\n",
       " 'Kur emergency services Kleiner',\n",
       " 'Twain emergency services Fort',\n",
       " 'Alexa emergency services City',\n",
       " 'Choice emergency services Márquez',\n",
       " 'Teater emergency services Yer',\n",
       " 'England emergency services Bernard',\n",
       " 'Lopes emergency services Diaz',\n",
       " 'Nirvana emergency services Madre',\n",
       " 'Home emergency services Asturias',\n",
       " 'Rookie emergency services FF',\n",
       " 'Jesus emergency services Dis',\n",
       " 'Nota emergency services Lorentz',\n",
       " 'Carpenter emergency services SM',\n",
       " 'Calderón emergency services Sempre',\n",
       " 'RN emergency services Portsmouth',\n",
       " 'Miracle emergency services Irena',\n",
       " 'Alpi emergency services Oaks',\n",
       " 'President emergency services Fighting',\n",
       " 'Caesar emergency services Bilbao',\n",
       " 'Cádiz emergency services Sharon',\n",
       " 'Animal emergency services Allium',\n",
       " 'Powell emergency services Haiti',\n",
       " 'Penelope emergency services Isabella',\n",
       " 'Braga emergency services Olsson',\n",
       " 'Kamen emergency services Pure',\n",
       " 'Lindl emergency services Patrol',\n",
       " 'Roberts emergency services Siege',\n",
       " 'Sofia emergency services Galiza',\n",
       " 'Pitt emergency services Lucky',\n",
       " 'Esther emergency services Pfeiffer',\n",
       " 'Villiers emergency services FX',\n",
       " 'CAS emergency services Energy',\n",
       " 'Rutherford emergency services Joanne',\n",
       " 'Alvin emergency services Erik',\n",
       " 'Fritz emergency services EM',\n",
       " 'Ferreira emergency services Raum',\n",
       " 'Imperium emergency services VL',\n",
       " 'Rollins emergency services Sabina',\n",
       " 'Peck emergency services Krupp',\n",
       " 'Osman emergency services October',\n",
       " 'Cidade emergency services Davidson',\n",
       " 'Regional emergency services Allah',\n",
       " 'Gunung emergency services Larva',\n",
       " 'Cu emergency services Khu',\n",
       " 'Holocaust emergency services Lahore',\n",
       " 'Return emergency services Dorset',\n",
       " 'Kemp emergency services Stay',\n",
       " 'Late emergency services Sit',\n",
       " 'Rayon emergency services Wayne',\n",
       " 'Joy emergency services Flag',\n",
       " 'Graves emergency services Ferns',\n",
       " 'Bavaria emergency services Jaguar',\n",
       " 'Norwich emergency services Brenda',\n",
       " 'Advance emergency services Mina',\n",
       " 'Illusion emergency services Harding',\n",
       " 'Nice emergency services Giants',\n",
       " 'BL emergency services Buta',\n",
       " 'Kinos emergency services Faye',\n",
       " 'Severus emergency services Generation',\n",
       " 'Lowell emergency services Schottland',\n",
       " 'Ole emergency services USD',\n",
       " 'Platinum emergency services Eu',\n",
       " 'Zo color Haji',\n",
       " 'Odin color Yahoo',\n",
       " 'Cardiff color Stal',\n",
       " 'Linh color FC',\n",
       " 'Plains color Dad',\n",
       " 'Irena color Kenia',\n",
       " 'Montpellier color CCD',\n",
       " 'Manor color Riau',\n",
       " 'Trung color Ky',\n",
       " 'Frères color Billie',\n",
       " 'Cash color Elbe',\n",
       " 'Far color Paraíso',\n",
       " 'Pleasure color TD',\n",
       " 'Aid color Luther',\n",
       " 'Rioja color Roi',\n",
       " 'UD color Pole',\n",
       " 'CC color Page',\n",
       " 'Cambridge color Baron',\n",
       " 'Teddy color Libia',\n",
       " 'Crown color Cuenca',\n",
       " 'Bing color Kálmán',\n",
       " 'Face color ET',\n",
       " 'Marne color Mainstream',\n",
       " 'Hawker color Agency',\n",
       " 'Parry color Mata',\n",
       " 'Borussia color Mineral',\n",
       " 'Greece color Figaro',\n",
       " 'Telephone color Trung',\n",
       " 'MBA color Sabha',\n",
       " 'Latreille color Guimarães',\n",
       " 'Principal color Disneyland',\n",
       " 'Dad color Wes',\n",
       " 'Belfast color Gesù',\n",
       " 'Ages color Cinq',\n",
       " 'Massacre color Silla',\n",
       " 'Hub color JR',\n",
       " 'Newton color Bryant',\n",
       " 'Stand color Klaus',\n",
       " 'McKamey color Kleiner',\n",
       " 'Powers color Fort',\n",
       " 'Leta color City',\n",
       " 'Mad color Márquez',\n",
       " 'Samba color Yer',\n",
       " 'Senators color Bernard',\n",
       " 'Fel color Diaz',\n",
       " 'Hughes color Madre',\n",
       " 'Potok color Asturias',\n",
       " 'Maxim color FF',\n",
       " 'Winter color Dis',\n",
       " 'Morley color Lorentz',\n",
       " 'Bassa color SM',\n",
       " 'Bush color Sempre',\n",
       " 'Train color Portsmouth',\n",
       " 'Marea color Irena',\n",
       " 'Mahler color Oaks',\n",
       " 'Kati color Fighting',\n",
       " 'Lyle color Bilbao',\n",
       " 'Preto color Sharon',\n",
       " 'Königsberg color Allium',\n",
       " 'Uit color Haiti',\n",
       " 'Farm color Isabella',\n",
       " 'Midden color Olsson',\n",
       " 'Chicago color Pure',\n",
       " 'Dol color Patrol',\n",
       " 'Pero color Siege',\n",
       " 'Toten color Galiza',\n",
       " 'Tears color Lucky',\n",
       " 'Minister color Pfeiffer',\n",
       " 'Valenciana color FX',\n",
       " 'Dunkerque color Energy',\n",
       " 'Sartre color Joanne',\n",
       " 'Front color Erik',\n",
       " 'CAF color EM',\n",
       " 'WR color Raum',\n",
       " 'Donovan color VL',\n",
       " 'Bey color Sabina',\n",
       " 'Alexa color Krupp',\n",
       " 'Porter color October',\n",
       " 'Euro color Davidson',\n",
       " 'Shri color Allah',\n",
       " 'Atelier color Larva',\n",
       " 'Classic color Khu',\n",
       " 'Noche color Lahore',\n",
       " 'Cavendish color Dorset',\n",
       " 'Sinh color Stay',\n",
       " 'Sports color Sit',\n",
       " 'Denne color Wayne',\n",
       " 'Sonntag color Flag',\n",
       " 'Terceira color Ferns',\n",
       " 'Au color Jaguar',\n",
       " 'Jammu color Brenda',\n",
       " 'Carnaval color Mina',\n",
       " 'Ferreira color Harding',\n",
       " 'Fenner color Giants',\n",
       " 'Vader color Buta',\n",
       " 'Bray color Faye',\n",
       " 'Barnard color Generation',\n",
       " 'Earl color Schottland',\n",
       " 'Manu color USD',\n",
       " 'Dara color Eu',\n",
       " 'Wesley expected completeness Haji',\n",
       " 'Hai expected completeness Yahoo',\n",
       " 'Wilder expected completeness Stal',\n",
       " 'Porter expected completeness FC',\n",
       " 'Mirage expected completeness Dad',\n",
       " 'Dharma expected completeness Kenia',\n",
       " 'Santo expected completeness CCD',\n",
       " 'Josep expected completeness Riau',\n",
       " 'Burgess expected completeness Ky',\n",
       " 'Mortimer expected completeness Billie',\n",
       " 'Zbigniew expected completeness Elbe',\n",
       " 'Design expected completeness Paraíso',\n",
       " 'Hotel expected completeness TD',\n",
       " 'Robot expected completeness Luther',\n",
       " 'Guayaquil expected completeness Roi',\n",
       " 'Laurie expected completeness Pole',\n",
       " 'Vittoria expected completeness Page',\n",
       " 'Motion expected completeness Baron',\n",
       " 'Cor expected completeness Libia',\n",
       " 'Naga expected completeness Cuenca',\n",
       " 'Alexa expected completeness Kálmán',\n",
       " 'WBC expected completeness ET',\n",
       " 'Collins expected completeness Mainstream',\n",
       " 'Akademie expected completeness Agency',\n",
       " 'Faber expected completeness Mata',\n",
       " 'Cullen expected completeness Mineral',\n",
       " 'Hiroshima expected completeness Figaro',\n",
       " 'Auckland expected completeness Trung',\n",
       " 'Belmont expected completeness Sabha',\n",
       " 'RCA expected completeness Guimarães',\n",
       " 'Träger expected completeness Disneyland',\n",
       " 'CC expected completeness Wes',\n",
       " 'Deus expected completeness Gesù',\n",
       " 'Mustang expected completeness Cinq',\n",
       " 'Benjamin expected completeness Silla',\n",
       " 'Ronnie expected completeness JR',\n",
       " 'AD expected completeness Bryant',\n",
       " 'Oru expected completeness Klaus',\n",
       " 'Baronet expected completeness Kleiner',\n",
       " 'Nos expected completeness Fort',\n",
       " 'Certain expected completeness City',\n",
       " 'Kid expected completeness Márquez',\n",
       " 'Ami expected completeness Yer',\n",
       " 'AVN expected completeness Bernard',\n",
       " 'Maya expected completeness Diaz',\n",
       " 'Oasis expected completeness Madre',\n",
       " 'Ordu expected completeness Asturias',\n",
       " 'FAA expected completeness FF',\n",
       " 'Automatic expected completeness Dis',\n",
       " 'Chronicle expected completeness Lorentz',\n",
       " 'NH expected completeness SM',\n",
       " 'Door expected completeness Sempre',\n",
       " 'Profil expected completeness Portsmouth',\n",
       " 'Words expected completeness Irena',\n",
       " 'PE expected completeness Oaks',\n",
       " 'Hamar expected completeness Fighting',\n",
       " 'Ribera expected completeness Bilbao',\n",
       " 'Crash expected completeness Sharon',\n",
       " 'Stella expected completeness Allium',\n",
       " 'Béla expected completeness Haiti',\n",
       " 'Ferro expected completeness Isabella',\n",
       " 'Writer expected completeness Olsson',\n",
       " 'Aviation expected completeness Pure',\n",
       " 'da expected completeness Patrol',\n",
       " 'Hercules expected completeness Siege',\n",
       " 'Elmi expected completeness Galiza',\n",
       " 'Heide expected completeness Lucky',\n",
       " 'Armstrong expected completeness Pfeiffer',\n",
       " 'Remote expected completeness FX',\n",
       " 'Nagar expected completeness Energy',\n",
       " 'Moro expected completeness Joanne',\n",
       " 'Ami expected completeness Erik',\n",
       " 'CL expected completeness EM',\n",
       " 'Toren expected completeness Raum',\n",
       " 'Ph expected completeness VL',\n",
       " 'Margaretha expected completeness Sabina',\n",
       " 'Heide expected completeness Krupp',\n",
       " 'Banja expected completeness October',\n",
       " 'Spot expected completeness Davidson',\n",
       " 'Elias expected completeness Allah',\n",
       " 'LP expected completeness Larva',\n",
       " 'ML expected completeness Khu',\n",
       " 'Wait expected completeness Lahore',\n",
       " 'Rome expected completeness Dorset',\n",
       " 'Citizen expected completeness Stay',\n",
       " 'Allier expected completeness Sit',\n",
       " 'Hava expected completeness Wayne',\n",
       " 'Sada expected completeness Flag',\n",
       " 'Era expected completeness Ferns',\n",
       " 'Graham expected completeness Jaguar',\n",
       " 'Tone expected completeness Brenda',\n",
       " 'Madeleine expected completeness Mina',\n",
       " 'Midnight expected completeness Harding',\n",
       " 'GDP expected completeness Giants',\n",
       " 'Salt expected completeness Buta',\n",
       " 'Sima expected completeness Faye',\n",
       " 'Steinicke expected completeness Generation',\n",
       " 'Nové expected completeness Schottland',\n",
       " 'Checa expected completeness USD',\n",
       " 'Stanley expected completeness Eu',\n",
       " 'Canadá measured physical quantity Haji',\n",
       " 'Garden measured physical quantity Yahoo',\n",
       " 'Walters measured physical quantity Stal',\n",
       " 'Salon measured physical quantity FC',\n",
       " 'Canada measured physical quantity Dad',\n",
       " 'Cambridge measured physical quantity Kenia',\n",
       " 'Az measured physical quantity CCD',\n",
       " 'Ne measured physical quantity Riau',\n",
       " 'Bulu measured physical quantity Ky',\n",
       " 'Cramer measured physical quantity Billie',\n",
       " 'Speedway measured physical quantity Elbe',\n",
       " 'Paderborn measured physical quantity Paraíso',\n",
       " 'Hazel measured physical quantity TD',\n",
       " 'Ipswich measured physical quantity Luther',\n",
       " 'Nowe measured physical quantity Roi',\n",
       " 'Foi measured physical quantity Pole',\n",
       " 'Powers measured physical quantity Page',\n",
       " 'Ingles measured physical quantity Baron',\n",
       " 'Welsh measured physical quantity Libia',\n",
       " 'Ludwik measured physical quantity Cuenca',\n",
       " 'Wedding measured physical quantity Kálmán',\n",
       " 'Ludovic measured physical quantity ET',\n",
       " 'Genus measured physical quantity Mainstream',\n",
       " 'Fortuna measured physical quantity Agency',\n",
       " 'Norris measured physical quantity Mata',\n",
       " 'MC measured physical quantity Mineral',\n",
       " 'Mass measured physical quantity Figaro',\n",
       " 'Thái measured physical quantity Trung',\n",
       " 'Fresno measured physical quantity Sabha',\n",
       " 'Bresse measured physical quantity Guimarães',\n",
       " 'Essex measured physical quantity Disneyland',\n",
       " 'Conte measured physical quantity Wes',\n",
       " 'Deeds measured physical quantity Gesù',\n",
       " 'Woods measured physical quantity Cinq',\n",
       " 'Lara measured physical quantity Silla',\n",
       " 'Perdana measured physical quantity JR',\n",
       " 'Shanghai measured physical quantity Bryant',\n",
       " 'IRAS measured physical quantity Klaus',\n",
       " 'Music measured physical quantity Kleiner',\n",
       " 'Aiken measured physical quantity Fort',\n",
       " 'Zion measured physical quantity City',\n",
       " 'Hardy measured physical quantity Márquez',\n",
       " 'While measured physical quantity Yer',\n",
       " 'Cuenca measured physical quantity Bernard',\n",
       " 'Wittgenstein measured physical quantity Diaz',\n",
       " 'Chatham measured physical quantity Madre',\n",
       " 'Hertfordshire measured physical quantity Asturias',\n",
       " 'Em measured physical quantity FF',\n",
       " 'Velázquez measured physical quantity Dis',\n",
       " 'Barrett measured physical quantity Lorentz',\n",
       " 'Kensley measured physical quantity SM',\n",
       " 'Bartlett measured physical quantity Sempre',\n",
       " 'Aves measured physical quantity Portsmouth',\n",
       " 'Phantom measured physical quantity Irena',\n",
       " 'Lori measured physical quantity Oaks',\n",
       " 'Lors measured physical quantity Fighting',\n",
       " 'Gutenberg measured physical quantity Bilbao',\n",
       " 'Dänemark measured physical quantity Sharon',\n",
       " 'Sá measured physical quantity Allium',\n",
       " 'Oman measured physical quantity Haiti',\n",
       " 'EN measured physical quantity Isabella',\n",
       " 'Gate measured physical quantity Olsson',\n",
       " 'Ticino measured physical quantity Pure',\n",
       " 'Eclipse measured physical quantity Patrol',\n",
       " 'UCB measured physical quantity Siege',\n",
       " 'Arcade measured physical quantity Galiza',\n",
       " 'Buzz measured physical quantity Lucky',\n",
       " 'Renault measured physical quantity Pfeiffer',\n",
       " 'Laval measured physical quantity FX',\n",
       " 'Molina measured physical quantity Energy',\n",
       " 'RN measured physical quantity Joanne',\n",
       " 'Monmouth measured physical quantity Erik',\n",
       " 'Turin measured physical quantity EM',\n",
       " 'Smith measured physical quantity Raum',\n",
       " 'Hume measured physical quantity VL',\n",
       " 'Gan measured physical quantity Sabina',\n",
       " 'Rotten measured physical quantity Krupp',\n",
       " 'Poet measured physical quantity October',\n",
       " 'Assembly measured physical quantity Davidson',\n",
       " 'Toren measured physical quantity Allah',\n",
       " 'Roberto measured physical quantity Larva',\n",
       " 'Beatrice measured physical quantity Khu',\n",
       " 'Mario measured physical quantity Lahore',\n",
       " 'SR measured physical quantity Dorset',\n",
       " 'Marge measured physical quantity Stay',\n",
       " 'Weir measured physical quantity Sit',\n",
       " 'Adobe measured physical quantity Wayne',\n",
       " 'Hallan measured physical quantity Flag',\n",
       " 'Viscount measured physical quantity Ferns',\n",
       " 'Dante measured physical quantity Jaguar',\n",
       " 'Sarmiento measured physical quantity Brenda',\n",
       " 'Nikolaj measured physical quantity Mina',\n",
       " 'Lamarck measured physical quantity Harding',\n",
       " 'Safe measured physical quantity Giants',\n",
       " 'UN measured physical quantity Buta',\n",
       " 'Edmonton measured physical quantity Faye',\n",
       " 'Zelda measured physical quantity Generation',\n",
       " 'Planck measured physical quantity Schottland',\n",
       " 'Ne measured physical quantity USD',\n",
       " 'Isle measured physical quantity Eu']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict['sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc60d9",
   "metadata": {},
   "source": [
    "#### -> Test my hypothesis if (f, r, e) or (e, r_de, f) exist more?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60662516",
   "metadata": {},
   "source": [
    "Evaluate if for (e, r, f) we know more often (e, r_de, f) or (f, r, e), i.e. Knowledge Transfer vs symmetric rule.\n",
    "This can also help us understand which way we get (f, r_de, e).\n",
    "\n",
    "Since when we train on (e, r_de, f), we rarely get (f, r_de, e), it already implies that we would go the way:\n",
    "(e, r, f) -RULE-> (f, r, e) -KT-> (f, r_de, e)\n",
    "\n",
    "1800 facts are training the rule (900<->900)\n",
    "1800-1900 are facts that are used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37646447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap(a, b):\n",
    "    a_multiset = Counter(a)\n",
    "    b_multiset = Counter(b)\n",
    "\n",
    "    overlap = list((a_multiset & b_multiset).elements())\n",
    "    \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0110d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: taxonomischer Rang\n",
      "Relation Target: taxon rank\n",
      "Accuracy for (f, r, e): 0.73\n",
      "Accuracy for (e, r_t, f): 0.62\n",
      "Accuracy for (f, r_t, e): 0.63\n",
      "Size (f, r, e): 73\n",
      "Size (e, r_t, f): 62\n",
      "Overlap between (f, r, e) and (e, r_t, f): 49\n",
      "Transfer from (e, r_t, f) to (f, r_t, e): 0.6774193548387096\n",
      "Transfer from (f, r, e) to (f, r_t, e): 0.8356164383561644\n",
      "\n",
      "Relation: Notfalleinrichtungen\n",
      "Relation Target: emergency services\n",
      "Accuracy for (f, r, e): 0.81\n",
      "Accuracy for (e, r_t, f): 0.21\n",
      "Accuracy for (f, r_t, e): 0.6\n",
      "Size (f, r, e): 81\n",
      "Size (e, r_t, f): 21\n",
      "Overlap between (f, r, e) and (e, r_t, f): 17\n",
      "Transfer from (e, r_t, f) to (f, r_t, e): 0.6190476190476191\n",
      "Transfer from (f, r, e) to (f, r_t, e): 0.7283950617283951\n",
      "\n",
      "Relation: Farbe\n",
      "Relation Target: color\n",
      "Accuracy for (f, r, e): 0.71\n",
      "Accuracy for (e, r_t, f): 0.9\n",
      "Accuracy for (f, r_t, e): 0.71\n",
      "Size (f, r, e): 71\n",
      "Size (e, r_t, f): 90\n",
      "Overlap between (f, r, e) and (e, r_t, f): 63\n",
      "Transfer from (e, r_t, f) to (f, r_t, e): 0.7\n",
      "Transfer from (f, r, e) to (f, r_t, e): 1.0\n",
      "\n",
      "Relation: erwartete Vollständigkeit\n",
      "Relation Target: expected completeness\n",
      "Accuracy for (f, r, e): 0.72\n",
      "Accuracy for (e, r_t, f): 0.02\n",
      "Accuracy for (f, r_t, e): 0.45\n",
      "Size (f, r, e): 72\n",
      "Size (e, r_t, f): 2\n",
      "Overlap between (f, r, e) and (e, r_t, f): 1\n",
      "Transfer from (e, r_t, f) to (f, r_t, e): 0.0\n",
      "Transfer from (f, r, e) to (f, r_t, e): 0.6111111111111112\n",
      "\n",
      "Relation: gemessene physikalische Größe\n",
      "Relation Target: measured physical quantity\n",
      "Accuracy for (f, r, e): 0.69\n",
      "Accuracy for (e, r_t, f): 0.14\n",
      "Accuracy for (f, r_t, e): 0.55\n",
      "Size (f, r, e): 69\n",
      "Size (e, r_t, f): 14\n",
      "Overlap between (f, r, e) and (e, r_t, f): 7\n",
      "Transfer from (e, r_t, f) to (f, r_t, e): 0.5\n",
      "Transfer from (f, r, e) to (f, r_t, e): 0.7391304347826086\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over relations, take the training samples that were trained on\n",
    "for i in range(n_relations):\n",
    "    trained_test = train_dict['sample'][1800+i*1900:(i+1)*1900]\n",
    "\n",
    "    acc_r = 0\n",
    "    correct_entities_r = []\n",
    "    \n",
    "    acc_rde = 0\n",
    "    correct_entities_rde = []\n",
    "    \n",
    "    acc_test = 0\n",
    "    correct_entities_test = []\n",
    "    \n",
    "    r = relations['de'].iloc[i]\n",
    "    r_t = relations['en'].iloc[i]\n",
    "\n",
    "    for sample in trained_test:\n",
    "\n",
    "        # Test (f, r, e)\n",
    "        f = sample.rsplit(' ', 1)[1] \n",
    "        e = sample.split(' ', 1)[0]\n",
    "\n",
    "        label_token = tokenizer.convert_tokens_to_ids(e)\n",
    "\n",
    "        prompt = f + ' ' + r + ' [MASK]'\n",
    "        # print(prompt)\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        # Pick the [MASK] candidates with the highest logits\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_r += 1\n",
    "            correct_entities_r.append(e)\n",
    "\n",
    "        # Test (e, r_de, f)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(f)\n",
    "\n",
    "        prompt = e + ' ' + r_t + ' [MASK]'\n",
    "        # print(prompt)\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        # Pick the [MASK] candidates with the highest logits\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_rde += 1\n",
    "            correct_entities_rde.append(f)\n",
    "            \n",
    "        # Test (f, r_de, e)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(e)\n",
    "\n",
    "        prompt = f + ' ' + r_t + ' [MASK]'\n",
    "        # print(prompt)\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        # Pick the [MASK] candidates with the highest logits\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_test += 1\n",
    "            correct_entities_test.append(e)\n",
    "        \n",
    "\n",
    "    acc_r /= 100\n",
    "    acc_rde /= 100\n",
    "    acc_test /= 100\n",
    "\n",
    "    print(f'Relation: {r}')\n",
    "    print(f'Relation Target: {r_t}')\n",
    "    print(f'Accuracy for (f, r, e): {acc_r}')\n",
    "    print(f'Accuracy for (e, r_t, f): {acc_rde}')\n",
    "    print(f'Accuracy for (f, r_t, e): {acc_test}')\n",
    "    print(f'Size (f, r, e): {len(correct_entities_r)}')\n",
    "    print(f'Size (e, r_t, f): {len(correct_entities_rde)}')\n",
    "    print(f'Overlap between (f, r, e) and (e, r_t, f): {len(compute_overlap(correct_entities_r, correct_entities_rde))}')\n",
    "    if len(correct_entities_rde) == 0:\n",
    "        print(f'Transfer from (e, r_t, f) to (f, r_t, e): {0}')\n",
    "    else:\n",
    "        print(f'Transfer from (e, r_t, f) to (f, r_t, e): {len(compute_overlap(correct_entities_rde, correct_entities_test))/len(correct_entities_rde)}')\n",
    "    \n",
    "    if len(correct_entities_r) == 0:\n",
    "        print(f'Transfer from (f, r, e) to (f, r_t, e): {0}')\n",
    "    else:\n",
    "        print(f'Transfer from (f, r, e) to (f, r_t, e): {len(compute_overlap(correct_entities_r, correct_entities_test))/len(correct_entities_r)}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045671a",
   "metadata": {},
   "source": [
    "#### -> Is every relation symmetric now? What about relations that aren't part of the training?\n",
    "For this sample n_relations from general and 100 entities and test them if what they predict in one direction, they also predict in the other. This was also quite flawed in Symbolic Reasoner. Here they didn't finetune\n",
    "\n",
    "- What about training with general and then testing on them if they are symmetric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925c8a5",
   "metadata": {},
   "source": [
    "**Are General Relations (aka generate_random) symmetric?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82df31e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: manner of death\n",
      "Accuracy: 0.882\n",
      "Percentage of Symmetry: 0.017\n",
      "Relation: final event\n",
      "Accuracy: 0.82\n",
      "Percentage of Symmetry: 0.023\n",
      "Relation: birthday\n",
      "Accuracy: 0.904\n",
      "Percentage of Symmetry: 0.12\n",
      "Relation: supercharger\n",
      "Accuracy: 0.892\n",
      "Percentage of Symmetry: 0.026\n",
      "Relation: running mate\n",
      "Accuracy: 0.875\n",
      "Percentage of Symmetry: 0.065\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for idx, rel in relations_random.iterrows():\n",
    "    print(f\"Relation: {rel['en']}\")\n",
    "    \n",
    "    trained_test = train_random[i*1000:(i+1)*1000]\n",
    "\n",
    "    acc = 0\n",
    "    sym = 0\n",
    "\n",
    "    for sample in trained_test:\n",
    "                \n",
    "        # (e, r, f)\n",
    "        e = sample.split(' ', 1)[0]\n",
    "        f = sample.rsplit(' ', 1)[1] \n",
    "        \n",
    "        label_token = tokenizer.convert_tokens_to_ids(f)\n",
    "\n",
    "        # Use this single token entity to get a pair\n",
    "        prompt = e + ' ' + rel['en'] + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()\n",
    "        \n",
    "        if label_token in top_1_token:\n",
    "            acc += 1\n",
    "\n",
    "        # Check if the pair is symmetry\n",
    "        label_token = tokenizer.convert_tokens_to_ids(e)\n",
    "\n",
    "        # Use this single token entity to get a pair\n",
    "        prompt = f + ' ' + rel['en'] + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        top_1_token = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "        \n",
    "        if label_token in top_1_token:\n",
    "            sym += 1\n",
    "            \n",
    "    i += 1\n",
    "\n",
    "    print(f'Accuracy: {acc/1000}')\n",
    "    print(f'Percentage of Symmetry: {sym/1000}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb31eb",
   "metadata": {},
   "source": [
    "**Are General Relations that weren't trained on symmetric?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02335492",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_test, relations_test = load_data(Relation.Equivalence, source_language, target_language, False, False)\n",
    "\n",
    "relations_sampled = relations_test.sample(n_relations)\n",
    "relations_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79cf03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, rel in relations_sampled.iterrows():\n",
    "    print(f\"Relation: {rel['en']}\")\n",
    "\n",
    "    sym = 0\n",
    "\n",
    "    # Get random entities for e\n",
    "    entities1 = generate_unique_indices(entities_test.shape[0], 100)\n",
    "\n",
    "    for i, e_id in enumerate(entities1):\n",
    "        e = entities_test['label'][e_id]\n",
    "\n",
    "        e_token = tokenizer.encode(e)[1]\n",
    "\n",
    "        # Use this single token entity to get a pair\n",
    "        prompt = e + ' ' + rel['en'] + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        f = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        # Check if the pair is symmetry\n",
    "        prompt = tokenizer.decode([f]) + ' ' + rel['en'] + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        top_1_token = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "        if e_token in top_1_token:\n",
    "            sym += 1\n",
    "\n",
    "    print(f'Percentage of Symmetry: {sym/100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554bc979",
   "metadata": {},
   "source": [
    "### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2accb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8829473684210526\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "total = len(train_dict['sample'])\n",
    "i = 0\n",
    "\n",
    "for txt in train_dict['sample'][:10000]:\n",
    "    i += 1\n",
    "    \n",
    "    # Add [MASK] for object\n",
    "    sample = txt.rsplit(' ', 1)[0] + ' [MASK]'\n",
    "    label_token = tokenizer.convert_tokens_to_ids(txt.rsplit(' ', 1)[1])\n",
    "    \n",
    "    encoded_input = tokenizer(sample, return_tensors='pt')\n",
    "    token_logits = model(**encoded_input).logits\n",
    "    \n",
    "    mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    \n",
    "    # Pick the [MASK] candidates with the highest logits\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()\n",
    "    \n",
    "    if label_token in top_5_tokens:\n",
    "        k += 1\n",
    "print(k/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"lens manner of [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n>>> {tokenizer.decode([chunk])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbe90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_dict['sample']:\n",
    "    if 'Alex' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc6b4c",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "- Training with just symmetric, doesn't necessarily mean that everything is symmetric. Maybe BERT in Symbolic Reasoner was just overfitting since it isnt finetuning but actually pretraining, i.e. it never sees evidence of non symmetry but a lot of symmetry.\n",
    "\n",
    "\n",
    "\n",
    "- See Obsidian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ea4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
