{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e4031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer, TrainingArguments, Trainer, \\\n",
    "    DataCollatorForLanguageModeling, IntervalStrategy\n",
    "\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "from data_generation_relation import *\n",
    "from utils import *\n",
    "from custom_trainer import CustomTrainer\n",
    "from datasets import load_metric\n",
    "import logging\n",
    "from transformers import logging as tlogging\n",
    "import wandb\n",
    "import sys\n",
    "from utils import set_seed\n",
    "from transformers.integrations import WandbCallback, TensorBoardCallback\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735ddbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "run_name = 'Implication_en_de_20'\n",
    "epochs = 200\n",
    "batch_size = 256\n",
    "lr = 5e-5\n",
    "\n",
    "relation = 'implication'\n",
    "source_language = ['en']\n",
    "target_language = ['de']\n",
    "n_relations = 10\n",
    "n_facts = 1000\n",
    "n_pairs = 20\n",
    "\n",
    "use_random = False\n",
    "\n",
    "precision_k = 1\n",
    "\n",
    "use_pretrained = False\n",
    "use_target = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cd5ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        id                          en                             de  \\\n",
       " 694   P105                  taxon rank             taxonomischer Rang   \n",
       " 598   P462                       color                          Farbe   \n",
       " 120   P111  measured physical quantity  gemessene physikalische Größe   \n",
       " 281   P400                    platform                      Plattform   \n",
       " 137  P8345             media franchise               Medien-Franchise   \n",
       " 204  P1606        natural reservoir of           Erregerreservoir von   \n",
       " 231  P2675                    reply to                    Antwort auf   \n",
       " 213  P1909                 side effect                   Nebenwirkung   \n",
       " 235  P1363       points/goal scored by    Punkt/Treffer erzielt durch   \n",
       " 711   P607                    conflict                  Kriegseinsatz   \n",
       " \n",
       "                             es                         fr    count  \n",
       " 694       categoría taxonómica           rang taxinomique  3580266  \n",
       " 598                      color                    couleur   194389  \n",
       " 120     cantidad física medida  grandeur physique mesurée     3610  \n",
       " 281                 plataforma                 plateforme    95318  \n",
       " 137       franquicia de medios       franchise médiatique    27415  \n",
       " 204      reservorio natural de       réservoir naturel de       17  \n",
       " 231                respuesta a                  réponse à      381  \n",
       " 213          efecto secundario           effet secondaire       40  \n",
       " 235  puntos/goles marcados por       point/but marqué par     2441  \n",
       " 711  participó en el conflicto                    conflit   220972  ,\n",
       "         id                         en                            de  \\\n",
       " 528  P6855         emergency services          Notfalleinrichtungen   \n",
       " 606  P2429      expected completeness     erwartete Vollständigkeit   \n",
       " 63   P3027           open period from        geöffnet von Zeitpunkt   \n",
       " 515  P7727      legislative committee             Legislativkomitee   \n",
       " 587  P9597               type of lens                     Linsentyp   \n",
       " 218  P8852                facial hair                  Gesichtshaar   \n",
       " 66   P1455              list of works                     Werkliste   \n",
       " 754   P129  physically interacts with  interagiert physikalisch mit   \n",
       " 118  P6271                 demonym of                    Demonym zu   \n",
       " 522  P2596                    culture                        Kultur   \n",
       " \n",
       "                              es                                  fr   count  \n",
       " 528     servicios de emergencia  accueil et traitement des urgences     766  \n",
       " 606        grado de completitud                 degré de complétude    3826  \n",
       " 63                abierto desde     début de la période d'ouverture      16  \n",
       " 515          comité legislativo                   comité législatif  123710  \n",
       " 587               tipo de lente            type de lentille optique    1721  \n",
       " 218                vello facial                    pilosité faciale     362  \n",
       " 66               lista de obras                    liste des œuvres    1227  \n",
       " 754  interactúa físicamente con         interagit physiquement avec    9480  \n",
       " 118               gentilicio de                          gentilé de    2629  \n",
       " 522                     cultura                             culture   10007  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test, relations, entities = generate_reasoning(relation=Relation(relation),\n",
    "                                                        source_language=source_language,\n",
    "                                                        target_language=target_language,\n",
    "                                                        n_relations=n_relations,\n",
    "                                                        n_facts=n_facts,\n",
    "                                                        use_pretrained=use_pretrained,\n",
    "                                                        use_target=use_target,\n",
    "                                                        use_enhanced=False,\n",
    "                                                        use_same_relations=False,\n",
    "                                                        n_pairs=n_pairs)\n",
    "\n",
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a01af80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>P6855</td>\n",
       "      <td>emergency services</td>\n",
       "      <td>Notfalleinrichtungen</td>\n",
       "      <td>servicios de emergencia</td>\n",
       "      <td>accueil et traitement des urgences</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>P2429</td>\n",
       "      <td>expected completeness</td>\n",
       "      <td>erwartete Vollständigkeit</td>\n",
       "      <td>grado de completitud</td>\n",
       "      <td>degré de complétude</td>\n",
       "      <td>3826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>P3027</td>\n",
       "      <td>open period from</td>\n",
       "      <td>geöffnet von Zeitpunkt</td>\n",
       "      <td>abierto desde</td>\n",
       "      <td>début de la période d'ouverture</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>P7727</td>\n",
       "      <td>legislative committee</td>\n",
       "      <td>Legislativkomitee</td>\n",
       "      <td>comité legislativo</td>\n",
       "      <td>comité législatif</td>\n",
       "      <td>123710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>P9597</td>\n",
       "      <td>type of lens</td>\n",
       "      <td>Linsentyp</td>\n",
       "      <td>tipo de lente</td>\n",
       "      <td>type de lentille optique</td>\n",
       "      <td>1721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>P8852</td>\n",
       "      <td>facial hair</td>\n",
       "      <td>Gesichtshaar</td>\n",
       "      <td>vello facial</td>\n",
       "      <td>pilosité faciale</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>P1455</td>\n",
       "      <td>list of works</td>\n",
       "      <td>Werkliste</td>\n",
       "      <td>lista de obras</td>\n",
       "      <td>liste des œuvres</td>\n",
       "      <td>1227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>P129</td>\n",
       "      <td>physically interacts with</td>\n",
       "      <td>interagiert physikalisch mit</td>\n",
       "      <td>interactúa físicamente con</td>\n",
       "      <td>interagit physiquement avec</td>\n",
       "      <td>9480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>P6271</td>\n",
       "      <td>demonym of</td>\n",
       "      <td>Demonym zu</td>\n",
       "      <td>gentilicio de</td>\n",
       "      <td>gentilé de</td>\n",
       "      <td>2629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>P2596</td>\n",
       "      <td>culture</td>\n",
       "      <td>Kultur</td>\n",
       "      <td>cultura</td>\n",
       "      <td>culture</td>\n",
       "      <td>10007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                         en                            de  \\\n",
       "528  P6855         emergency services          Notfalleinrichtungen   \n",
       "606  P2429      expected completeness     erwartete Vollständigkeit   \n",
       "63   P3027           open period from        geöffnet von Zeitpunkt   \n",
       "515  P7727      legislative committee             Legislativkomitee   \n",
       "587  P9597               type of lens                     Linsentyp   \n",
       "218  P8852                facial hair                  Gesichtshaar   \n",
       "66   P1455              list of works                     Werkliste   \n",
       "754   P129  physically interacts with  interagiert physikalisch mit   \n",
       "118  P6271                 demonym of                    Demonym zu   \n",
       "522  P2596                    culture                        Kultur   \n",
       "\n",
       "                             es                                  fr   count  \n",
       "528     servicios de emergencia  accueil et traitement des urgences     766  \n",
       "606        grado de completitud                 degré de complétude    3826  \n",
       "63                abierto desde     début de la période d'ouverture      16  \n",
       "515          comité legislativo                   comité législatif  123710  \n",
       "587               tipo de lente            type de lentille optique    1721  \n",
       "218                vello facial                    pilosité faciale     362  \n",
       "66               lista de obras                    liste des œuvres    1227  \n",
       "754  interactúa físicamente con         interagit physiquement avec    9480  \n",
       "118               gentilicio de                          gentilé de    2629  \n",
       "522                     cultura                             culture   10007  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5349baf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_random = []\n",
    "\n",
    "if use_random:\n",
    "    # Generate half/half\n",
    "    factor = 1.0\n",
    "    n_random = factor * n_facts\n",
    "\n",
    "    train_random, relations_random = generate_random(source_language, target_language, n_random, n_relations)\n",
    "    train += train_random\n",
    "\n",
    "relations_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81631d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING\n",
    "# Load mBERT model and Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Load Data Collator for Prediction and Evaluation\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "eval_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5618d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b2ec8be0fb4ea081d547cb88427ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c230b7d9c4bb453a821ad5f708150156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ~~ PRE-PROCESSING ~~\n",
    "train_dict = {'sample': train}\n",
    "test_dict = {'sample': flatten_dict2_list(copy.deepcopy(test))}\n",
    "train_ds = Dataset.from_dict(train_dict)\n",
    "test_ds = Dataset.from_dict(test_dict)\n",
    "\n",
    "# Tokenize Training and Test Data\n",
    "tokenized_train = tokenize(tokenizer, train_ds)  # Train is shuffled by Huggingface\n",
    "tokenized_test = tokenize(tokenizer, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae700a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train and Test Data\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "test_complete_df = pd.DataFrame(test)\n",
    "test_flat_df = pd.DataFrame(test_dict)\n",
    "\n",
    "data_dir = './output/' + run_name + '/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "train_df.to_csv(data_dir + 'train_set', index=False)\n",
    "test_complete_df.to_json(data_dir + 'test_set_complete')\n",
    "test_flat_df.to_csv(data_dir + 'test_set', index=False)\n",
    "\n",
    "if use_random:\n",
    "    train_random_df = pd.DataFrame({'sample': train_random})\n",
    "    train_random_df.to_csv(data_dir + 'train_random', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0d66d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='./output/' + run_name + '/models/',\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=128,\n",
    "        learning_rate=lr,\n",
    "        logging_dir='./output/' + run_name + '/tb_logs/',\n",
    "        logging_strategy=IntervalStrategy.EPOCH,\n",
    "        evaluation_strategy=IntervalStrategy.EPOCH,\n",
    "        save_strategy=IntervalStrategy.EPOCH,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    eval_data_collator=eval_data_collator,\n",
    "    compute_metrics=precision_at_one,\n",
    "    precision_at=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b90787af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19000\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7600\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7600' max='7600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7600/7600 1:52:07, Epoch 200/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.754000</td>\n",
       "      <td>8.455544</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.343000</td>\n",
       "      <td>6.775641</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.828100</td>\n",
       "      <td>6.018909</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.599000</td>\n",
       "      <td>5.475370</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.426800</td>\n",
       "      <td>5.167844</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.391100</td>\n",
       "      <td>5.355132</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.386300</td>\n",
       "      <td>5.105166</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.263200</td>\n",
       "      <td>5.025634</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.256400</td>\n",
       "      <td>4.958439</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.216100</td>\n",
       "      <td>4.944651</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.212800</td>\n",
       "      <td>4.983780</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.172800</td>\n",
       "      <td>5.043835</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.170300</td>\n",
       "      <td>4.848181</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.155200</td>\n",
       "      <td>4.845535</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.192900</td>\n",
       "      <td>4.630761</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.111800</td>\n",
       "      <td>4.822012</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.125100</td>\n",
       "      <td>4.649027</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.142400</td>\n",
       "      <td>4.669083</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.125900</td>\n",
       "      <td>4.574639</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.127100</td>\n",
       "      <td>4.520109</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.161700</td>\n",
       "      <td>4.618351</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.098100</td>\n",
       "      <td>4.440432</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.132500</td>\n",
       "      <td>4.725621</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>4.884236</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.091500</td>\n",
       "      <td>4.573143</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.122700</td>\n",
       "      <td>4.802151</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.142500</td>\n",
       "      <td>4.640013</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.081400</td>\n",
       "      <td>4.903117</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.099900</td>\n",
       "      <td>4.781009</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.049600</td>\n",
       "      <td>4.679241</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.038800</td>\n",
       "      <td>5.073473</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.083400</td>\n",
       "      <td>4.823790</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.032100</td>\n",
       "      <td>4.723872</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.063900</td>\n",
       "      <td>4.850721</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.081400</td>\n",
       "      <td>4.839449</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.100700</td>\n",
       "      <td>4.800258</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.064000</td>\n",
       "      <td>5.080610</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.104900</td>\n",
       "      <td>5.195995</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.024600</td>\n",
       "      <td>5.413558</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.953700</td>\n",
       "      <td>5.122548</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.918200</td>\n",
       "      <td>5.745680</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.880300</td>\n",
       "      <td>5.997728</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.780300</td>\n",
       "      <td>6.614691</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.740400</td>\n",
       "      <td>7.124413</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.693500</td>\n",
       "      <td>6.810805</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.651300</td>\n",
       "      <td>7.591344</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.614800</td>\n",
       "      <td>7.138199</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.594700</td>\n",
       "      <td>6.376750</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.568000</td>\n",
       "      <td>6.405297</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.643900</td>\n",
       "      <td>6.579255</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.564000</td>\n",
       "      <td>6.147459</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.580100</td>\n",
       "      <td>6.143785</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.556800</td>\n",
       "      <td>6.172322</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.559500</td>\n",
       "      <td>5.779029</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.527800</td>\n",
       "      <td>5.852683</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.518600</td>\n",
       "      <td>6.004194</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.515000</td>\n",
       "      <td>5.917855</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.503300</td>\n",
       "      <td>5.692895</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.536600</td>\n",
       "      <td>5.819894</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.488600</td>\n",
       "      <td>5.571473</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.487600</td>\n",
       "      <td>5.666573</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.425300</td>\n",
       "      <td>5.464211</td>\n",
       "      <td>0.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.405100</td>\n",
       "      <td>5.350902</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.384700</td>\n",
       "      <td>5.354737</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.332000</td>\n",
       "      <td>5.303081</td>\n",
       "      <td>0.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.272700</td>\n",
       "      <td>5.442781</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.267500</td>\n",
       "      <td>5.321414</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.265000</td>\n",
       "      <td>5.657205</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.225800</td>\n",
       "      <td>5.350082</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.179000</td>\n",
       "      <td>5.680816</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.175700</td>\n",
       "      <td>5.438413</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.163700</td>\n",
       "      <td>5.491056</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.139300</td>\n",
       "      <td>5.388869</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.158200</td>\n",
       "      <td>5.472659</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.152200</td>\n",
       "      <td>5.402950</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.146900</td>\n",
       "      <td>5.787894</td>\n",
       "      <td>0.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.150900</td>\n",
       "      <td>5.536516</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>5.402076</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.125900</td>\n",
       "      <td>5.848182</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.121400</td>\n",
       "      <td>5.560151</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.114800</td>\n",
       "      <td>5.777606</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.117300</td>\n",
       "      <td>5.713777</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.090300</td>\n",
       "      <td>5.920877</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.102800</td>\n",
       "      <td>5.621034</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.070700</td>\n",
       "      <td>5.739488</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.081900</td>\n",
       "      <td>5.850207</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.117700</td>\n",
       "      <td>5.883021</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.078200</td>\n",
       "      <td>5.724964</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.076100</td>\n",
       "      <td>5.895420</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.144900</td>\n",
       "      <td>5.349353</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.060600</td>\n",
       "      <td>5.865839</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.095900</td>\n",
       "      <td>5.732002</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.105500</td>\n",
       "      <td>5.609621</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.099100</td>\n",
       "      <td>5.756639</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.069000</td>\n",
       "      <td>5.741012</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.085100</td>\n",
       "      <td>5.785892</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.090300</td>\n",
       "      <td>5.776430</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.117800</td>\n",
       "      <td>6.175798</td>\n",
       "      <td>0.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.048400</td>\n",
       "      <td>5.942752</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.068600</td>\n",
       "      <td>5.880477</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.078300</td>\n",
       "      <td>6.064230</td>\n",
       "      <td>0.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.064800</td>\n",
       "      <td>6.071047</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.063600</td>\n",
       "      <td>6.225387</td>\n",
       "      <td>0.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.026600</td>\n",
       "      <td>5.986238</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.067100</td>\n",
       "      <td>6.001571</td>\n",
       "      <td>0.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.024400</td>\n",
       "      <td>6.279285</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.098400</td>\n",
       "      <td>5.902085</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.058100</td>\n",
       "      <td>5.851429</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.071600</td>\n",
       "      <td>6.174625</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.107700</td>\n",
       "      <td>6.232257</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.077100</td>\n",
       "      <td>6.321647</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.041600</td>\n",
       "      <td>6.759705</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.097700</td>\n",
       "      <td>6.564131</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.065900</td>\n",
       "      <td>6.636618</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.044000</td>\n",
       "      <td>7.093683</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.047700</td>\n",
       "      <td>6.657194</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.028600</td>\n",
       "      <td>6.407941</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.029700</td>\n",
       "      <td>6.181804</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.058800</td>\n",
       "      <td>6.426991</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.029900</td>\n",
       "      <td>6.715791</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.029900</td>\n",
       "      <td>6.677304</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>6.688199</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.059000</td>\n",
       "      <td>6.854685</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.014000</td>\n",
       "      <td>6.443277</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.034200</td>\n",
       "      <td>6.630472</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.061300</td>\n",
       "      <td>7.088502</td>\n",
       "      <td>0.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.053200</td>\n",
       "      <td>7.440797</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.037300</td>\n",
       "      <td>6.781946</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.046300</td>\n",
       "      <td>6.617473</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.048500</td>\n",
       "      <td>6.678993</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.074800</td>\n",
       "      <td>6.748588</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.027700</td>\n",
       "      <td>6.536355</td>\n",
       "      <td>0.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.052600</td>\n",
       "      <td>6.404638</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.041000</td>\n",
       "      <td>6.461285</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.015900</td>\n",
       "      <td>6.897132</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.011600</td>\n",
       "      <td>6.762846</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.049600</td>\n",
       "      <td>6.774919</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.029000</td>\n",
       "      <td>6.989051</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.033300</td>\n",
       "      <td>6.740828</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.033300</td>\n",
       "      <td>6.629301</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.029700</td>\n",
       "      <td>6.790821</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.067300</td>\n",
       "      <td>6.935053</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.042500</td>\n",
       "      <td>7.069460</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.039800</td>\n",
       "      <td>7.403725</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.016900</td>\n",
       "      <td>7.452702</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.005400</td>\n",
       "      <td>7.636630</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.018200</td>\n",
       "      <td>7.521691</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.020500</td>\n",
       "      <td>7.225771</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.057200</td>\n",
       "      <td>7.343232</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.047600</td>\n",
       "      <td>7.351997</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.002300</td>\n",
       "      <td>7.272107</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.037500</td>\n",
       "      <td>6.850135</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.044300</td>\n",
       "      <td>7.093752</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.039000</td>\n",
       "      <td>7.215963</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.995500</td>\n",
       "      <td>7.680001</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.034500</td>\n",
       "      <td>7.703540</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.978600</td>\n",
       "      <td>7.477534</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.041300</td>\n",
       "      <td>7.406715</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>1.059700</td>\n",
       "      <td>7.162232</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.036600</td>\n",
       "      <td>7.180219</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.059600</td>\n",
       "      <td>7.292656</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>1.041100</td>\n",
       "      <td>6.975492</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.034000</td>\n",
       "      <td>7.009853</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.016400</td>\n",
       "      <td>6.840820</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.018100</td>\n",
       "      <td>6.970851</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.018700</td>\n",
       "      <td>6.851768</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>6.893246</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.035900</td>\n",
       "      <td>6.883679</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>1.016000</td>\n",
       "      <td>6.840860</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.026900</td>\n",
       "      <td>6.931923</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>1.018200</td>\n",
       "      <td>7.151585</td>\n",
       "      <td>0.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>1.005800</td>\n",
       "      <td>7.215382</td>\n",
       "      <td>0.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>1.021400</td>\n",
       "      <td>7.279340</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>1.035900</td>\n",
       "      <td>7.221902</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.034400</td>\n",
       "      <td>7.400295</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.007400</td>\n",
       "      <td>7.262840</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.008900</td>\n",
       "      <td>7.318617</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1.037700</td>\n",
       "      <td>7.141273</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>1.002500</td>\n",
       "      <td>7.166141</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.035400</td>\n",
       "      <td>7.064623</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>1.020900</td>\n",
       "      <td>7.137972</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>1.021800</td>\n",
       "      <td>7.011970</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>1.024200</td>\n",
       "      <td>6.888032</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.978400</td>\n",
       "      <td>7.050982</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.992700</td>\n",
       "      <td>7.020638</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>1.014600</td>\n",
       "      <td>7.048275</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>1.021600</td>\n",
       "      <td>7.022153</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>1.033800</td>\n",
       "      <td>7.089497</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.986100</td>\n",
       "      <td>7.162094</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.026500</td>\n",
       "      <td>7.224936</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>7.258439</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.045900</td>\n",
       "      <td>7.234823</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1.026500</td>\n",
       "      <td>7.273380</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>1.019300</td>\n",
       "      <td>7.207878</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.012500</td>\n",
       "      <td>7.192502</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.994600</td>\n",
       "      <td>7.165200</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>1.013600</td>\n",
       "      <td>7.178828</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>1.007500</td>\n",
       "      <td>7.192487</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>1.001500</td>\n",
       "      <td>7.189723</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.019300</td>\n",
       "      <td>7.194109</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-38\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-38/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-38/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-38/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-38/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-76\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-76/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-76/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-76/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-76/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-114\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-114/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-114/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-114/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-114/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-38] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-152\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-152/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-152/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-152/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-152/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-76] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-190\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-190/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-190/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-190/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-190/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-114] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-228\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-228/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-228/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-228/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-228/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-152] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-266\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-266/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-266/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-266/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-266/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-190] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-304\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-304/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-266] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-342\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-342/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-342/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-342/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-342/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-304] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-380\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-380/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-342] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-418\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-418/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-418/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-418/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-418/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-380] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-456\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-456/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-456/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-456/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-456/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-418] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-494\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-494/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-494/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-494/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-494/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-456] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-532\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-532/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-532/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-532/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-532/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-494] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-570\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-570/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-570/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-570/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-570/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-532] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-608\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-608/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-608/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-608/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-608/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-570] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-646\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-646/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-646/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-646/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-646/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-608] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-684\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-684/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-684/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-684/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-684/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-228] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-722\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-722/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-722/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-722/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-722/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-646] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-760/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-760/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-760/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-760/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-684] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-798\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-798/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-798/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-798/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-798/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-722] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-836\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-836/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-836/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-836/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-836/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-798] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-874\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-874/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-874/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-874/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-874/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-836] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-912\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-912/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-912/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-912/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-912/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-874] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-950\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-950/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-950/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-912] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-988\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-988/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-988/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-988/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-988/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-950] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1026\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1026/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1026/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1026/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1026/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-988] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1064\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1064/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1064/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1064/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1064/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1026] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1102\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1102/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1102/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1102/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1102/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1064] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1140\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1140/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1140/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1140/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1140/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1102] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1178\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1178/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1178/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1178/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1178/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1140] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1216\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1216/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1216/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1216/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1216/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1178] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1254\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1254/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1254/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1254/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1254/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1216] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1292\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1292/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1292/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1292/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1292/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1254] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1330\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1330/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1330/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1330/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1330/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1292] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1368\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1368/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1368/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1368/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1368/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1330] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1406\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1406/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1406/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1406/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1406/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1368] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1444\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1444/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1444/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1444/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1444/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1406] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1482\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1482/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1482/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1482/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1482/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1444] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1520\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1520/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1520/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1520/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1520/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1482] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1558\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1558/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1558/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1558/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1558/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1520] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1596\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1596/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1596/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1596/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1596/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1558] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1634\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1634/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1634/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1634/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1634/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1596] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1672\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1672/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1672/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1672/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1672/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1634] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1710\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1710/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1710/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1710/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1710/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1672] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1748\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1748/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1748/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1748/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1748/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1710] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1786\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1786/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1786/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1786/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1786/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1748] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1824\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1824/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1824/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1824/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1824/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-760] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1862\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1862/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1862/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1862/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1862/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1786] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1900\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1900/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1900/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1824] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1938\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1938/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1938/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1938/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1938/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1862] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-1976\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-1976/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-1976/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-1976/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-1976/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1900] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2014\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2014/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2014/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2014/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2014/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1938] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2052\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2052/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2052/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2052/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2052/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-1976] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2090\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2090/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2090/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2090/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2090/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2052] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2128\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2128/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2128/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2128/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2128/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2090] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2166\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2166/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2166/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2166/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2166/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2128] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2204\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2204/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2204/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2204/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2204/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2166] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2242\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2242/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2242/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2242/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2242/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2204] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2280\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2280/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2280/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2280/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2280/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2242] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2318\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2318/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2318/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2318/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2318/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2280] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2356\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2356/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2356/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2356/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2356/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2318] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2394\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2394/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2394/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2394/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2394/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2356] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2432\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2432/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2432/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2432/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2432/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2394] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2470\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2470/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2470/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2470/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2470/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2014] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2508\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2508/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2508/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2508/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2508/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2432] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2546\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2546/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2546/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2546/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2546/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2508] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2584\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2584/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2584/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2584/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2584/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2546] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2622\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2622/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2622/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2622/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2622/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2584] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2660\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2660/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2660/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2660/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2660/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2622] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2698\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2698/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2698/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2698/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2698/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2660] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2736\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2736/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2736/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2736/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2736/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2698] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2774\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2774/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2774/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2774/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2774/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2736] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2812\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2812/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2812/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2812/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2812/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2774] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2850\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2850/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2850/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2812] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2888\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2888/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2888/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2888/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2888/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2850] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2926\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2926/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2926/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2926/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2926/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2888] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-2964\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-2964/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-2964/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-2964/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-2964/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2926] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3002\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3002/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3002/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3002/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3002/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-2964] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3040\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3040/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3040/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3040/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3040/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3002] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3078\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3078/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3078/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3078/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3078/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3040] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3116\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3116/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3116/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3116/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3116/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3078] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3154\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3154/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3154/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3154/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3154/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3116] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3192\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3192/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3192/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3192/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3192/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3154] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3230\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3230/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3230/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3230/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3230/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3192] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3268\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3268/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3268/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3268/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3268/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3230] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3306\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3306/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3306/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3306/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3306/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3268] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3344\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3344/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3344/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3344/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3344/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3306] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3382\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3382/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3382/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3382/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3382/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3344] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3420\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3420/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3420/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3420/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3420/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3382] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3458\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3458/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3458/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3458/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3458/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3420] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3496\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3496/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3496/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3496/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3496/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3458] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3534\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3534/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3534/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3534/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3534/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3496] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3572\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3572/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3572/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3572/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3572/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3534] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3610\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3610/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3610/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3610/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3610/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3572] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3648\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3648/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3648/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3648/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3648/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3610] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3686\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3686/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3686/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3686/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3686/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3648] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3724\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3724/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3724/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3724/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3724/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3686] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3762\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3762/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3762/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3762/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3762/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3724] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3800\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3800/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3800/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3762] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3838\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3838/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3838/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3838/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3838/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3800] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3876\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3876/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3876/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3876/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3876/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3838] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3914\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3914/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3914/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3914/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3914/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3876] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3952\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3952/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3952/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3952/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3952/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3914] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-3990\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-3990/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-3990/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-3990/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-3990/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3952] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4028\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4028/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4028/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4028/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4028/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-3990] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4066\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4066/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4066/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4066/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4066/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4028] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4104\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4104/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4104/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4104/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4104/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4066] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4142\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4142/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4142/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4142/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4142/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4104] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4180\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4180/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4180/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4180/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4180/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4142] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4218\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4218/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4218/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4218/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4218/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4180] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4256\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4256/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4256/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4256/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4256/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4218] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4294\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4294/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4294/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4294/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4294/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4256] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4332\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4332/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4332/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4332/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4332/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4294] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4370\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4370/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4370/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4370/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4370/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4332] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4408\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4408/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4408/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4408/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4408/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4370] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4446\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4446/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4446/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4446/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4446/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4408] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4484\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4484/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4484/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4484/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4484/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4446] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4522\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4522/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4522/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4522/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4522/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4484] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4560\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4560/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4560/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4560/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4560/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4522] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4598\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4598/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4598/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4598/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4598/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4560] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4636\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4636/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4636/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4636/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4636/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4598] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4674\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4674/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4674/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4674/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4674/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4636] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4712\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4712/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4712/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4712/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4712/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4674] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4750\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4750/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4750/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4712] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4788\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4788/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4788/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4788/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4788/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4750] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4826\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4826/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4826/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4826/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4826/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4788] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4864\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4864/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4864/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4864/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4864/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4826] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4902\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4902/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4902/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4902/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4902/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4864] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4940\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4940/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4940/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4940/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4940/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4902] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-4978\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-4978/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-4978/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-4978/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-4978/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4940] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5016\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5016/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5016/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5016/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5016/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-4978] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5054\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5054/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5054/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5054/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5054/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5016] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5092\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5092/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5092/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5092/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5092/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5054] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5130\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5130/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5130/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5130/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5130/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5092] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5168\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5168/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5168/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5168/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5168/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5130] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5206\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5206/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5206/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5206/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5206/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5168] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5244\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5244/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5244/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5244/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5244/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5206] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5282\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5282/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5282/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5282/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5282/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5244] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5320\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5320/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5320/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5320/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5320/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5282] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5358\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5358/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5358/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5358/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5358/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5320] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5396\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5396/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5396/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5396/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5396/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5358] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5434\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5434/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5434/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5434/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5434/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5396] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5472\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5472/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5472/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5472/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5472/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5434] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5510\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5510/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5510/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5510/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5510/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5472] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5548\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5548/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5548/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5548/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5548/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5510] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5586\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5586/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5586/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5586/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5586/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5548] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5624\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5624/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5624/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5624/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5624/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5586] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5662\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5662/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5662/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5662/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5662/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5624] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5700\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5700/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5662] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5738\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5738/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5738/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5738/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5738/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5700] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5776\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5776/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5776/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5776/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5776/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5738] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5814\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5814/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5814/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5814/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5814/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5776] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5852\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5852/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5852/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5852/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5852/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5814] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5890\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5890/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5890/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5890/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5890/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5852] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5928\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5928/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5928/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5928/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5928/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5890] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-5966\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-5966/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-5966/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-5966/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-5966/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5928] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6004\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6004/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6004/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6004/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6004/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-5966] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6042\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6042/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6042/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6042/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6042/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6004] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6080\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6080/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6080/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6080/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6080/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6042] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6118\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6118/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6118/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6118/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6118/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6080] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6156\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6156/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6156/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6156/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6156/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6118] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6194\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6194/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6194/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6194/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6194/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6156] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6232\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6232/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6232/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6232/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6232/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6194] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6270\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6270/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6270/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6270/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6270/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6232] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6308\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6308/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6308/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6308/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6308/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6270] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6346\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6346/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6346/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6346/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6346/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6308] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6384\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6384/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6384/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6384/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6384/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6346] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6422\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6422/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6422/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6422/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6422/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6384] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6460\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6460/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6460/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6460/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6460/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6422] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6498\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6498/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6498/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6498/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6498/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6460] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6536\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6536/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6536/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6536/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6536/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6498] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6574\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6574/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6574/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6574/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6574/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6536] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6612\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6612/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6612/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6612/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6612/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6574] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6650\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6650/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6650/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6612] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6688\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6688/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6688/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6688/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6688/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6650] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6726\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6726/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6726/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6726/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6726/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6688] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6764\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6764/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6764/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6764/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6764/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6726] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6802\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6802/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6802/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6802/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6802/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6764] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6840\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6840/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6840/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6840/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6840/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6802] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6878\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6878/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6878/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6878/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6878/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6840] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6916\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6916/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6916/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6916/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6916/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6878] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6954\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6954/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6954/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6954/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6954/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6916] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-6992\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-6992/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-6992/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-6992/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-6992/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6954] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7030\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7030/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7030/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7030/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7030/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-6992] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7068\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7068/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7068/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7068/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7068/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7030] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7106\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7106/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7106/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7106/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7106/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7068] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7144\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7144/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7144/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7144/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7144/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7106] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7182\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7182/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7182/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7182/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7182/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7144] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7220\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7220/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7220/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7220/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7220/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7182] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7258\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7258/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7258/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7258/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7258/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7220] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7296\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7296/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7296/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7296/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7296/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7258] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7334\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7334/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7334/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7334/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7334/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7296] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7372\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7372/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7372/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7372/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7372/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7334] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7410\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7410/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7410/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7410/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7410/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7372] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7448\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7448/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7448/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7448/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7448/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7410] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7486\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7486/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7486/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7486/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7486/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7448] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7524\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7524/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7524/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7524/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7524/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7486] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7562\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7562/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7562/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7562/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7562/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7524] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Implication_en_de_20/models/checkpoint-7600\n",
      "Configuration saved in ./output/Implication_en_de_20/models/checkpoint-7600/config.json\n",
      "Model weights saved in ./output/Implication_en_de_20/models/checkpoint-7600/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Implication_en_de_20/models/checkpoint-7600/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/Implication_en_de_20/models/checkpoint-7600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Implication_en_de_20/models/checkpoint-7562] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/Implication_en_de_20/models/checkpoint-2470 (score: 0.061).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7600, training_loss=1.3646312668449, metrics={'train_runtime': 6730.2371, 'train_samples_per_second': 564.616, 'train_steps_per_second': 1.129, 'total_flos': 1.75993251844284e+16, 'train_loss': 1.3646312668449, 'epoch': 200.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d1dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.061,\n",
       " 'eval_loss': 5.3030805587768555,\n",
       " 'eval_runtime': 1.6259,\n",
       " 'eval_samples_per_second': 615.046,\n",
       " 'eval_steps_per_second': 2.46,\n",
       " 'epoch': 200.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Test\n",
    "trainer.evaluate(eval_dataset=tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff21830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c6e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation Symmetry per Relation\n",
    "evaluation_implication(trainer, tokenizer, relations, copy.deepcopy(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2702e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_implication(trainer, tokenizer, relation_pairs, test):\n",
    "    for target in test:\n",
    "\n",
    "        # Iterate over all relations per target language\n",
    "        for (idx1, relation), (idx2, implication) in zip(relation_pairs[0].iterrows(), relation_pairs[1].iterrows()):\n",
    "            # IMPLICATION\n",
    "            if not test[target][implication[target]]:\n",
    "                continue\n",
    "\n",
    "            # Relation from test set dict\n",
    "            relation_test = test[target][implication[target]]\n",
    "\n",
    "            # Tokenize\n",
    "            relation_test_ds = Dataset.from_dict({'sample': relation_test})\n",
    "            tokenized_relation_ds = tokenize(tokenizer, relation_test_ds)\n",
    "\n",
    "            # Evaluate\n",
    "            metrics = trainer.evaluate(eval_dataset=tokenized_relation_ds, custom_eval=True)\n",
    "            output_metrics = remove_key_dict(metrics, 'eval_correct_predictions')\n",
    "            print(output_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214e34f",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "- How is (if at all) implication learned?\n",
    "- Pretrained?\n",
    "- Target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dict['sample'][:1901])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38896e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d2fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entities[900:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb14c2d",
   "metadata": {},
   "source": [
    "#### Rule: (e, r, f) -> (e, s, a), (e, s, b), (e, s, c)  =>  (e, r_de, f) -> (e, s_de, a), (e, s, b), (e, s, c) \n",
    "\n",
    "Test:\n",
    "- Are the implications learned in source language? (e, s, a) (e, s, b) (e, s, c)\n",
    "- Is there a general transfer to the target? (e, r_de, f)\n",
    "\n",
    "1800 facts are training the rule (900<->900)\n",
    "1800-1900 are facts that are used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2ff06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Iterate over relations, take the training samples that were trained on\n",
    "for i in range(n_relations):\n",
    "    trained_test = train_dict['sample'][1800+i*1900:(i+1)*1900]\n",
    "    entities_sampled = entities[900+i*1000:(i+1)*1000]\n",
    "\n",
    "    acc_imp_source = 0\n",
    "    acc_rde = 0\n",
    "    \n",
    "    # Relation pairs!\n",
    "    r = relations[0]['en'].iloc[i]\n",
    "    r_de = relations[0]['de'].iloc[i]\n",
    "    s = relations[1]['en'].iloc[i]\n",
    "    s_de = relations[1]['de'].iloc[i]\n",
    "\n",
    "    for j, sample in enumerate(trained_test):\n",
    "        \n",
    "        ents = entities_sampled[j]\n",
    "        e = ents[0]\n",
    "        f = ents[1]\n",
    "        \n",
    "        # Test (e, s, a) (e, s, b) (e, s, c)\n",
    "        for ent in ents[2:]:\n",
    "            label_token = tokenizer.convert_tokens_to_ids(ent)\n",
    "\n",
    "            prompt = e + ' ' + s + ' [MASK]'\n",
    "\n",
    "            encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "            token_logits = model(**encoded_input).logits\n",
    "\n",
    "            mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "            mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "            top_token = torch.topk(mask_token_logits, len(ents[2:]), dim=1).indices[0].tolist()\n",
    "\n",
    "            if label_token in top_token:\n",
    "                acc_imp_source += 1\n",
    "\n",
    "                \n",
    "        # Test (e, r_de, f)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(f)\n",
    "\n",
    "        prompt = e + ' ' + r_de + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_rde += 1        \n",
    "\n",
    "    acc_imp_source /= (len(ents[2:])*100)\n",
    "    acc_rde /= 100\n",
    "\n",
    "    print(f'Relation: {r}')\n",
    "    print(f'Accuracy for Implication Source (e, s, a) (e, s, b) (e, s, c): {acc_imp_source}')\n",
    "    print(f'Accuracy for KT (e, r_de, f): {acc_rde}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cf623",
   "metadata": {},
   "source": [
    "### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4920784d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9906\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "total = len(train_dict['sample'])\n",
    "i = 0\n",
    "\n",
    "for txt in train_dict['sample'][:10000]:\n",
    "    i += 1\n",
    "    \n",
    "    # Add [MASK] for object\n",
    "    sample = txt.rsplit(' ', 1)[0] + ' [MASK]'\n",
    "    label_token = tokenizer.convert_tokens_to_ids(txt.rsplit(' ', 1)[1])\n",
    "    \n",
    "    encoded_input = tokenizer(sample, return_tensors='pt')\n",
    "    token_logits = model(**encoded_input).logits\n",
    "    \n",
    "    mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    \n",
    "    # Pick the [MASK] candidates with the highest logits\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()\n",
    "    \n",
    "    if label_token in top_5_tokens:\n",
    "        k += 1\n",
    "        \n",
    "print(k/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"lens manner of [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n>>> {tokenizer.decode([chunk])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ff4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_dict['sample']:\n",
    "    if 'Alex' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e8aa5",
   "metadata": {},
   "source": [
    "### Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548995a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
