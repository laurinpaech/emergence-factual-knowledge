{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e4031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer, TrainingArguments, Trainer, \\\n",
    "    DataCollatorForLanguageModeling, IntervalStrategy\n",
    "\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "from data_generation_relation import *\n",
    "from utils import *\n",
    "from custom_trainer import CustomTrainer\n",
    "from datasets import load_metric\n",
    "import logging\n",
    "from transformers import logging as tlogging\n",
    "import wandb\n",
    "import sys\n",
    "from utils import set_seed\n",
    "from transformers.integrations import WandbCallback, TensorBoardCallback\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735ddbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "run_name = 'CompositionDefault'\n",
    "epochs = 200\n",
    "batch_size = 256\n",
    "lr = 5e-5\n",
    "\n",
    "relation = 'composition'\n",
    "source_language = ['en']\n",
    "target_language = ['de']\n",
    "n_relations = 10\n",
    "n_facts = 1000\n",
    "n_pairs = 100\n",
    "\n",
    "use_random = False\n",
    "\n",
    "use_pretrained = False\n",
    "use_target = False\n",
    "use_enhanced = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cd5ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        id                              en                                de  \\\n",
       " 694   P105                      taxon rank                taxonomischer Rang   \n",
       " 606  P2429           expected completeness         erwartete Vollständigkeit   \n",
       " 281   P400                        platform                         Plattform   \n",
       " 587  P9597                    type of lens                         Linsentyp   \n",
       " 231  P2675                        reply to                       Antwort auf   \n",
       " 754   P129       physically interacts with      interagiert physikalisch mit   \n",
       " 711   P607                        conflict                     Kriegseinsatz   \n",
       " 196  P5353                 school district                       Schulbezirk   \n",
       " 300  P3461      designated as terrorist by  als terroristisch eingestuft von   \n",
       " 326    P65  site of astronomical discovery     astronomischer Entdeckungsort   \n",
       " \n",
       "                                       es                               fr  \\\n",
       " 694                 categoría taxonómica                 rang taxinomique   \n",
       " 606                 grado de completitud              degré de complétude   \n",
       " 281                           plataforma                       plateforme   \n",
       " 587                        tipo de lente         type de lentille optique   \n",
       " 231                          respuesta a                        réponse à   \n",
       " 754           interactúa físicamente con      interagit physiquement avec   \n",
       " 711            participó en el conflicto                          conflit   \n",
       " 196                     distrito escolar                district scolaire   \n",
       " 300           considerado terrorista por     désigné comme terroriste par   \n",
       " 326  lugar de descubrimiento astronómico  lieu de découverte astronomique   \n",
       " \n",
       "        count  \n",
       " 694  3580266  \n",
       " 606     3826  \n",
       " 281    95318  \n",
       " 587     1721  \n",
       " 231      381  \n",
       " 754     9480  \n",
       " 711   220972  \n",
       " 196     9776  \n",
       " 300      236  \n",
       " 326    53748  ,\n",
       "         id                          en                             de  \\\n",
       " 528  P6855          emergency services           Notfalleinrichtungen   \n",
       " 120   P111  measured physical quantity  gemessene physikalische Größe   \n",
       " 515  P7727       legislative committee              Legislativkomitee   \n",
       " 204  P1606        natural reservoir of           Erregerreservoir von   \n",
       " 66   P1455               list of works                      Werkliste   \n",
       " 235  P1363       points/goal scored by    Punkt/Treffer erzielt durch   \n",
       " 522  P2596                     culture                         Kultur   \n",
       " 383  P1876                     vehicle                       Fahrzeug   \n",
       " 30   P7163             typically sells        verkauft im Allgemeinen   \n",
       " 380  P1302        primary destinations                      Hauptorte   \n",
       " \n",
       "                             es                                  fr   count  \n",
       " 528    servicios de emergencia  accueil et traitement des urgences     766  \n",
       " 120     cantidad física medida           grandeur physique mesurée    3610  \n",
       " 515         comité legislativo                   comité législatif  123710  \n",
       " 204      reservorio natural de                réservoir naturel de      17  \n",
       " 66              lista de obras                    liste des œuvres    1227  \n",
       " 235  puntos/goles marcados por                point/but marqué par    2441  \n",
       " 522                    cultura                             culture   10007  \n",
       " 383                       nave                            vaisseau     840  \n",
       " 30          vende generalmente                   vend généralement     299  \n",
       " 380       destinos principales    principales localités desservies    3923  ,\n",
       "         id                           en                            de  \\\n",
       " 598   P462                        color                         Farbe   \n",
       " 63   P3027             open period from        geöffnet von Zeitpunkt   \n",
       " 137  P8345              media franchise              Medien-Franchise   \n",
       " 218  P8852                  facial hair                  Gesichtshaar   \n",
       " 213  P1909                  side effect                  Nebenwirkung   \n",
       " 118  P6271                   demonym of                    Demonym zu   \n",
       " 411  P2550  recording or performance of  Aufnahme oder Ausführung von   \n",
       " 456    P21                sex or gender                    Geschlecht   \n",
       " 432   P193     main building contractor         Generalbauunternehmer   \n",
       " 740  P1889               different from               verschieden von   \n",
       " \n",
       "                            es                                   fr    count  \n",
       " 598                     color                              couleur   194389  \n",
       " 63              abierto desde      début de la période d'ouverture       16  \n",
       " 137      franquicia de medios                 franchise médiatique    27415  \n",
       " 218              vello facial                     pilosité faciale      362  \n",
       " 213         efecto secundario                     effet secondaire       40  \n",
       " 118             gentilicio de                           gentilé de     2629  \n",
       " 411  grabación o ejecución de  enregistrement ou interprétation de    14735  \n",
       " 456             sexo o género                        sexe ou genre  7855753  \n",
       " 432               constructor                       maître d'œuvre     2893  \n",
       " 740              diferente de              à ne pas confondre avec   797811  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test, relations, entities = generate_reasoning(relation=Relation(relation),\n",
    "                                                        source_language=source_language,\n",
    "                                                        target_language=target_language,\n",
    "                                                        n_relations=n_relations,\n",
    "                                                        n_facts=n_facts,\n",
    "                                                        use_pretrained=use_pretrained,\n",
    "                                                        use_target=use_target,\n",
    "                                                        use_enhanced=use_enhanced,\n",
    "                                                        use_same_relations=False,\n",
    "                                                        n_pairs=n_pairs)\n",
    "\n",
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f750f094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>P6855</td>\n",
       "      <td>emergency services</td>\n",
       "      <td>Notfalleinrichtungen</td>\n",
       "      <td>servicios de emergencia</td>\n",
       "      <td>accueil et traitement des urgences</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>P111</td>\n",
       "      <td>measured physical quantity</td>\n",
       "      <td>gemessene physikalische Größe</td>\n",
       "      <td>cantidad física medida</td>\n",
       "      <td>grandeur physique mesurée</td>\n",
       "      <td>3610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>P7727</td>\n",
       "      <td>legislative committee</td>\n",
       "      <td>Legislativkomitee</td>\n",
       "      <td>comité legislativo</td>\n",
       "      <td>comité législatif</td>\n",
       "      <td>123710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>P1606</td>\n",
       "      <td>natural reservoir of</td>\n",
       "      <td>Erregerreservoir von</td>\n",
       "      <td>reservorio natural de</td>\n",
       "      <td>réservoir naturel de</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>P1455</td>\n",
       "      <td>list of works</td>\n",
       "      <td>Werkliste</td>\n",
       "      <td>lista de obras</td>\n",
       "      <td>liste des œuvres</td>\n",
       "      <td>1227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>P1363</td>\n",
       "      <td>points/goal scored by</td>\n",
       "      <td>Punkt/Treffer erzielt durch</td>\n",
       "      <td>puntos/goles marcados por</td>\n",
       "      <td>point/but marqué par</td>\n",
       "      <td>2441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>P2596</td>\n",
       "      <td>culture</td>\n",
       "      <td>Kultur</td>\n",
       "      <td>cultura</td>\n",
       "      <td>culture</td>\n",
       "      <td>10007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>P1876</td>\n",
       "      <td>vehicle</td>\n",
       "      <td>Fahrzeug</td>\n",
       "      <td>nave</td>\n",
       "      <td>vaisseau</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>P7163</td>\n",
       "      <td>typically sells</td>\n",
       "      <td>verkauft im Allgemeinen</td>\n",
       "      <td>vende generalmente</td>\n",
       "      <td>vend généralement</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>P1302</td>\n",
       "      <td>primary destinations</td>\n",
       "      <td>Hauptorte</td>\n",
       "      <td>destinos principales</td>\n",
       "      <td>principales localités desservies</td>\n",
       "      <td>3923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                          en                             de  \\\n",
       "528  P6855          emergency services           Notfalleinrichtungen   \n",
       "120   P111  measured physical quantity  gemessene physikalische Größe   \n",
       "515  P7727       legislative committee              Legislativkomitee   \n",
       "204  P1606        natural reservoir of           Erregerreservoir von   \n",
       "66   P1455               list of works                      Werkliste   \n",
       "235  P1363       points/goal scored by    Punkt/Treffer erzielt durch   \n",
       "522  P2596                     culture                         Kultur   \n",
       "383  P1876                     vehicle                       Fahrzeug   \n",
       "30   P7163             typically sells        verkauft im Allgemeinen   \n",
       "380  P1302        primary destinations                      Hauptorte   \n",
       "\n",
       "                            es                                  fr   count  \n",
       "528    servicios de emergencia  accueil et traitement des urgences     766  \n",
       "120     cantidad física medida           grandeur physique mesurée    3610  \n",
       "515         comité legislativo                   comité législatif  123710  \n",
       "204      reservorio natural de                réservoir naturel de      17  \n",
       "66              lista de obras                    liste des œuvres    1227  \n",
       "235  puntos/goles marcados por                point/but marqué par    2441  \n",
       "522                    cultura                             culture   10007  \n",
       "383                       nave                            vaisseau     840  \n",
       "30          vende generalmente                   vend généralement     299  \n",
       "380       destinos principales    principales localités desservies    3923  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbf9397f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>P462</td>\n",
       "      <td>color</td>\n",
       "      <td>Farbe</td>\n",
       "      <td>color</td>\n",
       "      <td>couleur</td>\n",
       "      <td>194389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>P3027</td>\n",
       "      <td>open period from</td>\n",
       "      <td>geöffnet von Zeitpunkt</td>\n",
       "      <td>abierto desde</td>\n",
       "      <td>début de la période d'ouverture</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>P8345</td>\n",
       "      <td>media franchise</td>\n",
       "      <td>Medien-Franchise</td>\n",
       "      <td>franquicia de medios</td>\n",
       "      <td>franchise médiatique</td>\n",
       "      <td>27415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>P8852</td>\n",
       "      <td>facial hair</td>\n",
       "      <td>Gesichtshaar</td>\n",
       "      <td>vello facial</td>\n",
       "      <td>pilosité faciale</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>P1909</td>\n",
       "      <td>side effect</td>\n",
       "      <td>Nebenwirkung</td>\n",
       "      <td>efecto secundario</td>\n",
       "      <td>effet secondaire</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>P6271</td>\n",
       "      <td>demonym of</td>\n",
       "      <td>Demonym zu</td>\n",
       "      <td>gentilicio de</td>\n",
       "      <td>gentilé de</td>\n",
       "      <td>2629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>P2550</td>\n",
       "      <td>recording or performance of</td>\n",
       "      <td>Aufnahme oder Ausführung von</td>\n",
       "      <td>grabación o ejecución de</td>\n",
       "      <td>enregistrement ou interprétation de</td>\n",
       "      <td>14735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>P21</td>\n",
       "      <td>sex or gender</td>\n",
       "      <td>Geschlecht</td>\n",
       "      <td>sexo o género</td>\n",
       "      <td>sexe ou genre</td>\n",
       "      <td>7855753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>P193</td>\n",
       "      <td>main building contractor</td>\n",
       "      <td>Generalbauunternehmer</td>\n",
       "      <td>constructor</td>\n",
       "      <td>maître d'œuvre</td>\n",
       "      <td>2893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>P1889</td>\n",
       "      <td>different from</td>\n",
       "      <td>verschieden von</td>\n",
       "      <td>diferente de</td>\n",
       "      <td>à ne pas confondre avec</td>\n",
       "      <td>797811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                           en                            de  \\\n",
       "598   P462                        color                         Farbe   \n",
       "63   P3027             open period from        geöffnet von Zeitpunkt   \n",
       "137  P8345              media franchise              Medien-Franchise   \n",
       "218  P8852                  facial hair                  Gesichtshaar   \n",
       "213  P1909                  side effect                  Nebenwirkung   \n",
       "118  P6271                   demonym of                    Demonym zu   \n",
       "411  P2550  recording or performance of  Aufnahme oder Ausführung von   \n",
       "456    P21                sex or gender                    Geschlecht   \n",
       "432   P193     main building contractor         Generalbauunternehmer   \n",
       "740  P1889               different from               verschieden von   \n",
       "\n",
       "                           es                                   fr    count  \n",
       "598                     color                              couleur   194389  \n",
       "63              abierto desde      début de la période d'ouverture       16  \n",
       "137      franquicia de medios                 franchise médiatique    27415  \n",
       "218              vello facial                     pilosité faciale      362  \n",
       "213         efecto secundario                     effet secondaire       40  \n",
       "118             gentilicio de                           gentilé de     2629  \n",
       "411  grabación o ejecución de  enregistrement ou interprétation de    14735  \n",
       "456             sexo o género                        sexe ou genre  7855753  \n",
       "432               constructor                       maître d'œuvre     2893  \n",
       "740              diferente de              à ne pas confondre avec   797811  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81631d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING\n",
    "# Load mBERT model and Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Load Data Collator for Prediction and Evaluation\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "eval_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5618d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b935b064ae42fb86cd891e9cf5f084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65340e1bf0d74ac487e6b4b99f55d06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ~~ PRE-PROCESSING ~~\n",
    "train_dict = {'sample': train}\n",
    "test_dict = {'sample': flatten_dict2_list(copy.deepcopy(test))}\n",
    "train_ds = Dataset.from_dict(train_dict)\n",
    "test_ds = Dataset.from_dict(test_dict)\n",
    "\n",
    "# Tokenize Training and Test Data\n",
    "tokenized_train = tokenize(tokenizer, train_ds)  # Train is shuffled by Huggingface\n",
    "tokenized_test = tokenize(tokenizer, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7fbbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train and Test Data\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "test_complete_df = pd.DataFrame(test)\n",
    "test_flat_df = pd.DataFrame(test_dict)\n",
    "\n",
    "data_dir = './output/' + run_name + '/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "train_df.to_csv(data_dir + 'train_set', index=False)\n",
    "test_complete_df.to_json(data_dir + 'test_set_complete')\n",
    "test_flat_df.to_csv(data_dir + 'test_set', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0d66d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='./output/' + run_name + '/models/',\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=128,\n",
    "        learning_rate=lr,\n",
    "        logging_dir='./output/' + run_name + '/tb_logs/',\n",
    "        logging_strategy=IntervalStrategy.EPOCH,\n",
    "        evaluation_strategy=IntervalStrategy.EPOCH,\n",
    "        save_strategy=IntervalStrategy.EPOCH,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    eval_data_collator=eval_data_collator,\n",
    "    compute_metrics=precision_at_one\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b90787af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 29000\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11400\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11400' max='11400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11400/11400 3:00:03, Epoch 200/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.367200</td>\n",
       "      <td>8.520420</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.333000</td>\n",
       "      <td>7.969775</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.185500</td>\n",
       "      <td>7.787424</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.170800</td>\n",
       "      <td>7.638084</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.027400</td>\n",
       "      <td>7.538991</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.883800</td>\n",
       "      <td>7.293577</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.786900</td>\n",
       "      <td>7.135616</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.694400</td>\n",
       "      <td>6.910761</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.513500</td>\n",
       "      <td>6.946599</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.484100</td>\n",
       "      <td>6.712662</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.354400</td>\n",
       "      <td>6.620102</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.269800</td>\n",
       "      <td>6.622962</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.180500</td>\n",
       "      <td>6.560471</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.127200</td>\n",
       "      <td>6.799276</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.006000</td>\n",
       "      <td>6.743473</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.907600</td>\n",
       "      <td>6.704802</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.822000</td>\n",
       "      <td>6.807217</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.802500</td>\n",
       "      <td>6.721432</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.789900</td>\n",
       "      <td>6.633260</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.743600</td>\n",
       "      <td>6.595062</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.728600</td>\n",
       "      <td>6.655901</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.681100</td>\n",
       "      <td>6.507675</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.689700</td>\n",
       "      <td>6.523175</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.677600</td>\n",
       "      <td>6.573184</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.691900</td>\n",
       "      <td>6.866583</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.668600</td>\n",
       "      <td>7.225891</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.691600</td>\n",
       "      <td>7.176287</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.702600</td>\n",
       "      <td>6.903597</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.684300</td>\n",
       "      <td>7.140883</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.673600</td>\n",
       "      <td>7.126407</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.637000</td>\n",
       "      <td>7.773983</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.621200</td>\n",
       "      <td>7.408462</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.658500</td>\n",
       "      <td>7.612993</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.578400</td>\n",
       "      <td>7.400016</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.621200</td>\n",
       "      <td>7.041313</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.604900</td>\n",
       "      <td>7.336185</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.592000</td>\n",
       "      <td>7.351769</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.605500</td>\n",
       "      <td>7.541814</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.567700</td>\n",
       "      <td>7.422419</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.590800</td>\n",
       "      <td>7.297859</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.545400</td>\n",
       "      <td>7.174154</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.558400</td>\n",
       "      <td>7.549750</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.527900</td>\n",
       "      <td>7.693130</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.521900</td>\n",
       "      <td>7.697776</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.481800</td>\n",
       "      <td>7.845807</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.465500</td>\n",
       "      <td>7.708357</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.452300</td>\n",
       "      <td>7.670907</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.418700</td>\n",
       "      <td>7.565661</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.402400</td>\n",
       "      <td>7.925544</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.376100</td>\n",
       "      <td>7.983027</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.334400</td>\n",
       "      <td>7.768872</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.331600</td>\n",
       "      <td>7.845186</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.292800</td>\n",
       "      <td>7.573962</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.288500</td>\n",
       "      <td>7.778593</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.259400</td>\n",
       "      <td>7.656396</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.205400</td>\n",
       "      <td>7.364123</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.200500</td>\n",
       "      <td>7.898446</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.159400</td>\n",
       "      <td>7.675439</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.131900</td>\n",
       "      <td>7.186105</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.107600</td>\n",
       "      <td>7.120734</td>\n",
       "      <td>0.065000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.067700</td>\n",
       "      <td>7.160460</td>\n",
       "      <td>0.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.056700</td>\n",
       "      <td>7.076907</td>\n",
       "      <td>0.076000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>6.729596</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.016300</td>\n",
       "      <td>6.707332</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.983700</td>\n",
       "      <td>6.610064</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.000800</td>\n",
       "      <td>6.541978</td>\n",
       "      <td>0.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.946500</td>\n",
       "      <td>6.250025</td>\n",
       "      <td>0.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.956200</td>\n",
       "      <td>6.011765</td>\n",
       "      <td>0.145000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.926800</td>\n",
       "      <td>6.069297</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.913800</td>\n",
       "      <td>6.225326</td>\n",
       "      <td>0.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.908600</td>\n",
       "      <td>6.110768</td>\n",
       "      <td>0.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.889200</td>\n",
       "      <td>5.985119</td>\n",
       "      <td>0.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.875800</td>\n",
       "      <td>5.614743</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.844600</td>\n",
       "      <td>5.875984</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>5.951866</td>\n",
       "      <td>0.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.813900</td>\n",
       "      <td>5.667977</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.808900</td>\n",
       "      <td>5.639537</td>\n",
       "      <td>0.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.803300</td>\n",
       "      <td>6.049553</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.767300</td>\n",
       "      <td>5.948754</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.763900</td>\n",
       "      <td>6.153883</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.756900</td>\n",
       "      <td>5.848179</td>\n",
       "      <td>0.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.770700</td>\n",
       "      <td>5.803998</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.731700</td>\n",
       "      <td>5.739009</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>5.696541</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.736400</td>\n",
       "      <td>5.521401</td>\n",
       "      <td>0.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>5.439589</td>\n",
       "      <td>0.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>5.652916</td>\n",
       "      <td>0.206000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.723600</td>\n",
       "      <td>5.543191</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.688400</td>\n",
       "      <td>5.900853</td>\n",
       "      <td>0.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.717100</td>\n",
       "      <td>5.693152</td>\n",
       "      <td>0.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.707900</td>\n",
       "      <td>6.157600</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.698600</td>\n",
       "      <td>5.891107</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.703800</td>\n",
       "      <td>5.608339</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.713400</td>\n",
       "      <td>5.622273</td>\n",
       "      <td>0.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.660200</td>\n",
       "      <td>5.745045</td>\n",
       "      <td>0.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.680300</td>\n",
       "      <td>5.835866</td>\n",
       "      <td>0.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>5.637281</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.668300</td>\n",
       "      <td>5.716133</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.659100</td>\n",
       "      <td>5.667148</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.676300</td>\n",
       "      <td>5.569866</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.665200</td>\n",
       "      <td>5.650997</td>\n",
       "      <td>0.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.638100</td>\n",
       "      <td>5.536514</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.647900</td>\n",
       "      <td>5.304692</td>\n",
       "      <td>0.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>5.566198</td>\n",
       "      <td>0.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.668100</td>\n",
       "      <td>5.365607</td>\n",
       "      <td>0.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.646400</td>\n",
       "      <td>5.139904</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.633000</td>\n",
       "      <td>5.325068</td>\n",
       "      <td>0.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.636800</td>\n",
       "      <td>5.500080</td>\n",
       "      <td>0.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.648800</td>\n",
       "      <td>5.595031</td>\n",
       "      <td>0.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.635300</td>\n",
       "      <td>5.716195</td>\n",
       "      <td>0.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>5.841389</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>5.756978</td>\n",
       "      <td>0.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.646600</td>\n",
       "      <td>5.631215</td>\n",
       "      <td>0.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>5.506938</td>\n",
       "      <td>0.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.666700</td>\n",
       "      <td>5.509510</td>\n",
       "      <td>0.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.645600</td>\n",
       "      <td>5.683694</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>5.464496</td>\n",
       "      <td>0.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.648100</td>\n",
       "      <td>5.616609</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.619700</td>\n",
       "      <td>5.483227</td>\n",
       "      <td>0.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.617300</td>\n",
       "      <td>5.833184</td>\n",
       "      <td>0.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.598300</td>\n",
       "      <td>5.823830</td>\n",
       "      <td>0.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.618000</td>\n",
       "      <td>5.623483</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.636200</td>\n",
       "      <td>5.405248</td>\n",
       "      <td>0.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.600900</td>\n",
       "      <td>5.535714</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.618600</td>\n",
       "      <td>5.373235</td>\n",
       "      <td>0.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.624300</td>\n",
       "      <td>5.449249</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.623300</td>\n",
       "      <td>5.526503</td>\n",
       "      <td>0.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.608200</td>\n",
       "      <td>5.665980</td>\n",
       "      <td>0.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.610300</td>\n",
       "      <td>5.600879</td>\n",
       "      <td>0.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>5.575889</td>\n",
       "      <td>0.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.614500</td>\n",
       "      <td>5.860071</td>\n",
       "      <td>0.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.612500</td>\n",
       "      <td>5.734211</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.592400</td>\n",
       "      <td>5.789748</td>\n",
       "      <td>0.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.609200</td>\n",
       "      <td>5.649408</td>\n",
       "      <td>0.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>5.670642</td>\n",
       "      <td>0.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.624100</td>\n",
       "      <td>5.741916</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.603500</td>\n",
       "      <td>5.871453</td>\n",
       "      <td>0.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.610600</td>\n",
       "      <td>5.702960</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.626200</td>\n",
       "      <td>5.329832</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.619200</td>\n",
       "      <td>5.417943</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.599600</td>\n",
       "      <td>5.576516</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.598700</td>\n",
       "      <td>5.625018</td>\n",
       "      <td>0.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.618300</td>\n",
       "      <td>5.378581</td>\n",
       "      <td>0.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.631800</td>\n",
       "      <td>5.373924</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>5.556571</td>\n",
       "      <td>0.177000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.609900</td>\n",
       "      <td>5.349820</td>\n",
       "      <td>0.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.605600</td>\n",
       "      <td>5.548957</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>5.653975</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.612600</td>\n",
       "      <td>5.548265</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.609600</td>\n",
       "      <td>5.635396</td>\n",
       "      <td>0.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.616200</td>\n",
       "      <td>5.567688</td>\n",
       "      <td>0.164000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>5.596154</td>\n",
       "      <td>0.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.594200</td>\n",
       "      <td>5.535422</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.583100</td>\n",
       "      <td>5.555802</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.618100</td>\n",
       "      <td>5.437286</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.599300</td>\n",
       "      <td>5.483039</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.587700</td>\n",
       "      <td>5.213150</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.597300</td>\n",
       "      <td>5.440530</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.591800</td>\n",
       "      <td>5.635961</td>\n",
       "      <td>0.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.610500</td>\n",
       "      <td>5.534814</td>\n",
       "      <td>0.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.597200</td>\n",
       "      <td>5.424732</td>\n",
       "      <td>0.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.576200</td>\n",
       "      <td>5.457769</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.602500</td>\n",
       "      <td>5.506425</td>\n",
       "      <td>0.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.590200</td>\n",
       "      <td>5.539977</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.605500</td>\n",
       "      <td>5.335140</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.554100</td>\n",
       "      <td>5.548883</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.592400</td>\n",
       "      <td>5.518165</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.592700</td>\n",
       "      <td>5.593760</td>\n",
       "      <td>0.195000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.583500</td>\n",
       "      <td>5.606080</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.576100</td>\n",
       "      <td>5.611994</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.592400</td>\n",
       "      <td>5.305257</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.574900</td>\n",
       "      <td>5.367775</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.595500</td>\n",
       "      <td>5.473322</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.572800</td>\n",
       "      <td>5.471691</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.589200</td>\n",
       "      <td>5.486359</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.578500</td>\n",
       "      <td>5.498105</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.585800</td>\n",
       "      <td>5.459182</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.595100</td>\n",
       "      <td>5.473244</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.564000</td>\n",
       "      <td>5.520756</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.594800</td>\n",
       "      <td>5.478836</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.606200</td>\n",
       "      <td>5.481313</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.578100</td>\n",
       "      <td>5.487682</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.572400</td>\n",
       "      <td>5.572434</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>5.490815</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.585600</td>\n",
       "      <td>5.505614</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>5.459146</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.575800</td>\n",
       "      <td>5.458550</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>5.428932</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.576800</td>\n",
       "      <td>5.433502</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.560200</td>\n",
       "      <td>5.460128</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.592900</td>\n",
       "      <td>5.529508</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.570800</td>\n",
       "      <td>5.499937</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.576200</td>\n",
       "      <td>5.471141</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.556800</td>\n",
       "      <td>5.517353</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.572100</td>\n",
       "      <td>5.507347</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.578700</td>\n",
       "      <td>5.512800</td>\n",
       "      <td>0.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.569100</td>\n",
       "      <td>5.503930</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.559200</td>\n",
       "      <td>5.494923</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.596700</td>\n",
       "      <td>5.498721</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.575500</td>\n",
       "      <td>5.499226</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-57\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-57/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-57/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-57/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-57/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-114\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-114/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-114/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-114/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-114/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-171\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-171/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-171/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-171/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-171/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-57] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-228\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-228/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-228/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-228/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-228/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-114] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-285\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-285/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-285/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-285/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-285/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-228] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-342\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-342/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-342/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-342/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-342/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-171] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-399\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-399/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-399/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-399/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-399/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-285] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-456\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-456/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-456/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-456/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-456/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-342] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-513\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-513/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-513/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-513/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-513/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-456] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-570\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-570/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-570/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-570/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-570/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-399] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-627\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-627/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-627/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-627/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-627/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-513] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-684\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-684/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-684/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-684/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-684/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-570] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-741\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-741/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-741/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-741/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-741/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-684] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-798\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-798/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-798/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-798/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-798/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-741] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-855\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-855/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-855/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-855/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-855/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-798] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-912\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-912/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-912/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-912/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-912/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-855] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-969\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-969/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-969/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-969/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-969/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-912] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1026\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1026/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1026/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1026/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1026/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-969] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1083\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1083/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1083/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1083/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1083/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1026] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1140\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1140/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1140/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1140/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1140/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1083] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1197\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1197/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1197/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1197/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1197/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1140] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1254\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1254/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1254/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1254/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1254/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1197] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1311\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1311/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1311/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1311/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1311/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1254] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1368\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1368/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1368/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1368/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1368/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1311] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1425\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1425/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1425/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1425/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1425/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1368] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1482\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1482/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1482/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1482/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1482/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1425] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1539\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1539/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1539/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1539/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1539/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1482] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1596\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1596/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1596/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1596/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1596/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1539] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1653\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1653/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1653/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1653/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1653/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1596] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1710\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1710/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1710/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1710/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1710/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1653] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1767\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1767/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1767/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1767/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1767/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1710] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1824\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1824/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1824/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1824/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1824/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1767] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1881\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1881/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1881/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1881/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1881/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1824] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1938\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1938/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1938/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1938/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1938/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1881] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-1995\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-1995/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-1995/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-1995/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-1995/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1938] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2052\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2052/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2052/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2052/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2052/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-1995] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2109\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2109/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2109/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2109/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2109/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2052] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2166\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2166/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2166/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2166/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2166/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2109] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2223\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2223/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2223/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2223/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2223/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2166] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2280\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2280/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2280/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2280/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2280/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2223] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2337\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2337/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2337/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2337/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2337/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2280] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2394\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2394/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2394/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2394/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2394/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2337] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2451\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2451/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2451/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2451/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2451/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2394] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2508\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2508/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2508/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2508/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2508/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2451] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2565\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2565/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2565/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2565/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2565/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2508] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2622\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2622/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2622/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2622/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2622/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2565] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2679\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2679/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2679/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2679/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2679/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2622] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2736\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2736/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2736/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2736/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2736/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-627] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2793\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2793/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2793/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2793/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2793/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2679] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2850\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2850/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2850/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2736] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2907\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2907/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2907/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2907/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2907/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2850] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-2964\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-2964/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-2964/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-2964/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-2964/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2793] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3021\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3021/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3021/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3021/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3021/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2907] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3078\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3078/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3078/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3078/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3078/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-2964] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3135\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3135/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3135/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3135/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3135/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3021] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3192\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3192/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3192/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3192/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3192/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3078] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3249\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3249/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3249/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3249/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3249/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3192] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3306\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3306/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3306/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3306/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3306/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3135] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3363\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3363/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3363/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3363/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3363/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3249] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3420\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3420/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3420/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3420/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3420/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3306] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3477\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3477/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3477/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3477/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3477/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3363] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3534\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3534/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3534/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3534/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3534/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3420] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3591\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3591/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3591/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3591/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3591/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3477] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3648\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3648/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3648/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3648/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3648/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3534] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3705\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3705/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3705/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3705/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3705/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3648] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3762\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3762/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3762/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3762/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3762/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3591] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3819\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3819/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3819/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3819/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3819/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3705] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3876\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3876/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3876/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3876/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3876/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3762] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3933\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3933/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3933/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3933/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3933/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3876] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-3990\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-3990/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-3990/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-3990/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-3990/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3933] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4047\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4047/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4047/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4047/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4047/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3819] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4104\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4104/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4104/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4104/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4104/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-3990] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4161\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4161/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4161/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4161/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4161/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4047] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4218\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4218/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4218/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4218/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4218/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4104] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4275\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4275/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4275/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4275/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4275/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4218] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4332\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4332/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4332/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4332/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4332/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4275] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4389\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4389/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4389/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4389/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4389/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4161] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4446\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4446/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4446/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4446/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4446/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4332] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4503\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4503/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4503/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4503/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4503/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4389] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4560\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4560/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4560/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4560/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4560/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4446] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4617\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4617/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4617/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4617/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4617/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4560] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4674\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4674/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4674/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4674/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4674/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4617] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4731\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4731/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4731/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4731/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4731/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4674] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4788\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4788/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4788/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4788/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4788/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4731] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4845\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4845/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4845/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4845/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4845/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4788] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4902\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4902/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4902/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4902/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4902/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4503] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-4959\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-4959/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-4959/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-4959/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-4959/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4845] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5016\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5016/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5016/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5016/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5016/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4959] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5073\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5073/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5073/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5073/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5073/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5016] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5130\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5130/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5130/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5130/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5130/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5073] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5187\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5187/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5187/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5187/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5187/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5130] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5244\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5244/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5244/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5244/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5244/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5187] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5301\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5301/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5301/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5301/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5301/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5244] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5358\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5358/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5358/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5358/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5358/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5301] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5415\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5415/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5415/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5415/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5415/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5358] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5472\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5472/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5472/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5472/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5472/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5415] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5529\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5529/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5529/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5529/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5529/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5472] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5586\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5586/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5586/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5586/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5586/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5529] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5643\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5643/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5643/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5643/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5643/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5586] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5700\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5700/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5643] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5757\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5757/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5757/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5757/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5757/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5700] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5814\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5814/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5814/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5814/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5814/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5757] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5871\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5871/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5871/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5871/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5871/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5814] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5928\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5928/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5928/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5928/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5928/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5871] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-5985\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-5985/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-5985/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-5985/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-5985/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-4902] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6042\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6042/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6042/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6042/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6042/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-5928] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6099\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6099/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6099/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6099/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6099/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6042] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6156\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6156/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6156/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6156/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6156/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6099] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6213\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6213/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6213/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6213/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6213/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6156] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6270\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6270/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6270/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6270/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6270/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6213] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6327\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6327/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6327/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6327/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6327/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6270] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6384\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6384/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6384/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6384/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6384/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6327] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6441\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6441/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6441/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6441/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6441/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6384] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6498\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6498/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6498/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6498/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6498/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6441] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6555\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6555/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6555/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6555/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6555/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6498] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6612\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6612/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6612/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6612/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6612/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6555] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6669\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6669/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6669/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6669/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6669/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6612] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6726\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6726/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6726/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6726/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6726/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6669] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6783\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6783/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6783/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6783/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6783/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6726] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6840\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6840/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6840/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6840/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6840/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6783] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6897\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6897/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6897/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6897/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6897/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6840] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-6954\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-6954/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-6954/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-6954/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-6954/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6897] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7011\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7011/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7011/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7011/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7011/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-6954] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7068\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7068/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7068/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7068/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7068/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7011] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7125\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7125/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7125/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7125/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7125/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7068] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7182\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7182/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7182/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7182/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7182/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7125] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7239\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7239/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7239/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7239/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7239/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7182] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7296\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7296/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7296/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7296/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7296/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7239] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7353\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7353/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7353/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7353/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7353/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7296] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7410\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7410/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7410/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7410/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7410/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7353] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7467\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7467/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7467/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7467/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7467/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7410] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7524\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7524/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7524/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7524/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7524/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7467] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7581\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7581/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7581/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7581/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7581/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7524] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7638\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7638/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7638/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7638/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7638/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7581] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7695\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7695/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7695/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7695/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7695/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7638] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7752\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7752/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7752/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7752/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7752/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7695] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7809\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7809/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7809/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7809/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7809/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7752] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7866\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7866/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7866/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7866/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7866/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7809] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7923\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7923/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7923/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7923/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7923/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7866] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-7980\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-7980/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-7980/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-7980/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-7980/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7923] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8037\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8037/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8037/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8037/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8037/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-7980] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8094\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8094/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8094/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8094/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8094/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8037] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8151\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8151/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8151/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8151/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8151/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8094] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8208\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8208/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8208/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8208/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8208/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8151] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8265\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8265/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8265/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8265/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8265/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8208] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8322\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8322/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8322/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8322/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8322/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8265] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8379\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8379/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8379/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8379/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8379/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8322] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8436\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8436/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8436/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8436/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8436/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8379] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8493\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8493/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8493/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8493/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8493/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8436] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8550\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8550/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8550/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8493] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8607\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8607/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8607/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8607/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8607/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8550] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8664\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8664/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8664/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8664/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8664/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8607] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8721\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8721/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8721/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8721/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8721/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8664] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8778\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8778/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8778/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8778/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8778/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8721] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8835\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8835/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8835/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8835/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8835/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8778] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8892\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8892/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8892/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8892/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8892/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8835] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-8949\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-8949/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-8949/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-8949/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-8949/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8892] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9006\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9006/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9006/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9006/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9006/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-8949] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9063\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9063/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9063/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9063/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9063/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9006] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9120\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9120/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9120/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9120/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9120/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9063] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9177\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9177/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9177/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9177/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9177/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9120] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9234\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9234/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9234/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9234/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9234/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9177] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9291\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9291/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9291/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9291/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9291/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9234] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9348\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9348/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9348/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9348/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9348/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9291] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9405\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9405/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9405/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9405/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9405/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9348] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9462\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9462/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9462/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9462/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9462/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9405] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9519\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9519/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9519/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9519/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9519/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9462] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9576\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9576/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9576/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9576/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9576/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9519] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9633\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9633/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9633/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9633/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9633/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9576] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9690\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9690/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9690/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9690/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9690/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9633] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9747\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9747/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9747/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9747/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9747/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9690] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9804\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9804/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9804/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9804/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9804/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9747] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9861\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9861/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9861/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9861/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9861/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9804] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9918\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9918/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9918/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9918/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9918/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9861] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-9975\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-9975/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-9975/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-9975/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-9975/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9918] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10032\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10032/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10032/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10032/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10032/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-9975] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10089\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10089/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10089/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10089/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10089/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10032] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10146\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10146/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10146/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10146/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10146/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10089] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10203\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10203/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10203/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10203/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10203/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10146] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10260\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10260/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10260/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10260/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10260/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10203] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10317\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10317/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10317/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10317/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10317/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10260] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10374\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10374/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10374/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10374/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10374/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10317] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10431\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10431/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10431/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10431/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10431/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10374] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10488\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10488/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10488/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10488/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10488/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10431] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10545\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10545/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10545/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10545/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10545/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10488] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10602\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10602/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10602/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10602/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10602/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10545] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10659\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10659/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10659/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10659/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10659/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10602] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10716\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10716/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10716/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10716/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10716/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10659] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10773\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10773/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10773/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10773/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10773/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10716] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10830\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10830/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10830/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10830/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10830/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10773] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10887\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10887/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10887/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10887/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10887/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10830] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-10944\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-10944/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-10944/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-10944/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-10944/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10887] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-11001\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-11001/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-11001/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-11001/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-11001/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-10944] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-11058\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-11058/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-11058/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-11058/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-11058/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-11001] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-11115\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-11115/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-11115/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-11115/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-11115/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-11058] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-11172\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-11172/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-11172/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-11172/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-11172/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-11115] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-11229\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-11229/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-11229/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-11229/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-11229/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-11172] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-11286\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-11286/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-11286/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-11286/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-11286/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-11229] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-11343\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-11343/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-11343/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-11343/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-11343/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-11286] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/CompositionDefault/models/checkpoint-11400\n",
      "Configuration saved in ./output/CompositionDefault/models/checkpoint-11400/config.json\n",
      "Model weights saved in ./output/CompositionDefault/models/checkpoint-11400/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/CompositionDefault/models/checkpoint-11400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/CompositionDefault/models/checkpoint-11400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/CompositionDefault/models/checkpoint-11343] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/CompositionDefault/models/checkpoint-5985 (score: 0.224).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11400, training_loss=1.0165458928493032, metrics={'train_runtime': 10805.7205, 'train_samples_per_second': 536.753, 'train_steps_per_second': 1.055, 'total_flos': 2.686243581e+16, 'train_loss': 1.0165458928493032, 'epoch': 200.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d1dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 04:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.224,\n",
       " 'eval_loss': 5.365607261657715,\n",
       " 'eval_runtime': 1.6513,\n",
       " 'eval_samples_per_second': 605.571,\n",
       " 'eval_steps_per_second': 2.422,\n",
       " 'epoch': 200.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Test\n",
    "trainer.evaluate(eval_dataset=tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "360c6e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation - source: taxon rank Relation - target: taxonomischer Rang\n",
      "Relation - source: emergency services Relation - target: Notfalleinrichtungen\n",
      "Relation - source: color Relation - target: Farbe\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9dbbdccc0a49bba9e97607f932b6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.11, 'eval_loss': 3.7856783866882324, 'eval_runtime': 0.8323, 'eval_samples_per_second': 120.15, 'eval_steps_per_second': 1.201}\n",
      "Relation - source: expected completeness Relation - target: erwartete Vollständigkeit\n",
      "Relation - source: measured physical quantity Relation - target: gemessene physikalische Größe\n",
      "Relation - source: open period from Relation - target: geöffnet von Zeitpunkt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949d9c0c894c4c3c8f5e3c8c91f48464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.06, 'eval_loss': 6.303734302520752, 'eval_runtime': 0.6847, 'eval_samples_per_second': 146.048, 'eval_steps_per_second': 1.46}\n",
      "Relation - source: platform Relation - target: Plattform\n",
      "Relation - source: legislative committee Relation - target: Legislativkomitee\n",
      "Relation - source: media franchise Relation - target: Medien-Franchise\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344b8081070e4310ba7dc4a2c5701531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.0, 'eval_loss': 12.631064414978027, 'eval_runtime': 0.6724, 'eval_samples_per_second': 148.719, 'eval_steps_per_second': 1.487}\n",
      "Relation - source: type of lens Relation - target: Linsentyp\n",
      "Relation - source: natural reservoir of Relation - target: Erregerreservoir von\n",
      "Relation - source: facial hair Relation - target: Gesichtshaar\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748effb8ac78499589602aad80704ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.04, 'eval_loss': 7.2143940925598145, 'eval_runtime': 0.6595, 'eval_samples_per_second': 151.623, 'eval_steps_per_second': 1.516}\n",
      "Relation - source: reply to Relation - target: Antwort auf\n",
      "Relation - source: list of works Relation - target: Werkliste\n",
      "Relation - source: side effect Relation - target: Nebenwirkung\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f382efb9b50541d7a6c482b79d5a78e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.04, 'eval_loss': 5.758947372436523, 'eval_runtime': 0.6251, 'eval_samples_per_second': 159.962, 'eval_steps_per_second': 1.6}\n",
      "Relation - source: physically interacts with Relation - target: interagiert physikalisch mit\n",
      "Relation - source: points/goal scored by Relation - target: Punkt/Treffer erzielt durch\n",
      "Relation - source: demonym of Relation - target: Demonym zu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169ae50c99b941b58f1c713df4c0804b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.27, 'eval_loss': 3.7629101276397705, 'eval_runtime': 0.6228, 'eval_samples_per_second': 160.556, 'eval_steps_per_second': 1.606}\n",
      "Relation - source: conflict Relation - target: Kriegseinsatz\n",
      "Relation - source: culture Relation - target: Kultur\n",
      "Relation - source: recording or performance of Relation - target: Aufnahme oder Ausführung von\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10288c9db0344d4783e189ca1c4f1f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.87, 'eval_loss': 0.39176636934280396, 'eval_runtime': 0.6612, 'eval_samples_per_second': 151.238, 'eval_steps_per_second': 1.512}\n",
      "Relation - source: school district Relation - target: Schulbezirk\n",
      "Relation - source: vehicle Relation - target: Fahrzeug\n",
      "Relation - source: sex or gender Relation - target: Geschlecht\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61683bc48b74c73adbfb8878adf0537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.0, 'eval_loss': 8.432419776916504, 'eval_runtime': 0.613, 'eval_samples_per_second': 163.143, 'eval_steps_per_second': 1.631}\n",
      "Relation - source: designated as terrorist by Relation - target: als terroristisch eingestuft von\n",
      "Relation - source: typically sells Relation - target: verkauft im Allgemeinen\n",
      "Relation - source: main building contractor Relation - target: Generalbauunternehmer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85c27e27f934986b32d585bedbb0dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.67, 'eval_loss': 1.1832425594329834, 'eval_runtime': 0.6232, 'eval_samples_per_second': 160.475, 'eval_steps_per_second': 1.605}\n",
      "Relation - source: site of astronomical discovery Relation - target: astronomischer Entdeckungsort\n",
      "Relation - source: primary destinations Relation - target: Hauptorte\n",
      "Relation - source: different from Relation - target: verschieden von\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b56c9394e0b47f898fc32ed7a501ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.18, 'eval_loss': 4.191916465759277, 'eval_runtime': 0.637, 'eval_samples_per_second': 156.995, 'eval_steps_per_second': 1.57}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Symmetry per Relation\n",
    "evaluation_composition(trainer, tokenizer, relations, source_language, copy.deepcopy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214e34f",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "- Why does it not learn compositions?\n",
    "- pretrained\n",
    "- target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b37f3489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4966256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Medina taxon rank Terminal', 'Terminal emergency services Malone', 'Medina color Malone', 'Invasion taxon rank EV', 'EV emergency services Soccerway', 'Invasion color Soccerway', 'Burke taxon rank Amadeus', 'Amadeus emergency services President', 'Burke color President', 'Drama taxon rank Simpson', 'Simpson emergency services Mitch', 'Drama color Mitch', 'Master taxon rank Jang', 'Jang emergency services Hütte', 'Master color Hütte', 'Dari taxon rank Kosmos', 'Kosmos emergency services Bohemia', 'Dari color Bohemia', 'Chihuahua taxon rank Lyman', 'Lyman emergency services Ravenna', 'Chihuahua color Ravenna', 'EP taxon rank Lakes', 'Lakes emergency services Shri', 'EP color Shri', 'Chase taxon rank Seite', 'Seite emergency services Delgado', 'Chase color Delgado', 'Worcester taxon rank Antoinette', 'Antoinette emergency services Midway', 'Worcester color Midway', 'Albert taxon rank Henley', 'Henley emergency services Od', 'Albert color Od', 'Ibiza taxon rank Cruise', 'Cruise emergency services Missouri', 'Ibiza color Missouri', 'Câmara taxon rank Ennen', 'Ennen emergency services Hodges', 'Câmara color Hodges', 'Eleanor taxon rank Turm', 'Turm emergency services Publié', 'Eleanor color Publié', 'Campus taxon rank Gloucester', 'Gloucester emergency services Gyula', 'Campus color Gyula', 'Schaus taxon rank Diana', 'Diana emergency services DSM', 'Schaus color DSM', 'Chaos taxon rank Bing', 'Bing emergency services Ost', 'Chaos color Ost', 'Alice taxon rank Industria', 'Industria emergency services Candy', 'Alice color Candy', 'Franklin taxon rank President', 'President emergency services Ted', 'Franklin color Ted', 'Colombo taxon rank Sprint', 'Sprint emergency services Eaton', 'Colombo color Eaton', 'Graf taxon rank Jess', 'Jess emergency services Irving', 'Graf color Irving', 'Darwin taxon rank Indianapolis', 'Indianapolis emergency services Palatinat', 'Darwin color Palatinat', 'Nisan taxon rank Mouse', 'Mouse emergency services Sai', 'Nisan color Sai', 'Hartman taxon rank Hindenburg', 'Hindenburg emergency services Principal', 'Hartman color Principal', 'Lur taxon rank Shawn', 'Shawn emergency services Arrow', 'Lur color Arrow', 'Fiesta taxon rank Ehren', 'Ehren emergency services Berg', 'Fiesta color Berg', 'Alexandra taxon rank Florida', 'Florida emergency services Bora', 'Alexandra color Bora', 'Mérida taxon rank Córdoba', 'Córdoba emergency services Soleil', 'Mérida color Soleil', 'Shire taxon rank Otis', 'Otis emergency services Modena', 'Shire color Modena', 'Anthology taxon rank Waves', 'Waves emergency services Odd', 'Anthology color Odd', 'Monroe taxon rank Special', 'Special emergency services Cullen', 'Monroe color Cullen', 'Hell taxon rank Chaco', 'Chaco emergency services Nashville', 'Hell color Nashville', 'Bandet taxon rank Sally', 'Sally emergency services Guadalupe', 'Bandet color Guadalupe', 'Bertrand taxon rank Midlands', 'Midlands emergency services Server', 'Bertrand color Server', 'Drum taxon rank Check', 'Check emergency services Catedral', 'Drum color Catedral', 'IBM taxon rank Carlos', 'Carlos emergency services McGill', 'IBM color McGill', 'Christchurch taxon rank Regia', 'Regia emergency services Linden', 'Christchurch color Linden', 'Broadcast taxon rank Records', 'Records emergency services Schlacht', 'Broadcast color Schlacht', 'Clay taxon rank Salisbury', 'Salisbury emergency services Helmut', 'Clay color Helmut', 'Sound taxon rank Poslední', 'Poslední emergency services HP', 'Sound color HP', 'Liga taxon rank Brain', 'Brain emergency services Ipswich', 'Liga color Ipswich', 'Vatican taxon rank Qara', 'Qara emergency services Den', 'Vatican color Den', 'Libro taxon rank Bobby', 'Bobby emergency services Hügel', 'Libro color Hügel', 'Vanderbilt taxon rank Potosí', 'Potosí emergency services pad', 'Vanderbilt color pad', 'Austrian taxon rank Byron', 'Byron emergency services Irena', 'Austrian color Irena', 'Varese taxon rank Gary', 'Gary emergency services Estate', 'Varese color Estate', 'Ninja taxon rank Henning', 'Henning emergency services EL', 'Ninja color EL', 'Lafayette taxon rank Apollo', 'Apollo emergency services India', 'Lafayette color India', 'Catalina taxon rank Monkey', 'Monkey emergency services Hills', 'Catalina color Hills', 'Companion taxon rank Empire', 'Empire emergency services Pan', 'Companion color Pan', 'Marshal taxon rank Robertson', 'Robertson emergency services Mint', 'Marshal color Mint', 'Molina taxon rank Harcourt', 'Harcourt emergency services Capitaine', 'Molina color Capitaine', 'Dow taxon rank Savoy', 'Savoy emergency services Coburg', 'Dow color Coburg', 'Platte taxon rank Lorenz', 'Lorenz emergency services Satellite', 'Platte color Satellite', 'Giant taxon rank EP', 'EP emergency services Cameron', 'Giant color Cameron', 'Mulder taxon rank TNI', 'TNI emergency services JK', 'Mulder color JK', 'Hancock taxon rank Christensen', 'Christensen emergency services Sport', 'Hancock color Sport', 'Deutschland taxon rank Jozef', 'Jozef emergency services Mitch', 'Deutschland color Mitch', 'Della taxon rank Lotto', 'Lotto emergency services Schaus', 'Della color Schaus', 'Niko taxon rank Darling', 'Darling emergency services York', 'Niko color York', 'Romans taxon rank Westminster', 'Westminster emergency services Sentinel', 'Romans color Sentinel', 'Salman taxon rank Camino', 'Camino emergency services Savoy', 'Salman color Savoy', 'Medi taxon rank Bees', 'Bees emergency services Spain', 'Medi color Spain', 'Monaco taxon rank Friesland', 'Friesland emergency services Lucia', 'Monaco color Lucia', 'Release taxon rank Use', 'Use emergency services Points', 'Release color Points', 'Daughter taxon rank Hubbard', 'Hubbard emergency services Prima', 'Daughter color Prima', 'Kendall taxon rank Lego', 'Lego emergency services Siena', 'Kendall color Siena', 'PCR taxon rank Campos', 'Campos emergency services Lillehammer', 'PCR color Lillehammer', 'Lorena taxon rank Horne', 'Horne emergency services Luce', 'Lorena color Luce', 'Twain taxon rank Palestine', 'Palestine emergency services Hindi', 'Twain color Hindi', 'Head taxon rank ET', 'ET emergency services Aire', 'Head color Aire', 'Porta taxon rank di', 'di emergency services Bund', 'Porta color Bund', 'Yunan taxon rank Lindsay', 'Lindsay emergency services Heide', 'Yunan color Heide', 'MGM taxon rank Application', 'Application emergency services Horse', 'MGM color Horse', 'Queste taxon rank Elke', 'Elke emergency services Alt', 'Queste color Alt', 'Geld taxon rank Almanya', 'Almanya emergency services Raymond', 'Geld color Raymond', 'Jesus taxon rank Bet', 'Bet emergency services Bari', 'Jesus color Bari', 'Potter taxon rank Ming', 'Ming emergency services Vladimir', 'Potter color Vladimir', 'Crown taxon rank Morris', 'Morris emergency services Gates', 'Crown color Gates', 'Adi taxon rank Quintana', 'Quintana emergency services Isole', 'Adi color Isole', 'Sinn taxon rank Moore', 'Moore emergency services Quatre', 'Sinn color Quatre', 'Vacelet taxon rank Katy', 'Katy emergency services Darkness', 'Vacelet color Darkness', 'Sagan taxon rank Data', 'Data emergency services Aya', 'Sagan color Aya', 'Madsen taxon rank AF', 'AF emergency services Atatürk', 'Madsen color Atatürk', 'Robinson taxon rank Dante', 'Dante emergency services Apocalypse', 'Robinson color Apocalypse', 'Fayette taxon rank Falling', 'Falling emergency services Cliff', 'Fayette color Cliff', 'Grant taxon rank Acre', 'Acre emergency services Seitz', 'Grant color Seitz', 'Julian taxon rank Rum', 'Rum emergency services Alcalá', 'Julian color Alcalá', 'Carrillo taxon rank Oscara', 'Oscara emergency services Marburg', 'Carrillo color Marburg', 'Aus taxon rank Patria', 'Patria emergency services Fuego', 'Aus color Fuego', 'BBC taxon rank Pluto', 'Pluto emergency services Sainte', 'BBC color Sainte', 'Kongo taxon rank Membre', 'Membre emergency services Guanajuato', 'Kongo color Guanajuato', 'Gaur taxon rank Champs', 'Champs emergency services Bertram', 'Gaur color Bertram', 'Shining taxon rank Klinik', 'Klinik emergency services Diablo', 'Shining color Diablo', 'Simpson taxon rank Moreau', 'Moreau emergency services Isles', 'Simpson color Isles', 'Douglas taxon rank Soldier', 'Soldier emergency services Cinema', 'Douglas color Cinema', 'Continental taxon rank Mille', 'Mille emergency services Eaton', 'Continental color Eaton', 'West taxon rank Julie', 'Julie emergency services Dorothea', 'West color Dorothea', 'Haas taxon rank Apocalypse', 'Apocalypse emergency services Nota', 'Haas color Nota', 'Lynch taxon rank Volga', 'Volga emergency services ABA', 'Lynch color ABA', 'Cornwall taxon rank Terminal', 'Terminal emergency services Malone', 'Cornwall color Malone', 'Jalan taxon rank EV', 'EV emergency services Soccerway', 'Jalan color Soccerway', 'Dhaka taxon rank Amadeus', 'Amadeus emergency services President', 'Dhaka color President', 'Reading taxon rank Simpson', 'Simpson emergency services Mitch', 'Reading color Mitch', 'Auckland taxon rank Jang', 'Jang emergency services Hütte', 'Auckland color Hütte', 'Chancellor taxon rank Kosmos', 'Kosmos emergency services Bohemia', 'Chancellor color Bohemia', 'Dakota taxon rank Lyman', 'Lyman emergency services Ravenna', 'Dakota color Ravenna', 'Arles taxon rank Lakes', 'Lakes emergency services Shri', 'Arles color Shri', 'Loan taxon rank Seite', 'Seite emergency services Delgado', 'Loan color Delgado', 'Stab taxon rank Antoinette', 'Antoinette emergency services Midway', 'Stab color Midway', 'Golden taxon rank Henley', 'Henley emergency services Od', 'Golden color Od', 'Jakiel taxon rank Cruise', 'Cruise emergency services Missouri', 'Jakiel color Missouri', 'Wade taxon rank Ennen', 'Ennen emergency services Hodges', 'Wade color Hodges', 'Ebro taxon rank Turm', 'Turm emergency services Publié', 'Ebro color Publié', 'Dorothea taxon rank Gloucester', 'Gloucester emergency services Gyula', 'Dorothea color Gyula', 'Urban taxon rank Diana', 'Diana emergency services DSM', 'Urban color DSM', 'HF taxon rank Bing', 'Bing emergency services Ost', 'HF color Ost', 'Spectrum taxon rank Industria', 'Industria emergency services Candy', 'Spectrum color Candy', 'Cass taxon rank President', 'President emergency services Ted', 'Cass color Ted', 'Riva taxon rank Sprint', 'Sprint emergency services Eaton', 'Riva color Eaton', 'Winston taxon rank Jess', 'Jess emergency services Irving', 'Winston color Irving', 'Punk taxon rank Indianapolis', 'Indianapolis emergency services Palatinat', 'Punk color Palatinat', 'SMS taxon rank Mouse', 'Mouse emergency services Sai', 'SMS color Sai', 'Paglinawan taxon rank Hindenburg', 'Hindenburg emergency services Principal', 'Paglinawan color Principal', 'McKay taxon rank Shawn', 'Shawn emergency services Arrow', 'McKay color Arrow', 'Harmony taxon rank Ehren', 'Ehren emergency services Berg', 'Harmony color Berg', 'Chantal taxon rank Florida', 'Florida emergency services Bora', 'Chantal color Bora', 'Taurus taxon rank Córdoba', 'Córdoba emergency services Soleil', 'Taurus color Soleil', 'Crosby taxon rank Otis', 'Otis emergency services Modena', 'Crosby color Modena', 'GSC taxon rank Waves', 'Waves emergency services Odd', 'GSC color Odd', 'Moonlight taxon rank Special', 'Special emergency services Cullen', 'Moonlight color Cullen', 'Goodman taxon rank Chaco', 'Chaco emergency services Nashville', 'Goodman color Nashville', 'Stéphane taxon rank Sally', 'Sally emergency services Guadalupe', 'Stéphane color Guadalupe', 'Kader taxon rank Midlands', 'Midlands emergency services Server', 'Kader color Server', 'Ferro taxon rank Check', 'Check emergency services Catedral', 'Ferro color Catedral', 'Anglo taxon rank Carlos', 'Carlos emergency services McGill', 'Anglo color McGill', 'Flesh taxon rank Regia', 'Regia emergency services Linden', 'Flesh color Linden', 'Ubuntu taxon rank Records', 'Records emergency services Schlacht', 'Ubuntu color Schlacht', 'GMT taxon rank Salisbury', 'Salisbury emergency services Helmut', 'GMT color Helmut', 'Quintana taxon rank Poslední', 'Poslední emergency services HP', 'Quintana color HP', 'Lublin taxon rank Brain', 'Brain emergency services Ipswich', 'Lublin color Ipswich', 'Nagar taxon rank Qara', 'Qara emergency services Den', 'Nagar color Den', 'Dictionary taxon rank Bobby', 'Bobby emergency services Hügel', 'Dictionary color Hügel', 'Cats taxon rank Potosí', 'Potosí emergency services pad', 'Cats color pad', 'Beyoncé taxon rank Byron', 'Byron emergency services Irena', 'Beyoncé color Irena', 'Mendoza taxon rank Gary', 'Gary emergency services Estate', 'Mendoza color Estate', 'Roja taxon rank Henning', 'Henning emergency services EL', 'Roja color EL', 'Beckett taxon rank Apollo', 'Apollo emergency services India', 'Beckett color India', 'Toulouse taxon rank Monkey', 'Monkey emergency services Hills', 'Toulouse color Hills', 'Summit taxon rank Empire', 'Empire emergency services Pan', 'Summit color Pan', 'Champion taxon rank Robertson', 'Robertson emergency services Mint', 'Champion color Mint', 'Twins taxon rank Harcourt', 'Harcourt emergency services Capitaine', 'Twins color Capitaine', 'Halo taxon rank Savoy', 'Savoy emergency services Coburg', 'Halo color Coburg', 'Deborah taxon rank Lorenz', 'Lorenz emergency services Satellite', 'Deborah color Satellite', 'SF taxon rank EP', 'EP emergency services Cameron', 'SF color Cameron', 'Commonwealth taxon rank TNI', 'TNI emergency services JK', 'Commonwealth color JK', 'Dublin taxon rank Christensen', 'Christensen emergency services Sport', 'Dublin color Sport', 'Apocalypse taxon rank Jozef', 'Jozef emergency services Mitch', 'Apocalypse color Mitch', 'Drake taxon rank Lotto', 'Lotto emergency services Schaus', 'Drake color Schaus', 'Padang taxon rank Darling', 'Darling emergency services York', 'Padang color York', 'Royal taxon rank Westminster', 'Westminster emergency services Sentinel', 'Royal color Sentinel', 'Quick taxon rank Camino', 'Camino emergency services Savoy', 'Quick color Savoy', 'Remote taxon rank Bees', 'Bees emergency services Spain', 'Remote color Spain', 'Player taxon rank Friesland', 'Friesland emergency services Lucia', 'Player color Lucia', 'Laba taxon rank Use', 'Use emergency services Points', 'Laba color Points', 'Wiener taxon rank Hubbard', 'Hubbard emergency services Prima', 'Wiener color Prima', 'Tripoli taxon rank Lego', 'Lego emergency services Siena', 'Tripoli color Siena', 'Glasgow taxon rank Campos', 'Campos emergency services Lillehammer', 'Glasgow color Lillehammer', 'Hansen taxon rank Horne', 'Horne emergency services Luce', 'Hansen color Luce', 'Ortes taxon rank Palestine', 'Palestine emergency services Hindi', 'Ortes color Hindi', 'Mozilla taxon rank ET', 'ET emergency services Aire', 'Mozilla color Aire', 'Mur taxon rank di', 'di emergency services Bund', 'Mur color Bund', 'Iberia taxon rank Lindsay', 'Lindsay emergency services Heide', 'Iberia color Heide', 'Vir taxon rank Application', 'Application emergency services Horse', 'Vir color Horse', 'Prato taxon rank Elke', 'Elke emergency services Alt', 'Prato color Alt', 'Ferris taxon rank Almanya', 'Almanya emergency services Raymond', 'Ferris color Raymond', 'Racing taxon rank Bet', 'Bet emergency services Bari', 'Racing color Bari', 'Steiner taxon rank Ming', 'Ming emergency services Vladimir', 'Steiner color Vladimir', 'Sala taxon rank Morris', 'Morris emergency services Gates', 'Sala color Gates', 'Connection taxon rank Quintana', 'Quintana emergency services Isole', 'Connection color Isole', 'Hastings taxon rank Moore', 'Moore emergency services Quatre', 'Hastings color Quatre', 'Siegfried taxon rank Katy', 'Katy emergency services Darkness', 'Siegfried color Darkness', 'Kale taxon rank Data', 'Data emergency services Aya', 'Kale color Aya', 'Fourier taxon rank AF', 'AF emergency services Atatürk', 'Fourier color Atatürk', 'Pada taxon rank Dante', 'Dante emergency services Apocalypse', 'Pada color Apocalypse', 'Roberto taxon rank Falling', 'Falling emergency services Cliff', 'Roberto color Cliff', 'Struggle taxon rank Acre', 'Acre emergency services Seitz', 'Struggle color Seitz', 'Elijah taxon rank Rum', 'Rum emergency services Alcalá', 'Elijah color Alcalá', 'Rote taxon rank Oscara', 'Oscara emergency services Marburg', 'Rote color Marburg', 'Christi taxon rank Patria', 'Patria emergency services Fuego', 'Christi color Fuego', 'Berge taxon rank Pluto', 'Pluto emergency services Sainte', 'Berge color Sainte', 'Wimbledon taxon rank Membre', 'Membre emergency services Guanajuato', 'Wimbledon color Guanajuato', 'Bees taxon rank Champs', 'Champs emergency services Bertram', 'Bees color Bertram', 'Palm taxon rank Klinik', 'Klinik emergency services Diablo', 'Palm color Diablo', 'Nor taxon rank Moreau', 'Moreau emergency services Isles', 'Nor color Isles', 'Maria taxon rank Soldier', 'Soldier emergency services Cinema', 'Maria color Cinema', 'Stadio taxon rank Mille', 'Mille emergency services Eaton', 'Stadio color Eaton', 'Belo taxon rank Julie', 'Julie emergency services Dorothea', 'Belo color Dorothea', 'Boga taxon rank Apocalypse', 'Apocalypse emergency services Nota', 'Boga color Nota', 'NSW taxon rank Volga', 'Volga emergency services ABA', 'NSW color ABA', 'Roller taxon rank Terminal', 'Terminal emergency services Malone', 'Roller color Malone', 'León taxon rank EV', 'EV emergency services Soccerway', 'León color Soccerway', 'Rollen taxon rank Amadeus', 'Amadeus emergency services President', 'Rollen color President', 'Agama taxon rank Simpson', 'Simpson emergency services Mitch', 'Agama color Mitch', 'Seminary taxon rank Jang', 'Jang emergency services Hütte', 'Seminary color Hütte', 'Ensemble taxon rank Kosmos', 'Kosmos emergency services Bohemia', 'Ensemble color Bohemia', 'Blade taxon rank Lyman', 'Lyman emergency services Ravenna', 'Blade color Ravenna', 'Cochrane taxon rank Lakes', 'Lakes emergency services Shri', 'Cochrane color Shri', 'Indiana taxon rank Seite', 'Seite emergency services Delgado', 'Indiana color Delgado', 'Carvalho taxon rank Antoinette', 'Antoinette emergency services Midway', 'Carvalho color Midway', 'René taxon rank Henley', 'Henley emergency services Od', 'René color Od', 'Mask taxon rank Cruise', 'Cruise emergency services Missouri', 'Mask color Missouri', 'Dacia taxon rank Ennen', 'Ennen emergency services Hodges', 'Dacia color Hodges', 'Cea taxon rank Turm', 'Turm emergency services Publié', 'Cea color Publié', 'Bulgaria taxon rank Gloucester', 'Gloucester emergency services Gyula', 'Bulgaria color Gyula', 'Rocket taxon rank Diana', 'Diana emergency services DSM', 'Rocket color DSM', 'Pro taxon rank Bing', 'Bing emergency services Ost', 'Pro color Ost', 'Portugal taxon rank Industria', 'Industria emergency services Candy', 'Portugal color Candy', 'Blanco taxon rank President', 'President emergency services Ted', 'Blanco color Ted', 'Addison taxon rank Sprint', 'Sprint emergency services Eaton', 'Addison color Eaton', 'Como taxon rank Jess', 'Jess emergency services Irving', 'Como color Irving', 'Suomi taxon rank Indianapolis', 'Indianapolis emergency services Palatinat', 'Suomi color Palatinat', 'Alt taxon rank Mouse', 'Mouse emergency services Sai', 'Alt color Sai', 'Esther taxon rank Hindenburg', 'Hindenburg emergency services Principal', 'Esther color Principal', 'Sick taxon rank Shawn', 'Shawn emergency services Arrow', 'Sick color Arrow', 'Bowman taxon rank Ehren', 'Ehren emergency services Berg', 'Bowman color Berg', 'Wells taxon rank Florida', 'Florida emergency services Bora', 'Wells color Bora', 'NT taxon rank Córdoba', 'Córdoba emergency services Soleil', 'NT color Soleil', 'Titanic taxon rank Otis', 'Otis emergency services Modena', 'Titanic color Modena', 'Chamber taxon rank Waves', 'Waves emergency services Odd', 'Chamber color Odd', 'Satellite taxon rank Special', 'Special emergency services Cullen', 'Satellite color Cullen', 'Niels taxon rank Chaco', 'Chaco emergency services Nashville', 'Niels color Nashville', 'Borneo taxon rank Sally', 'Sally emergency services Guadalupe', 'Borneo color Guadalupe', 'Tigre taxon rank Midlands', 'Midlands emergency services Server', 'Tigre color Server', 'Madagascar taxon rank Check', 'Check emergency services Catedral', 'Madagascar color Catedral', 'Vincent taxon rank Carlos', 'Carlos emergency services McGill', 'Vincent color McGill', 'Midlands taxon rank Regia', 'Regia emergency services Linden', 'Midlands color Linden', 'Siam taxon rank Records', 'Records emergency services Schlacht', 'Siam color Schlacht', 'Uit taxon rank Salisbury', 'Salisbury emergency services Helmut', 'Uit color Helmut', 'CDC taxon rank Poslední', 'Poslední emergency services HP', 'CDC color HP', 'Sawyer taxon rank Brain', 'Brain emergency services Ipswich', 'Sawyer color Ipswich', 'Ranking taxon rank Qara', 'Qara emergency services Den', 'Ranking color Den', 'Babylon taxon rank Bobby', 'Bobby emergency services Hügel', 'Babylon color Hügel', 'Côte taxon rank Potosí', 'Potosí emergency services pad', 'Côte color pad', 'IS taxon rank Byron', 'Byron emergency services Irena', 'IS color Irena', 'Frost taxon rank Gary', 'Gary emergency services Estate', 'Frost color Estate', 'Mariana taxon rank Henning', 'Henning emergency services EL', 'Mariana color EL', 'Baza taxon rank Apollo', 'Apollo emergency services India', 'Baza color India', 'Washington taxon rank Monkey', 'Monkey emergency services Hills', 'Washington color Hills', 'Giles taxon rank Empire', 'Empire emergency services Pan', 'Giles color Pan', 'Benton taxon rank Robertson', 'Robertson emergency services Mint', 'Benton color Mint', 'Balázs taxon rank Harcourt', 'Harcourt emergency services Capitaine', 'Balázs color Capitaine', 'Pays taxon rank Savoy', 'Savoy emergency services Coburg', 'Pays color Coburg', 'Alman taxon rank Lorenz', 'Lorenz emergency services Satellite', 'Alman color Satellite', 'Hitchcock taxon rank EP', 'EP emergency services Cameron', 'Hitchcock color Cameron', 'Danube taxon rank TNI', 'TNI emergency services JK', 'Danube color JK', 'Quattro taxon rank Christensen', 'Christensen emergency services Sport', 'Quattro color Sport', 'Troy taxon rank Jozef', 'Jozef emergency services Mitch', 'Troy color Mitch', 'Hector taxon rank Lotto', 'Lotto emergency services Schaus', 'Hector color Schaus', 'Hip taxon rank Darling', 'Darling emergency services York', 'Hip color York', 'Face taxon rank Westminster', 'Westminster emergency services Sentinel', 'Face color Sentinel', 'Funk taxon rank Camino', 'Camino emergency services Savoy', 'Funk color Savoy', 'Hazel taxon rank Bees', 'Bees emergency services Spain', 'Hazel color Spain', 'Romawi taxon rank Friesland', 'Friesland emergency services Lucia', 'Romawi color Lucia', 'Straits taxon rank Use', 'Use emergency services Points', 'Straits color Points', 'Put taxon rank Hubbard', 'Hubbard emergency services Prima', 'Put color Prima', 'ATP taxon rank Lego', 'Lego emergency services Siena', 'ATP color Siena', 'Abucay taxon rank Campos', 'Campos emergency services Lillehammer', 'Abucay color Lillehammer', 'Spor taxon rank Horne', 'Horne emergency services Luce', 'Spor color Luce', 'Cathedral taxon rank Palestine', 'Palestine emergency services Hindi', 'Cathedral color Hindi', 'Vijay taxon rank ET', 'ET emergency services Aire', 'Vijay color Aire', 'Géza taxon rank di', 'di emergency services Bund', 'Géza color Bund', 'Werk taxon rank Lindsay', 'Lindsay emergency services Heide', 'Werk color Heide', 'Jungen taxon rank Application', 'Application emergency services Horse', 'Jungen color Horse', 'Harri taxon rank Elke', 'Elke emergency services Alt', 'Harri color Alt', 'Wood taxon rank Almanya', 'Almanya emergency services Raymond', 'Wood color Raymond', 'XP taxon rank Bet', 'Bet emergency services Bari', 'XP color Bari', 'Henning taxon rank Ming', 'Ming emergency services Vladimir', 'Henning color Vladimir', 'Daniel taxon rank Morris', 'Morris emergency services Gates', 'Daniel color Gates', 'Lac taxon rank Quintana', 'Quintana emergency services Isole', 'Lac color Isole', 'Niger taxon rank Moore', 'Moore emergency services Quatre', 'Niger color Quatre', 'Bon taxon rank Katy', 'Katy emergency services Darkness', 'Bon color Darkness', 'Hess taxon rank Data', 'Data emergency services Aya', 'Hess color Aya', 'Asunción taxon rank AF', 'AF emergency services Atatürk', 'Asunción color Atatürk', 'ba taxon rank Dante', 'Dante emergency services Apocalypse', 'ba color Apocalypse', 'Roll taxon rank Falling', 'Falling emergency services Cliff', 'Roll color Cliff', 'Nada taxon rank Acre', 'Acre emergency services Seitz', 'Nada color Seitz', 'Saale taxon rank Rum', 'Rum emergency services Alcalá', 'Saale color Alcalá', 'Minden taxon rank Oscara', 'Oscara emergency services Marburg', 'Minden color Marburg', 'Imperial taxon rank Patria', 'Patria emergency services Fuego', 'Imperial color Fuego', 'Moreau taxon rank Pluto', 'Pluto emergency services Sainte', 'Moreau color Sainte', 'Provence taxon rank Membre', 'Membre emergency services Guanajuato', 'Provence color Guanajuato', 'Archer taxon rank Champs', 'Champs emergency services Bertram', 'Archer color Bertram', 'JNA taxon rank Klinik', 'Klinik emergency services Diablo', 'JNA color Diablo', 'Punjabi taxon rank Moreau', 'Moreau emergency services Isles', 'Punjabi color Isles', 'Tarragona taxon rank Soldier', 'Soldier emergency services Cinema', 'Tarragona color Cinema', 'Pirates taxon rank Mille', 'Mille emergency services Eaton', 'Pirates color Eaton', 'Mat taxon rank Julie', 'Julie emergency services Dorothea', 'Mat color Dorothea', 'Berlin taxon rank Apocalypse', 'Apocalypse emergency services Nota', 'Berlin color Nota', 'Thành taxon rank Volga', 'Volga emergency services ABA', 'Thành color ABA', 'Vance taxon rank Terminal', 'Terminal emergency services Malone', 'Vance color Malone', 'Banja taxon rank EV', 'EV emergency services Soccerway', 'Banja color Soccerway', 'Champ taxon rank Amadeus', 'Amadeus emergency services President', 'Champ color President', 'Trieste taxon rank Simpson', 'Simpson emergency services Mitch', 'Trieste color Mitch', 'Pi taxon rank Jang', 'Jang emergency services Hütte', 'Pi color Hütte', 'Cisco taxon rank Kosmos', 'Kosmos emergency services Bohemia', 'Cisco color Bohemia', 'Cologne taxon rank Lyman', 'Lyman emergency services Ravenna', 'Cologne color Ravenna', 'Banks taxon rank Lakes', 'Lakes emergency services Shri', 'Banks color Shri', 'MotoGP taxon rank Seite', 'Seite emergency services Delgado', 'MotoGP color Delgado', 'Spiegel taxon rank Antoinette', 'Antoinette emergency services Midway', 'Spiegel color Midway', 'Dara taxon rank Henley', 'Henley emergency services Od', 'Dara color Od', 'Platinum taxon rank Cruise', 'Cruise emergency services Missouri', 'Platinum color Missouri', 'Calvin taxon rank Ennen', 'Ennen emergency services Hodges', 'Calvin color Hodges', 'Gauss taxon rank Turm', 'Turm emergency services Publié', 'Gauss color Publié', 'Lena taxon rank Gloucester', 'Gloucester emergency services Gyula', 'Lena color Gyula', 'Voltaire taxon rank Diana', 'Diana emergency services DSM', 'Voltaire color DSM', 'Polar taxon rank Bing', 'Bing emergency services Ost', 'Polar color Ost', 'Depot taxon rank Industria', 'Industria emergency services Candy', 'Depot color Candy', 'Petit taxon rank President', 'President emergency services Ted', 'Petit color Ted', 'Einstein taxon rank Sprint', 'Sprint emergency services Eaton', 'Einstein color Eaton', 'Bay taxon rank Jess', 'Jess emergency services Irving', 'Bay color Irving', 'Skin taxon rank Indianapolis', 'Indianapolis emergency services Palatinat', 'Skin color Palatinat', 'Donna taxon rank Mouse', 'Mouse emergency services Sai', 'Donna color Sai', 'Pasteur taxon rank Hindenburg', 'Hindenburg emergency services Principal', 'Pasteur color Principal', 'Seul taxon rank Shawn', 'Shawn emergency services Arrow', 'Seul color Arrow', 'Middlesex taxon rank Ehren', 'Ehren emergency services Berg', 'Middlesex color Berg', 'Closer taxon rank Florida', 'Florida emergency services Bora', 'Closer color Bora', 'Hollow taxon rank Córdoba', 'Córdoba emergency services Soleil', 'Hollow color Soleil', 'Malang taxon rank Otis', 'Otis emergency services Modena', 'Malang color Modena', 'KPD taxon rank Waves', 'Waves emergency services Odd', 'KPD color Odd', 'Palma taxon rank Special', 'Special emergency services Cullen', 'Palma color Cullen', 'AFL taxon rank Chaco', 'Chaco emergency services Nashville', 'AFL color Nashville', 'Accademia taxon rank Sally', 'Sally emergency services Guadalupe', 'Accademia color Guadalupe', 'Leafs taxon rank Midlands', 'Midlands emergency services Server', 'Leafs color Server', 'Principal taxon rank Check', 'Check emergency services Catedral', 'Principal color Catedral', 'Carla taxon rank Carlos', 'Carlos emergency services McGill', 'Carla color McGill', 'Rally taxon rank Regia', 'Regia emergency services Linden', 'Rally color Linden', 'Lilla taxon rank Records', 'Records emergency services Schlacht', 'Lilla color Schlacht', 'Gamble taxon rank Salisbury', 'Salisbury emergency services Helmut', 'Gamble color Helmut', 'Alec taxon rank Poslední', 'Poslední emergency services HP', 'Alec color HP', 'Medvedev taxon rank Brain', 'Brain emergency services Ipswich', 'Medvedev color Ipswich', 'Nino taxon rank Qara', 'Qara emergency services Den', 'Nino color Den', 'Willard taxon rank Bobby', 'Bobby emergency services Hügel', 'Willard color Hügel', 'Seoul taxon rank Potosí', 'Potosí emergency services pad', 'Seoul color pad', 'Bala taxon rank Byron', 'Byron emergency services Irena', 'Bala color Irena', 'Stift taxon rank Gary', 'Gary emergency services Estate', 'Stift color Estate', 'Albania taxon rank Henning', 'Henning emergency services EL', 'Albania color EL', 'Grund taxon rank Apollo', 'Apollo emergency services India', 'Grund color India', 'Indre taxon rank Monkey', 'Monkey emergency services Hills', 'Indre color Hills', 'Poslední taxon rank Empire', 'Empire emergency services Pan', 'Poslední color Pan', 'Switch taxon rank Robertson', 'Robertson emergency services Mint', 'Switch color Mint', 'Coral taxon rank Harcourt', 'Harcourt emergency services Capitaine', 'Coral color Capitaine', 'Ethel taxon rank Savoy', 'Savoy emergency services Coburg', 'Ethel color Coburg', 'Charlie taxon rank Lorenz', 'Lorenz emergency services Satellite', 'Charlie color Satellite', 'Warren taxon rank EP', 'EP emergency services Cameron', 'Warren color Cameron', 'Irvine taxon rank TNI', 'TNI emergency services JK', 'Irvine color JK', 'Quinta taxon rank Christensen', 'Christensen emergency services Sport', 'Quinta color Sport', 'Up taxon rank Jozef', 'Jozef emergency services Mitch', 'Up color Mitch', 'Heft taxon rank Lotto', 'Lotto emergency services Schaus', 'Heft color Schaus', 'Cap taxon rank Darling', 'Darling emergency services York', 'Cap color York', 'NN taxon rank Westminster', 'Westminster emergency services Sentinel', 'NN color Sentinel', 'Romas taxon rank Camino', 'Camino emergency services Savoy', 'Romas color Savoy', 'Benedict taxon rank Bees', 'Bees emergency services Spain', 'Benedict color Spain', 'Zurich taxon rank Friesland', 'Friesland emergency services Lucia', 'Zurich color Lucia', 'Capitaine taxon rank Use', 'Use emergency services Points', 'Capitaine color Points', 'Camilla taxon rank Hubbard', 'Hubbard emergency services Prima', 'Camilla color Prima', 'Theatre taxon rank Lego', 'Lego emergency services Siena', 'Theatre color Siena', 'Eo taxon rank Campos', 'Campos emergency services Lillehammer', 'Eo color Lillehammer', 'Kelley taxon rank Horne', 'Horne emergency services Luce', 'Kelley color Luce', 'Ola taxon rank Palestine', 'Palestine emergency services Hindi', 'Ola color Hindi', 'Gibraltar taxon rank ET', 'ET emergency services Aire', 'Gibraltar color Aire', 'Alabama taxon rank di', 'di emergency services Bund', 'Alabama color Bund', 'Revue taxon rank Lindsay', 'Lindsay emergency services Heide', 'Revue color Heide', 'Duran taxon rank Application', 'Application emergency services Horse', 'Duran color Horse', 'Guía taxon rank Elke', 'Elke emergency services Alt', 'Guía color Alt', 'Wanda taxon rank Almanya', 'Almanya emergency services Raymond', 'Wanda color Raymond', 'Piemonte taxon rank Bet', 'Bet emergency services Bari', 'Piemonte color Bari', 'Impact taxon rank Ming', 'Ming emergency services Vladimir', 'Impact color Vladimir', 'Vene taxon rank Morris', 'Morris emergency services Gates', 'Vene color Gates', 'KBS taxon rank Quintana', 'Quintana emergency services Isole', 'KBS color Isole', 'Emery taxon rank Moore', 'Moore emergency services Quatre', 'Emery color Quatre', 'Alexandria taxon rank Katy', 'Katy emergency services Darkness', 'Alexandria color Darkness', 'Bey taxon rank Data', 'Data emergency services Aya', 'Bey color Aya', 'Studi taxon rank AF', 'AF emergency services Atatürk', 'Studi color Atatürk', 'Coll taxon rank Dante', 'Dante emergency services Apocalypse', 'Coll color Apocalypse', 'Jeffries taxon rank Falling', 'Falling emergency services Cliff', 'Jeffries color Cliff', 'Forst taxon rank Acre', 'Acre emergency services Seitz', 'Forst color Seitz', 'Casablanca taxon rank Rum', 'Rum emergency services Alcalá', 'Casablanca color Alcalá', 'Kati taxon rank Oscara', 'Oscara emergency services Marburg', 'Kati color Marburg', 'Kort taxon rank Patria', 'Patria emergency services Fuego', 'Kort color Fuego', 'Teluk taxon rank Pluto', 'Pluto emergency services Sainte', 'Teluk color Sainte', 'Farrell taxon rank Membre', 'Membre emergency services Guanajuato', 'Farrell color Guanajuato', 'Ehren taxon rank Champs', 'Champs emergency services Bertram', 'Ehren color Bertram', 'Tampere taxon rank Klinik', 'Klinik emergency services Diablo', 'Tampere color Diablo', 'Kepler taxon rank Moreau', 'Moreau emergency services Isles', 'Kepler color Isles', 'Madeleine taxon rank Soldier', 'Soldier emergency services Cinema', 'Madeleine color Cinema', 'Milan taxon rank Mille', 'Mille emergency services Eaton', 'Milan color Eaton', 'Translation taxon rank Julie', 'Julie emergency services Dorothea', 'Translation color Dorothea', 'Koch taxon rank Apocalypse', 'Apocalypse emergency services Nota', 'Koch color Nota', 'Butler taxon rank Volga', 'Volga emergency services ABA', 'Butler color ABA', 'Neuchâtel taxon rank Terminal', 'Terminal emergency services Malone', 'Neuchâtel color Malone', 'Delaware taxon rank EV', 'EV emergency services Soccerway', 'Delaware color Soccerway', 'Wittenberg taxon rank Amadeus', 'Amadeus emergency services President', 'Wittenberg color President', 'Cidade taxon rank Simpson', 'Simpson emergency services Mitch', 'Cidade color Mitch', 'Murphy taxon rank Jang', 'Jang emergency services Hütte', 'Murphy color Hütte', 'su taxon rank Kosmos', 'Kosmos emergency services Bohemia', 'su color Bohemia', 'Ses taxon rank Lyman', 'Lyman emergency services Ravenna', 'Ses color Ravenna', 'Progreso taxon rank Lakes', 'Lakes emergency services Shri', 'Progreso color Shri', 'Curie taxon rank Seite', 'Seite emergency services Delgado', 'Curie color Delgado', 'Ned taxon rank Antoinette', 'Antoinette emergency services Midway', 'Ned color Midway', 'Zealand taxon rank Henley', 'Henley emergency services Od', 'Zealand color Od', 'Bennett taxon rank Cruise', 'Cruise emergency services Missouri', 'Bennett color Missouri', 'Porsche taxon rank Ennen', 'Ennen emergency services Hodges', 'Porsche color Hodges', 'Villiers taxon rank Turm', 'Turm emergency services Publié', 'Villiers color Publié', 'Niño taxon rank Gloucester', 'Gloucester emergency services Gyula', 'Niño color Gyula', 'Balance taxon rank Diana', 'Diana emergency services DSM', 'Balance color DSM', 'Barth taxon rank Bing', 'Bing emergency services Ost', 'Barth color Ost', 'Robot taxon rank Industria', 'Industria emergency services Candy', 'Robot color Candy', 'Sinh taxon rank President', 'President emergency services Ted', 'Sinh color Ted', 'Gillespie taxon rank Sprint', 'Sprint emergency services Eaton', 'Gillespie color Eaton', 'Titan taxon rank Jess', 'Jess emergency services Irving', 'Titan color Irving', 'Tierra taxon rank Indianapolis', 'Indianapolis emergency services Palatinat', 'Tierra color Palatinat', 'ID taxon rank Mouse', 'Mouse emergency services Sai', 'ID color Sai', 'WWF taxon rank Hindenburg', 'Hindenburg emergency services Principal', 'WWF color Principal', 'Azur taxon rank Shawn', 'Shawn emergency services Arrow', 'Azur color Arrow', 'Reason taxon rank Ehren', 'Ehren emergency services Berg', 'Reason color Berg', 'Luke taxon rank Florida', 'Florida emergency services Bora', 'Luke color Bora', 'Trees taxon rank Córdoba', 'Córdoba emergency services Soleil', 'Trees color Soleil', 'Morris taxon rank Otis', 'Otis emergency services Modena', 'Morris color Modena', 'Monate taxon rank Waves', 'Waves emergency services Odd', 'Monate color Odd', 'Norway taxon rank Special', 'Special emergency services Cullen', 'Norway color Cullen', 'Han taxon rank Chaco', 'Chaco emergency services Nashville', 'Han color Nashville', 'Cuban taxon rank Sally', 'Sally emergency services Guadalupe', 'Cuban color Guadalupe', 'Melbourne taxon rank Midlands', 'Midlands emergency services Server', 'Melbourne color Server', 'Rooma taxon rank Check', 'Check emergency services Catedral', 'Rooma color Catedral', 'Ad taxon rank Carlos', 'Carlos emergency services McGill', 'Ad color McGill', 'Christ taxon rank Regia', 'Regia emergency services Linden', 'Christ color Linden', 'PSA taxon rank Records', 'Records emergency services Schlacht', 'PSA color Schlacht', 'Omer taxon rank Salisbury', 'Salisbury emergency services Helmut', 'Omer color Helmut', 'THE taxon rank Poslední', 'Poslední emergency services HP', 'THE color HP', 'Hand taxon rank Brain', 'Brain emergency services Ipswich', 'Hand color Ipswich', 'Urgell taxon rank Qara', 'Qara emergency services Den', 'Urgell color Den', 'Liv taxon rank Bobby', 'Bobby emergency services Hügel', 'Liv color Hügel', 'Bonaparte taxon rank Potosí', 'Potosí emergency services pad', 'Bonaparte color pad', 'Tempo taxon rank Byron', 'Byron emergency services Irena', 'Tempo color Irena', 'Abel taxon rank Gary', 'Gary emergency services Estate', 'Abel color Estate', 'Gegen taxon rank Henning', 'Henning emergency services EL', 'Gegen color EL', 'Hoy taxon rank Apollo', 'Apollo emergency services India', 'Hoy color India', 'Stil taxon rank Monkey', 'Monkey emergency services Hills', 'Stil color Hills', 'CDP taxon rank Empire', 'Empire emergency services Pan', 'CDP color Pan', 'Kanal taxon rank Robertson', 'Robertson emergency services Mint', 'Kanal color Mint', 'Izrael taxon rank Harcourt', 'Harcourt emergency services Capitaine', 'Izrael color Capitaine', 'Genesis taxon rank Savoy', 'Savoy emergency services Coburg', 'Genesis color Coburg', 'Eylül taxon rank Lorenz', 'Lorenz emergency services Satellite', 'Eylül color Satellite', 'Vivaldi taxon rank EP', 'EP emergency services Cameron', 'Vivaldi color Cameron', 'Mario taxon rank TNI', 'TNI emergency services JK', 'Mario color JK', 'Power taxon rank Christensen', 'Christensen emergency services Sport', 'Power color Sport', 'Saussure taxon rank Jozef', 'Jozef emergency services Mitch', 'Saussure color Mitch', 'Muir taxon rank Lotto', 'Lotto emergency services Schaus', 'Muir color Schaus', 'Alonso taxon rank Darling', 'Darling emergency services York', 'Alonso color York', 'Largo taxon rank Westminster', 'Westminster emergency services Sentinel', 'Largo color Sentinel', 'Phi taxon rank Camino', 'Camino emergency services Savoy', 'Phi color Savoy', 'While taxon rank Bees', 'Bees emergency services Spain', 'While color Spain', 'Rain taxon rank Friesland', 'Friesland emergency services Lucia', 'Rain color Lucia', 'Canary taxon rank Use', 'Use emergency services Points', 'Canary color Points', 'Arms taxon rank Hubbard', 'Hubbard emergency services Prima', 'Arms color Prima', 'Bil taxon rank Lego', 'Lego emergency services Siena', 'Bil color Siena', 'Cause taxon rank Campos', 'Campos emergency services Lillehammer', 'Cause color Lillehammer', 'Hitler taxon rank Horne', 'Horne emergency services Luce', 'Hitler color Luce', 'Kimberly taxon rank Palestine', 'Palestine emergency services Hindi', 'Kimberly color Hindi', 'Swan taxon rank ET', 'ET emergency services Aire', 'Swan color Aire', 'Parker taxon rank di', 'di emergency services Bund', 'Parker color Bund', 'WDR taxon rank Lindsay', 'Lindsay emergency services Heide', 'WDR color Heide', 'Gama taxon rank Application', 'Application emergency services Horse', 'Gama color Horse', 'Pam taxon rank Elke', 'Elke emergency services Alt', 'Pam color Alt', 'Leif taxon rank Almanya', 'Almanya emergency services Raymond', 'Leif color Raymond', 'IM taxon rank Bet', 'Bet emergency services Bari', 'IM color Bari', 'Comte taxon rank Ming', 'Ming emergency services Vladimir', 'Comte color Vladimir', 'Stella taxon rank Morris', 'Morris emergency services Gates', 'Stella color Gates', 'Speedway taxon rank Quintana', 'Quintana emergency services Isole', 'Speedway color Isole', 'Linda taxon rank Moore', 'Moore emergency services Quatre', 'Linda color Quatre', 'Dok taxon rank Katy', 'Katy emergency services Darkness', 'Dok color Darkness', 'Earth taxon rank Data', 'Data emergency services Aya', 'Earth color Aya', 'Ele taxon rank AF', 'AF emergency services Atatürk', 'Ele color Atatürk', 'Limited taxon rank Dante', 'Dante emergency services Apocalypse', 'Limited color Apocalypse', 'GM taxon rank Falling', 'Falling emergency services Cliff', 'GM color Cliff', 'Isola taxon rank Acre', 'Acre emergency services Seitz', 'Isola color Seitz', 'Kahn taxon rank Rum', 'Rum emergency services Alcalá', 'Kahn color Alcalá', 'Slavic taxon rank Oscara', 'Oscara emergency services Marburg', 'Slavic color Marburg', 'Subway taxon rank Patria', 'Patria emergency services Fuego', 'Subway color Fuego', 'Tracy taxon rank Pluto', 'Pluto emergency services Sainte', 'Tracy color Sainte', 'Stage taxon rank Membre', 'Membre emergency services Guanajuato', 'Stage color Guanajuato', 'Rady taxon rank Champs', 'Champs emergency services Bertram', 'Rady color Bertram', 'Tanner taxon rank Klinik', 'Klinik emergency services Diablo', 'Tanner color Diablo', 'Newport taxon rank Moreau', 'Moreau emergency services Isles', 'Newport color Isles', 'Holt taxon rank Soldier', 'Soldier emergency services Cinema', 'Holt color Cinema', 'Ion taxon rank Mille', 'Mille emergency services Eaton', 'Ion color Eaton', 'Amigos taxon rank Julie', 'Julie emergency services Dorothea', 'Amigos color Dorothea', 'Bruges taxon rank Apocalypse', 'Apocalypse emergency services Nota', 'Bruges color Nota', 'Astra taxon rank Volga', 'Volga emergency services ABA', 'Astra color ABA', 'KK taxon rank Terminal', 'Terminal emergency services Malone', 'KK color Malone', 'Alliance taxon rank EV', 'EV emergency services Soccerway', 'Alliance color Soccerway', 'Dana taxon rank Amadeus', 'Amadeus emergency services President', 'Dana color President', 'Rep taxon rank Simpson', 'Simpson emergency services Mitch', 'Rep color Mitch', 'Albin taxon rank Jang', 'Jang emergency services Hütte', 'Albin color Hütte', 'Nissan taxon rank Kosmos', 'Kosmos emergency services Bohemia', 'Nissan color Bohemia', 'Aberdeen taxon rank Lyman', 'Lyman emergency services Ravenna', 'Aberdeen color Ravenna', 'Az taxon rank Lakes', 'Lakes emergency services Shri', 'Az color Shri', 'Jenna taxon rank Seite', 'Seite emergency services Delgado', 'Jenna color Delgado', 'Hamlet taxon rank Antoinette', 'Antoinette emergency services Midway', 'Hamlet color Midway', 'Pretoria taxon rank Henley', 'Henley emergency services Od', 'Pretoria color Od', 'Latin taxon rank Cruise', 'Cruise emergency services Missouri', 'Latin color Missouri', 'ESPN taxon rank Ennen', 'Ennen emergency services Hodges', 'ESPN color Hodges', 'Florence taxon rank Turm', 'Turm emergency services Publié', 'Florence color Publié', 'Philippe taxon rank Gloucester', 'Gloucester emergency services Gyula', 'Philippe color Gyula', 'Phelps taxon rank Diana', 'Diana emergency services DSM', 'Phelps color DSM', 'Zoom taxon rank Bing', 'Bing emergency services Ost', 'Zoom color Ost', 'Isto taxon rank Industria', 'Industria emergency services Candy', 'Isto color Candy', 'Sporting taxon rank President', 'President emergency services Ted', 'Sporting color Ted', 'Star taxon rank Sprint', 'Sprint emergency services Eaton', 'Star color Eaton', 'Padre taxon rank Jess', 'Jess emergency services Irving', 'Padre color Irving', 'Perth taxon rank Indianapolis', 'Indianapolis emergency services Palatinat', 'Perth color Palatinat', 'Cecil taxon rank Mouse', 'Mouse emergency services Sai', 'Cecil color Sai', 'Rec taxon rank Hindenburg', 'Hindenburg emergency services Principal', 'Rec color Principal', 'Katrina taxon rank Shawn', 'Shawn emergency services Arrow', 'Katrina color Arrow', 'Lincoln taxon rank Ehren', 'Ehren emergency services Berg', 'Lincoln color Berg', 'Xuân taxon rank Florida', 'Florida emergency services Bora', 'Xuân color Bora', 'Pet taxon rank Córdoba', 'Córdoba emergency services Soleil', 'Pet color Soleil', 'Hora taxon rank Otis', 'Otis emergency services Modena', 'Hora color Modena', 'Heads taxon rank Waves', 'Waves emergency services Odd', 'Heads color Odd', 'Coimbra taxon rank Special', 'Special emergency services Cullen', 'Coimbra color Cullen', 'Magister taxon rank Chaco', 'Chaco emergency services Nashville', 'Magister color Nashville', 'Kristen taxon rank Sally', 'Sally emergency services Guadalupe', 'Kristen color Guadalupe', 'Benson taxon rank Midlands', 'Midlands emergency services Server', 'Benson color Server', 'ES taxon rank Check', 'Check emergency services Catedral', 'ES color Catedral', 'Ayn taxon rank Carlos', 'Carlos emergency services McGill', 'Ayn color McGill', 'Cu taxon rank Regia', 'Regia emergency services Linden', 'Cu color Linden', 'Champs taxon rank Records', 'Records emergency services Schlacht', 'Champs color Schlacht', 'Lydia taxon rank Salisbury', 'Salisbury emergency services Helmut', 'Lydia color Helmut', 'Greg taxon rank Poslední', 'Poslední emergency services HP', 'Greg color HP', 'Fortaleza taxon rank Brain', 'Brain emergency services Ipswich', 'Fortaleza color Ipswich', 'TSV taxon rank Qara', 'Qara emergency services Den', 'TSV color Den', 'Bulu taxon rank Bobby', 'Bobby emergency services Hügel', 'Bulu color Hügel', 'Finale taxon rank Potosí', 'Potosí emergency services pad', 'Finale color pad', 'Mitt taxon rank Byron', 'Byron emergency services Irena', 'Mitt color Irena', 'Lyman taxon rank Gary', 'Gary emergency services Estate', 'Lyman color Estate', 'Zeeland taxon rank Henning', 'Henning emergency services EL', 'Zeeland color EL', 'Algeria taxon rank Apollo', 'Apollo emergency services India', 'Algeria color India', 'Dada taxon rank Monkey', 'Monkey emergency services Hills', 'Dada color Hills', 'Sheridan taxon rank Empire', 'Empire emergency services Pan', 'Sheridan color Pan', 'Pinto taxon rank Robertson', 'Robertson emergency services Mint', 'Pinto color Mint', 'Kelly taxon rank Harcourt', 'Harcourt emergency services Capitaine', 'Kelly color Capitaine', 'Soria taxon rank Savoy', 'Savoy emergency services Coburg', 'Soria color Coburg', 'Fairfax taxon rank Lorenz', 'Lorenz emergency services Satellite', 'Fairfax color Satellite', 'Tutte taxon rank EP', 'EP emergency services Cameron', 'Tutte color Cameron', 'Cent taxon rank TNI', 'TNI emergency services JK', 'Cent color JK', 'AG taxon rank Christensen', 'Christensen emergency services Sport', 'AG color Sport', 'Fund taxon rank Jozef', 'Jozef emergency services Mitch', 'Fund color Mitch', 'Louisville taxon rank Lotto', 'Lotto emergency services Schaus', 'Louisville color Schaus', 'Veracruz taxon rank Darling', 'Darling emergency services York', 'Veracruz color York', 'Sabbath taxon rank Westminster', 'Westminster emergency services Sentinel', 'Sabbath color Sentinel', 'Duck taxon rank Camino', 'Camino emergency services Savoy', 'Duck color Savoy', 'Jack taxon rank Bees', 'Bees emergency services Spain', 'Jack color Spain', 'IRAS taxon rank Friesland', 'Friesland emergency services Lucia', 'IRAS color Lucia', 'Cherokee taxon rank Use', 'Use emergency services Points', 'Cherokee color Points', 'Oper taxon rank Hubbard', 'Hubbard emergency services Prima', 'Oper color Prima', 'Raphaël taxon rank Lego', 'Lego emergency services Siena', 'Raphaël color Siena', 'Women taxon rank Campos', 'Campos emergency services Lillehammer', 'Women color Lillehammer', 'Patton taxon rank Horne', 'Horne emergency services Luce', 'Patton color Luce', 'Al taxon rank Palestine', 'Palestine emergency services Hindi', 'Al color Hindi', 'Oaxaca taxon rank ET', 'ET emergency services Aire', 'Oaxaca color Aire', 'Direito taxon rank di', 'di emergency services Bund', 'Direito color Bund', 'Province taxon rank Lindsay', 'Lindsay emergency services Heide', 'Province color Heide', 'Grimaldi taxon rank Application', 'Application emergency services Horse', 'Grimaldi color Horse', 'Ruska taxon rank Elke', 'Elke emergency services Alt', 'Ruska color Alt', 'Jess taxon rank Almanya', 'Almanya emergency services Raymond', 'Jess color Raymond', 'Lugar taxon rank Bet', 'Bet emergency services Bari', 'Lugar color Bari', 'Paul taxon rank Ming', 'Ming emergency services Vladimir', 'Paul color Vladimir', 'CAD taxon rank Morris', 'Morris emergency services Gates', 'CAD color Gates', 'Adrian taxon rank Quintana', 'Quintana emergency services Isole', 'Adrian color Isole', 'Color taxon rank Moore', 'Moore emergency services Quatre', 'Color color Quatre', 'Thornton taxon rank Katy', 'Katy emergency services Darkness', 'Thornton color Darkness', 'Kampung taxon rank Data', 'Data emergency services Aya', 'Kampung color Aya', 'Britten taxon rank AF', 'AF emergency services Atatürk', 'Britten color Atatürk', 'Eagle taxon rank Dante', 'Dante emergency services Apocalypse', 'Eagle color Apocalypse', 'Spirit taxon rank Falling', 'Falling emergency services Cliff', 'Spirit color Cliff', 'Command taxon rank Acre', 'Acre emergency services Seitz', 'Command color Seitz', 'Day taxon rank Rum', 'Rum emergency services Alcalá', 'Day color Alcalá', 'America taxon rank Oscara', 'Oscara emergency services Marburg', 'America color Marburg', 'Blanca taxon rank Patria', 'Patria emergency services Fuego', 'Blanca color Fuego', 'Sherlock taxon rank Pluto', 'Pluto emergency services Sainte', 'Sherlock color Sainte', 'Qara taxon rank Membre', 'Membre emergency services Guanajuato', 'Qara color Guanajuato', 'Enterprise taxon rank Champs', 'Champs emergency services Bertram', 'Enterprise color Bertram', 'Pedra taxon rank Klinik', 'Klinik emergency services Diablo', 'Pedra color Diablo', 'Milli taxon rank Moreau', 'Moreau emergency services Isles', 'Milli color Isles', 'Lost taxon rank Soldier', 'Soldier emergency services Cinema', 'Lost color Cinema', 'Johnny taxon rank Mille', 'Mille emergency services Eaton', 'Johnny color Eaton', 'Macau taxon rank Julie', 'Julie emergency services Dorothea', 'Macau color Dorothea', 'Manhattan taxon rank Apocalypse', 'Apocalypse emergency services Nota', 'Manhattan color Nota', 'Monde taxon rank Volga', 'Volga emergency services ABA', 'Monde color ABA', 'Turquia taxon rank Terminal', 'Terminal emergency services Malone', 'Turquia color Malone', 'Cast taxon rank EV', 'EV emergency services Soccerway', 'Cast color Soccerway', 'Balkan taxon rank Amadeus', 'Amadeus emergency services President', 'Balkan color President', 'Mountain taxon rank Simpson', 'Simpson emergency services Mitch', 'Mountain color Mitch', 'Madonna taxon rank Jang', 'Jang emergency services Hütte', 'Madonna color Hütte', 'Huelva taxon rank Kosmos', 'Kosmos emergency services Bohemia', 'Huelva color Bohemia', 'Brady taxon rank Lyman', 'Lyman emergency services Ravenna', 'Brady color Ravenna', 'Mustang taxon rank Lakes', 'Lakes emergency services Shri', 'Mustang color Shri', 'Caen taxon rank Seite', 'Seite emergency services Delgado', 'Caen color Delgado', 'Wert taxon rank Antoinette', 'Antoinette emergency services Midway', 'Wert color Midway', 'Savage taxon rank Henley', 'Henley emergency services Od', 'Savage color Od', 'WBA taxon rank Cruise', 'Cruise emergency services Missouri', 'WBA color Missouri', 'Elias taxon rank Ennen', 'Ennen emergency services Hodges', 'Elias color Hodges', 'Houten taxon rank Turm', 'Turm emergency services Publié', 'Houten color Publié', 'Rogers taxon rank Gloucester', 'Gloucester emergency services Gyula', 'Rogers color Gyula', 'Change taxon rank Diana', 'Diana emergency services DSM', 'Change color DSM', 'WRC taxon rank Bing', 'Bing emergency services Ost', 'WRC color Ost', 'Gia taxon rank Industria', 'Industria emergency services Candy', 'Gia color Candy', 'Jason taxon rank President', 'President emergency services Ted', 'Jason color Ted', 'Th taxon rank Sprint', 'Sprint emergency services Eaton', 'Th color Eaton', 'Prairie taxon rank Jess', 'Jess emergency services Irving', 'Prairie color Irving', 'Tyne taxon rank Indianapolis', 'Indianapolis emergency services Palatinat', 'Tyne color Palatinat', 'Culture taxon rank Mouse', 'Mouse emergency services Sai', 'Culture color Sai', 'Rang taxon rank Hindenburg', 'Hindenburg emergency services Principal', 'Rang color Principal', 'Ph taxon rank Shawn', 'Shawn emergency services Arrow', 'Ph color Arrow', 'Macbeth taxon rank Ehren', 'Ehren emergency services Berg', 'Macbeth color Berg', 'KHL taxon rank Florida', 'Florida emergency services Bora', 'KHL color Bora', 'Escape taxon rank Córdoba', 'Córdoba emergency services Soleil', 'Escape color Soleil', 'Racine taxon rank Otis', 'Otis emergency services Modena', 'Racine color Modena', 'Alus taxon rank Waves', 'Waves emergency services Odd', 'Alus color Odd', 'Jenny taxon rank Special', 'Special emergency services Cullen', 'Jenny color Cullen', 'Conquest taxon rank Chaco', 'Chaco emergency services Nashville', 'Conquest color Nashville', 'Bambino taxon rank Sally', 'Sally emergency services Guadalupe', 'Bambino color Guadalupe', 'Hagen taxon rank Midlands', 'Midlands emergency services Server']\n"
     ]
    }
   ],
   "source": [
    "print(train_dict['sample'][:1901])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c698760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Haji Farbe Malone',\n",
       " 'Yahoo Farbe Soccerway',\n",
       " 'Stal Farbe President',\n",
       " 'FC Farbe Mitch',\n",
       " 'Dad Farbe Hütte',\n",
       " 'Kenia Farbe Bohemia',\n",
       " 'CCD Farbe Ravenna',\n",
       " 'Riau Farbe Shri',\n",
       " 'Ky Farbe Delgado',\n",
       " 'Billie Farbe Midway',\n",
       " 'Elbe Farbe Od',\n",
       " 'Paraíso Farbe Missouri',\n",
       " 'TD Farbe Hodges',\n",
       " 'Luther Farbe Publié',\n",
       " 'Roi Farbe Gyula',\n",
       " 'Pole Farbe DSM',\n",
       " 'Page Farbe Ost',\n",
       " 'Baron Farbe Candy',\n",
       " 'Libia Farbe Ted',\n",
       " 'Cuenca Farbe Eaton',\n",
       " 'Kálmán Farbe Irving',\n",
       " 'ET Farbe Palatinat',\n",
       " 'Mainstream Farbe Sai',\n",
       " 'Agency Farbe Principal',\n",
       " 'Mata Farbe Arrow',\n",
       " 'Mineral Farbe Berg',\n",
       " 'Figaro Farbe Bora',\n",
       " 'Trung Farbe Soleil',\n",
       " 'Sabha Farbe Modena',\n",
       " 'Guimarães Farbe Odd',\n",
       " 'Disneyland Farbe Cullen',\n",
       " 'Wes Farbe Nashville',\n",
       " 'Gesù Farbe Guadalupe',\n",
       " 'Cinq Farbe Server',\n",
       " 'Silla Farbe Catedral',\n",
       " 'JR Farbe McGill',\n",
       " 'Bryant Farbe Linden',\n",
       " 'Klaus Farbe Schlacht',\n",
       " 'Kleiner Farbe Helmut',\n",
       " 'Fort Farbe HP',\n",
       " 'City Farbe Ipswich',\n",
       " 'Márquez Farbe Den',\n",
       " 'Yer Farbe Hügel',\n",
       " 'Bernard Farbe pad',\n",
       " 'Diaz Farbe Irena',\n",
       " 'Madre Farbe Estate',\n",
       " 'Asturias Farbe EL',\n",
       " 'FF Farbe India',\n",
       " 'Dis Farbe Hills',\n",
       " 'Lorentz Farbe Pan',\n",
       " 'SM Farbe Mint',\n",
       " 'Sempre Farbe Capitaine',\n",
       " 'Portsmouth Farbe Coburg',\n",
       " 'Irena Farbe Satellite',\n",
       " 'Oaks Farbe Cameron',\n",
       " 'Fighting Farbe JK',\n",
       " 'Bilbao Farbe Sport',\n",
       " 'Sharon Farbe Mitch',\n",
       " 'Allium Farbe Schaus',\n",
       " 'Haiti Farbe York',\n",
       " 'Isabella Farbe Sentinel',\n",
       " 'Olsson Farbe Savoy',\n",
       " 'Pure Farbe Spain',\n",
       " 'Patrol Farbe Lucia',\n",
       " 'Siege Farbe Points',\n",
       " 'Galiza Farbe Prima',\n",
       " 'Lucky Farbe Siena',\n",
       " 'Pfeiffer Farbe Lillehammer',\n",
       " 'FX Farbe Luce',\n",
       " 'Energy Farbe Hindi',\n",
       " 'Joanne Farbe Aire',\n",
       " 'Erik Farbe Bund',\n",
       " 'EM Farbe Heide',\n",
       " 'Raum Farbe Horse',\n",
       " 'VL Farbe Alt',\n",
       " 'Sabina Farbe Raymond',\n",
       " 'Krupp Farbe Bari',\n",
       " 'October Farbe Vladimir',\n",
       " 'Davidson Farbe Gates',\n",
       " 'Allah Farbe Isole',\n",
       " 'Larva Farbe Quatre',\n",
       " 'Khu Farbe Darkness',\n",
       " 'Lahore Farbe Aya',\n",
       " 'Dorset Farbe Atatürk',\n",
       " 'Stay Farbe Apocalypse',\n",
       " 'Sit Farbe Cliff',\n",
       " 'Wayne Farbe Seitz',\n",
       " 'Flag Farbe Alcalá',\n",
       " 'Ferns Farbe Marburg',\n",
       " 'Jaguar Farbe Fuego',\n",
       " 'Brenda Farbe Sainte',\n",
       " 'Mina Farbe Guanajuato',\n",
       " 'Harding Farbe Bertram',\n",
       " 'Giants Farbe Diablo',\n",
       " 'Buta Farbe Isles',\n",
       " 'Faye Farbe Cinema',\n",
       " 'Generation Farbe Eaton',\n",
       " 'Schottland Farbe Dorothea',\n",
       " 'USD Farbe Nota',\n",
       " 'Eu Farbe ABA',\n",
       " 'Haji geöffnet von Zeitpunkt Ludovic',\n",
       " 'Yahoo geöffnet von Zeitpunkt WBC',\n",
       " 'Stal geöffnet von Zeitpunkt Randall',\n",
       " 'FC geöffnet von Zeitpunkt Blonde',\n",
       " 'Dad geöffnet von Zeitpunkt Else',\n",
       " 'Kenia geöffnet von Zeitpunkt Vienne',\n",
       " 'CCD geöffnet von Zeitpunkt Addis',\n",
       " 'Riau geöffnet von Zeitpunkt Rail',\n",
       " 'Ky geöffnet von Zeitpunkt Bande',\n",
       " 'Billie geöffnet von Zeitpunkt NN',\n",
       " 'Elbe geöffnet von Zeitpunkt FL',\n",
       " 'Paraíso geöffnet von Zeitpunkt BM',\n",
       " 'TD geöffnet von Zeitpunkt Eo',\n",
       " 'Luther geöffnet von Zeitpunkt Pretoria',\n",
       " 'Roi geöffnet von Zeitpunkt Giro',\n",
       " 'Pole geöffnet von Zeitpunkt Duty',\n",
       " 'Page geöffnet von Zeitpunkt Zanzibar',\n",
       " 'Baron geöffnet von Zeitpunkt Buda',\n",
       " 'Libia geöffnet von Zeitpunkt Barrow',\n",
       " 'Cuenca geöffnet von Zeitpunkt Desert',\n",
       " 'Kálmán geöffnet von Zeitpunkt Ship',\n",
       " 'ET geöffnet von Zeitpunkt Direito',\n",
       " 'Mainstream geöffnet von Zeitpunkt Aragón',\n",
       " 'Agency geöffnet von Zeitpunkt Asylum',\n",
       " 'Mata geöffnet von Zeitpunkt Poco',\n",
       " 'Mineral geöffnet von Zeitpunkt Cambridge',\n",
       " 'Figaro geöffnet von Zeitpunkt Churchill',\n",
       " 'Trung geöffnet von Zeitpunkt Bund',\n",
       " 'Sabha geöffnet von Zeitpunkt Vela',\n",
       " 'Guimarães geöffnet von Zeitpunkt Saskatchewan',\n",
       " 'Disneyland geöffnet von Zeitpunkt KZ',\n",
       " 'Wes geöffnet von Zeitpunkt Salman',\n",
       " 'Gesù geöffnet von Zeitpunkt Passo',\n",
       " 'Cinq geöffnet von Zeitpunkt Nissan',\n",
       " 'Silla geöffnet von Zeitpunkt Duck',\n",
       " 'JR geöffnet von Zeitpunkt Muell',\n",
       " 'Bryant geöffnet von Zeitpunkt Beck',\n",
       " 'Klaus geöffnet von Zeitpunkt Lok',\n",
       " 'Kleiner geöffnet von Zeitpunkt Dmitri',\n",
       " 'Fort geöffnet von Zeitpunkt Voltaire',\n",
       " 'City geöffnet von Zeitpunkt Al',\n",
       " 'Márquez geöffnet von Zeitpunkt Snow',\n",
       " 'Yer geöffnet von Zeitpunkt Vu',\n",
       " 'Bernard geöffnet von Zeitpunkt Alec',\n",
       " 'Diaz geöffnet von Zeitpunkt Jeffries',\n",
       " 'Madre geöffnet von Zeitpunkt Gould',\n",
       " 'Asturias geöffnet von Zeitpunkt Yunan',\n",
       " 'FF geöffnet von Zeitpunkt Telegraph',\n",
       " 'Dis geöffnet von Zeitpunkt Bavière',\n",
       " 'Lorentz geöffnet von Zeitpunkt Gerd',\n",
       " 'SM geöffnet von Zeitpunkt KHL',\n",
       " 'Sempre geöffnet von Zeitpunkt Fulham',\n",
       " 'Portsmouth geöffnet von Zeitpunkt Ashton',\n",
       " 'Irena geöffnet von Zeitpunkt Nixon',\n",
       " 'Oaks geöffnet von Zeitpunkt Apollo',\n",
       " 'Fighting geöffnet von Zeitpunkt Hey',\n",
       " 'Bilbao geöffnet von Zeitpunkt Holz',\n",
       " 'Sharon geöffnet von Zeitpunkt Poland',\n",
       " 'Allium geöffnet von Zeitpunkt Canadian',\n",
       " 'Haiti geöffnet von Zeitpunkt Esch',\n",
       " 'Isabella geöffnet von Zeitpunkt Berliner',\n",
       " 'Olsson geöffnet von Zeitpunkt Banská',\n",
       " 'Pure geöffnet von Zeitpunkt Cor',\n",
       " 'Patrol geöffnet von Zeitpunkt Morocco',\n",
       " 'Siege geöffnet von Zeitpunkt Chatham',\n",
       " 'Galiza geöffnet von Zeitpunkt Shock',\n",
       " 'Lucky geöffnet von Zeitpunkt Orne',\n",
       " 'Pfeiffer geöffnet von Zeitpunkt Malone',\n",
       " 'FX geöffnet von Zeitpunkt Kosova',\n",
       " 'Energy geöffnet von Zeitpunkt Surat',\n",
       " 'Joanne geöffnet von Zeitpunkt Ros',\n",
       " 'Erik geöffnet von Zeitpunkt Bursa',\n",
       " 'EM geöffnet von Zeitpunkt Poison',\n",
       " 'Raum geöffnet von Zeitpunkt Bug',\n",
       " 'VL geöffnet von Zeitpunkt Kawas',\n",
       " 'Sabina geöffnet von Zeitpunkt Lourdes',\n",
       " 'Krupp geöffnet von Zeitpunkt Vai',\n",
       " 'October geöffnet von Zeitpunkt Cuenca',\n",
       " 'Davidson geöffnet von Zeitpunkt Odin',\n",
       " 'Allah geöffnet von Zeitpunkt Tabachnick',\n",
       " 'Larva geöffnet von Zeitpunkt Lucia',\n",
       " 'Khu geöffnet von Zeitpunkt Dentro',\n",
       " 'Lahore geöffnet von Zeitpunkt Luke',\n",
       " 'Dorset geöffnet von Zeitpunkt Ride',\n",
       " 'Stay geöffnet von Zeitpunkt Pokémon',\n",
       " 'Sit geöffnet von Zeitpunkt Botafogo',\n",
       " 'Wayne geöffnet von Zeitpunkt Stettin',\n",
       " 'Flag geöffnet von Zeitpunkt RT',\n",
       " 'Ferns geöffnet von Zeitpunkt Bonnet',\n",
       " 'Jaguar geöffnet von Zeitpunkt Milli',\n",
       " 'Brenda geöffnet von Zeitpunkt Loan',\n",
       " 'Mina geöffnet von Zeitpunkt Bar',\n",
       " 'Harding geöffnet von Zeitpunkt Danmark',\n",
       " 'Giants geöffnet von Zeitpunkt Kepler',\n",
       " 'Buta geöffnet von Zeitpunkt Kirkwood',\n",
       " 'Faye geöffnet von Zeitpunkt Talent',\n",
       " 'Generation geöffnet von Zeitpunkt Sven',\n",
       " 'Schottland geöffnet von Zeitpunkt Isabella',\n",
       " 'USD geöffnet von Zeitpunkt Bush',\n",
       " 'Eu geöffnet von Zeitpunkt Krone',\n",
       " 'Haji Medien-Franchise Paso',\n",
       " 'Yahoo Medien-Franchise Orléans',\n",
       " 'Stal Medien-Franchise Photography',\n",
       " 'FC Medien-Franchise Austin',\n",
       " 'Dad Medien-Franchise Albion',\n",
       " 'Kenia Medien-Franchise Florencia',\n",
       " 'CCD Medien-Franchise PRL',\n",
       " 'Riau Medien-Franchise Talent',\n",
       " 'Ky Medien-Franchise Viola',\n",
       " 'Billie Medien-Franchise Nos',\n",
       " 'Elbe Medien-Franchise Secrets',\n",
       " 'Paraíso Medien-Franchise Erin',\n",
       " 'TD Medien-Franchise Genel',\n",
       " 'Luther Medien-Franchise Avant',\n",
       " 'Roi Medien-Franchise Regular',\n",
       " 'Pole Medien-Franchise Kazan',\n",
       " 'Page Medien-Franchise Tutte',\n",
       " 'Baron Medien-Franchise Reese',\n",
       " 'Libia Medien-Franchise Disse',\n",
       " 'Cuenca Medien-Franchise Zelanda',\n",
       " 'Kálmán Medien-Franchise Billy',\n",
       " 'ET Medien-Franchise Forma',\n",
       " 'Mainstream Medien-Franchise Carlisle',\n",
       " 'Agency Medien-Franchise DK',\n",
       " 'Mata Medien-Franchise Silvio',\n",
       " 'Mineral Medien-Franchise Senegal',\n",
       " 'Figaro Medien-Franchise Schweden',\n",
       " 'Trung Medien-Franchise PGA',\n",
       " 'Sabha Medien-Franchise Rathaus',\n",
       " 'Guimarães Medien-Franchise Wish',\n",
       " 'Disneyland Medien-Franchise Yukon',\n",
       " 'Wes Medien-Franchise Harri',\n",
       " 'Gesù Medien-Franchise Truman',\n",
       " 'Cinq Medien-Franchise Peterborough',\n",
       " 'Silla Medien-Franchise Wing',\n",
       " 'JR Medien-Franchise Artur',\n",
       " 'Bryant Medien-Franchise Cent',\n",
       " 'Klaus Medien-Franchise Castello',\n",
       " 'Kleiner Medien-Franchise Qua',\n",
       " 'Fort Medien-Franchise Alt',\n",
       " 'City Medien-Franchise Tribune',\n",
       " 'Márquez Medien-Franchise Charlotte',\n",
       " 'Yer Medien-Franchise Castelo',\n",
       " 'Bernard Medien-Franchise Trang',\n",
       " 'Diaz Medien-Franchise Alexis',\n",
       " 'Madre Medien-Franchise Irwin',\n",
       " 'Asturias Medien-Franchise Nizza',\n",
       " 'FF Medien-Franchise Monza',\n",
       " 'Dis Medien-Franchise Torpedo',\n",
       " 'Lorentz Medien-Franchise Teil',\n",
       " 'SM Medien-Franchise Homme',\n",
       " 'Sempre Medien-Franchise Heidelberg',\n",
       " 'Portsmouth Medien-Franchise Zeus',\n",
       " 'Irena Medien-Franchise Tunnel',\n",
       " 'Oaks Medien-Franchise Boga',\n",
       " 'Fighting Medien-Franchise Benedict',\n",
       " 'Bilbao Medien-Franchise Die',\n",
       " 'Sharon Medien-Franchise Margareta',\n",
       " 'Allium Medien-Franchise Mem',\n",
       " 'Haiti Medien-Franchise Aur',\n",
       " 'Isabella Medien-Franchise Sims',\n",
       " 'Olsson Medien-Franchise Res',\n",
       " 'Pure Medien-Franchise Canadian',\n",
       " 'Patrol Medien-Franchise Vallée',\n",
       " 'Siege Medien-Franchise Harper',\n",
       " 'Galiza Medien-Franchise Edit',\n",
       " 'Lucky Medien-Franchise Cervantes',\n",
       " 'Pfeiffer Medien-Franchise Gino',\n",
       " 'FX Medien-Franchise Brenda',\n",
       " 'Energy Medien-Franchise Gruyter',\n",
       " 'Joanne Medien-Franchise Fashion',\n",
       " 'Erik Medien-Franchise Occitanie',\n",
       " 'EM Medien-Franchise Naast',\n",
       " 'Raum Medien-Franchise Flag',\n",
       " 'VL Medien-Franchise Aria',\n",
       " 'Sabina Medien-Franchise Henrik',\n",
       " 'Krupp Medien-Franchise File',\n",
       " 'October Medien-Franchise Aves',\n",
       " 'Davidson Medien-Franchise Würzburg',\n",
       " 'Allah Medien-Franchise Kutler',\n",
       " 'Larva Medien-Franchise Albion',\n",
       " 'Khu Medien-Franchise Bernstein',\n",
       " 'Lahore Medien-Franchise Maas',\n",
       " 'Dorset Medien-Franchise Andorra',\n",
       " 'Stay Medien-Franchise Cleveland',\n",
       " 'Sit Medien-Franchise Asturias',\n",
       " 'Wayne Medien-Franchise Maan',\n",
       " 'Flag Medien-Franchise Dudley',\n",
       " 'Ferns Medien-Franchise Satellite',\n",
       " 'Jaguar Medien-Franchise Ventura',\n",
       " 'Brenda Medien-Franchise Welch',\n",
       " 'Mina Medien-Franchise Ashton',\n",
       " 'Harding Medien-Franchise Southampton',\n",
       " 'Giants Medien-Franchise Lillehammer',\n",
       " 'Buta Medien-Franchise Room',\n",
       " 'Faye Medien-Franchise Seat',\n",
       " 'Generation Medien-Franchise Mono',\n",
       " 'Schottland Medien-Franchise Stockholm',\n",
       " 'USD Medien-Franchise RPG',\n",
       " 'Eu Medien-Franchise Vic',\n",
       " 'Haji Gesichtshaar Colbert',\n",
       " 'Yahoo Gesichtshaar Van',\n",
       " 'Stal Gesichtshaar Tun',\n",
       " 'FC Gesichtshaar Crash',\n",
       " 'Dad Gesichtshaar Mercy',\n",
       " 'Kenia Gesichtshaar Egitto',\n",
       " 'CCD Gesichtshaar Ministro',\n",
       " 'Riau Gesichtshaar Rousseau',\n",
       " 'Ky Gesichtshaar Limita',\n",
       " 'Billie Gesichtshaar Cuenca',\n",
       " 'Elbe Gesichtshaar Stal',\n",
       " 'Paraíso Gesichtshaar Junkers',\n",
       " 'TD Gesichtshaar Velvet',\n",
       " 'Luther Gesichtshaar TT',\n",
       " 'Roi Gesichtshaar Nos',\n",
       " 'Pole Gesichtshaar Kleiner',\n",
       " 'Page Gesichtshaar Emir',\n",
       " 'Baron Gesichtshaar Danny',\n",
       " 'Libia Gesichtshaar Garrison',\n",
       " 'Cuenca Gesichtshaar Morrison',\n",
       " 'Kálmán Gesichtshaar Seed',\n",
       " 'ET Gesichtshaar Rookie',\n",
       " 'Mainstream Gesichtshaar DNS',\n",
       " 'Agency Gesichtshaar York',\n",
       " 'Mata Gesichtshaar Rosie',\n",
       " 'Mineral Gesichtshaar Cola',\n",
       " 'Figaro Gesichtshaar Torpedo',\n",
       " 'Trung Gesichtshaar Cruzeiro',\n",
       " 'Sabha Gesichtshaar Noire',\n",
       " 'Guimarães Gesichtshaar Tunnel',\n",
       " 'Disneyland Gesichtshaar Amiens',\n",
       " 'Wes Gesichtshaar Una',\n",
       " 'Gesù Gesichtshaar Berlín',\n",
       " 'Cinq Gesichtshaar Ra',\n",
       " 'Silla Gesichtshaar Medvedev',\n",
       " 'JR Gesichtshaar Blacks',\n",
       " 'Bryant Gesichtshaar Cincinnati',\n",
       " 'Klaus Gesichtshaar SAR',\n",
       " 'Kleiner Gesichtshaar Heidi',\n",
       " 'Fort Gesichtshaar Hahn',\n",
       " 'City Gesichtshaar City',\n",
       " 'Márquez Gesichtshaar Witness',\n",
       " 'Yer Gesichtshaar Rae',\n",
       " 'Bernard Gesichtshaar Lord',\n",
       " 'Diaz Gesichtshaar RJ',\n",
       " 'Madre Gesichtshaar Kampung',\n",
       " 'Asturias Gesichtshaar Enigma',\n",
       " 'FF Gesichtshaar Runde',\n",
       " 'Dis Gesichtshaar Belfast',\n",
       " 'Lorentz Gesichtshaar Abdel',\n",
       " 'SM Gesichtshaar Buku',\n",
       " 'Sempre Gesichtshaar Ny',\n",
       " 'Portsmouth Gesichtshaar Raja',\n",
       " 'Irena Gesichtshaar Challenger',\n",
       " 'Oaks Gesichtshaar Kew',\n",
       " 'Fighting Gesichtshaar Ryu',\n",
       " 'Bilbao Gesichtshaar Galatasaray',\n",
       " 'Sharon Gesichtshaar Frida',\n",
       " 'Allium Gesichtshaar Muslimani',\n",
       " 'Haiti Gesichtshaar College',\n",
       " 'Isabella Gesichtshaar Hepburn',\n",
       " 'Olsson Gesichtshaar Arap',\n",
       " 'Pure Gesichtshaar Edouard',\n",
       " 'Patrol Gesichtshaar WDR',\n",
       " 'Siege Gesichtshaar Kata',\n",
       " 'Galiza Gesichtshaar Vai',\n",
       " 'Lucky Gesichtshaar Hindenburg',\n",
       " 'Pfeiffer Gesichtshaar can',\n",
       " 'FX Gesichtshaar Soc',\n",
       " 'Energy Gesichtshaar Famous',\n",
       " 'Joanne Gesichtshaar Brain',\n",
       " 'Erik Gesichtshaar Antonio',\n",
       " 'EM Gesichtshaar Export',\n",
       " 'Raum Gesichtshaar Slater',\n",
       " 'VL Gesichtshaar Cheryl',\n",
       " 'Sabina Gesichtshaar Samo',\n",
       " 'Krupp Gesichtshaar Jos',\n",
       " 'October Gesichtshaar PV',\n",
       " 'Davidson Gesichtshaar Naast',\n",
       " 'Allah Gesichtshaar Dänemark',\n",
       " 'Larva Gesichtshaar Bug',\n",
       " 'Khu Gesichtshaar Birinci',\n",
       " 'Lahore Gesichtshaar Cassidy',\n",
       " 'Dorset Gesichtshaar Pure',\n",
       " 'Stay Gesichtshaar Icarus',\n",
       " 'Sit Gesichtshaar Nagasaki',\n",
       " 'Wayne Gesichtshaar Homer',\n",
       " 'Flag Gesichtshaar Rupert',\n",
       " 'Ferns Gesichtshaar Burr',\n",
       " 'Jaguar Gesichtshaar Mark',\n",
       " 'Brenda Gesichtshaar Battlefield',\n",
       " 'Mina Gesichtshaar FBI',\n",
       " 'Harding Gesichtshaar Larva',\n",
       " 'Giants Gesichtshaar Clash',\n",
       " 'Buta Gesichtshaar Us',\n",
       " 'Faye Gesichtshaar Xoán',\n",
       " 'Generation Gesichtshaar Spain',\n",
       " 'Schottland Gesichtshaar Jupiter',\n",
       " 'USD Gesichtshaar Chem',\n",
       " 'Eu Gesichtshaar Ware',\n",
       " 'Haji Nebenwirkung Medvedev',\n",
       " 'Yahoo Nebenwirkung SVT',\n",
       " 'Stal Nebenwirkung Champions',\n",
       " 'FC Nebenwirkung Conway',\n",
       " 'Dad Nebenwirkung Stella',\n",
       " 'Kenia Nebenwirkung Portuguesa',\n",
       " 'CCD Nebenwirkung Wire',\n",
       " 'Riau Nebenwirkung Newell',\n",
       " 'Ky Nebenwirkung Medan',\n",
       " 'Billie Nebenwirkung TKO',\n",
       " 'Elbe Nebenwirkung Graves',\n",
       " 'Paraíso Nebenwirkung UC',\n",
       " 'TD Nebenwirkung Mer',\n",
       " 'Luther Nebenwirkung Franche',\n",
       " 'Roi Nebenwirkung Salman',\n",
       " 'Pole Nebenwirkung Bumi',\n",
       " 'Page Nebenwirkung Vice',\n",
       " 'Baron Nebenwirkung Pablo',\n",
       " 'Libia Nebenwirkung Aires',\n",
       " 'Cuenca Nebenwirkung Path',\n",
       " 'Kálmán Nebenwirkung Friendship',\n",
       " 'ET Nebenwirkung Zoltán',\n",
       " 'Mainstream Nebenwirkung Rome',\n",
       " 'Agency Nebenwirkung Meredith',\n",
       " 'Mata Nebenwirkung NL',\n",
       " 'Mineral Nebenwirkung Gustav',\n",
       " 'Figaro Nebenwirkung Vancouver',\n",
       " 'Trung Nebenwirkung Diaz',\n",
       " 'Sabha Nebenwirkung Bresse',\n",
       " 'Guimarães Nebenwirkung Péter',\n",
       " 'Disneyland Nebenwirkung Animal',\n",
       " 'Wes Nebenwirkung Thành',\n",
       " 'Gesù Nebenwirkung Hamar',\n",
       " 'Cinq Nebenwirkung Beaumont',\n",
       " 'Silla Nebenwirkung Middleton',\n",
       " 'JR Nebenwirkung Ross',\n",
       " 'Bryant Nebenwirkung Montero',\n",
       " 'Klaus Nebenwirkung Hulk',\n",
       " 'Kleiner Nebenwirkung Quick',\n",
       " 'Fort Nebenwirkung Malta',\n",
       " 'City Nebenwirkung TNI',\n",
       " 'Márquez Nebenwirkung RSS',\n",
       " 'Yer Nebenwirkung Robson',\n",
       " 'Bernard Nebenwirkung Bertrand',\n",
       " 'Diaz Nebenwirkung Triumph',\n",
       " 'Madre Nebenwirkung CCD',\n",
       " 'Asturias Nebenwirkung Esto',\n",
       " 'FF Nebenwirkung Ultra',\n",
       " 'Dis Nebenwirkung Baker',\n",
       " 'Lorentz Nebenwirkung Expo',\n",
       " 'SM Nebenwirkung Dezember',\n",
       " 'Sempre Nebenwirkung Mircea',\n",
       " 'Portsmouth Nebenwirkung Stewart',\n",
       " 'Irena Nebenwirkung BRT',\n",
       " 'Oaks Nebenwirkung Vierge',\n",
       " 'Fighting Nebenwirkung Mitch',\n",
       " 'Bilbao Nebenwirkung Springfield',\n",
       " 'Sharon Nebenwirkung Lemon',\n",
       " 'Allium Nebenwirkung Cats',\n",
       " 'Haiti Nebenwirkung Chaplin',\n",
       " 'Isabella Nebenwirkung Wege',\n",
       " 'Olsson Nebenwirkung Racing',\n",
       " 'Pure Nebenwirkung PP',\n",
       " 'Patrol Nebenwirkung Sidney',\n",
       " 'Siege Nebenwirkung Linia',\n",
       " 'Galiza Nebenwirkung Jakiel',\n",
       " 'Lucky Nebenwirkung Ying',\n",
       " 'Pfeiffer Nebenwirkung Magister',\n",
       " 'FX Nebenwirkung Gegner',\n",
       " 'Energy Nebenwirkung ASCII',\n",
       " 'Joanne Nebenwirkung Pax',\n",
       " 'Erik Nebenwirkung Mills',\n",
       " 'EM Nebenwirkung Clock',\n",
       " 'Raum Nebenwirkung Marlon',\n",
       " 'VL Nebenwirkung AP',\n",
       " 'Sabina Nebenwirkung Budapest',\n",
       " 'Krupp Nebenwirkung Aalborg',\n",
       " 'October Nebenwirkung Schneider',\n",
       " 'Davidson Nebenwirkung Charlton',\n",
       " 'Allah Nebenwirkung Doe',\n",
       " 'Larva Nebenwirkung Hurricane',\n",
       " 'Khu Nebenwirkung Malta',\n",
       " 'Lahore Nebenwirkung Como',\n",
       " 'Dorset Nebenwirkung Allen',\n",
       " 'Stay Nebenwirkung Farmer',\n",
       " 'Sit Nebenwirkung IN',\n",
       " 'Wayne Nebenwirkung Seele',\n",
       " 'Flag Nebenwirkung Mississippi',\n",
       " 'Ferns Nebenwirkung Père',\n",
       " 'Jaguar Nebenwirkung Early',\n",
       " 'Brenda Nebenwirkung Palacio',\n",
       " 'Mina Nebenwirkung Merlin',\n",
       " 'Harding Nebenwirkung USD',\n",
       " 'Giants Nebenwirkung Eugen',\n",
       " 'Buta Nebenwirkung Schweden',\n",
       " 'Faye Nebenwirkung Même',\n",
       " 'Generation Nebenwirkung Amber',\n",
       " 'Schottland Nebenwirkung Goose',\n",
       " 'USD Nebenwirkung Wizard',\n",
       " 'Eu Nebenwirkung Freiburg',\n",
       " 'Haji Demonym zu Dyke',\n",
       " 'Yahoo Demonym zu Würzburg',\n",
       " 'Stal Demonym zu Reno',\n",
       " 'FC Demonym zu Lindberg',\n",
       " 'Dad Demonym zu Mato',\n",
       " 'Kenia Demonym zu Racine',\n",
       " 'CCD Demonym zu cat',\n",
       " 'Riau Demonym zu Cedar',\n",
       " 'Ky Demonym zu Ebert',\n",
       " 'Billie Demonym zu Rico',\n",
       " 'Elbe Demonym zu Fields',\n",
       " 'Paraíso Demonym zu Wells',\n",
       " 'TD Demonym zu Ryder',\n",
       " 'Luther Demonym zu Christ',\n",
       " 'Roi Demonym zu Dayton',\n",
       " 'Pole Demonym zu Sanremo',\n",
       " 'Page Demonym zu Rimini',\n",
       " 'Baron Demonym zu Jefferson',\n",
       " 'Libia Demonym zu Kosmos',\n",
       " 'Cuenca Demonym zu Atkins',\n",
       " 'Kálmán Demonym zu Brothers',\n",
       " 'ET Demonym zu McLaughlin',\n",
       " 'Mainstream Demonym zu Tema',\n",
       " 'Agency Demonym zu Brunnen',\n",
       " 'Mata Demonym zu Cessna',\n",
       " 'Mineral Demonym zu Aragón',\n",
       " 'Figaro Demonym zu Eugenia',\n",
       " 'Trung Demonym zu Thánh',\n",
       " 'Sabha Demonym zu Liberation',\n",
       " 'Guimarães Demonym zu Eugenia',\n",
       " 'Disneyland Demonym zu Home',\n",
       " 'Wes Demonym zu Siegen',\n",
       " 'Gesù Demonym zu Apple',\n",
       " 'Cinq Demonym zu Bilbao',\n",
       " 'Silla Demonym zu Sport',\n",
       " 'JR Demonym zu NE',\n",
       " 'Bryant Demonym zu Bulgaria',\n",
       " 'Klaus Demonym zu Morley',\n",
       " 'Kleiner Demonym zu Dixon',\n",
       " 'Fort Demonym zu Dre',\n",
       " 'City Demonym zu Rembrandt',\n",
       " 'Márquez Demonym zu Brücke',\n",
       " 'Yer Demonym zu Friends',\n",
       " 'Bernard Demonym zu Delia',\n",
       " 'Diaz Demonym zu Fashion',\n",
       " 'Madre Demonym zu Od',\n",
       " 'Asturias Demonym zu Stéphane',\n",
       " 'FF Demonym zu HD',\n",
       " 'Dis Demonym zu Vanderbilt',\n",
       " 'Lorentz Demonym zu Roberta',\n",
       " 'SM Demonym zu Richter',\n",
       " 'Sempre Demonym zu Kuzey',\n",
       " 'Portsmouth Demonym zu Baza',\n",
       " 'Irena Demonym zu Desmond',\n",
       " 'Oaks Demonym zu Minden',\n",
       " 'Fighting Demonym zu SD',\n",
       " 'Bilbao Demonym zu Pulau',\n",
       " 'Sharon Demonym zu Bila',\n",
       " 'Allium Demonym zu Jenny',\n",
       " 'Haiti Demonym zu Empire',\n",
       " 'Isabella Demonym zu Cecil',\n",
       " 'Olsson Demonym zu Norris',\n",
       " 'Pure Demonym zu Skin',\n",
       " 'Patrol Demonym zu Avant',\n",
       " 'Siege Demonym zu Koch',\n",
       " 'Galiza Demonym zu Männchen',\n",
       " 'Lucky Demonym zu Nacht',\n",
       " 'Pfeiffer Demonym zu Londra',\n",
       " 'FX Demonym zu Somerset',\n",
       " 'Energy Demonym zu Ramsay',\n",
       " 'Joanne Demonym zu Drama',\n",
       " 'Erik Demonym zu Auburn',\n",
       " 'EM Demonym zu Kristen',\n",
       " 'Raum Demonym zu Ottawa',\n",
       " 'VL Demonym zu Neustadt',\n",
       " 'Sabina Demonym zu Transvaal',\n",
       " 'Krupp Demonym zu Cinema',\n",
       " 'October Demonym zu Romans',\n",
       " 'Davidson Demonym zu Fire',\n",
       " 'Allah Demonym zu Ingles',\n",
       " 'Larva Demonym zu Atlantic',\n",
       " 'Khu Demonym zu Aten',\n",
       " 'Lahore Demonym zu Pico',\n",
       " 'Dorset Demonym zu Tolosa',\n",
       " 'Stay Demonym zu Medina',\n",
       " 'Sit Demonym zu Edouard',\n",
       " 'Wayne Demonym zu IM',\n",
       " 'Flag Demonym zu Manu',\n",
       " 'Ferns Demonym zu Carmen',\n",
       " 'Jaguar Demonym zu Fulda',\n",
       " 'Brenda Demonym zu Mora',\n",
       " 'Mina Demonym zu Ghose',\n",
       " 'Harding Demonym zu Reason',\n",
       " 'Giants Demonym zu Halen',\n",
       " 'Buta Demonym zu extremo',\n",
       " 'Faye Demonym zu Cada',\n",
       " 'Generation Demonym zu PSV',\n",
       " 'Schottland Demonym zu Hang',\n",
       " 'USD Demonym zu Davies',\n",
       " 'Eu Demonym zu Ordu',\n",
       " 'Haji Aufnahme oder Ausführung von Gotland',\n",
       " 'Yahoo Aufnahme oder Ausführung von Books',\n",
       " 'Stal Aufnahme oder Ausführung von Padre',\n",
       " 'FC Aufnahme oder Ausführung von Smith',\n",
       " 'Dad Aufnahme oder Ausführung von Stare',\n",
       " 'Kenia Aufnahme oder Ausführung von Lost',\n",
       " 'CCD Aufnahme oder Ausführung von Schwartz',\n",
       " 'Riau Aufnahme oder Ausführung von Cervantes',\n",
       " 'Ky Aufnahme oder Ausführung von Klaus',\n",
       " 'Billie Aufnahme oder Ausführung von Graham',\n",
       " 'Elbe Aufnahme oder Ausführung von Parigi',\n",
       " 'Paraíso Aufnahme oder Ausführung von Jerusalén',\n",
       " 'TD Aufnahme oder Ausführung von Un',\n",
       " 'Luther Aufnahme oder Ausführung von Osiris',\n",
       " 'Roi Aufnahme oder Ausführung von Ordu',\n",
       " 'Pole Aufnahme oder Ausführung von Luca',\n",
       " 'Page Aufnahme oder Ausführung von Timothy',\n",
       " 'Baron Aufnahme oder Ausführung von Town',\n",
       " 'Libia Aufnahme oder Ausführung von Rather',\n",
       " 'Cuenca Aufnahme oder Ausführung von Atlantic',\n",
       " 'Kálmán Aufnahme oder Ausführung von Sacro',\n",
       " 'ET Aufnahme oder Ausführung von Stig',\n",
       " 'Mainstream Aufnahme oder Ausführung von Neuchâtel',\n",
       " 'Agency Aufnahme oder Ausführung von Alison',\n",
       " 'Mata Aufnahme oder Ausführung von Fiesta',\n",
       " 'Mineral Aufnahme oder Ausführung von Linden',\n",
       " 'Figaro Aufnahme oder Ausführung von Lindberg',\n",
       " 'Trung Aufnahme oder Ausführung von Ille',\n",
       " 'Sabha Aufnahme oder Ausführung von Selva',\n",
       " 'Guimarães Aufnahme oder Ausführung von Pál',\n",
       " 'Disneyland Aufnahme oder Ausführung von Capo',\n",
       " 'Wes Aufnahme oder Ausführung von Mato',\n",
       " 'Gesù Aufnahme oder Ausführung von Prayer',\n",
       " 'Cinq Aufnahme oder Ausführung von Percy',\n",
       " 'Silla Aufnahme oder Ausführung von Ying',\n",
       " 'JR Aufnahme oder Ausführung von Michel',\n",
       " 'Bryant Aufnahme oder Ausführung von Club',\n",
       " 'Klaus Aufnahme oder Ausführung von Gales',\n",
       " 'Kleiner Aufnahme oder Ausführung von Forbes',\n",
       " 'Fort Aufnahme oder Ausführung von Bitte',\n",
       " 'City Aufnahme oder Ausführung von Köln',\n",
       " 'Márquez Aufnahme oder Ausführung von Hubbard',\n",
       " 'Yer Aufnahme oder Ausführung von Boris',\n",
       " 'Bernard Aufnahme oder Ausführung von Planet',\n",
       " 'Diaz Aufnahme oder Ausführung von Koska',\n",
       " 'Madre Aufnahme oder Ausführung von Natal',\n",
       " 'Asturias Aufnahme oder Ausführung von Ernest',\n",
       " 'FF Aufnahme oder Ausführung von Weather',\n",
       " 'Dis Aufnahme oder Ausführung von Nada',\n",
       " 'Lorentz Aufnahme oder Ausführung von Faro',\n",
       " 'SM Aufnahme oder Ausführung von Hero',\n",
       " 'Sempre Aufnahme oder Ausführung von Christensen',\n",
       " 'Portsmouth Aufnahme oder Ausführung von Cherokee',\n",
       " 'Irena Aufnahme oder Ausführung von Guy',\n",
       " 'Oaks Aufnahme oder Ausführung von Lieutenant',\n",
       " 'Fighting Aufnahme oder Ausführung von Carmen',\n",
       " 'Bilbao Aufnahme oder Ausführung von Uno',\n",
       " 'Sharon Aufnahme oder Ausführung von Ostrava',\n",
       " 'Allium Aufnahme oder Ausführung von Kathryn',\n",
       " 'Haiti Aufnahme oder Ausführung von Madre',\n",
       " 'Isabella Aufnahme oder Ausführung von Brooklyn',\n",
       " 'Olsson Aufnahme oder Ausführung von Winkel',\n",
       " 'Pure Aufnahme oder Ausführung von Leary',\n",
       " 'Patrol Aufnahme oder Ausführung von Illinois',\n",
       " 'Siege Aufnahme oder Ausführung von Jones',\n",
       " 'Galiza Aufnahme oder Ausführung von Counter',\n",
       " 'Lucky Aufnahme oder Ausführung von Georg',\n",
       " 'Pfeiffer Aufnahme oder Ausführung von McDonnell',\n",
       " 'FX Aufnahme oder Ausführung von Caldwell',\n",
       " 'Energy Aufnahme oder Ausführung von NC',\n",
       " 'Joanne Aufnahme oder Ausführung von Bombay',\n",
       " 'Erik Aufnahme oder Ausführung von Buku',\n",
       " 'EM Aufnahme oder Ausführung von Essential',\n",
       " 'Raum Aufnahme oder Ausführung von Onder',\n",
       " 'VL Aufnahme oder Ausführung von Jenna',\n",
       " 'Sabina Aufnahme oder Ausführung von Areas',\n",
       " 'Krupp Aufnahme oder Ausführung von Egitto',\n",
       " 'October Aufnahme oder Ausführung von Monate',\n",
       " 'Davidson Aufnahme oder Ausführung von Salto',\n",
       " 'Allah Aufnahme oder Ausführung von Kenneth',\n",
       " 'Larva Aufnahme oder Ausführung von Copeland',\n",
       " 'Khu Aufnahme oder Ausführung von NF',\n",
       " 'Lahore Aufnahme oder Ausführung von Islander',\n",
       " 'Dorset Aufnahme oder Ausführung von Jack',\n",
       " 'Stay Aufnahme oder Ausführung von Sébastien',\n",
       " 'Sit Aufnahme oder Ausführung von Carol',\n",
       " 'Wayne Aufnahme oder Ausführung von Allen',\n",
       " 'Flag Aufnahme oder Ausführung von Seni',\n",
       " 'Ferns Aufnahme oder Ausführung von KV',\n",
       " 'Jaguar Aufnahme oder Ausführung von GS',\n",
       " 'Brenda Aufnahme oder Ausführung von League',\n",
       " 'Mina Aufnahme oder Ausführung von Paradise',\n",
       " 'Harding Aufnahme oder Ausführung von Zoltán',\n",
       " 'Giants Aufnahme oder Ausführung von Burlington',\n",
       " 'Buta Aufnahme oder Ausführung von Barton',\n",
       " 'Faye Aufnahme oder Ausführung von Lindberg',\n",
       " 'Generation Aufnahme oder Ausführung von Davis',\n",
       " 'Schottland Aufnahme oder Ausführung von Lago',\n",
       " 'USD Aufnahme oder Ausführung von Sweden',\n",
       " 'Eu Aufnahme oder Ausführung von Sumatra',\n",
       " 'Haji Geschlecht Team',\n",
       " 'Yahoo Geschlecht Caribe',\n",
       " 'Stal Geschlecht Winchester',\n",
       " 'FC Geschlecht Marlon',\n",
       " 'Dad Geschlecht Indra',\n",
       " 'Kenia Geschlecht Merkel',\n",
       " 'CCD Geschlecht Novel',\n",
       " 'Riau Geschlecht Arabian',\n",
       " 'Ky Geschlecht Quatre',\n",
       " 'Billie Geschlecht Kemp',\n",
       " 'Elbe Geschlecht Colts',\n",
       " 'Paraíso Geschlecht Nate',\n",
       " 'TD Geschlecht Groove',\n",
       " 'Luther Geschlecht Harris',\n",
       " 'Roi Geschlecht Kurt',\n",
       " 'Pole Geschlecht Forma',\n",
       " 'Page Geschlecht Sharks',\n",
       " 'Baron Geschlecht Kane',\n",
       " 'Libia Geschlecht Middleton',\n",
       " 'Cuenca Geschlecht PRL',\n",
       " 'Kálmán Geschlecht ville',\n",
       " 'ET Geschlecht Recreation',\n",
       " 'Mainstream Geschlecht Jasper',\n",
       " 'Agency Geschlecht Genoa',\n",
       " 'Mata Geschlecht Oosten',\n",
       " 'Mineral Geschlecht Campbell',\n",
       " 'Figaro Geschlecht Rock',\n",
       " 'Trung Geschlecht Bremen',\n",
       " 'Sabha Geschlecht IFK',\n",
       " 'Guimarães Geschlecht Tarn',\n",
       " 'Disneyland Geschlecht Mircea',\n",
       " 'Wes Geschlecht Happy',\n",
       " 'Gesù Geschlecht Lyman',\n",
       " 'Cinq Geschlecht Sweeney',\n",
       " 'Silla Geschlecht Carl',\n",
       " 'JR Geschlecht Oslo',\n",
       " 'Bryant Geschlecht Mon',\n",
       " 'Klaus Geschlecht PIB',\n",
       " 'Kleiner Geschlecht Marathon',\n",
       " 'Fort Geschlecht Girls',\n",
       " 'City Geschlecht Luck',\n",
       " 'Márquez Geschlecht Karlsruhe',\n",
       " 'Yer Geschlecht Violet',\n",
       " 'Bernard Geschlecht Cosimo',\n",
       " 'Diaz Geschlecht Sera',\n",
       " 'Madre Geschlecht Cerca',\n",
       " 'Asturias Geschlecht Argentina',\n",
       " 'FF Geschlecht Rochester',\n",
       " 'Dis Geschlecht Sibiu',\n",
       " 'Lorentz Geschlecht Paglinawan',\n",
       " 'SM Geschlecht Wrong',\n",
       " 'Sempre Geschlecht Agung',\n",
       " 'Portsmouth Geschlecht Happy',\n",
       " 'Irena Geschlecht Kunst',\n",
       " 'Oaks Geschlecht Londres',\n",
       " 'Fighting Geschlecht Indianapolis',\n",
       " 'Bilbao Geschlecht Hardy',\n",
       " 'Sharon Geschlecht Carl',\n",
       " 'Allium Geschlecht BB',\n",
       " 'Haiti Geschlecht Greenwood',\n",
       " 'Isabella Geschlecht Mendoza',\n",
       " 'Olsson Geschlecht Nile',\n",
       " 'Pure Geschlecht Justicia',\n",
       " 'Patrol Geschlecht Onthophagus',\n",
       " 'Siege Geschlecht Bloom',\n",
       " 'Galiza Geschlecht Holloway',\n",
       " 'Lucky Geschlecht Calderón',\n",
       " 'Pfeiffer Geschlecht Adana',\n",
       " 'FX Geschlecht Franjo',\n",
       " 'Energy Geschlecht Buna',\n",
       " 'Joanne Geschlecht Mühle',\n",
       " 'Erik Geschlecht Buffalo',\n",
       " 'EM Geschlecht Floyd',\n",
       " 'Raum Geschlecht Silent',\n",
       " 'VL Geschlecht Baltic',\n",
       " 'Sabina Geschlecht Bartlett',\n",
       " 'Krupp Geschlecht Bari',\n",
       " 'October Geschlecht Nirvana',\n",
       " 'Davidson Geschlecht Kanal',\n",
       " 'Allah Geschlecht Exeter',\n",
       " 'Larva Geschlecht Melolonthidae',\n",
       " 'Khu Geschlecht Clerk',\n",
       " 'Lahore Geschlecht Au',\n",
       " 'Dorset Geschlecht AT',\n",
       " 'Stay Geschlecht Oriente',\n",
       " 'Sit Geschlecht Queste',\n",
       " 'Wayne Geschlecht Viking',\n",
       " 'Flag Geschlecht Navarro',\n",
       " 'Ferns Geschlecht System',\n",
       " 'Jaguar Geschlecht Phi',\n",
       " 'Brenda Geschlecht Schuster',\n",
       " 'Mina Geschlecht Midlands',\n",
       " 'Harding Geschlecht Verne',\n",
       " 'Giants Geschlecht Bend',\n",
       " 'Buta Geschlecht Ole',\n",
       " 'Faye Geschlecht Henrik',\n",
       " 'Generation Geschlecht Mallorca',\n",
       " 'Schottland Geschlecht Mutter',\n",
       " 'USD Geschlecht Artemis',\n",
       " 'Eu Geschlecht Alma',\n",
       " 'Haji Generalbauunternehmer Nuovo',\n",
       " 'Yahoo Generalbauunternehmer Comtat',\n",
       " 'Stal Generalbauunternehmer Pass',\n",
       " 'FC Generalbauunternehmer Mircea',\n",
       " 'Dad Generalbauunternehmer Kassel',\n",
       " 'Kenia Generalbauunternehmer Sailor',\n",
       " 'CCD Generalbauunternehmer Colégio',\n",
       " 'Riau Generalbauunternehmer Esch',\n",
       " 'Ky Generalbauunternehmer Frederico',\n",
       " 'Billie Generalbauunternehmer Elite',\n",
       " 'Elbe Generalbauunternehmer Noah',\n",
       " 'Paraíso Generalbauunternehmer Draft',\n",
       " 'TD Generalbauunternehmer Karya',\n",
       " 'Luther Generalbauunternehmer Hodges',\n",
       " 'Roi Generalbauunternehmer Urbino',\n",
       " 'Pole Generalbauunternehmer Canadian',\n",
       " 'Page Generalbauunternehmer Saksa',\n",
       " 'Baron Generalbauunternehmer Mutter',\n",
       " 'Libia Generalbauunternehmer Flow',\n",
       " 'Cuenca Generalbauunternehmer Granada',\n",
       " 'Kálmán Generalbauunternehmer Sainte',\n",
       " 'ET Generalbauunternehmer Kathryn',\n",
       " 'Mainstream Generalbauunternehmer Afrika',\n",
       " 'Agency Generalbauunternehmer Stare',\n",
       " 'Mata Generalbauunternehmer Below',\n",
       " 'Mineral Generalbauunternehmer Jill',\n",
       " 'Figaro Generalbauunternehmer Mansfield',\n",
       " 'Trung Generalbauunternehmer PSA',\n",
       " 'Sabha Generalbauunternehmer Horace',\n",
       " 'Guimarães Generalbauunternehmer Pay',\n",
       " 'Disneyland Generalbauunternehmer MBC',\n",
       " 'Wes Generalbauunternehmer Canada',\n",
       " 'Gesù Generalbauunternehmer Tübingen',\n",
       " 'Cinq Generalbauunternehmer Béla',\n",
       " 'Silla Generalbauunternehmer Sparks',\n",
       " 'JR Generalbauunternehmer Maddalena',\n",
       " 'Bryant Generalbauunternehmer Aziz',\n",
       " 'Klaus Generalbauunternehmer Mission',\n",
       " 'Kleiner Generalbauunternehmer AK',\n",
       " 'Fort Generalbauunternehmer Maha',\n",
       " 'City Generalbauunternehmer Nagar',\n",
       " 'Márquez Generalbauunternehmer Namur',\n",
       " 'Yer Generalbauunternehmer Moldavia',\n",
       " 'Bernard Generalbauunternehmer Hidden',\n",
       " 'Diaz Generalbauunternehmer Fritz',\n",
       " 'Madre Generalbauunternehmer Kansas',\n",
       " 'Asturias Generalbauunternehmer Conquest',\n",
       " 'FF Generalbauunternehmer Louvain',\n",
       " 'Dis Generalbauunternehmer Schaus',\n",
       " 'Lorentz Generalbauunternehmer Britannia',\n",
       " 'SM Generalbauunternehmer Hindenburg',\n",
       " 'Sempre Generalbauunternehmer Oriental',\n",
       " 'Portsmouth Generalbauunternehmer Israeli',\n",
       " 'Irena Generalbauunternehmer Norma',\n",
       " 'Oaks Generalbauunternehmer Salud',\n",
       " 'Fighting Generalbauunternehmer Amelia',\n",
       " 'Bilbao Generalbauunternehmer Turbo',\n",
       " 'Sharon Generalbauunternehmer Saksa',\n",
       " 'Allium Generalbauunternehmer Miki',\n",
       " 'Haiti Generalbauunternehmer Levante',\n",
       " 'Isabella Generalbauunternehmer Wakil',\n",
       " 'Olsson Generalbauunternehmer Helden',\n",
       " 'Pure Generalbauunternehmer Vid',\n",
       " 'Patrol Generalbauunternehmer Strong',\n",
       " 'Siege Generalbauunternehmer Esto',\n",
       " 'Galiza Generalbauunternehmer Paradise',\n",
       " 'Lucky Generalbauunternehmer Valley',\n",
       " 'Pfeiffer Generalbauunternehmer Dixon',\n",
       " 'FX Generalbauunternehmer Ivana',\n",
       " 'Energy Generalbauunternehmer Peters',\n",
       " 'Joanne Generalbauunternehmer Trip',\n",
       " 'Erik Generalbauunternehmer Jakiel',\n",
       " 'EM Generalbauunternehmer Wedding',\n",
       " 'Raum Generalbauunternehmer Fairfield',\n",
       " 'VL Generalbauunternehmer Regno',\n",
       " 'Sabina Generalbauunternehmer Mainstream',\n",
       " 'Krupp Generalbauunternehmer ARM',\n",
       " 'October Generalbauunternehmer Frankie',\n",
       " 'Davidson Generalbauunternehmer Ruben',\n",
       " 'Allah Generalbauunternehmer Pasha',\n",
       " 'Larva Generalbauunternehmer Colonia',\n",
       " 'Khu Generalbauunternehmer Clive',\n",
       " 'Lahore Generalbauunternehmer CJ',\n",
       " 'Dorset Generalbauunternehmer Spain',\n",
       " 'Stay Generalbauunternehmer SFR',\n",
       " 'Sit Generalbauunternehmer Karya',\n",
       " 'Wayne Generalbauunternehmer Freedom',\n",
       " 'Flag Generalbauunternehmer Bright',\n",
       " 'Ferns Generalbauunternehmer Greatest',\n",
       " 'Jaguar Generalbauunternehmer Damascus',\n",
       " 'Brenda Generalbauunternehmer Flesh',\n",
       " 'Mina Generalbauunternehmer Putnam',\n",
       " 'Harding Generalbauunternehmer Storm',\n",
       " 'Giants Generalbauunternehmer Chief',\n",
       " 'Buta Generalbauunternehmer Brasileiro',\n",
       " 'Faye Generalbauunternehmer Mahler',\n",
       " 'Generation Generalbauunternehmer Universe',\n",
       " 'Schottland Generalbauunternehmer Rooma',\n",
       " 'USD Generalbauunternehmer Like',\n",
       " 'Eu Generalbauunternehmer Heidelberg',\n",
       " 'Haji verschieden von Face',\n",
       " 'Yahoo verschieden von Kelley',\n",
       " 'Stal verschieden von Björn',\n",
       " 'FC verschieden von Fairfax',\n",
       " 'Dad verschieden von Anderson',\n",
       " 'Kenia verschieden von Sylvia',\n",
       " 'CCD verschieden von Nevada',\n",
       " 'Riau verschieden von SF',\n",
       " 'Ky verschieden von Schmidt',\n",
       " 'Billie verschieden von CC',\n",
       " 'Elbe verschieden von Graz',\n",
       " 'Paraíso verschieden von War',\n",
       " 'TD verschieden von Seigneur',\n",
       " 'Luther verschieden von Mon',\n",
       " 'Roi verschieden von Safe',\n",
       " 'Pole verschieden von Soto',\n",
       " 'Page verschieden von Ireland',\n",
       " 'Baron verschieden von Common',\n",
       " 'Libia verschieden von Pinto',\n",
       " 'Cuenca verschieden von Butte',\n",
       " 'Kálmán verschieden von Londra',\n",
       " 'ET verschieden von Latino',\n",
       " 'Mainstream verschieden von Trap',\n",
       " 'Agency verschieden von Atas',\n",
       " 'Mata verschieden von Mississippi',\n",
       " 'Mineral verschieden von Elaine',\n",
       " 'Figaro verschieden von Sunset',\n",
       " 'Trung verschieden von Welfare',\n",
       " 'Sabha verschieden von Halo',\n",
       " 'Guimarães verschieden von Jenny',\n",
       " 'Disneyland verschieden von Friendship',\n",
       " 'Wes verschieden von Copenhagen',\n",
       " 'Gesù verschieden von Tracy',\n",
       " 'Cinq verschieden von NE',\n",
       " 'Silla verschieden von Kort',\n",
       " 'JR verschieden von Mens',\n",
       " 'Bryant verschieden von Shadows',\n",
       " 'Klaus verschieden von Curtis',\n",
       " 'Kleiner verschieden von Star',\n",
       " 'Fort verschieden von Alicante',\n",
       " 'City verschieden von Kelas',\n",
       " 'Márquez verschieden von Sabadell',\n",
       " 'Yer verschieden von Gold',\n",
       " 'Bernard verschieden von Bey',\n",
       " 'Diaz verschieden von Pitkin',\n",
       " 'Madre verschieden von Largo',\n",
       " 'Asturias verschieden von ur',\n",
       " 'FF verschieden von Verona',\n",
       " 'Dis verschieden von UA',\n",
       " 'Lorentz verschieden von Titan',\n",
       " 'SM verschieden von Quelle',\n",
       " 'Sempre verschieden von Più',\n",
       " 'Portsmouth verschieden von Surat',\n",
       " 'Irena verschieden von Rhodes',\n",
       " 'Oaks verschieden von Linh',\n",
       " 'Fighting verschieden von Teacher',\n",
       " 'Bilbao verschieden von Giants',\n",
       " 'Sharon verschieden von Ward',\n",
       " 'Allium verschieden von Freedom',\n",
       " 'Haiti verschieden von UHF',\n",
       " 'Isabella verschieden von Leicester',\n",
       " 'Olsson verschieden von ci',\n",
       " 'Pure verschieden von MB',\n",
       " 'Patrol verschieden von Gattung',\n",
       " 'Siege verschieden von Everton',\n",
       " 'Galiza verschieden von Greatest',\n",
       " 'Lucky verschieden von Marion',\n",
       " 'Pfeiffer verschieden von Reyes',\n",
       " 'FX verschieden von EPA',\n",
       " 'Energy verschieden von Schalke',\n",
       " 'Joanne verschieden von Julian',\n",
       " 'Erik verschieden von Carnegie',\n",
       " 'EM verschieden von Camus',\n",
       " 'Raum verschieden von Heidi',\n",
       " 'VL verschieden von Eye',\n",
       " 'Sabina verschieden von Tabachnick',\n",
       " 'Krupp verschieden von Sentinel',\n",
       " 'October verschieden von Omaha',\n",
       " 'Davidson verschieden von ACM',\n",
       " 'Allah verschieden von Harris',\n",
       " 'Larva verschieden von Alzheimer',\n",
       " 'Khu verschieden von Sous',\n",
       " 'Lahore verschieden von Scholar',\n",
       " 'Dorset verschieden von STS',\n",
       " 'Stay verschieden von Abucay',\n",
       " 'Sit verschieden von UFC',\n",
       " 'Wayne verschieden von Paglinawan',\n",
       " 'Flag verschieden von Edit',\n",
       " 'Ferns verschieden von Brooklyn',\n",
       " 'Jaguar verschieden von Case',\n",
       " 'Brenda verschieden von Grâce',\n",
       " 'Mina verschieden von Eure',\n",
       " 'Harding verschieden von Anh',\n",
       " 'Giants verschieden von July',\n",
       " 'Buta verschieden von CRC',\n",
       " 'Faye verschieden von Gypsy',\n",
       " 'Generation verschieden von Mozilla',\n",
       " 'Schottland verschieden von Esther',\n",
       " 'USD verschieden von Conquest',\n",
       " 'Eu verschieden von AR']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "713123dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Haji', 'Terminal', 'Malone'],\n",
       " ['Yahoo', 'EV', 'Soccerway'],\n",
       " ['Stal', 'Amadeus', 'President'],\n",
       " ['FC', 'Simpson', 'Mitch'],\n",
       " ['Dad', 'Jang', 'Hütte'],\n",
       " ['Kenia', 'Kosmos', 'Bohemia'],\n",
       " ['CCD', 'Lyman', 'Ravenna'],\n",
       " ['Riau', 'Lakes', 'Shri'],\n",
       " ['Ky', 'Seite', 'Delgado'],\n",
       " ['Billie', 'Antoinette', 'Midway'],\n",
       " ['Elbe', 'Henley', 'Od'],\n",
       " ['Paraíso', 'Cruise', 'Missouri'],\n",
       " ['TD', 'Ennen', 'Hodges'],\n",
       " ['Luther', 'Turm', 'Publié'],\n",
       " ['Roi', 'Gloucester', 'Gyula'],\n",
       " ['Pole', 'Diana', 'DSM'],\n",
       " ['Page', 'Bing', 'Ost'],\n",
       " ['Baron', 'Industria', 'Candy'],\n",
       " ['Libia', 'President', 'Ted'],\n",
       " ['Cuenca', 'Sprint', 'Eaton'],\n",
       " ['Kálmán', 'Jess', 'Irving'],\n",
       " ['ET', 'Indianapolis', 'Palatinat'],\n",
       " ['Mainstream', 'Mouse', 'Sai'],\n",
       " ['Agency', 'Hindenburg', 'Principal'],\n",
       " ['Mata', 'Shawn', 'Arrow'],\n",
       " ['Mineral', 'Ehren', 'Berg'],\n",
       " ['Figaro', 'Florida', 'Bora'],\n",
       " ['Trung', 'Córdoba', 'Soleil'],\n",
       " ['Sabha', 'Otis', 'Modena'],\n",
       " ['Guimarães', 'Waves', 'Odd'],\n",
       " ['Disneyland', 'Special', 'Cullen'],\n",
       " ['Wes', 'Chaco', 'Nashville'],\n",
       " ['Gesù', 'Sally', 'Guadalupe'],\n",
       " ['Cinq', 'Midlands', 'Server'],\n",
       " ['Silla', 'Check', 'Catedral'],\n",
       " ['JR', 'Carlos', 'McGill'],\n",
       " ['Bryant', 'Regia', 'Linden'],\n",
       " ['Klaus', 'Records', 'Schlacht'],\n",
       " ['Kleiner', 'Salisbury', 'Helmut'],\n",
       " ['Fort', 'Poslední', 'HP'],\n",
       " ['City', 'Brain', 'Ipswich'],\n",
       " ['Márquez', 'Qara', 'Den'],\n",
       " ['Yer', 'Bobby', 'Hügel'],\n",
       " ['Bernard', 'Potosí', 'pad'],\n",
       " ['Diaz', 'Byron', 'Irena'],\n",
       " ['Madre', 'Gary', 'Estate'],\n",
       " ['Asturias', 'Henning', 'EL'],\n",
       " ['FF', 'Apollo', 'India'],\n",
       " ['Dis', 'Monkey', 'Hills'],\n",
       " ['Lorentz', 'Empire', 'Pan'],\n",
       " ['SM', 'Robertson', 'Mint'],\n",
       " ['Sempre', 'Harcourt', 'Capitaine'],\n",
       " ['Portsmouth', 'Savoy', 'Coburg'],\n",
       " ['Irena', 'Lorenz', 'Satellite'],\n",
       " ['Oaks', 'EP', 'Cameron'],\n",
       " ['Fighting', 'TNI', 'JK'],\n",
       " ['Bilbao', 'Christensen', 'Sport'],\n",
       " ['Sharon', 'Jozef', 'Mitch'],\n",
       " ['Allium', 'Lotto', 'Schaus'],\n",
       " ['Haiti', 'Darling', 'York'],\n",
       " ['Isabella', 'Westminster', 'Sentinel'],\n",
       " ['Olsson', 'Camino', 'Savoy'],\n",
       " ['Pure', 'Bees', 'Spain'],\n",
       " ['Patrol', 'Friesland', 'Lucia'],\n",
       " ['Siege', 'Use', 'Points'],\n",
       " ['Galiza', 'Hubbard', 'Prima'],\n",
       " ['Lucky', 'Lego', 'Siena'],\n",
       " ['Pfeiffer', 'Campos', 'Lillehammer'],\n",
       " ['FX', 'Horne', 'Luce'],\n",
       " ['Energy', 'Palestine', 'Hindi'],\n",
       " ['Joanne', 'ET', 'Aire'],\n",
       " ['Erik', 'di', 'Bund'],\n",
       " ['EM', 'Lindsay', 'Heide'],\n",
       " ['Raum', 'Application', 'Horse'],\n",
       " ['VL', 'Elke', 'Alt'],\n",
       " ['Sabina', 'Almanya', 'Raymond'],\n",
       " ['Krupp', 'Bet', 'Bari'],\n",
       " ['October', 'Ming', 'Vladimir'],\n",
       " ['Davidson', 'Morris', 'Gates'],\n",
       " ['Allah', 'Quintana', 'Isole'],\n",
       " ['Larva', 'Moore', 'Quatre'],\n",
       " ['Khu', 'Katy', 'Darkness'],\n",
       " ['Lahore', 'Data', 'Aya'],\n",
       " ['Dorset', 'AF', 'Atatürk'],\n",
       " ['Stay', 'Dante', 'Apocalypse'],\n",
       " ['Sit', 'Falling', 'Cliff'],\n",
       " ['Wayne', 'Acre', 'Seitz'],\n",
       " ['Flag', 'Rum', 'Alcalá'],\n",
       " ['Ferns', 'Oscara', 'Marburg'],\n",
       " ['Jaguar', 'Patria', 'Fuego'],\n",
       " ['Brenda', 'Pluto', 'Sainte'],\n",
       " ['Mina', 'Membre', 'Guanajuato'],\n",
       " ['Harding', 'Champs', 'Bertram'],\n",
       " ['Giants', 'Klinik', 'Diablo'],\n",
       " ['Buta', 'Moreau', 'Isles'],\n",
       " ['Faye', 'Soldier', 'Cinema'],\n",
       " ['Generation', 'Mille', 'Eaton'],\n",
       " ['Schottland', 'Julie', 'Dorothea'],\n",
       " ['USD', 'Apocalypse', 'Nota'],\n",
       " ['Eu', 'Volga', 'ABA']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities[900:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f826ec1d",
   "metadata": {},
   "source": [
    "#### Rule: (e, r, f ) $\\land$ (f, s, g) -> (e, t, g) => (e, r_de, f ) $\\land$ (f, s_de, g) -> (e, t_de, g)\n",
    "\n",
    "- Are the composition learned in source language? (e, t, g)\n",
    "- Is there a general transfer to the target? (e, r_de, f) and (f, s_de, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814221b",
   "metadata": {},
   "source": [
    "1800 facts are training the rule (900<->900)\n",
    "1800-1900 are facts that are used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9c5eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation1: taxon rank\n",
      "Relation2: emergency services\n",
      "Composition: color\n",
      "Accuracy for learning source rule (e, t, g): 0.96\n",
      "Accuracy for learning target rule (e, t_de, g): 0.11\n",
      "Accuracy for KT2 (e, r_de, f): 0.56\n",
      "Accuracy for KT1 (f, s_de, g): 0.39\n",
      "\n",
      "\n",
      "Relation1: expected completeness\n",
      "Relation2: measured physical quantity\n",
      "Composition: open period from\n",
      "Accuracy for learning source rule (e, t, g): 0.96\n",
      "Accuracy for learning target rule (e, t_de, g): 0.06\n",
      "Accuracy for KT2 (e, r_de, f): 0.25\n",
      "Accuracy for KT1 (f, s_de, g): 0.37\n",
      "\n",
      "\n",
      "Relation1: platform\n",
      "Relation2: legislative committee\n",
      "Composition: media franchise\n",
      "Accuracy for learning source rule (e, t, g): 0.91\n",
      "Accuracy for learning target rule (e, t_de, g): 0.0\n",
      "Accuracy for KT2 (e, r_de, f): 0.19\n",
      "Accuracy for KT1 (f, s_de, g): 0.89\n",
      "\n",
      "\n",
      "Relation1: type of lens\n",
      "Relation2: natural reservoir of\n",
      "Composition: facial hair\n",
      "Accuracy for learning source rule (e, t, g): 0.97\n",
      "Accuracy for learning target rule (e, t_de, g): 0.04\n",
      "Accuracy for KT2 (e, r_de, f): 0.02\n",
      "Accuracy for KT1 (f, s_de, g): 0.88\n",
      "\n",
      "\n",
      "Relation1: reply to\n",
      "Relation2: list of works\n",
      "Composition: side effect\n",
      "Accuracy for learning source rule (e, t, g): 0.99\n",
      "Accuracy for learning target rule (e, t_de, g): 0.04\n",
      "Accuracy for KT2 (e, r_de, f): 0.3\n",
      "Accuracy for KT1 (f, s_de, g): 0.74\n",
      "\n",
      "\n",
      "Relation1: physically interacts with\n",
      "Relation2: points/goal scored by\n",
      "Composition: demonym of\n",
      "Accuracy for learning source rule (e, t, g): 0.94\n",
      "Accuracy for learning target rule (e, t_de, g): 0.27\n",
      "Accuracy for KT2 (e, r_de, f): 0.96\n",
      "Accuracy for KT1 (f, s_de, g): 1.0\n",
      "\n",
      "\n",
      "Relation1: conflict\n",
      "Relation2: culture\n",
      "Composition: recording or performance of\n",
      "Accuracy for learning source rule (e, t, g): 0.96\n",
      "Accuracy for learning target rule (e, t_de, g): 0.87\n",
      "Accuracy for KT2 (e, r_de, f): 0.03\n",
      "Accuracy for KT1 (f, s_de, g): 0.96\n",
      "\n",
      "\n",
      "Relation1: school district\n",
      "Relation2: vehicle\n",
      "Composition: sex or gender\n",
      "Accuracy for learning source rule (e, t, g): 0.94\n",
      "Accuracy for learning target rule (e, t_de, g): 0.0\n",
      "Accuracy for KT2 (e, r_de, f): 0.18\n",
      "Accuracy for KT1 (f, s_de, g): 0.95\n",
      "\n",
      "\n",
      "Relation1: designated as terrorist by\n",
      "Relation2: typically sells\n",
      "Composition: main building contractor\n",
      "Accuracy for learning source rule (e, t, g): 0.94\n",
      "Accuracy for learning target rule (e, t_de, g): 0.67\n",
      "Accuracy for KT2 (e, r_de, f): 1.0\n",
      "Accuracy for KT1 (f, s_de, g): 0.67\n",
      "\n",
      "\n",
      "Relation1: site of astronomical discovery\n",
      "Relation2: primary destinations\n",
      "Composition: different from\n",
      "Accuracy for learning source rule (e, t, g): 0.97\n",
      "Accuracy for learning target rule (e, t_de, g): 0.18\n",
      "Accuracy for KT2 (e, r_de, f): 0.91\n",
      "Accuracy for KT1 (f, s_de, g): 0.75\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over relations, take the training samples that were trained on\n",
    "for i in range(n_relations):\n",
    "    trained_test = train_dict['sample'][1800+i*1900:(i+1)*1900]\n",
    "    entities_sampled = entities[900+i*1000:(i+1)*1000]\n",
    "    \n",
    "    acc_source = 0\n",
    "    acc_target = 0\n",
    "    acc_transfer1 = 0\n",
    "    acc_transfer2 = 0  \n",
    "    \n",
    "    # Relation pairs!\n",
    "    r = relations[0]['en'].iloc[i]\n",
    "    s = relations[1]['en'].iloc[i]\n",
    "    t = relations[2]['en'].iloc[i]\n",
    "\n",
    "    r_de = relations[0]['de'].iloc[i]\n",
    "    s_de = relations[1]['de'].iloc[i]\n",
    "    t_de = relations[2]['de'].iloc[i]\n",
    "\n",
    "    for j, sample in enumerate(trained_test):\n",
    "\n",
    "        ents = entities_sampled[j]\n",
    "        e = ents[0]\n",
    "        f = ents[1]\n",
    "        g = ents[2]\n",
    "\n",
    "        # Test (e, t, g)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(g)\n",
    "\n",
    "        prompt = e + ' ' + t + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_source += 1\n",
    "            \n",
    "        # Test (e, t, g)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(g)\n",
    "\n",
    "        prompt = e + ' ' + t_de + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_target += 1\n",
    "\n",
    "        # Test (e, r_de, f)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(f)\n",
    "\n",
    "        prompt = e + ' ' + r_de + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_transfer1 += 1\n",
    "            \n",
    "        # Test (f, s_de, g)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(g)\n",
    "\n",
    "        prompt = f + ' ' + s_de + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_transfer2 += 1\n",
    "        \n",
    "\n",
    "    acc_source /= 100\n",
    "    acc_target /= 100\n",
    "    acc_transfer1 /= 100\n",
    "    acc_transfer2 /= 100\n",
    "\n",
    "    print(f'Relation1: {r}')\n",
    "    print(f'Relation2: {s}')\n",
    "    print(f'Composition: {t}')\n",
    "    print(f'Accuracy for learning source rule (e, t, g): {acc_source}')\n",
    "    print(f'Accuracy for learning target rule (e, t_de, g): {acc_target}')\n",
    "    print(f'Accuracy for KT2 (e, r_de, f): {acc_transfer1}')\n",
    "    print(f'Accuracy for KT1 (f, s_de, g): {acc_transfer2}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6b182",
   "metadata": {},
   "source": [
    "### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bcc1227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "total = len(train_dict['sample'])\n",
    "i = 0\n",
    "\n",
    "for txt in train_dict['sample'][:10000]:\n",
    "    i += 1\n",
    "    \n",
    "    # Add [MASK] for object\n",
    "    sample = txt.rsplit(' ', 1)[0] + ' [MASK]'\n",
    "    label_token = tokenizer.convert_tokens_to_ids(txt.rsplit(' ', 1)[1])\n",
    "    \n",
    "    encoded_input = tokenizer(sample, return_tensors='pt')\n",
    "    token_logits = model(**encoded_input).logits\n",
    "    \n",
    "    mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    \n",
    "    # Pick the [MASK] candidates with the highest logits\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "    \n",
    "    if label_token in top_5_tokens:\n",
    "        k += 1\n",
    "\n",
    "print(k/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b299262",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"lens manner of [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n>>> {tokenizer.decode([chunk])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b25e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_dict['sample']:\n",
    "    if 'Alex' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc95dd",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aecc52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
