{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e4031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer, TrainingArguments, Trainer, \\\n",
    "    DataCollatorForLanguageModeling, IntervalStrategy\n",
    "\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "from data_generation_relation import *\n",
    "from utils import *\n",
    "from custom_trainer import CustomTrainer\n",
    "from datasets import load_metric\n",
    "import logging\n",
    "from transformers import logging as tlogging\n",
    "import wandb\n",
    "import sys\n",
    "from utils import set_seed\n",
    "from transformers.integrations import WandbCallback, TensorBoardCallback\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735ddbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "run_name = 'Negation_en_de_20_opposite'\n",
    "epochs = 200\n",
    "batch_size = 256\n",
    "lr = 5e-5\n",
    "\n",
    "relation = 'negation'\n",
    "source_language = ['en']\n",
    "target_language = ['de']\n",
    "n_relations = 10\n",
    "n_facts = 1000\n",
    "n_pairs = 20\n",
    "\n",
    "use_random = False\n",
    "\n",
    "use_general = False\n",
    "use_pretrained = False\n",
    "use_target = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cd5ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "      <th>count</th>\n",
       "      <th>en_negated</th>\n",
       "      <th>de_negated</th>\n",
       "      <th>fr_negated</th>\n",
       "      <th>es_negated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>690</td>\n",
       "      <td>P4032</td>\n",
       "      <td>reviewed by</td>\n",
       "      <td>überprüft von</td>\n",
       "      <td>revisado por</td>\n",
       "      <td>examiné par</td>\n",
       "      <td>507</td>\n",
       "      <td>not reviewed by</td>\n",
       "      <td>nicht überprüft von</td>\n",
       "      <td>ne pas examiné par</td>\n",
       "      <td>no revisado por</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>775</td>\n",
       "      <td>P514</td>\n",
       "      <td>interleaves with</td>\n",
       "      <td>verzahnt mit</td>\n",
       "      <td>intercalada con</td>\n",
       "      <td>s'imbrique avec</td>\n",
       "      <td>31</td>\n",
       "      <td>not interleaves with</td>\n",
       "      <td>nicht verzahnt mit</td>\n",
       "      <td>ne pas s'imbrique avec</td>\n",
       "      <td>no intercalada con</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>P3027</td>\n",
       "      <td>open period from</td>\n",
       "      <td>geöffnet von Zeitpunkt</td>\n",
       "      <td>abierto desde</td>\n",
       "      <td>début de la période d'ouverture</td>\n",
       "      <td>16</td>\n",
       "      <td>not open period from</td>\n",
       "      <td>nicht geöffnet von Zeitpunkt</td>\n",
       "      <td>ne pas début de la période d'ouverture</td>\n",
       "      <td>no abierto desde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>531</td>\n",
       "      <td>P3301</td>\n",
       "      <td>broadcast by</td>\n",
       "      <td>ausgestrahlt von</td>\n",
       "      <td>transmitido por</td>\n",
       "      <td>diffusé par</td>\n",
       "      <td>7411</td>\n",
       "      <td>not broadcast by</td>\n",
       "      <td>nicht ausgestrahlt von</td>\n",
       "      <td>ne pas diffusé par</td>\n",
       "      <td>no transmitido por</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>P1455</td>\n",
       "      <td>list of works</td>\n",
       "      <td>Werkliste</td>\n",
       "      <td>lista de obras</td>\n",
       "      <td>liste des œuvres</td>\n",
       "      <td>1227</td>\n",
       "      <td>not list of works</td>\n",
       "      <td>nicht Werkliste</td>\n",
       "      <td>ne pas liste des œuvres</td>\n",
       "      <td>no lista de obras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>748</td>\n",
       "      <td>P1038</td>\n",
       "      <td>relative</td>\n",
       "      <td>Verwandte</td>\n",
       "      <td>pariente</td>\n",
       "      <td>parentèle</td>\n",
       "      <td>51652</td>\n",
       "      <td>not relative</td>\n",
       "      <td>nicht Verwandte</td>\n",
       "      <td>ne pas parentèle</td>\n",
       "      <td>no pariente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>382</td>\n",
       "      <td>P1302</td>\n",
       "      <td>primary destinations</td>\n",
       "      <td>Hauptorte</td>\n",
       "      <td>destinos principales</td>\n",
       "      <td>principales localités desservies</td>\n",
       "      <td>3923</td>\n",
       "      <td>not primary destinations</td>\n",
       "      <td>nicht Hauptorte</td>\n",
       "      <td>ne pas principales localités desservies</td>\n",
       "      <td>no destinos principales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>682</td>\n",
       "      <td>P552</td>\n",
       "      <td>handedness</td>\n",
       "      <td>Händigkeit</td>\n",
       "      <td>lateralidad</td>\n",
       "      <td>latéralisation</td>\n",
       "      <td>2256</td>\n",
       "      <td>not handedness</td>\n",
       "      <td>nicht Händigkeit</td>\n",
       "      <td>ne pas latéralisation</td>\n",
       "      <td>no lateralidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>554</td>\n",
       "      <td>P4878</td>\n",
       "      <td>symbolizes</td>\n",
       "      <td>symbolisiert</td>\n",
       "      <td>simboliza</td>\n",
       "      <td>symbolise</td>\n",
       "      <td>51</td>\n",
       "      <td>not symbolizes</td>\n",
       "      <td>nicht symbolisiert</td>\n",
       "      <td>ne pas symbolise</td>\n",
       "      <td>no simboliza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>624</td>\n",
       "      <td>P3438</td>\n",
       "      <td>vehicle normally used</td>\n",
       "      <td>normalerweise verwendetes Fahrzeug</td>\n",
       "      <td>vehículo usado normalmente</td>\n",
       "      <td>véhicule habituellement utilisé</td>\n",
       "      <td>2892</td>\n",
       "      <td>not vehicle normally used</td>\n",
       "      <td>nicht normalerweise verwendetes Fahrzeug</td>\n",
       "      <td>ne pas véhicule habituellement utilisé</td>\n",
       "      <td>no vehículo usado normalmente</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     id                     en  \\\n",
       "690         690  P4032            reviewed by   \n",
       "775         775   P514       interleaves with   \n",
       "63           63  P3027       open period from   \n",
       "531         531  P3301           broadcast by   \n",
       "66           66  P1455          list of works   \n",
       "748         748  P1038               relative   \n",
       "382         382  P1302   primary destinations   \n",
       "682         682   P552             handedness   \n",
       "554         554  P4878             symbolizes   \n",
       "624         624  P3438  vehicle normally used   \n",
       "\n",
       "                                     de                          es  \\\n",
       "690                       überprüft von                revisado por   \n",
       "775                        verzahnt mit             intercalada con   \n",
       "63               geöffnet von Zeitpunkt               abierto desde   \n",
       "531                    ausgestrahlt von             transmitido por   \n",
       "66                            Werkliste              lista de obras   \n",
       "748                           Verwandte                    pariente   \n",
       "382                           Hauptorte        destinos principales   \n",
       "682                          Händigkeit                 lateralidad   \n",
       "554                        symbolisiert                   simboliza   \n",
       "624  normalerweise verwendetes Fahrzeug  vehículo usado normalmente   \n",
       "\n",
       "                                   fr  count                 en_negated  \\\n",
       "690                       examiné par    507            not reviewed by   \n",
       "775                   s'imbrique avec     31       not interleaves with   \n",
       "63    début de la période d'ouverture     16       not open period from   \n",
       "531                       diffusé par   7411           not broadcast by   \n",
       "66                   liste des œuvres   1227          not list of works   \n",
       "748                         parentèle  51652               not relative   \n",
       "382  principales localités desservies   3923   not primary destinations   \n",
       "682                    latéralisation   2256             not handedness   \n",
       "554                         symbolise     51             not symbolizes   \n",
       "624   véhicule habituellement utilisé   2892  not vehicle normally used   \n",
       "\n",
       "                                   de_negated  \\\n",
       "690                       nicht überprüft von   \n",
       "775                        nicht verzahnt mit   \n",
       "63               nicht geöffnet von Zeitpunkt   \n",
       "531                    nicht ausgestrahlt von   \n",
       "66                            nicht Werkliste   \n",
       "748                           nicht Verwandte   \n",
       "382                           nicht Hauptorte   \n",
       "682                          nicht Händigkeit   \n",
       "554                        nicht symbolisiert   \n",
       "624  nicht normalerweise verwendetes Fahrzeug   \n",
       "\n",
       "                                  fr_negated                     es_negated  \n",
       "690                       ne pas examiné par                no revisado por  \n",
       "775                   ne pas s'imbrique avec             no intercalada con  \n",
       "63    ne pas début de la période d'ouverture               no abierto desde  \n",
       "531                       ne pas diffusé par             no transmitido por  \n",
       "66                   ne pas liste des œuvres              no lista de obras  \n",
       "748                         ne pas parentèle                    no pariente  \n",
       "382  ne pas principales localités desservies        no destinos principales  \n",
       "682                    ne pas latéralisation                 no lateralidad  \n",
       "554                         ne pas symbolise                   no simboliza  \n",
       "624   ne pas véhicule habituellement utilisé  no vehículo usado normalmente  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test, relations, entities = generate_reasoning(relation=Relation(relation),\n",
    "                                                        source_language=source_language,\n",
    "                                                        target_language=target_language,\n",
    "                                                        n_relations=n_relations,\n",
    "                                                        n_facts=n_facts,\n",
    "                                                        use_pretrained=use_pretrained,\n",
    "                                                        use_target=use_target,\n",
    "                                                        use_enhanced=False,\n",
    "                                                        use_same_relations=False,\n",
    "                                                        n_pairs=n_pairs)\n",
    "\n",
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52da60a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Medina', 'Italie', 'Bora'],\n",
       " ['Invasion', 'Hus', 'epi'],\n",
       " ['Burke', 'Wilfried', 'Fach'],\n",
       " ['Drama', 'Inge', 'Elite'],\n",
       " ['Master', 'Portland', 'Eliza'],\n",
       " ['Dari', 'Weir', 'Antoine'],\n",
       " ['Chihuahua', 'Universitas', 'Collins'],\n",
       " ['EP', 'Condor', 'Gospel'],\n",
       " ['Chase', 'MS', 'Ramsey'],\n",
       " ['Worcester', 'Katz', 'Saussure'],\n",
       " ['Albert', 'BRT', 'Carioca'],\n",
       " ['Ibiza', 'Note', 'Ultra'],\n",
       " ['Câmara', 'Né', 'Fidel'],\n",
       " ['Eleanor', 'Einer', 'Everybody'],\n",
       " ['Campus', 'Pizza', 'Koska'],\n",
       " ['Schaus', 'Ariane', 'Exil'],\n",
       " ['Chaos', 'Holy', 'Haar'],\n",
       " ['Alice', 'Ringo', 'Theresa'],\n",
       " ['Franklin', 'Zone', 'Benth'],\n",
       " ['Colombo', 'Baja', 'Ruska'],\n",
       " ['Graf', 'Italie', 'Bora'],\n",
       " ['Darwin', 'Hus', 'epi'],\n",
       " ['Nisan', 'Wilfried', 'Fach'],\n",
       " ['Hartman', 'Inge', 'Elite'],\n",
       " ['Lur', 'Portland', 'Eliza'],\n",
       " ['Fiesta', 'Weir', 'Antoine'],\n",
       " ['Alexandra', 'Universitas', 'Collins'],\n",
       " ['Mérida', 'Condor', 'Gospel'],\n",
       " ['Shire', 'MS', 'Ramsey'],\n",
       " ['Anthology', 'Katz', 'Saussure'],\n",
       " ['Monroe', 'BRT', 'Carioca'],\n",
       " ['Hell', 'Note', 'Ultra'],\n",
       " ['Bandet', 'Né', 'Fidel'],\n",
       " ['Bertrand', 'Einer', 'Everybody'],\n",
       " ['Drum', 'Pizza', 'Koska'],\n",
       " ['IBM', 'Ariane', 'Exil'],\n",
       " ['Christchurch', 'Holy', 'Haar'],\n",
       " ['Broadcast', 'Ringo', 'Theresa'],\n",
       " ['Clay', 'Zone', 'Benth'],\n",
       " ['Sound', 'Baja', 'Ruska'],\n",
       " ['Liga', 'Italie', 'Bora'],\n",
       " ['Vatican', 'Hus', 'epi'],\n",
       " ['Libro', 'Wilfried', 'Fach'],\n",
       " ['Vanderbilt', 'Inge', 'Elite'],\n",
       " ['Austrian', 'Portland', 'Eliza'],\n",
       " ['Varese', 'Weir', 'Antoine'],\n",
       " ['Ninja', 'Universitas', 'Collins'],\n",
       " ['Lafayette', 'Condor', 'Gospel'],\n",
       " ['Catalina', 'MS', 'Ramsey'],\n",
       " ['Companion', 'Katz', 'Saussure'],\n",
       " ['Marshal', 'BRT', 'Carioca'],\n",
       " ['Molina', 'Note', 'Ultra'],\n",
       " ['Dow', 'Né', 'Fidel'],\n",
       " ['Platte', 'Einer', 'Everybody'],\n",
       " ['Giant', 'Pizza', 'Koska'],\n",
       " ['Mulder', 'Ariane', 'Exil'],\n",
       " ['Hancock', 'Holy', 'Haar'],\n",
       " ['Deutschland', 'Ringo', 'Theresa'],\n",
       " ['Della', 'Zone', 'Benth'],\n",
       " ['Niko', 'Baja', 'Ruska'],\n",
       " ['Romans', 'Italie', 'Bora'],\n",
       " ['Salman', 'Hus', 'epi'],\n",
       " ['Medi', 'Wilfried', 'Fach'],\n",
       " ['Monaco', 'Inge', 'Elite'],\n",
       " ['Release', 'Portland', 'Eliza'],\n",
       " ['Daughter', 'Weir', 'Antoine'],\n",
       " ['Kendall', 'Universitas', 'Collins'],\n",
       " ['PCR', 'Condor', 'Gospel'],\n",
       " ['Lorena', 'MS', 'Ramsey'],\n",
       " ['Twain', 'Katz', 'Saussure'],\n",
       " ['Head', 'BRT', 'Carioca'],\n",
       " ['Porta', 'Note', 'Ultra'],\n",
       " ['Yunan', 'Né', 'Fidel'],\n",
       " ['MGM', 'Einer', 'Everybody'],\n",
       " ['Queste', 'Pizza', 'Koska'],\n",
       " ['Geld', 'Ariane', 'Exil'],\n",
       " ['Jesus', 'Holy', 'Haar'],\n",
       " ['Potter', 'Ringo', 'Theresa'],\n",
       " ['Crown', 'Zone', 'Benth'],\n",
       " ['Adi', 'Baja', 'Ruska'],\n",
       " ['Sinn', 'Italie', 'Bora'],\n",
       " ['Vacelet', 'Hus', 'epi'],\n",
       " ['Sagan', 'Wilfried', 'Fach'],\n",
       " ['Madsen', 'Inge', 'Elite'],\n",
       " ['Robinson', 'Portland', 'Eliza'],\n",
       " ['Fayette', 'Weir', 'Antoine'],\n",
       " ['Grant', 'Universitas', 'Collins'],\n",
       " ['Julian', 'Condor', 'Gospel'],\n",
       " ['Carrillo', 'MS', 'Ramsey'],\n",
       " ['Aus', 'Katz', 'Saussure'],\n",
       " ['BBC', 'BRT', 'Carioca'],\n",
       " ['Kongo', 'Note', 'Ultra'],\n",
       " ['Gaur', 'Né', 'Fidel'],\n",
       " ['Shining', 'Einer', 'Everybody'],\n",
       " ['Simpson', 'Pizza', 'Koska'],\n",
       " ['Douglas', 'Ariane', 'Exil'],\n",
       " ['Continental', 'Holy', 'Haar'],\n",
       " ['West', 'Ringo', 'Theresa'],\n",
       " ['Haas', 'Zone', 'Benth'],\n",
       " ['Lynch', 'Baja', 'Ruska'],\n",
       " ['Cornwall', 'Italie', 'Bora'],\n",
       " ['Jalan', 'Hus', 'epi'],\n",
       " ['Dhaka', 'Wilfried', 'Fach'],\n",
       " ['Reading', 'Inge', 'Elite'],\n",
       " ['Auckland', 'Portland', 'Eliza'],\n",
       " ['Chancellor', 'Weir', 'Antoine'],\n",
       " ['Dakota', 'Universitas', 'Collins'],\n",
       " ['Arles', 'Condor', 'Gospel'],\n",
       " ['Loan', 'MS', 'Ramsey'],\n",
       " ['Stab', 'Katz', 'Saussure'],\n",
       " ['Golden', 'BRT', 'Carioca'],\n",
       " ['Jakiel', 'Note', 'Ultra'],\n",
       " ['Wade', 'Né', 'Fidel'],\n",
       " ['Ebro', 'Einer', 'Everybody'],\n",
       " ['Dorothea', 'Pizza', 'Koska'],\n",
       " ['Urban', 'Ariane', 'Exil'],\n",
       " ['HF', 'Holy', 'Haar'],\n",
       " ['Spectrum', 'Ringo', 'Theresa'],\n",
       " ['Cass', 'Zone', 'Benth'],\n",
       " ['Riva', 'Baja', 'Ruska'],\n",
       " ['Winston', 'Italie', 'Bora'],\n",
       " ['Punk', 'Hus', 'epi'],\n",
       " ['SMS', 'Wilfried', 'Fach'],\n",
       " ['Paglinawan', 'Inge', 'Elite'],\n",
       " ['McKay', 'Portland', 'Eliza'],\n",
       " ['Harmony', 'Weir', 'Antoine'],\n",
       " ['Chantal', 'Universitas', 'Collins'],\n",
       " ['Taurus', 'Condor', 'Gospel'],\n",
       " ['Crosby', 'MS', 'Ramsey'],\n",
       " ['GSC', 'Katz', 'Saussure'],\n",
       " ['Moonlight', 'BRT', 'Carioca'],\n",
       " ['Goodman', 'Note', 'Ultra'],\n",
       " ['Stéphane', 'Né', 'Fidel'],\n",
       " ['Kader', 'Einer', 'Everybody'],\n",
       " ['Ferro', 'Pizza', 'Koska'],\n",
       " ['Anglo', 'Ariane', 'Exil'],\n",
       " ['Flesh', 'Holy', 'Haar'],\n",
       " ['Ubuntu', 'Ringo', 'Theresa'],\n",
       " ['GMT', 'Zone', 'Benth'],\n",
       " ['Quintana', 'Baja', 'Ruska'],\n",
       " ['Lublin', 'Italie', 'Bora'],\n",
       " ['Nagar', 'Hus', 'epi'],\n",
       " ['Dictionary', 'Wilfried', 'Fach'],\n",
       " ['Cats', 'Inge', 'Elite'],\n",
       " ['Beyoncé', 'Portland', 'Eliza'],\n",
       " ['Mendoza', 'Weir', 'Antoine'],\n",
       " ['Roja', 'Universitas', 'Collins'],\n",
       " ['Beckett', 'Condor', 'Gospel'],\n",
       " ['Toulouse', 'MS', 'Ramsey'],\n",
       " ['Summit', 'Katz', 'Saussure'],\n",
       " ['Champion', 'BRT', 'Carioca'],\n",
       " ['Twins', 'Note', 'Ultra'],\n",
       " ['Halo', 'Né', 'Fidel'],\n",
       " ['Deborah', 'Einer', 'Everybody'],\n",
       " ['SF', 'Pizza', 'Koska'],\n",
       " ['Commonwealth', 'Ariane', 'Exil'],\n",
       " ['Dublin', 'Holy', 'Haar'],\n",
       " ['Apocalypse', 'Ringo', 'Theresa'],\n",
       " ['Drake', 'Zone', 'Benth'],\n",
       " ['Padang', 'Baja', 'Ruska'],\n",
       " ['Royal', 'Italie', 'Bora'],\n",
       " ['Quick', 'Hus', 'epi'],\n",
       " ['Remote', 'Wilfried', 'Fach'],\n",
       " ['Player', 'Inge', 'Elite'],\n",
       " ['Laba', 'Portland', 'Eliza'],\n",
       " ['Wiener', 'Weir', 'Antoine'],\n",
       " ['Tripoli', 'Universitas', 'Collins'],\n",
       " ['Glasgow', 'Condor', 'Gospel'],\n",
       " ['Hansen', 'MS', 'Ramsey'],\n",
       " ['Ortes', 'Katz', 'Saussure'],\n",
       " ['Mozilla', 'BRT', 'Carioca'],\n",
       " ['Mur', 'Note', 'Ultra'],\n",
       " ['Iberia', 'Né', 'Fidel'],\n",
       " ['Vir', 'Einer', 'Everybody'],\n",
       " ['Prato', 'Pizza', 'Koska'],\n",
       " ['Ferris', 'Ariane', 'Exil'],\n",
       " ['Racing', 'Holy', 'Haar'],\n",
       " ['Steiner', 'Ringo', 'Theresa'],\n",
       " ['Sala', 'Zone', 'Benth'],\n",
       " ['Connection', 'Baja', 'Ruska'],\n",
       " ['Hastings', 'Italie', 'Bora'],\n",
       " ['Siegfried', 'Hus', 'epi'],\n",
       " ['Kale', 'Wilfried', 'Fach'],\n",
       " ['Fourier', 'Inge', 'Elite'],\n",
       " ['Pada', 'Portland', 'Eliza'],\n",
       " ['Roberto', 'Weir', 'Antoine'],\n",
       " ['Struggle', 'Universitas', 'Collins'],\n",
       " ['Elijah', 'Condor', 'Gospel'],\n",
       " ['Rote', 'MS', 'Ramsey'],\n",
       " ['Christi', 'Katz', 'Saussure'],\n",
       " ['Berge', 'BRT', 'Carioca'],\n",
       " ['Wimbledon', 'Note', 'Ultra'],\n",
       " ['Bees', 'Né', 'Fidel'],\n",
       " ['Palm', 'Einer', 'Everybody'],\n",
       " ['Nor', 'Pizza', 'Koska'],\n",
       " ['Maria', 'Ariane', 'Exil'],\n",
       " ['Stadio', 'Holy', 'Haar'],\n",
       " ['Belo', 'Ringo', 'Theresa'],\n",
       " ['Boga', 'Zone', 'Benth'],\n",
       " ['NSW', 'Baja', 'Ruska'],\n",
       " ['Roller', 'Italie', 'Bora'],\n",
       " ['León', 'Hus', 'epi'],\n",
       " ['Rollen', 'Wilfried', 'Fach'],\n",
       " ['Agama', 'Inge', 'Elite'],\n",
       " ['Seminary', 'Portland', 'Eliza'],\n",
       " ['Ensemble', 'Weir', 'Antoine'],\n",
       " ['Blade', 'Universitas', 'Collins'],\n",
       " ['Cochrane', 'Condor', 'Gospel'],\n",
       " ['Indiana', 'MS', 'Ramsey'],\n",
       " ['Carvalho', 'Katz', 'Saussure'],\n",
       " ['René', 'BRT', 'Carioca'],\n",
       " ['Mask', 'Note', 'Ultra'],\n",
       " ['Dacia', 'Né', 'Fidel'],\n",
       " ['Cea', 'Einer', 'Everybody'],\n",
       " ['Bulgaria', 'Pizza', 'Koska'],\n",
       " ['Rocket', 'Ariane', 'Exil'],\n",
       " ['Pro', 'Holy', 'Haar'],\n",
       " ['Portugal', 'Ringo', 'Theresa'],\n",
       " ['Blanco', 'Zone', 'Benth'],\n",
       " ['Addison', 'Baja', 'Ruska'],\n",
       " ['Como', 'Italie', 'Bora'],\n",
       " ['Suomi', 'Hus', 'epi'],\n",
       " ['Alt', 'Wilfried', 'Fach'],\n",
       " ['Esther', 'Inge', 'Elite'],\n",
       " ['Sick', 'Portland', 'Eliza'],\n",
       " ['Bowman', 'Weir', 'Antoine'],\n",
       " ['Wells', 'Universitas', 'Collins'],\n",
       " ['NT', 'Condor', 'Gospel'],\n",
       " ['Titanic', 'MS', 'Ramsey'],\n",
       " ['Chamber', 'Katz', 'Saussure'],\n",
       " ['Satellite', 'BRT', 'Carioca'],\n",
       " ['Niels', 'Note', 'Ultra'],\n",
       " ['Borneo', 'Né', 'Fidel'],\n",
       " ['Tigre', 'Einer', 'Everybody'],\n",
       " ['Madagascar', 'Pizza', 'Koska'],\n",
       " ['Vincent', 'Ariane', 'Exil'],\n",
       " ['Midlands', 'Holy', 'Haar'],\n",
       " ['Siam', 'Ringo', 'Theresa'],\n",
       " ['Uit', 'Zone', 'Benth'],\n",
       " ['CDC', 'Baja', 'Ruska'],\n",
       " ['Sawyer', 'Italie', 'Bora'],\n",
       " ['Ranking', 'Hus', 'epi'],\n",
       " ['Babylon', 'Wilfried', 'Fach'],\n",
       " ['Côte', 'Inge', 'Elite'],\n",
       " ['IS', 'Portland', 'Eliza'],\n",
       " ['Frost', 'Weir', 'Antoine'],\n",
       " ['Mariana', 'Universitas', 'Collins'],\n",
       " ['Baza', 'Condor', 'Gospel'],\n",
       " ['Washington', 'MS', 'Ramsey'],\n",
       " ['Giles', 'Katz', 'Saussure'],\n",
       " ['Benton', 'BRT', 'Carioca'],\n",
       " ['Balázs', 'Note', 'Ultra'],\n",
       " ['Pays', 'Né', 'Fidel'],\n",
       " ['Alman', 'Einer', 'Everybody'],\n",
       " ['Hitchcock', 'Pizza', 'Koska'],\n",
       " ['Danube', 'Ariane', 'Exil'],\n",
       " ['Quattro', 'Holy', 'Haar'],\n",
       " ['Troy', 'Ringo', 'Theresa'],\n",
       " ['Hector', 'Zone', 'Benth'],\n",
       " ['Hip', 'Baja', 'Ruska'],\n",
       " ['Face', 'Italie', 'Bora'],\n",
       " ['Funk', 'Hus', 'epi'],\n",
       " ['Hazel', 'Wilfried', 'Fach'],\n",
       " ['Romawi', 'Inge', 'Elite'],\n",
       " ['Straits', 'Portland', 'Eliza'],\n",
       " ['Put', 'Weir', 'Antoine'],\n",
       " ['ATP', 'Universitas', 'Collins'],\n",
       " ['Abucay', 'Condor', 'Gospel'],\n",
       " ['Spor', 'MS', 'Ramsey'],\n",
       " ['Cathedral', 'Katz', 'Saussure'],\n",
       " ['Vijay', 'BRT', 'Carioca'],\n",
       " ['Géza', 'Note', 'Ultra'],\n",
       " ['Werk', 'Né', 'Fidel'],\n",
       " ['Jungen', 'Einer', 'Everybody'],\n",
       " ['Harri', 'Pizza', 'Koska'],\n",
       " ['Wood', 'Ariane', 'Exil'],\n",
       " ['XP', 'Holy', 'Haar'],\n",
       " ['Henning', 'Ringo', 'Theresa'],\n",
       " ['Daniel', 'Zone', 'Benth'],\n",
       " ['Lac', 'Baja', 'Ruska'],\n",
       " ['Niger', 'Italie', 'Bora'],\n",
       " ['Bon', 'Hus', 'epi'],\n",
       " ['Hess', 'Wilfried', 'Fach'],\n",
       " ['Asunción', 'Inge', 'Elite'],\n",
       " ['ba', 'Portland', 'Eliza'],\n",
       " ['Roll', 'Weir', 'Antoine'],\n",
       " ['Nada', 'Universitas', 'Collins'],\n",
       " ['Saale', 'Condor', 'Gospel'],\n",
       " ['Minden', 'MS', 'Ramsey'],\n",
       " ['Imperial', 'Katz', 'Saussure'],\n",
       " ['Moreau', 'BRT', 'Carioca'],\n",
       " ['Provence', 'Note', 'Ultra'],\n",
       " ['Archer', 'Né', 'Fidel'],\n",
       " ['JNA', 'Einer', 'Everybody'],\n",
       " ['Punjabi', 'Pizza', 'Koska'],\n",
       " ['Tarragona', 'Ariane', 'Exil'],\n",
       " ['Pirates', 'Holy', 'Haar'],\n",
       " ['Mat', 'Ringo', 'Theresa'],\n",
       " ['Berlin', 'Zone', 'Benth'],\n",
       " ['Thành', 'Baja', 'Ruska'],\n",
       " ['Vance', 'Italie', 'Bora'],\n",
       " ['Banja', 'Hus', 'epi'],\n",
       " ['Champ', 'Wilfried', 'Fach'],\n",
       " ['Trieste', 'Inge', 'Elite'],\n",
       " ['Pi', 'Portland', 'Eliza'],\n",
       " ['Cisco', 'Weir', 'Antoine'],\n",
       " ['Cologne', 'Universitas', 'Collins'],\n",
       " ['Banks', 'Condor', 'Gospel'],\n",
       " ['MotoGP', 'MS', 'Ramsey'],\n",
       " ['Spiegel', 'Katz', 'Saussure'],\n",
       " ['Dara', 'BRT', 'Carioca'],\n",
       " ['Platinum', 'Note', 'Ultra'],\n",
       " ['Calvin', 'Né', 'Fidel'],\n",
       " ['Gauss', 'Einer', 'Everybody'],\n",
       " ['Lena', 'Pizza', 'Koska'],\n",
       " ['Voltaire', 'Ariane', 'Exil'],\n",
       " ['Polar', 'Holy', 'Haar'],\n",
       " ['Depot', 'Ringo', 'Theresa'],\n",
       " ['Petit', 'Zone', 'Benth'],\n",
       " ['Einstein', 'Baja', 'Ruska'],\n",
       " ['Bay', 'Italie', 'Bora'],\n",
       " ['Skin', 'Hus', 'epi'],\n",
       " ['Donna', 'Wilfried', 'Fach'],\n",
       " ['Pasteur', 'Inge', 'Elite'],\n",
       " ['Seul', 'Portland', 'Eliza'],\n",
       " ['Middlesex', 'Weir', 'Antoine'],\n",
       " ['Closer', 'Universitas', 'Collins'],\n",
       " ['Hollow', 'Condor', 'Gospel'],\n",
       " ['Malang', 'MS', 'Ramsey'],\n",
       " ['KPD', 'Katz', 'Saussure'],\n",
       " ['Palma', 'BRT', 'Carioca'],\n",
       " ['AFL', 'Note', 'Ultra'],\n",
       " ['Accademia', 'Né', 'Fidel'],\n",
       " ['Leafs', 'Einer', 'Everybody'],\n",
       " ['Principal', 'Pizza', 'Koska'],\n",
       " ['Carla', 'Ariane', 'Exil'],\n",
       " ['Rally', 'Holy', 'Haar'],\n",
       " ['Lilla', 'Ringo', 'Theresa'],\n",
       " ['Gamble', 'Zone', 'Benth'],\n",
       " ['Alec', 'Baja', 'Ruska'],\n",
       " ['Medvedev', 'Italie', 'Bora'],\n",
       " ['Nino', 'Hus', 'epi'],\n",
       " ['Willard', 'Wilfried', 'Fach'],\n",
       " ['Seoul', 'Inge', 'Elite'],\n",
       " ['Bala', 'Portland', 'Eliza'],\n",
       " ['Stift', 'Weir', 'Antoine'],\n",
       " ['Albania', 'Universitas', 'Collins'],\n",
       " ['Grund', 'Condor', 'Gospel'],\n",
       " ['Indre', 'MS', 'Ramsey'],\n",
       " ['Poslední', 'Katz', 'Saussure'],\n",
       " ['Switch', 'BRT', 'Carioca'],\n",
       " ['Coral', 'Note', 'Ultra'],\n",
       " ['Ethel', 'Né', 'Fidel'],\n",
       " ['Charlie', 'Einer', 'Everybody'],\n",
       " ['Warren', 'Pizza', 'Koska'],\n",
       " ['Irvine', 'Ariane', 'Exil'],\n",
       " ['Quinta', 'Holy', 'Haar'],\n",
       " ['Up', 'Ringo', 'Theresa'],\n",
       " ['Heft', 'Zone', 'Benth'],\n",
       " ['Cap', 'Baja', 'Ruska'],\n",
       " ['NN', 'Italie', 'Bora'],\n",
       " ['Romas', 'Hus', 'epi'],\n",
       " ['Benedict', 'Wilfried', 'Fach'],\n",
       " ['Zurich', 'Inge', 'Elite'],\n",
       " ['Capitaine', 'Portland', 'Eliza'],\n",
       " ['Camilla', 'Weir', 'Antoine'],\n",
       " ['Theatre', 'Universitas', 'Collins'],\n",
       " ['Eo', 'Condor', 'Gospel'],\n",
       " ['Kelley', 'MS', 'Ramsey'],\n",
       " ['Ola', 'Katz', 'Saussure'],\n",
       " ['Gibraltar', 'BRT', 'Carioca'],\n",
       " ['Alabama', 'Note', 'Ultra'],\n",
       " ['Revue', 'Né', 'Fidel'],\n",
       " ['Duran', 'Einer', 'Everybody'],\n",
       " ['Guía', 'Pizza', 'Koska'],\n",
       " ['Wanda', 'Ariane', 'Exil'],\n",
       " ['Piemonte', 'Holy', 'Haar'],\n",
       " ['Impact', 'Ringo', 'Theresa'],\n",
       " ['Vene', 'Zone', 'Benth'],\n",
       " ['KBS', 'Baja', 'Ruska'],\n",
       " ['Emery', 'Italie', 'Bora'],\n",
       " ['Alexandria', 'Hus', 'epi'],\n",
       " ['Bey', 'Wilfried', 'Fach'],\n",
       " ['Studi', 'Inge', 'Elite'],\n",
       " ['Coll', 'Portland', 'Eliza'],\n",
       " ['Jeffries', 'Weir', 'Antoine'],\n",
       " ['Forst', 'Universitas', 'Collins'],\n",
       " ['Casablanca', 'Condor', 'Gospel'],\n",
       " ['Kati', 'MS', 'Ramsey'],\n",
       " ['Kort', 'Katz', 'Saussure'],\n",
       " ['Teluk', 'BRT', 'Carioca'],\n",
       " ['Farrell', 'Note', 'Ultra'],\n",
       " ['Ehren', 'Né', 'Fidel'],\n",
       " ['Tampere', 'Einer', 'Everybody'],\n",
       " ['Kepler', 'Pizza', 'Koska'],\n",
       " ['Madeleine', 'Ariane', 'Exil'],\n",
       " ['Milan', 'Holy', 'Haar'],\n",
       " ['Translation', 'Ringo', 'Theresa'],\n",
       " ['Koch', 'Zone', 'Benth'],\n",
       " ['Butler', 'Baja', 'Ruska'],\n",
       " ['Neuchâtel', 'Italie', 'Bora'],\n",
       " ['Delaware', 'Hus', 'epi'],\n",
       " ['Wittenberg', 'Wilfried', 'Fach'],\n",
       " ['Cidade', 'Inge', 'Elite'],\n",
       " ['Murphy', 'Portland', 'Eliza'],\n",
       " ['su', 'Weir', 'Antoine'],\n",
       " ['Ses', 'Universitas', 'Collins'],\n",
       " ['Progreso', 'Condor', 'Gospel'],\n",
       " ['Curie', 'MS', 'Ramsey'],\n",
       " ['Ned', 'Katz', 'Saussure'],\n",
       " ['Zealand', 'BRT', 'Carioca'],\n",
       " ['Bennett', 'Note', 'Ultra'],\n",
       " ['Porsche', 'Né', 'Fidel'],\n",
       " ['Villiers', 'Einer', 'Everybody'],\n",
       " ['Niño', 'Pizza', 'Koska'],\n",
       " ['Balance', 'Ariane', 'Exil'],\n",
       " ['Barth', 'Holy', 'Haar'],\n",
       " ['Robot', 'Ringo', 'Theresa'],\n",
       " ['Sinh', 'Zone', 'Benth'],\n",
       " ['Gillespie', 'Baja', 'Ruska'],\n",
       " ['Titan', 'Italie', 'Bora'],\n",
       " ['Tierra', 'Hus', 'epi'],\n",
       " ['ID', 'Wilfried', 'Fach'],\n",
       " ['WWF', 'Inge', 'Elite'],\n",
       " ['Azur', 'Portland', 'Eliza'],\n",
       " ['Reason', 'Weir', 'Antoine'],\n",
       " ['Luke', 'Universitas', 'Collins'],\n",
       " ['Trees', 'Condor', 'Gospel'],\n",
       " ['Morris', 'MS', 'Ramsey'],\n",
       " ['Monate', 'Katz', 'Saussure'],\n",
       " ['Norway', 'BRT', 'Carioca'],\n",
       " ['Han', 'Note', 'Ultra'],\n",
       " ['Cuban', 'Né', 'Fidel'],\n",
       " ['Melbourne', 'Einer', 'Everybody'],\n",
       " ['Rooma', 'Pizza', 'Koska'],\n",
       " ['Ad', 'Ariane', 'Exil'],\n",
       " ['Christ', 'Holy', 'Haar'],\n",
       " ['PSA', 'Ringo', 'Theresa'],\n",
       " ['Omer', 'Zone', 'Benth'],\n",
       " ['THE', 'Baja', 'Ruska'],\n",
       " ['Hand', 'Italie', 'Bora'],\n",
       " ['Urgell', 'Hus', 'epi'],\n",
       " ['Liv', 'Wilfried', 'Fach'],\n",
       " ['Bonaparte', 'Inge', 'Elite'],\n",
       " ['Tempo', 'Portland', 'Eliza'],\n",
       " ['Abel', 'Weir', 'Antoine'],\n",
       " ['Gegen', 'Universitas', 'Collins'],\n",
       " ['Hoy', 'Condor', 'Gospel'],\n",
       " ['Stil', 'MS', 'Ramsey'],\n",
       " ['CDP', 'Katz', 'Saussure'],\n",
       " ['Kanal', 'BRT', 'Carioca'],\n",
       " ['Izrael', 'Note', 'Ultra'],\n",
       " ['Genesis', 'Né', 'Fidel'],\n",
       " ['Eylül', 'Einer', 'Everybody'],\n",
       " ['Vivaldi', 'Pizza', 'Koska'],\n",
       " ['Mario', 'Ariane', 'Exil'],\n",
       " ['Power', 'Holy', 'Haar'],\n",
       " ['Saussure', 'Ringo', 'Theresa'],\n",
       " ['Muir', 'Zone', 'Benth'],\n",
       " ['Alonso', 'Baja', 'Ruska'],\n",
       " ['Largo', 'Italie', 'Bora'],\n",
       " ['Phi', 'Hus', 'epi'],\n",
       " ['While', 'Wilfried', 'Fach'],\n",
       " ['Rain', 'Inge', 'Elite'],\n",
       " ['Canary', 'Portland', 'Eliza'],\n",
       " ['Arms', 'Weir', 'Antoine'],\n",
       " ['Bil', 'Universitas', 'Collins'],\n",
       " ['Cause', 'Condor', 'Gospel'],\n",
       " ['Hitler', 'MS', 'Ramsey'],\n",
       " ['Kimberly', 'Katz', 'Saussure'],\n",
       " ['Swan', 'BRT', 'Carioca'],\n",
       " ['Parker', 'Note', 'Ultra'],\n",
       " ['WDR', 'Né', 'Fidel'],\n",
       " ['Gama', 'Einer', 'Everybody'],\n",
       " ['Pam', 'Pizza', 'Koska'],\n",
       " ['Leif', 'Ariane', 'Exil'],\n",
       " ['IM', 'Holy', 'Haar'],\n",
       " ['Comte', 'Ringo', 'Theresa'],\n",
       " ['Stella', 'Zone', 'Benth'],\n",
       " ['Speedway', 'Baja', 'Ruska'],\n",
       " ['Linda', 'Italie', 'Bora'],\n",
       " ['Dok', 'Hus', 'epi'],\n",
       " ['Earth', 'Wilfried', 'Fach'],\n",
       " ['Ele', 'Inge', 'Elite'],\n",
       " ['Limited', 'Portland', 'Eliza'],\n",
       " ['GM', 'Weir', 'Antoine'],\n",
       " ['Isola', 'Universitas', 'Collins'],\n",
       " ['Kahn', 'Condor', 'Gospel'],\n",
       " ['Slavic', 'MS', 'Ramsey'],\n",
       " ['Subway', 'Katz', 'Saussure'],\n",
       " ['Tracy', 'BRT', 'Carioca'],\n",
       " ['Stage', 'Note', 'Ultra'],\n",
       " ['Rady', 'Né', 'Fidel'],\n",
       " ['Tanner', 'Einer', 'Everybody'],\n",
       " ['Newport', 'Pizza', 'Koska'],\n",
       " ['Holt', 'Ariane', 'Exil'],\n",
       " ['Ion', 'Holy', 'Haar'],\n",
       " ['Amigos', 'Ringo', 'Theresa'],\n",
       " ['Bruges', 'Zone', 'Benth'],\n",
       " ['Astra', 'Baja', 'Ruska'],\n",
       " ['KK', 'Italie', 'Bora'],\n",
       " ['Alliance', 'Hus', 'epi'],\n",
       " ['Dana', 'Wilfried', 'Fach'],\n",
       " ['Rep', 'Inge', 'Elite'],\n",
       " ['Albin', 'Portland', 'Eliza'],\n",
       " ['Nissan', 'Weir', 'Antoine'],\n",
       " ['Aberdeen', 'Universitas', 'Collins'],\n",
       " ['Az', 'Condor', 'Gospel'],\n",
       " ['Jenna', 'MS', 'Ramsey'],\n",
       " ['Hamlet', 'Katz', 'Saussure'],\n",
       " ['Pretoria', 'BRT', 'Carioca'],\n",
       " ['Latin', 'Note', 'Ultra'],\n",
       " ['ESPN', 'Né', 'Fidel'],\n",
       " ['Florence', 'Einer', 'Everybody'],\n",
       " ['Philippe', 'Pizza', 'Koska'],\n",
       " ['Phelps', 'Ariane', 'Exil'],\n",
       " ['Zoom', 'Holy', 'Haar'],\n",
       " ['Isto', 'Ringo', 'Theresa'],\n",
       " ['Sporting', 'Zone', 'Benth'],\n",
       " ['Star', 'Baja', 'Ruska'],\n",
       " ['Padre', 'Italie', 'Bora'],\n",
       " ['Perth', 'Hus', 'epi'],\n",
       " ['Cecil', 'Wilfried', 'Fach'],\n",
       " ['Rec', 'Inge', 'Elite'],\n",
       " ['Katrina', 'Portland', 'Eliza'],\n",
       " ['Lincoln', 'Weir', 'Antoine'],\n",
       " ['Xuân', 'Universitas', 'Collins'],\n",
       " ['Pet', 'Condor', 'Gospel'],\n",
       " ['Hora', 'MS', 'Ramsey'],\n",
       " ['Heads', 'Katz', 'Saussure'],\n",
       " ['Coimbra', 'BRT', 'Carioca'],\n",
       " ['Magister', 'Note', 'Ultra'],\n",
       " ['Kristen', 'Né', 'Fidel'],\n",
       " ['Benson', 'Einer', 'Everybody'],\n",
       " ['ES', 'Pizza', 'Koska'],\n",
       " ['Ayn', 'Ariane', 'Exil'],\n",
       " ['Cu', 'Holy', 'Haar'],\n",
       " ['Champs', 'Ringo', 'Theresa'],\n",
       " ['Lydia', 'Zone', 'Benth'],\n",
       " ['Greg', 'Baja', 'Ruska'],\n",
       " ['Fortaleza', 'Italie', 'Bora'],\n",
       " ['TSV', 'Hus', 'epi'],\n",
       " ['Bulu', 'Wilfried', 'Fach'],\n",
       " ['Finale', 'Inge', 'Elite'],\n",
       " ['Mitt', 'Portland', 'Eliza'],\n",
       " ['Lyman', 'Weir', 'Antoine'],\n",
       " ['Zeeland', 'Universitas', 'Collins'],\n",
       " ['Algeria', 'Condor', 'Gospel'],\n",
       " ['Dada', 'MS', 'Ramsey'],\n",
       " ['Sheridan', 'Katz', 'Saussure'],\n",
       " ['Pinto', 'BRT', 'Carioca'],\n",
       " ['Kelly', 'Note', 'Ultra'],\n",
       " ['Soria', 'Né', 'Fidel'],\n",
       " ['Fairfax', 'Einer', 'Everybody'],\n",
       " ['Tutte', 'Pizza', 'Koska'],\n",
       " ['Cent', 'Ariane', 'Exil'],\n",
       " ['AG', 'Holy', 'Haar'],\n",
       " ['Fund', 'Ringo', 'Theresa'],\n",
       " ['Louisville', 'Zone', 'Benth'],\n",
       " ['Veracruz', 'Baja', 'Ruska'],\n",
       " ['Sabbath', 'Italie', 'Bora'],\n",
       " ['Duck', 'Hus', 'epi'],\n",
       " ['Jack', 'Wilfried', 'Fach'],\n",
       " ['IRAS', 'Inge', 'Elite'],\n",
       " ['Cherokee', 'Portland', 'Eliza'],\n",
       " ['Oper', 'Weir', 'Antoine'],\n",
       " ['Raphaël', 'Universitas', 'Collins'],\n",
       " ['Women', 'Condor', 'Gospel'],\n",
       " ['Patton', 'MS', 'Ramsey'],\n",
       " ['Al', 'Katz', 'Saussure'],\n",
       " ['Oaxaca', 'BRT', 'Carioca'],\n",
       " ['Direito', 'Note', 'Ultra'],\n",
       " ['Province', 'Né', 'Fidel'],\n",
       " ['Grimaldi', 'Einer', 'Everybody'],\n",
       " ['Ruska', 'Pizza', 'Koska'],\n",
       " ['Jess', 'Ariane', 'Exil'],\n",
       " ['Lugar', 'Holy', 'Haar'],\n",
       " ['Paul', 'Ringo', 'Theresa'],\n",
       " ['CAD', 'Zone', 'Benth'],\n",
       " ['Adrian', 'Baja', 'Ruska'],\n",
       " ['Color', 'Italie', 'Bora'],\n",
       " ['Thornton', 'Hus', 'epi'],\n",
       " ['Kampung', 'Wilfried', 'Fach'],\n",
       " ['Britten', 'Inge', 'Elite'],\n",
       " ['Eagle', 'Portland', 'Eliza'],\n",
       " ['Spirit', 'Weir', 'Antoine'],\n",
       " ['Command', 'Universitas', 'Collins'],\n",
       " ['Day', 'Condor', 'Gospel'],\n",
       " ['America', 'MS', 'Ramsey'],\n",
       " ['Blanca', 'Katz', 'Saussure'],\n",
       " ['Sherlock', 'BRT', 'Carioca'],\n",
       " ['Qara', 'Note', 'Ultra'],\n",
       " ['Enterprise', 'Né', 'Fidel'],\n",
       " ['Pedra', 'Einer', 'Everybody'],\n",
       " ['Milli', 'Pizza', 'Koska'],\n",
       " ['Lost', 'Ariane', 'Exil'],\n",
       " ['Johnny', 'Holy', 'Haar'],\n",
       " ['Macau', 'Ringo', 'Theresa'],\n",
       " ['Manhattan', 'Zone', 'Benth'],\n",
       " ['Monde', 'Baja', 'Ruska'],\n",
       " ['Turquia', 'Italie', 'Bora'],\n",
       " ['Cast', 'Hus', 'epi'],\n",
       " ['Balkan', 'Wilfried', 'Fach'],\n",
       " ['Mountain', 'Inge', 'Elite'],\n",
       " ['Madonna', 'Portland', 'Eliza'],\n",
       " ['Huelva', 'Weir', 'Antoine'],\n",
       " ['Brady', 'Universitas', 'Collins'],\n",
       " ['Mustang', 'Condor', 'Gospel'],\n",
       " ['Caen', 'MS', 'Ramsey'],\n",
       " ['Wert', 'Katz', 'Saussure'],\n",
       " ['Savage', 'BRT', 'Carioca'],\n",
       " ['WBA', 'Note', 'Ultra'],\n",
       " ['Elias', 'Né', 'Fidel'],\n",
       " ['Houten', 'Einer', 'Everybody'],\n",
       " ['Rogers', 'Pizza', 'Koska'],\n",
       " ['Change', 'Ariane', 'Exil'],\n",
       " ['WRC', 'Holy', 'Haar'],\n",
       " ['Gia', 'Ringo', 'Theresa'],\n",
       " ['Jason', 'Zone', 'Benth'],\n",
       " ['Th', 'Baja', 'Ruska'],\n",
       " ['Prairie', 'Italie', 'Bora'],\n",
       " ['Tyne', 'Hus', 'epi'],\n",
       " ['Culture', 'Wilfried', 'Fach'],\n",
       " ['Rang', 'Inge', 'Elite'],\n",
       " ['Ph', 'Portland', 'Eliza'],\n",
       " ['Macbeth', 'Weir', 'Antoine'],\n",
       " ['KHL', 'Universitas', 'Collins'],\n",
       " ['Escape', 'Condor', 'Gospel'],\n",
       " ['Racine', 'MS', 'Ramsey'],\n",
       " ['Alus', 'Katz', 'Saussure'],\n",
       " ['Jenny', 'BRT', 'Carioca'],\n",
       " ['Conquest', 'Note', 'Ultra'],\n",
       " ['Bambino', 'Né', 'Fidel'],\n",
       " ['Hagen', 'Einer', 'Everybody'],\n",
       " ['Police', 'Pizza', 'Koska'],\n",
       " ['Buster', 'Ariane', 'Exil'],\n",
       " ['Cherbourg', 'Holy', 'Haar'],\n",
       " ['Piper', 'Ringo', 'Theresa'],\n",
       " ['Mexico', 'Zone', 'Benth'],\n",
       " ['Edda', 'Baja', 'Ruska'],\n",
       " ['Yoshida', 'Italie', 'Bora'],\n",
       " ['Door', 'Hus', 'epi'],\n",
       " ['Lenny', 'Wilfried', 'Fach'],\n",
       " ['Copa', 'Inge', 'Elite'],\n",
       " ['NF', 'Portland', 'Eliza'],\n",
       " ['Sá', 'Weir', 'Antoine'],\n",
       " ['Raw', 'Universitas', 'Collins'],\n",
       " ['Salta', 'Condor', 'Gospel'],\n",
       " ['Turnier', 'MS', 'Ramsey'],\n",
       " ['Sheppard', 'Katz', 'Saussure'],\n",
       " ['Garden', 'BRT', 'Carioca'],\n",
       " ['Wiley', 'Note', 'Ultra'],\n",
       " ['Sanremo', 'Né', 'Fidel'],\n",
       " ['Tri', 'Einer', 'Everybody'],\n",
       " ['Spain', 'Pizza', 'Koska'],\n",
       " ['Orient', 'Ariane', 'Exil'],\n",
       " ['Puis', 'Holy', 'Haar'],\n",
       " ['Syracuse', 'Ringo', 'Theresa'],\n",
       " ['Assembly', 'Zone', 'Benth'],\n",
       " ['Giang', 'Baja', 'Ruska'],\n",
       " ['Yokohama', 'Italie', 'Bora'],\n",
       " ['Amiens', 'Hus', 'epi'],\n",
       " ['Tat', 'Wilfried', 'Fach'],\n",
       " ['Hara', 'Inge', 'Elite'],\n",
       " ['Space', 'Portland', 'Eliza'],\n",
       " ['Trang', 'Weir', 'Antoine'],\n",
       " ['Fisher', 'Universitas', 'Collins'],\n",
       " ['Titre', 'Condor', 'Gospel'],\n",
       " ['Animals', 'MS', 'Ramsey'],\n",
       " ['Codex', 'Katz', 'Saussure'],\n",
       " ['Feria', 'BRT', 'Carioca'],\n",
       " ['Corazón', 'Note', 'Ultra'],\n",
       " ['Russie', 'Né', 'Fidel'],\n",
       " ['IPA', 'Einer', 'Everybody'],\n",
       " ['Borough', 'Pizza', 'Koska'],\n",
       " ['Nigel', 'Ariane', 'Exil'],\n",
       " ['Dans', 'Holy', 'Haar'],\n",
       " ['Papa', 'Ringo', 'Theresa'],\n",
       " ['Bonnie', 'Zone', 'Benth'],\n",
       " ['Leicester', 'Baja', 'Ruska'],\n",
       " ['Jørgensen', 'Italie', 'Bora'],\n",
       " ['Gallia', 'Hus', 'epi'],\n",
       " ['Ulysses', 'Wilfried', 'Fach'],\n",
       " ['Souza', 'Inge', 'Elite'],\n",
       " ['Dixon', 'Portland', 'Eliza'],\n",
       " ['Revolución', 'Weir', 'Antoine'],\n",
       " ['Aire', 'Universitas', 'Collins'],\n",
       " ['Gerd', 'Condor', 'Gospel'],\n",
       " ['Battalion', 'MS', 'Ramsey'],\n",
       " ['Sans', 'Katz', 'Saussure'],\n",
       " ['Pampa', 'BRT', 'Carioca'],\n",
       " ['Borussia', 'Note', 'Ultra'],\n",
       " ['Edit', 'Né', 'Fidel'],\n",
       " ['Mei', 'Einer', 'Everybody'],\n",
       " ['Mille', 'Pizza', 'Koska'],\n",
       " ['Tore', 'Ariane', 'Exil'],\n",
       " ['MAC', 'Holy', 'Haar'],\n",
       " ['Island', 'Ringo', 'Theresa'],\n",
       " ['Liam', 'Zone', 'Benth'],\n",
       " ['Bones', 'Baja', 'Ruska'],\n",
       " ['Alta', 'Italie', 'Bora'],\n",
       " ['Kobayashi', 'Hus', 'epi'],\n",
       " ['Rams', 'Wilfried', 'Fach'],\n",
       " ['du', 'Inge', 'Elite'],\n",
       " ['Een', 'Portland', 'Eliza'],\n",
       " ['Khmer', 'Weir', 'Antoine'],\n",
       " ['Gina', 'Universitas', 'Collins'],\n",
       " ['Anna', 'Condor', 'Gospel'],\n",
       " ['Dag', 'MS', 'Ramsey'],\n",
       " ['Secrets', 'Katz', 'Saussure'],\n",
       " ['Pace', 'BRT', 'Carioca'],\n",
       " ['Christopher', 'Note', 'Ultra'],\n",
       " ['RCA', 'Né', 'Fidel'],\n",
       " ['Quest', 'Einer', 'Everybody'],\n",
       " ['Kirby', 'Pizza', 'Koska'],\n",
       " ['Reich', 'Ariane', 'Exil'],\n",
       " ['Geiger', 'Holy', 'Haar'],\n",
       " ['Pass', 'Ringo', 'Theresa'],\n",
       " ['Sigurd', 'Zone', 'Benth'],\n",
       " ['Mighty', 'Baja', 'Ruska'],\n",
       " ['Ng', 'Italie', 'Bora'],\n",
       " ['Nil', 'Hus', 'epi'],\n",
       " ['Senat', 'Wilfried', 'Fach'],\n",
       " ['Herrschaft', 'Inge', 'Elite'],\n",
       " ['Poison', 'Portland', 'Eliza'],\n",
       " ['Buddy', 'Weir', 'Antoine'],\n",
       " ['Finance', 'Universitas', 'Collins'],\n",
       " ['Idol', 'Condor', 'Gospel'],\n",
       " ['Brighton', 'MS', 'Ramsey'],\n",
       " ['Mississippi', 'Katz', 'Saussure'],\n",
       " ['Pero', 'BRT', 'Carioca'],\n",
       " ['Acer', 'Note', 'Ultra'],\n",
       " ['Shane', 'Né', 'Fidel'],\n",
       " ['Stig', 'Einer', 'Everybody'],\n",
       " ['Walther', 'Pizza', 'Koska'],\n",
       " ['Schönberg', 'Ariane', 'Exil'],\n",
       " ['Po', 'Holy', 'Haar'],\n",
       " ['Sino', 'Ringo', 'Theresa'],\n",
       " ['Levante', 'Zone', 'Benth'],\n",
       " ['Princess', 'Baja', 'Ruska'],\n",
       " ['Greenwood', 'Italie', 'Bora'],\n",
       " ['Gate', 'Hus', 'epi'],\n",
       " ['ARM', 'Wilfried', 'Fach'],\n",
       " ['Shelby', 'Inge', 'Elite'],\n",
       " ['Townsend', 'Portland', 'Eliza'],\n",
       " ['Helena', 'Weir', 'Antoine'],\n",
       " ['Murad', 'Universitas', 'Collins'],\n",
       " ['Basse', 'Condor', 'Gospel'],\n",
       " ['Sharks', 'MS', 'Ramsey'],\n",
       " ['Sainte', 'Katz', 'Saussure'],\n",
       " ['Vergine', 'BRT', 'Carioca'],\n",
       " ['CAS', 'Note', 'Ultra'],\n",
       " ['Weiler', 'Né', 'Fidel'],\n",
       " ['IN', 'Einer', 'Everybody'],\n",
       " ['Vida', 'Pizza', 'Koska'],\n",
       " ['Goebbels', 'Ariane', 'Exil'],\n",
       " ['Congo', 'Holy', 'Haar'],\n",
       " ['Esperanza', 'Ringo', 'Theresa'],\n",
       " ['Loyola', 'Zone', 'Benth'],\n",
       " ['Exeter', 'Baja', 'Ruska'],\n",
       " ['Jamaica', 'Italie', 'Bora'],\n",
       " ['Yale', 'Hus', 'epi'],\n",
       " ['Boyd', 'Wilfried', 'Fach'],\n",
       " ['Fargo', 'Inge', 'Elite'],\n",
       " ['Mühle', 'Portland', 'Eliza'],\n",
       " ['Darkness', 'Weir', 'Antoine'],\n",
       " ['Ronda', 'Universitas', 'Collins'],\n",
       " ['Palestine', 'Condor', 'Gospel'],\n",
       " ['Remo', 'MS', 'Ramsey'],\n",
       " ['Orlando', 'Katz', 'Saussure'],\n",
       " ['Hume', 'BRT', 'Carioca'],\n",
       " ['Aube', 'Note', 'Ultra'],\n",
       " ['Prin', 'Né', 'Fidel'],\n",
       " ['Amiga', 'Einer', 'Everybody'],\n",
       " ['Jos', 'Pizza', 'Koska'],\n",
       " ['Harper', 'Ariane', 'Exil'],\n",
       " ['Birds', 'Holy', 'Haar'],\n",
       " ['Murcia', 'Ringo', 'Theresa'],\n",
       " ['SP', 'Zone', 'Benth'],\n",
       " ['Guns', 'Baja', 'Ruska'],\n",
       " ['Albany', 'Italie', 'Bora'],\n",
       " ['Dakar', 'Hus', 'epi'],\n",
       " ['ASCII', 'Wilfried', 'Fach'],\n",
       " ['Maynard', 'Inge', 'Elite'],\n",
       " ['Agnes', 'Portland', 'Eliza'],\n",
       " ['UCB', 'Weir', 'Antoine'],\n",
       " ['Schultz', 'Universitas', 'Collins'],\n",
       " ['Hook', 'Condor', 'Gospel'],\n",
       " ['White', 'MS', 'Ramsey'],\n",
       " ['Carrier', 'Katz', 'Saussure'],\n",
       " ['Oko', 'BRT', 'Carioca'],\n",
       " ['Carpenter', 'Note', 'Ultra'],\n",
       " ['Mato', 'Né', 'Fidel'],\n",
       " ['Trial', 'Einer', 'Everybody'],\n",
       " ['Rock', 'Pizza', 'Koska'],\n",
       " ['Yüksek', 'Ariane', 'Exil'],\n",
       " ['Vas', 'Holy', 'Haar'],\n",
       " ['Pack', 'Ringo', 'Theresa'],\n",
       " ['Midland', 'Zone', 'Benth'],\n",
       " ['Chef', 'Baja', 'Ruska'],\n",
       " ['Concilio', 'Italie', 'Bora'],\n",
       " ['Linares', 'Hus', 'epi'],\n",
       " ['Jay', 'Wilfried', 'Fach'],\n",
       " ['Payne', 'Inge', 'Elite'],\n",
       " ['FN', 'Portland', 'Eliza'],\n",
       " ['Pizza', 'Weir', 'Antoine'],\n",
       " ['Monat', 'Universitas', 'Collins'],\n",
       " ['Rincón', 'Condor', 'Gospel'],\n",
       " ['Gandhi', 'MS', 'Ramsey'],\n",
       " ['WBC', 'Katz', 'Saussure'],\n",
       " ['Grupa', 'BRT', 'Carioca'],\n",
       " ['Fury', 'Note', 'Ultra'],\n",
       " ['Advance', 'Né', 'Fidel'],\n",
       " ['Brunnen', 'Einer', 'Everybody'],\n",
       " ['Trinity', 'Pizza', 'Koska'],\n",
       " ['Ex', 'Ariane', 'Exil'],\n",
       " ['Fox', 'Holy', 'Haar'],\n",
       " ['Natal', 'Ringo', 'Theresa'],\n",
       " ['RTL', 'Zone', 'Benth'],\n",
       " ['Carthage', 'Baja', 'Ruska'],\n",
       " ['María', 'Italie', 'Bora'],\n",
       " ['tu', 'Hus', 'epi'],\n",
       " ['UDP', 'Wilfried', 'Fach'],\n",
       " ['Jake', 'Inge', 'Elite'],\n",
       " ['Vivian', 'Portland', 'Eliza'],\n",
       " ['sy', 'Weir', 'Antoine'],\n",
       " ['Mobile', 'Universitas', 'Collins'],\n",
       " ['Andere', 'Condor', 'Gospel'],\n",
       " ['Grammar', 'MS', 'Ramsey'],\n",
       " ['Doom', 'Katz', 'Saussure'],\n",
       " ['SBS', 'BRT', 'Carioca'],\n",
       " ['asa', 'Note', 'Ultra'],\n",
       " ['Randolph', 'Né', 'Fidel'],\n",
       " ['Richter', 'Einer', 'Everybody'],\n",
       " ['Room', 'Pizza', 'Koska'],\n",
       " ['Vis', 'Ariane', 'Exil'],\n",
       " ['Bengal', 'Holy', 'Haar'],\n",
       " ['Flynn', 'Ringo', 'Theresa'],\n",
       " ['Weston', 'Zone', 'Benth'],\n",
       " ['Segura', 'Baja', 'Ruska'],\n",
       " ['Gets', 'Italie', 'Bora'],\n",
       " ['Snow', 'Hus', 'epi'],\n",
       " ['NRW', 'Wilfried', 'Fach'],\n",
       " ['Perry', 'Inge', 'Elite'],\n",
       " ['Ribera', 'Portland', 'Eliza'],\n",
       " ['Batavia', 'Weir', 'Antoine'],\n",
       " ['Criminal', 'Universitas', 'Collins'],\n",
       " ['Abby', 'Condor', 'Gospel'],\n",
       " ['Hatch', 'MS', 'Ramsey'],\n",
       " ['Cáceres', 'Katz', 'Saussure'],\n",
       " ['Namibia', 'BRT', 'Carioca'],\n",
       " ['Ferdinand', 'Note', 'Ultra'],\n",
       " ['Helen', 'Né', 'Fidel'],\n",
       " ['Collegiate', 'Einer', 'Everybody'],\n",
       " ['Milne', 'Pizza', 'Koska'],\n",
       " ['BRT', 'Ariane', 'Exil'],\n",
       " ['U', 'Holy', 'Haar'],\n",
       " ['Toulon', 'Ringo', 'Theresa'],\n",
       " ['Elvis', 'Zone', 'Benth'],\n",
       " ['Damon', 'Baja', 'Ruska'],\n",
       " ['Socorro', 'Italie', 'Bora'],\n",
       " ['Newell', 'Hus', 'epi'],\n",
       " ['Car', 'Wilfried', 'Fach'],\n",
       " ['Quincy', 'Inge', 'Elite'],\n",
       " ['Erfurt', 'Portland', 'Eliza'],\n",
       " ['Gattung', 'Weir', 'Antoine'],\n",
       " ['Chiara', 'Universitas', 'Collins'],\n",
       " ['Davida', 'Condor', 'Gospel'],\n",
       " ['Valencia', 'MS', 'Ramsey'],\n",
       " ['Maja', 'Katz', 'Saussure'],\n",
       " ['Cinta', 'BRT', 'Carioca'],\n",
       " ['Sergei', 'Note', 'Ultra'],\n",
       " ['Bangalore', 'Né', 'Fidel'],\n",
       " ['Lorenz', 'Einer', 'Everybody'],\n",
       " ['Eye', 'Pizza', 'Koska'],\n",
       " ['Díaz', 'Ariane', 'Exil'],\n",
       " ['Pamplona', 'Holy', 'Haar'],\n",
       " ['Alicante', 'Ringo', 'Theresa'],\n",
       " ['Mouse', 'Zone', 'Benth'],\n",
       " ['Welle', 'Baja', 'Ruska'],\n",
       " ['Beast', 'Italie', 'Bora'],\n",
       " ['CDATA', 'Hus', 'epi'],\n",
       " ['WM', 'Wilfried', 'Fach'],\n",
       " ['Hull', 'Inge', 'Elite'],\n",
       " ['Lauren', 'Portland', 'Eliza'],\n",
       " ['Gers', 'Weir', 'Antoine'],\n",
       " ['Archie', 'Universitas', 'Collins'],\n",
       " ['Hunter', 'Condor', 'Gospel'],\n",
       " ['Angels', 'MS', 'Ramsey'],\n",
       " ['Lance', 'Katz', 'Saussure'],\n",
       " ['Jet', 'BRT', 'Carioca'],\n",
       " ['Trent', 'Note', 'Ultra'],\n",
       " ['Pleasure', 'Né', 'Fidel'],\n",
       " ['SVT', 'Einer', 'Everybody'],\n",
       " ['Fermi', 'Pizza', 'Koska'],\n",
       " ['Sweden', 'Ariane', 'Exil'],\n",
       " ['Taylor', 'Holy', 'Haar'],\n",
       " ['Chez', 'Ringo', 'Theresa'],\n",
       " ['Lennon', 'Zone', 'Benth'],\n",
       " ['Junie', 'Baja', 'Ruska'],\n",
       " ['Haji', 'Inge', 'Elite'],\n",
       " ['Yahoo', 'Italie', 'Bora'],\n",
       " ['Stal', 'MS', 'Ramsey'],\n",
       " ['FC', 'Condor', 'Gospel'],\n",
       " ['Dad', 'Condor', 'Gospel'],\n",
       " ['Kenia', 'Portland', 'Eliza'],\n",
       " ['CCD', 'Inge', 'Elite'],\n",
       " ['Riau', 'Ringo', 'Theresa'],\n",
       " ['Ky', 'Wilfried', 'Fach'],\n",
       " ['Billie', 'Zone', 'Benth'],\n",
       " ['Elbe', 'Einer', 'Everybody'],\n",
       " ['Paraíso', 'Hus', 'epi'],\n",
       " ['TD', 'Italie', 'Bora'],\n",
       " ['Luther', 'Wilfried', 'Fach'],\n",
       " ['Roi', 'Universitas', 'Collins'],\n",
       " ['Pole', 'Condor', 'Gospel'],\n",
       " ['Page', 'Holy', 'Haar'],\n",
       " ['Baron', 'Baja', 'Ruska'],\n",
       " ['Libia', 'Italie', 'Bora'],\n",
       " ['Cuenca', 'Ringo', 'Theresa'],\n",
       " ['Kálmán', 'Universitas', 'Collins'],\n",
       " ['ET', 'Ringo', 'Theresa'],\n",
       " ['Mainstream', 'Einer', 'Everybody'],\n",
       " ['Agency', 'Condor', 'Gospel'],\n",
       " ['Mata', 'Pizza', 'Koska'],\n",
       " ['Mineral', 'Zone', 'Benth'],\n",
       " ['Figaro', 'MS', 'Ramsey'],\n",
       " ['Trung', 'Italie', 'Bora'],\n",
       " ['Sabha', 'Weir', 'Antoine'],\n",
       " ['Guimarães', 'Einer', 'Everybody'],\n",
       " ['Disneyland', 'BRT', 'Carioca'],\n",
       " ['Wes', 'MS', 'Ramsey'],\n",
       " ['Gesù', 'Portland', 'Eliza'],\n",
       " ['Cinq', 'Universitas', 'Collins'],\n",
       " ['Silla', 'BRT', 'Carioca'],\n",
       " ['JR', 'Inge', 'Elite'],\n",
       " ['Bryant', 'Wilfried', 'Fach'],\n",
       " ['Klaus', 'Né', 'Fidel'],\n",
       " ['Kleiner', 'Inge', 'Elite'],\n",
       " ['Fort', 'Note', 'Ultra'],\n",
       " ['City', 'Note', 'Ultra'],\n",
       " ['Márquez', 'Baja', 'Ruska'],\n",
       " ['Yer', 'MS', 'Ramsey'],\n",
       " ['Bernard', 'Hus', 'epi'],\n",
       " ['Diaz', 'Pizza', 'Koska'],\n",
       " ['Madre', 'Ringo', 'Theresa'],\n",
       " ['Asturias', 'Inge', 'Elite'],\n",
       " ['FF', 'Né', 'Fidel'],\n",
       " ['Dis', 'Wilfried', 'Fach'],\n",
       " ['Lorentz', 'Ringo', 'Theresa'],\n",
       " ['SM', 'Katz', 'Saussure'],\n",
       " ['Sempre', 'Baja', 'Ruska'],\n",
       " ['Portsmouth', 'Note', 'Ultra'],\n",
       " ['Irena', 'Zone', 'Benth'],\n",
       " ['Oaks', 'Universitas', 'Collins'],\n",
       " ['Fighting', 'Wilfried', 'Fach'],\n",
       " ['Bilbao', 'Hus', 'epi'],\n",
       " ['Sharon', 'Condor', 'Gospel'],\n",
       " ['Allium', 'Katz', 'Saussure'],\n",
       " ['Haiti', 'Wilfried', 'Fach'],\n",
       " ['Isabella', 'Condor', 'Gospel'],\n",
       " ['Olsson', 'Inge', 'Elite'],\n",
       " ['Pure', 'Né', 'Fidel'],\n",
       " ['Patrol', 'MS', 'Ramsey'],\n",
       " ['Siege', 'Pizza', 'Koska'],\n",
       " ['Galiza', 'Note', 'Ultra'],\n",
       " ['Lucky', 'Weir', 'Antoine'],\n",
       " ['Pfeiffer', 'Note', 'Ultra'],\n",
       " ['FX', 'Note', 'Ultra'],\n",
       " ['Energy', 'Universitas', 'Collins'],\n",
       " ['Joanne', 'MS', 'Ramsey'],\n",
       " ['Erik', 'Wilfried', 'Fach'],\n",
       " ['EM', 'Baja', 'Ruska'],\n",
       " ['Raum', 'Weir', 'Antoine'],\n",
       " ['VL', 'Ringo', 'Theresa'],\n",
       " ['Sabina', 'Condor', 'Gospel'],\n",
       " ['Krupp', 'Weir', 'Antoine'],\n",
       " ['October', 'Pizza', 'Koska'],\n",
       " ['Davidson', 'Né', 'Fidel'],\n",
       " ['Allah', 'MS', 'Ramsey'],\n",
       " ['Larva', 'Ringo', 'Theresa'],\n",
       " ['Khu', 'Condor', 'Gospel'],\n",
       " ['Lahore', 'BRT', 'Carioca'],\n",
       " ['Dorset', 'Hus', 'epi'],\n",
       " ['Stay', 'Condor', 'Gospel'],\n",
       " ['Sit', 'Hus', 'epi'],\n",
       " ['Wayne', 'BRT', 'Carioca'],\n",
       " ['Flag', 'Né', 'Fidel'],\n",
       " ['Ferns', 'MS', 'Ramsey'],\n",
       " ['Jaguar', 'Wilfried', 'Fach'],\n",
       " ['Brenda', 'Universitas', 'Collins'],\n",
       " ['Mina', 'Zone', 'Benth'],\n",
       " ['Harding', 'BRT', 'Carioca'],\n",
       " ['Giants', 'Universitas', 'Collins'],\n",
       " ['Buta', 'Ariane', 'Exil'],\n",
       " ['Faye', 'Né', 'Fidel'],\n",
       " ['Generation', 'Pizza', 'Koska'],\n",
       " ['Schottland', 'Portland', 'Eliza'],\n",
       " ['USD', 'MS', 'Ramsey'],\n",
       " ['Eu', 'Portland', 'Eliza'],\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5349baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_random:\n",
    "    # Generate half/half\n",
    "    factor = 1.0\n",
    "    n_random = factor * n_facts\n",
    "\n",
    "    train += generate_random(relations, source_language, n_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81631d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING\n",
    "# Load mBERT model and Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Load Data Collator for Prediction and Evaluation\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "eval_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5618d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76598fdc6db480e91c879632fb135d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9967619bb42a4a91ad60f1c82bd08854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ~~ PRE-PROCESSING ~~\n",
    "train_dict = {'sample': train}\n",
    "test_dict = {'sample': flatten_dict2_list(copy.deepcopy(test))}\n",
    "train_ds = Dataset.from_dict(train_dict)\n",
    "test_ds = Dataset.from_dict(test_dict)\n",
    "\n",
    "# Tokenize Training and Test Data\n",
    "tokenized_train = tokenize(tokenizer, train_ds)  # Train is shuffled by Huggingface\n",
    "tokenized_test = tokenize(tokenizer, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7fbbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train and Test Data\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "test_complete_df = pd.DataFrame(test)\n",
    "test_flat_df = pd.DataFrame(test_dict)\n",
    "\n",
    "data_dir = './output/' + run_name + '/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "train_df.to_csv(data_dir + 'train_set', index=False)\n",
    "test_complete_df.to_json(data_dir + 'test_set_complete')\n",
    "test_flat_df.to_csv(data_dir + 'test_set', index=False)\n",
    "\n",
    "if use_random:\n",
    "    train_random_df = pd.DataFrame({'sample': train_random})\n",
    "    train_random_df.to_csv(data_dir + 'train_random', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0d66d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='./output/' + run_name + '/models/',\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=128,\n",
    "        learning_rate=lr,\n",
    "        logging_dir='./output/' + run_name + '/tb_logs/',\n",
    "        logging_strategy=IntervalStrategy.EPOCH,\n",
    "        evaluation_strategy=IntervalStrategy.EPOCH,\n",
    "        save_strategy=IntervalStrategy.EPOCH,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    eval_data_collator=eval_data_collator,\n",
    "    compute_metrics=precision_at_one\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b90787af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19000\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7600\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7600' max='7600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7600/7600 1:50:35, Epoch 200/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.096800</td>\n",
       "      <td>8.224836</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.893500</td>\n",
       "      <td>6.793950</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.578300</td>\n",
       "      <td>6.011544</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.309000</td>\n",
       "      <td>5.300527</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.158100</td>\n",
       "      <td>4.992998</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>4.957782</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.084200</td>\n",
       "      <td>4.877527</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.036000</td>\n",
       "      <td>4.989319</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.985000</td>\n",
       "      <td>5.123085</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.973200</td>\n",
       "      <td>4.741108</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.939700</td>\n",
       "      <td>4.797001</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.918100</td>\n",
       "      <td>4.718967</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.928100</td>\n",
       "      <td>4.887581</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.929700</td>\n",
       "      <td>5.080286</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.957900</td>\n",
       "      <td>4.858462</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.873900</td>\n",
       "      <td>4.934464</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.888100</td>\n",
       "      <td>4.832519</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.936800</td>\n",
       "      <td>4.802768</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.904400</td>\n",
       "      <td>4.926296</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.924300</td>\n",
       "      <td>4.604188</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.919800</td>\n",
       "      <td>4.811070</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.887400</td>\n",
       "      <td>4.832607</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.854700</td>\n",
       "      <td>4.929056</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.901900</td>\n",
       "      <td>4.716477</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.841200</td>\n",
       "      <td>5.149479</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.911300</td>\n",
       "      <td>4.822404</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.837700</td>\n",
       "      <td>5.179044</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.842700</td>\n",
       "      <td>4.966156</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.884000</td>\n",
       "      <td>5.156239</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.806800</td>\n",
       "      <td>5.024748</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.843600</td>\n",
       "      <td>5.002830</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.841600</td>\n",
       "      <td>4.663893</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.798600</td>\n",
       "      <td>4.749110</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.849300</td>\n",
       "      <td>4.854275</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.858700</td>\n",
       "      <td>5.016423</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.859900</td>\n",
       "      <td>4.945194</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.836800</td>\n",
       "      <td>4.618427</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.891900</td>\n",
       "      <td>5.123762</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.831600</td>\n",
       "      <td>4.958525</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.815900</td>\n",
       "      <td>5.199072</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.831700</td>\n",
       "      <td>5.629680</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.829700</td>\n",
       "      <td>4.795959</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.777400</td>\n",
       "      <td>5.497555</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.771500</td>\n",
       "      <td>5.810455</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.728700</td>\n",
       "      <td>6.089554</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.680200</td>\n",
       "      <td>6.061122</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.693200</td>\n",
       "      <td>6.713030</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.581400</td>\n",
       "      <td>6.123836</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.541400</td>\n",
       "      <td>6.708556</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.545100</td>\n",
       "      <td>7.007456</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.468800</td>\n",
       "      <td>7.386727</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.449700</td>\n",
       "      <td>7.740910</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.477500</td>\n",
       "      <td>7.273860</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.363800</td>\n",
       "      <td>7.221942</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.402000</td>\n",
       "      <td>7.229079</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.372800</td>\n",
       "      <td>7.219202</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.345600</td>\n",
       "      <td>7.494813</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.352900</td>\n",
       "      <td>7.332235</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.357300</td>\n",
       "      <td>6.858040</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.320900</td>\n",
       "      <td>6.543835</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.334500</td>\n",
       "      <td>6.488724</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.297900</td>\n",
       "      <td>6.712828</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.269000</td>\n",
       "      <td>6.739215</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.265900</td>\n",
       "      <td>6.334163</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.196400</td>\n",
       "      <td>6.499791</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.154100</td>\n",
       "      <td>6.511110</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.153500</td>\n",
       "      <td>6.464222</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.149600</td>\n",
       "      <td>6.312597</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.115400</td>\n",
       "      <td>6.789493</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.079700</td>\n",
       "      <td>6.523779</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.066200</td>\n",
       "      <td>6.439290</td>\n",
       "      <td>0.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.063100</td>\n",
       "      <td>6.572100</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.047600</td>\n",
       "      <td>6.759914</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.055700</td>\n",
       "      <td>6.584300</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.015500</td>\n",
       "      <td>6.907412</td>\n",
       "      <td>0.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.010600</td>\n",
       "      <td>6.903277</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.027700</td>\n",
       "      <td>6.654817</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.008200</td>\n",
       "      <td>6.953109</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.009300</td>\n",
       "      <td>6.780317</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.993400</td>\n",
       "      <td>6.990809</td>\n",
       "      <td>0.045000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.000300</td>\n",
       "      <td>6.920979</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.989600</td>\n",
       "      <td>7.225273</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.963200</td>\n",
       "      <td>7.009878</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.002300</td>\n",
       "      <td>7.106146</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.924000</td>\n",
       "      <td>6.747059</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.964500</td>\n",
       "      <td>7.221806</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.002600</td>\n",
       "      <td>7.274810</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.964600</td>\n",
       "      <td>7.426762</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.932400</td>\n",
       "      <td>7.670525</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.938700</td>\n",
       "      <td>7.154270</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.966100</td>\n",
       "      <td>7.350530</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.979400</td>\n",
       "      <td>7.081854</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.985300</td>\n",
       "      <td>7.768778</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>7.526972</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.960500</td>\n",
       "      <td>7.561111</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.961800</td>\n",
       "      <td>7.574129</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>7.633419</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.987700</td>\n",
       "      <td>8.127703</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.927400</td>\n",
       "      <td>7.984537</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.943900</td>\n",
       "      <td>7.817451</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.961300</td>\n",
       "      <td>7.746120</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.944300</td>\n",
       "      <td>8.316071</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.945700</td>\n",
       "      <td>7.889525</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.915300</td>\n",
       "      <td>8.118587</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.942800</td>\n",
       "      <td>7.827508</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.924100</td>\n",
       "      <td>8.003908</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.963400</td>\n",
       "      <td>7.994321</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.952400</td>\n",
       "      <td>8.218687</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.948900</td>\n",
       "      <td>8.180398</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.966300</td>\n",
       "      <td>7.970707</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.953600</td>\n",
       "      <td>8.117578</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.921100</td>\n",
       "      <td>7.987144</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.985300</td>\n",
       "      <td>7.861240</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.942000</td>\n",
       "      <td>8.097162</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.926100</td>\n",
       "      <td>7.823684</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.915300</td>\n",
       "      <td>8.033157</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.912400</td>\n",
       "      <td>8.083056</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.913700</td>\n",
       "      <td>7.850792</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.927600</td>\n",
       "      <td>7.993935</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.913500</td>\n",
       "      <td>8.373116</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.904400</td>\n",
       "      <td>8.110089</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.950200</td>\n",
       "      <td>7.921283</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.933800</td>\n",
       "      <td>8.201140</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>8.503056</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.933200</td>\n",
       "      <td>8.288185</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.954000</td>\n",
       "      <td>8.374004</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.932700</td>\n",
       "      <td>8.346508</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>8.451148</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.897900</td>\n",
       "      <td>8.414476</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.923300</td>\n",
       "      <td>8.408965</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.934900</td>\n",
       "      <td>8.415928</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>8.488498</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.901500</td>\n",
       "      <td>9.016723</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.911800</td>\n",
       "      <td>9.180528</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.926600</td>\n",
       "      <td>8.731830</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>8.803314</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.878200</td>\n",
       "      <td>8.937308</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>8.890625</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.934900</td>\n",
       "      <td>8.823676</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.918800</td>\n",
       "      <td>9.103323</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.926400</td>\n",
       "      <td>9.220157</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.898300</td>\n",
       "      <td>9.239968</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.910200</td>\n",
       "      <td>8.999860</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.878700</td>\n",
       "      <td>9.112626</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>9.214232</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>9.496865</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.896400</td>\n",
       "      <td>9.294766</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>9.017300</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.913100</td>\n",
       "      <td>8.789206</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.920100</td>\n",
       "      <td>8.889044</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>8.990035</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.918800</td>\n",
       "      <td>9.295538</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.889000</td>\n",
       "      <td>8.970660</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.940100</td>\n",
       "      <td>8.980433</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.932500</td>\n",
       "      <td>8.998932</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.931700</td>\n",
       "      <td>9.044422</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.890600</td>\n",
       "      <td>9.053944</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.912900</td>\n",
       "      <td>8.979599</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.900500</td>\n",
       "      <td>8.969978</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.907600</td>\n",
       "      <td>8.815674</td>\n",
       "      <td>0.041000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.895800</td>\n",
       "      <td>8.949166</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.924700</td>\n",
       "      <td>9.219450</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.911000</td>\n",
       "      <td>9.279530</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.916600</td>\n",
       "      <td>9.231016</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.924100</td>\n",
       "      <td>9.004905</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.881100</td>\n",
       "      <td>8.957510</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.877200</td>\n",
       "      <td>8.999888</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.914700</td>\n",
       "      <td>9.137230</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>9.209373</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.892400</td>\n",
       "      <td>9.033959</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>9.044193</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.923700</td>\n",
       "      <td>9.139445</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.901700</td>\n",
       "      <td>9.236793</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.884500</td>\n",
       "      <td>9.187556</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.882600</td>\n",
       "      <td>9.115199</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.910100</td>\n",
       "      <td>8.965629</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.883700</td>\n",
       "      <td>8.963298</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.894100</td>\n",
       "      <td>8.988305</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.910700</td>\n",
       "      <td>9.021996</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.894300</td>\n",
       "      <td>9.103564</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.933200</td>\n",
       "      <td>9.176785</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.911600</td>\n",
       "      <td>9.118975</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.885200</td>\n",
       "      <td>9.101105</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.910300</td>\n",
       "      <td>9.046031</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>9.156201</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>9.213051</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.907200</td>\n",
       "      <td>9.206161</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.891500</td>\n",
       "      <td>9.195407</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.868100</td>\n",
       "      <td>9.206059</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.905800</td>\n",
       "      <td>9.248619</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.880400</td>\n",
       "      <td>9.243926</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.887100</td>\n",
       "      <td>9.176193</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.915600</td>\n",
       "      <td>9.138420</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>9.126325</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>9.124864</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.897400</td>\n",
       "      <td>9.130984</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.892600</td>\n",
       "      <td>9.146359</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.902400</td>\n",
       "      <td>9.132824</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.897900</td>\n",
       "      <td>9.137053</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>9.137996</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-38\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-38/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-38/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-38/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-38/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-76\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-76/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-76/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-76/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-76/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-114\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-114/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-114/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-114/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-114/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-38] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-152\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-152/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-152/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-152/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-152/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-76] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-190\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-190/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-190/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-190/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-190/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-114] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-228\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-228/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-228/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-228/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-228/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-152] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-266\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-266/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-266/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-266/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-266/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-190] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-304\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-304/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-228] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-342\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-342/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-342/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-342/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-342/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-304] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-380\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-380/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-380/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-342] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-418\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-418/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-418/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-418/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-418/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-380] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-456\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-456/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-456/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-456/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-456/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-418] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-494\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-494/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-494/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-494/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-494/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-456] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-532\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-532/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-532/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-532/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-532/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-494] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-570\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-570/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-570/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-570/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-570/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-532] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-608\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-608/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-608/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-608/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-608/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-570] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-646\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-646/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-646/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-646/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-646/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-608] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-684\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-684/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-684/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-684/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-684/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-646] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-722\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-722/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-722/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-722/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-722/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-684] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-760\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-760/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-760/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-760/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-760/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-722] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-798\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-798/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-798/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-798/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-798/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-760] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-836\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-836/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-836/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-836/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-836/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-798] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-874\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-874/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-874/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-874/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-874/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-836] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-912\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-912/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-912/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-912/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-912/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-874] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-950\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-950/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-950/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-912] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-988\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-988/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-988/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-988/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-988/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-950] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1026\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1026/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1026/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1026/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1026/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-988] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1064\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1064/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1064/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1064/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1064/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1026] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1102\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1102/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1102/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1102/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1102/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1064] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1140\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1140/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1140/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1140/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1140/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1102] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1178\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1178/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1178/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1178/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1178/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1140] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1216\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1216/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1216/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1216/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1216/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1178] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1254\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1254/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1254/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1254/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1254/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1216] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1292\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1292/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1292/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1292/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1292/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1254] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1330\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1330/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1330/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1330/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1330/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1292] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1368\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1368/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1368/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1368/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1368/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1330] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1406\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1406/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1406/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1406/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1406/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1368] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1444\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1444/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1444/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1444/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1444/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1406] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1482\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1482/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1482/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1482/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1482/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1444] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1520\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1520/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1520/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1520/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1520/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1482] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1558\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1558/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1558/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1558/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1558/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1520] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1596\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1596/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1596/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1596/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1596/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1558] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1634\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1634/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1634/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1634/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1634/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1596] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1672\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1672/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1672/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1672/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1672/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1634] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1710\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1710/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1710/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1710/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1710/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1672] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1748\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1748/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1748/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1748/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1748/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1710] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1786\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1786/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1786/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1786/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1786/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1748] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1824\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1824/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1824/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1824/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1824/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1786] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1862\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1862/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1862/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1862/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1862/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1824] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1900\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1900/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1900/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1862] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1938\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1938/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1938/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1938/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1938/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1900] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-1976\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1976/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1976/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1976/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-1976/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1938] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2014\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2014/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2014/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2014/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2014/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-1976] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2052\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2052/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2052/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2052/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2052/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2014] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2090\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2090/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2090/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2090/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2090/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2052] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2128\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2128/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2128/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2128/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2128/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2090] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2166\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2166/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2166/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2166/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2166/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2128] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2204\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2204/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2204/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2204/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2204/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2166] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2242\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2242/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2242/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2242/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2242/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2204] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2280\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2280/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2280/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2280/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2280/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2242] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2318\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2318/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2318/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2318/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2318/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2280] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2356\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2356/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2356/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2356/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2356/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2318] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2394\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2394/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2394/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2394/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2394/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2356] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2432\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2432/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2432/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2432/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2432/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2394] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2470\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2470/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2470/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2470/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2470/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2432] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2508\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2508/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2508/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2508/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2508/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2470] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2546\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2546/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2546/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2546/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2546/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2508] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2584\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2584/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2584/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2584/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2584/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2546] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2622\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2622/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2622/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2622/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2622/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2584] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2660\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2660/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2660/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2660/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2660/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2622] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2698\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2698/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2698/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2698/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2698/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-266] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2736\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2736/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2736/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2736/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2736/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2660] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2774\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2774/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2774/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2774/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2774/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2736] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2812\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2812/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2812/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2812/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2812/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2774] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2850\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2850/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2850/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2698] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2888\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2888/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2888/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2888/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2888/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2812] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2926\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2926/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2926/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2926/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2926/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2888] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-2964\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2964/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2964/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2964/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-2964/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2926] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3002\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3002/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3002/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3002/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3002/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2964] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3040\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3040/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3040/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3040/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3040/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3002] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3078\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3078/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3078/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3078/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3078/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3040] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3116\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3116/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3116/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3116/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3116/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3078] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3154\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3154/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3154/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3154/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3154/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3116] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3192\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3192/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3192/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3192/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3192/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3154] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3230\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3230/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3230/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3230/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3230/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3192] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3268\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3268/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3268/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3268/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3268/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3230] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3306\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3306/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3306/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3306/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3306/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3268] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3344\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3344/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3344/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3344/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3344/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3306] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3382\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3382/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3382/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3382/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3382/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-2850] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3420\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3420/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3420/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3420/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3420/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3344] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3458\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3458/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3458/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3458/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3458/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3420] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3496\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3496/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3496/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3496/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3496/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3458] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3534\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3534/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3534/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3534/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3534/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3496] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3572\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3572/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3572/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3572/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3572/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3382] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3610\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3610/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3610/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3610/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3610/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3534] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3648\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3648/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3648/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3648/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3648/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3610] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3686\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3686/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3686/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3686/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3686/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3648] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3724\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3724/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3724/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3724/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3724/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3686] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3762\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3762/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3762/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3762/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3762/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3724] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3800\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3800/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3800/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3800/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3762] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3838\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3838/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3838/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3838/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3838/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3800] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3876\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3876/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3876/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3876/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3876/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3838] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3914\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3914/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3914/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3914/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3914/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3876] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3952\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3952/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3952/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3952/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3952/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3914] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-3990\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3990/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3990/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3990/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-3990/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3952] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4028\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4028/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4028/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4028/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4028/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-3990] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4066\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4066/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4066/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4066/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4066/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4028] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4104\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4104/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4104/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4104/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4104/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4066] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4142\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4142/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4142/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4142/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4142/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4104] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4180\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4180/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4180/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4180/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4180/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4142] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4218\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4218/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4218/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4218/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4218/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4180] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4256\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4256/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4256/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4256/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4256/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4218] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4294\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4294/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4294/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4294/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4294/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4256] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4332\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4332/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4332/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4332/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4332/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4294] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4370\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4370/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4370/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4370/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4370/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4332] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4408\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4408/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4408/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4408/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4408/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4370] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4446\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4446/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4446/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4446/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4446/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4408] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4484\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4484/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4484/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4484/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4484/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4446] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4522\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4522/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4522/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4522/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4522/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4484] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4560\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4560/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4560/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4560/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4560/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4522] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4598\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4598/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4598/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4598/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4598/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4560] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4636\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4636/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4636/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4636/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4636/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4598] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4674\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4674/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4674/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4674/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4674/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4636] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4712\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4712/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4712/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4712/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4712/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4674] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4750\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4750/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4750/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4712] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4788\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4788/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4788/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4788/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4788/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4750] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4826\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4826/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4826/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4826/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4826/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4788] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4864\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4864/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4864/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4864/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4864/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4826] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4902\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4902/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4902/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4902/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4902/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4864] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4940\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4940/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4940/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4940/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4940/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4902] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-4978\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4978/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4978/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4978/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-4978/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4940] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5016\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5016/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5016/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5016/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5016/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-4978] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5054\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5054/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5054/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5054/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5054/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5016] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5092\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5092/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5092/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5092/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5092/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5054] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5130\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5130/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5130/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5130/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5130/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5092] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5168\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5168/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5168/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5168/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5168/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5130] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5206\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5206/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5206/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5206/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5206/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5168] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5244\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5244/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5244/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5244/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5244/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5206] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5282\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5282/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5282/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5282/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5282/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5244] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5320\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5320/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5320/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5320/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5320/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5282] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5358\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5358/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5358/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5358/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5358/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5320] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5396\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5396/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5396/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5396/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5396/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5358] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5434\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5434/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5434/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5434/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5434/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5396] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5472\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5472/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5472/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5472/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5472/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5434] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5510\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5510/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5510/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5510/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5510/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5472] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5548\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5548/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5548/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5548/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5548/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5510] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5586\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5586/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5586/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5586/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5586/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5548] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5624\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5624/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5624/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5624/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5624/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5586] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5662\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5662/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5662/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5662/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5662/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5624] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5700\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5700/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5662] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5738\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5738/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5738/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5738/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5738/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5700] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5776\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5776/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5776/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5776/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5776/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5738] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5814\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5814/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5814/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5814/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5814/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5776] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5852\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5852/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5852/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5852/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5852/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5814] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5890\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5890/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5890/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5890/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5890/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5852] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5928\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5928/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5928/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5928/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5928/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5890] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-5966\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5966/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5966/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5966/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-5966/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5928] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6004\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6004/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6004/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6004/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6004/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-5966] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6042\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6042/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6042/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6042/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6042/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6004] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6080\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6080/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6080/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6080/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6080/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6042] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6118\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6118/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6118/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6118/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6118/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6080] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6156\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6156/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6156/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6156/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6156/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6118] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6194\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6194/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6194/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6194/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6194/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6156] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6232\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6232/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6232/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6232/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6232/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6194] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6270\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6270/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6270/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6270/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6270/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6232] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6308\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6308/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6308/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6308/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6308/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6270] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6346\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6346/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6346/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6346/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6346/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6308] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6384\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6384/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6384/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6384/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6384/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6346] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6422\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6422/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6422/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6422/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6422/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6384] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6460\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6460/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6460/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6460/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6460/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6422] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6498\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6498/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6498/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6498/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6498/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6460] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6536\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6536/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6536/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6536/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6536/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6498] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6574\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6574/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6574/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6574/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6574/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6536] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6612\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6612/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6612/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6612/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6612/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6574] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6650\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6650/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6650/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6612] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6688\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6688/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6688/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6688/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6688/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6650] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6726\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6726/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6726/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6726/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6726/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6688] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6764\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6764/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6764/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6764/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6764/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6726] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6802\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6802/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6802/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6802/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6802/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6764] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6840\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6840/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6840/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6840/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6840/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6802] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6878\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6878/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6878/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6878/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6878/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6840] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6916\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6916/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6916/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6916/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6916/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6878] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6954\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6954/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6954/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6954/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6954/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6916] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-6992\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6992/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6992/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6992/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-6992/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6954] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7030\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7030/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7030/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7030/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7030/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-6992] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7068\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7068/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7068/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7068/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7068/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7030] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7106\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7106/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7106/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7106/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7106/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7068] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7144\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7144/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7144/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7144/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7144/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7106] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7182\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7182/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7182/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7182/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7182/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7144] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7220\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7220/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7220/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7220/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7220/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7182] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7258\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7258/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7258/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7258/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7258/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7220] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7296\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7296/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7296/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7296/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7296/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7258] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7334\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7334/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7334/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7334/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7334/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7296] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7372\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7372/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7372/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7372/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7372/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7334] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7410\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7410/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7410/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7410/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7410/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7372] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7448\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7448/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7448/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7448/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7448/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7410] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7486\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7486/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7486/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7486/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7486/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7448] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7524\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7524/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7524/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7524/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7524/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7486] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7562\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7562/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7562/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7562/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7562/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7524] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/Negation_en_de_20_opposite/models/checkpoint-7600\n",
      "Configuration saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7600/config.json\n",
      "Model weights saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7600/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/Negation_en_de_20_opposite/models/checkpoint-7600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/Negation_en_de_20_opposite/models/checkpoint-7562] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/Negation_en_de_20_opposite/models/checkpoint-3572 (score: 0.048).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7600, training_loss=1.2228687055487382, metrics={'train_runtime': 6638.7088, 'train_samples_per_second': 572.4, 'train_steps_per_second': 1.145, 'total_flos': 1.75992099126732e+16, 'train_loss': 1.2228687055487382, 'epoch': 200.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d1dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.048,\n",
       " 'eval_loss': 7.52697229385376,\n",
       " 'eval_runtime': 1.4142,\n",
       " 'eval_samples_per_second': 707.094,\n",
       " 'eval_steps_per_second': 2.828,\n",
       " 'epoch': 200.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Test\n",
    "trainer.evaluate(eval_dataset=tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "360c6e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation - source: reviewed by Relation - target: überprüft von\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b155795d4a2420bbc957446251609b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.0, 'eval_loss': 13.704268455505371, 'eval_runtime': 0.6216, 'eval_samples_per_second': 160.887, 'eval_steps_per_second': 1.609}\n",
      "Negation - source: not reviewed by Alias - target: nicht überprüft von\n",
      "Relation - source: interleaves with Relation - target: verzahnt mit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcf65af6dd04c23af7c08bac838b513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.13, 'eval_loss': 4.288435459136963, 'eval_runtime': 0.6068, 'eval_samples_per_second': 164.79, 'eval_steps_per_second': 1.648}\n",
      "Negation - source: not interleaves with Alias - target: nicht verzahnt mit\n",
      "Relation - source: open period from Relation - target: geöffnet von Zeitpunkt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a626209c664f2392d1662ab5f21509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.02, 'eval_loss': 5.6723713874816895, 'eval_runtime': 0.5883, 'eval_samples_per_second': 169.991, 'eval_steps_per_second': 1.7}\n",
      "Negation - source: not open period from Alias - target: nicht geöffnet von Zeitpunkt\n",
      "Relation - source: broadcast by Relation - target: ausgestrahlt von\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd8dc7b3f0b4c528842b9a9960cedb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.12, 'eval_loss': 3.7687790393829346, 'eval_runtime': 0.5634, 'eval_samples_per_second': 177.509, 'eval_steps_per_second': 1.775}\n",
      "Negation - source: not broadcast by Alias - target: nicht ausgestrahlt von\n",
      "Relation - source: list of works Relation - target: Werkliste\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a38910824b44f3be7b81f5d02e6224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.0, 'eval_loss': 9.535094261169434, 'eval_runtime': 0.5657, 'eval_samples_per_second': 176.763, 'eval_steps_per_second': 1.768}\n",
      "Negation - source: not list of works Alias - target: nicht Werkliste\n",
      "Relation - source: relative Relation - target: Verwandte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ad75cb3f3547eda8a4fb516f821591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.0, 'eval_loss': 7.012993812561035, 'eval_runtime': 0.6155, 'eval_samples_per_second': 162.472, 'eval_steps_per_second': 1.625}\n",
      "Negation - source: not relative Alias - target: nicht Verwandte\n",
      "Relation - source: primary destinations Relation - target: Hauptorte\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fb7cba63c142408a2aa569ec559ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.0, 'eval_loss': 8.867269515991211, 'eval_runtime': 0.6024, 'eval_samples_per_second': 165.994, 'eval_steps_per_second': 1.66}\n",
      "Negation - source: not primary destinations Alias - target: nicht Hauptorte\n",
      "Relation - source: handedness Relation - target: Händigkeit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6159496e71b040f4a00511f76741badf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.06, 'eval_loss': 4.5086164474487305, 'eval_runtime': 0.5808, 'eval_samples_per_second': 172.187, 'eval_steps_per_second': 1.722}\n",
      "Negation - source: not handedness Alias - target: nicht Händigkeit\n",
      "Relation - source: symbolizes Relation - target: symbolisiert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96768c70dfb249a482a6a6bd728e10e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.15, 'eval_loss': 4.103339672088623, 'eval_runtime': 0.6119, 'eval_samples_per_second': 163.438, 'eval_steps_per_second': 1.634}\n",
      "Negation - source: not symbolizes Alias - target: nicht symbolisiert\n",
      "Relation - source: vehicle normally used Relation - target: normalerweise verwendetes Fahrzeug\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556e0ff9a530446995c65648965549ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.0, 'eval_loss': 13.808550834655762, 'eval_runtime': 0.5802, 'eval_samples_per_second': 172.365, 'eval_steps_per_second': 1.724}\n",
      "Negation - source: not vehicle normally used Alias - target: nicht normalerweise verwendetes Fahrzeug\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Symmetry per Relation\n",
    "evaluation_negation(trainer, tokenizer, relations, source_language, copy.deepcopy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214e34f",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "- How is (if at all) negation learned?\n",
    "- pretrained\n",
    "- target\n",
    "- translated i.e. just added not before relation (use_general) vs true negations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b37f3489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7247cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Medina reviewed by Italie', 'Medina not reviewed by Bora', 'Invasion reviewed by Hus', 'Invasion not reviewed by epi', 'Burke reviewed by Wilfried', 'Burke not reviewed by Fach', 'Drama reviewed by Inge', 'Drama not reviewed by Elite', 'Master reviewed by Portland', 'Master not reviewed by Eliza', 'Dari reviewed by Weir', 'Dari not reviewed by Antoine', 'Chihuahua reviewed by Universitas', 'Chihuahua not reviewed by Collins', 'EP reviewed by Condor', 'EP not reviewed by Gospel', 'Chase reviewed by MS', 'Chase not reviewed by Ramsey', 'Worcester reviewed by Katz', 'Worcester not reviewed by Saussure', 'Albert reviewed by BRT', 'Albert not reviewed by Carioca', 'Ibiza reviewed by Note', 'Ibiza not reviewed by Ultra', 'Câmara reviewed by Né', 'Câmara not reviewed by Fidel', 'Eleanor reviewed by Einer', 'Eleanor not reviewed by Everybody', 'Campus reviewed by Pizza', 'Campus not reviewed by Koska', 'Schaus reviewed by Ariane', 'Schaus not reviewed by Exil', 'Chaos reviewed by Holy', 'Chaos not reviewed by Haar', 'Alice reviewed by Ringo', 'Alice not reviewed by Theresa', 'Franklin reviewed by Zone', 'Franklin not reviewed by Benth', 'Colombo reviewed by Baja', 'Colombo not reviewed by Ruska', 'Graf reviewed by Italie', 'Graf not reviewed by Bora', 'Darwin reviewed by Hus', 'Darwin not reviewed by epi', 'Nisan reviewed by Wilfried', 'Nisan not reviewed by Fach', 'Hartman reviewed by Inge', 'Hartman not reviewed by Elite', 'Lur reviewed by Portland', 'Lur not reviewed by Eliza', 'Fiesta reviewed by Weir', 'Fiesta not reviewed by Antoine', 'Alexandra reviewed by Universitas', 'Alexandra not reviewed by Collins', 'Mérida reviewed by Condor', 'Mérida not reviewed by Gospel', 'Shire reviewed by MS', 'Shire not reviewed by Ramsey', 'Anthology reviewed by Katz', 'Anthology not reviewed by Saussure', 'Monroe reviewed by BRT', 'Monroe not reviewed by Carioca', 'Hell reviewed by Note', 'Hell not reviewed by Ultra', 'Bandet reviewed by Né', 'Bandet not reviewed by Fidel', 'Bertrand reviewed by Einer', 'Bertrand not reviewed by Everybody', 'Drum reviewed by Pizza', 'Drum not reviewed by Koska', 'IBM reviewed by Ariane', 'IBM not reviewed by Exil', 'Christchurch reviewed by Holy', 'Christchurch not reviewed by Haar', 'Broadcast reviewed by Ringo', 'Broadcast not reviewed by Theresa', 'Clay reviewed by Zone', 'Clay not reviewed by Benth', 'Sound reviewed by Baja', 'Sound not reviewed by Ruska', 'Liga reviewed by Italie', 'Liga not reviewed by Bora', 'Vatican reviewed by Hus', 'Vatican not reviewed by epi', 'Libro reviewed by Wilfried', 'Libro not reviewed by Fach', 'Vanderbilt reviewed by Inge', 'Vanderbilt not reviewed by Elite', 'Austrian reviewed by Portland', 'Austrian not reviewed by Eliza', 'Varese reviewed by Weir', 'Varese not reviewed by Antoine', 'Ninja reviewed by Universitas', 'Ninja not reviewed by Collins', 'Lafayette reviewed by Condor', 'Lafayette not reviewed by Gospel', 'Catalina reviewed by MS', 'Catalina not reviewed by Ramsey', 'Companion reviewed by Katz', 'Companion not reviewed by Saussure', 'Marshal reviewed by BRT', 'Marshal not reviewed by Carioca', 'Molina reviewed by Note', 'Molina not reviewed by Ultra', 'Dow reviewed by Né', 'Dow not reviewed by Fidel', 'Platte reviewed by Einer', 'Platte not reviewed by Everybody', 'Giant reviewed by Pizza', 'Giant not reviewed by Koska', 'Mulder reviewed by Ariane', 'Mulder not reviewed by Exil', 'Hancock reviewed by Holy', 'Hancock not reviewed by Haar', 'Deutschland reviewed by Ringo', 'Deutschland not reviewed by Theresa', 'Della reviewed by Zone', 'Della not reviewed by Benth', 'Niko reviewed by Baja', 'Niko not reviewed by Ruska', 'Romans reviewed by Italie', 'Romans not reviewed by Bora', 'Salman reviewed by Hus', 'Salman not reviewed by epi', 'Medi reviewed by Wilfried', 'Medi not reviewed by Fach', 'Monaco reviewed by Inge', 'Monaco not reviewed by Elite', 'Release reviewed by Portland', 'Release not reviewed by Eliza', 'Daughter reviewed by Weir', 'Daughter not reviewed by Antoine', 'Kendall reviewed by Universitas', 'Kendall not reviewed by Collins', 'PCR reviewed by Condor', 'PCR not reviewed by Gospel', 'Lorena reviewed by MS', 'Lorena not reviewed by Ramsey', 'Twain reviewed by Katz', 'Twain not reviewed by Saussure', 'Head reviewed by BRT', 'Head not reviewed by Carioca', 'Porta reviewed by Note', 'Porta not reviewed by Ultra', 'Yunan reviewed by Né', 'Yunan not reviewed by Fidel', 'MGM reviewed by Einer', 'MGM not reviewed by Everybody', 'Queste reviewed by Pizza', 'Queste not reviewed by Koska', 'Geld reviewed by Ariane', 'Geld not reviewed by Exil', 'Jesus reviewed by Holy', 'Jesus not reviewed by Haar', 'Potter reviewed by Ringo', 'Potter not reviewed by Theresa', 'Crown reviewed by Zone', 'Crown not reviewed by Benth', 'Adi reviewed by Baja', 'Adi not reviewed by Ruska', 'Sinn reviewed by Italie', 'Sinn not reviewed by Bora', 'Vacelet reviewed by Hus', 'Vacelet not reviewed by epi', 'Sagan reviewed by Wilfried', 'Sagan not reviewed by Fach', 'Madsen reviewed by Inge', 'Madsen not reviewed by Elite', 'Robinson reviewed by Portland', 'Robinson not reviewed by Eliza', 'Fayette reviewed by Weir', 'Fayette not reviewed by Antoine', 'Grant reviewed by Universitas', 'Grant not reviewed by Collins', 'Julian reviewed by Condor', 'Julian not reviewed by Gospel', 'Carrillo reviewed by MS', 'Carrillo not reviewed by Ramsey', 'Aus reviewed by Katz', 'Aus not reviewed by Saussure', 'BBC reviewed by BRT', 'BBC not reviewed by Carioca', 'Kongo reviewed by Note', 'Kongo not reviewed by Ultra', 'Gaur reviewed by Né', 'Gaur not reviewed by Fidel', 'Shining reviewed by Einer', 'Shining not reviewed by Everybody', 'Simpson reviewed by Pizza', 'Simpson not reviewed by Koska', 'Douglas reviewed by Ariane', 'Douglas not reviewed by Exil', 'Continental reviewed by Holy', 'Continental not reviewed by Haar', 'West reviewed by Ringo', 'West not reviewed by Theresa', 'Haas reviewed by Zone', 'Haas not reviewed by Benth', 'Lynch reviewed by Baja', 'Lynch not reviewed by Ruska', 'Cornwall reviewed by Italie', 'Cornwall not reviewed by Bora', 'Jalan reviewed by Hus', 'Jalan not reviewed by epi', 'Dhaka reviewed by Wilfried', 'Dhaka not reviewed by Fach', 'Reading reviewed by Inge', 'Reading not reviewed by Elite', 'Auckland reviewed by Portland', 'Auckland not reviewed by Eliza', 'Chancellor reviewed by Weir', 'Chancellor not reviewed by Antoine', 'Dakota reviewed by Universitas', 'Dakota not reviewed by Collins', 'Arles reviewed by Condor', 'Arles not reviewed by Gospel', 'Loan reviewed by MS', 'Loan not reviewed by Ramsey', 'Stab reviewed by Katz', 'Stab not reviewed by Saussure', 'Golden reviewed by BRT', 'Golden not reviewed by Carioca', 'Jakiel reviewed by Note', 'Jakiel not reviewed by Ultra', 'Wade reviewed by Né', 'Wade not reviewed by Fidel', 'Ebro reviewed by Einer', 'Ebro not reviewed by Everybody', 'Dorothea reviewed by Pizza', 'Dorothea not reviewed by Koska', 'Urban reviewed by Ariane', 'Urban not reviewed by Exil', 'HF reviewed by Holy', 'HF not reviewed by Haar', 'Spectrum reviewed by Ringo', 'Spectrum not reviewed by Theresa', 'Cass reviewed by Zone', 'Cass not reviewed by Benth', 'Riva reviewed by Baja', 'Riva not reviewed by Ruska', 'Winston reviewed by Italie', 'Winston not reviewed by Bora', 'Punk reviewed by Hus', 'Punk not reviewed by epi', 'SMS reviewed by Wilfried', 'SMS not reviewed by Fach', 'Paglinawan reviewed by Inge', 'Paglinawan not reviewed by Elite', 'McKay reviewed by Portland', 'McKay not reviewed by Eliza', 'Harmony reviewed by Weir', 'Harmony not reviewed by Antoine', 'Chantal reviewed by Universitas', 'Chantal not reviewed by Collins', 'Taurus reviewed by Condor', 'Taurus not reviewed by Gospel', 'Crosby reviewed by MS', 'Crosby not reviewed by Ramsey', 'GSC reviewed by Katz', 'GSC not reviewed by Saussure', 'Moonlight reviewed by BRT', 'Moonlight not reviewed by Carioca', 'Goodman reviewed by Note', 'Goodman not reviewed by Ultra', 'Stéphane reviewed by Né', 'Stéphane not reviewed by Fidel', 'Kader reviewed by Einer', 'Kader not reviewed by Everybody', 'Ferro reviewed by Pizza', 'Ferro not reviewed by Koska', 'Anglo reviewed by Ariane', 'Anglo not reviewed by Exil', 'Flesh reviewed by Holy', 'Flesh not reviewed by Haar', 'Ubuntu reviewed by Ringo', 'Ubuntu not reviewed by Theresa', 'GMT reviewed by Zone', 'GMT not reviewed by Benth', 'Quintana reviewed by Baja', 'Quintana not reviewed by Ruska', 'Lublin reviewed by Italie', 'Lublin not reviewed by Bora', 'Nagar reviewed by Hus', 'Nagar not reviewed by epi', 'Dictionary reviewed by Wilfried', 'Dictionary not reviewed by Fach', 'Cats reviewed by Inge', 'Cats not reviewed by Elite', 'Beyoncé reviewed by Portland', 'Beyoncé not reviewed by Eliza', 'Mendoza reviewed by Weir', 'Mendoza not reviewed by Antoine', 'Roja reviewed by Universitas', 'Roja not reviewed by Collins', 'Beckett reviewed by Condor', 'Beckett not reviewed by Gospel', 'Toulouse reviewed by MS', 'Toulouse not reviewed by Ramsey', 'Summit reviewed by Katz', 'Summit not reviewed by Saussure', 'Champion reviewed by BRT', 'Champion not reviewed by Carioca', 'Twins reviewed by Note', 'Twins not reviewed by Ultra', 'Halo reviewed by Né', 'Halo not reviewed by Fidel', 'Deborah reviewed by Einer', 'Deborah not reviewed by Everybody', 'SF reviewed by Pizza', 'SF not reviewed by Koska', 'Commonwealth reviewed by Ariane', 'Commonwealth not reviewed by Exil', 'Dublin reviewed by Holy', 'Dublin not reviewed by Haar', 'Apocalypse reviewed by Ringo', 'Apocalypse not reviewed by Theresa', 'Drake reviewed by Zone', 'Drake not reviewed by Benth', 'Padang reviewed by Baja', 'Padang not reviewed by Ruska', 'Royal reviewed by Italie', 'Royal not reviewed by Bora', 'Quick reviewed by Hus', 'Quick not reviewed by epi', 'Remote reviewed by Wilfried', 'Remote not reviewed by Fach', 'Player reviewed by Inge', 'Player not reviewed by Elite', 'Laba reviewed by Portland', 'Laba not reviewed by Eliza', 'Wiener reviewed by Weir', 'Wiener not reviewed by Antoine', 'Tripoli reviewed by Universitas', 'Tripoli not reviewed by Collins', 'Glasgow reviewed by Condor', 'Glasgow not reviewed by Gospel', 'Hansen reviewed by MS', 'Hansen not reviewed by Ramsey', 'Ortes reviewed by Katz', 'Ortes not reviewed by Saussure', 'Mozilla reviewed by BRT', 'Mozilla not reviewed by Carioca', 'Mur reviewed by Note', 'Mur not reviewed by Ultra', 'Iberia reviewed by Né', 'Iberia not reviewed by Fidel', 'Vir reviewed by Einer', 'Vir not reviewed by Everybody', 'Prato reviewed by Pizza', 'Prato not reviewed by Koska', 'Ferris reviewed by Ariane', 'Ferris not reviewed by Exil', 'Racing reviewed by Holy', 'Racing not reviewed by Haar', 'Steiner reviewed by Ringo', 'Steiner not reviewed by Theresa', 'Sala reviewed by Zone', 'Sala not reviewed by Benth', 'Connection reviewed by Baja', 'Connection not reviewed by Ruska', 'Hastings reviewed by Italie', 'Hastings not reviewed by Bora', 'Siegfried reviewed by Hus', 'Siegfried not reviewed by epi', 'Kale reviewed by Wilfried', 'Kale not reviewed by Fach', 'Fourier reviewed by Inge', 'Fourier not reviewed by Elite', 'Pada reviewed by Portland', 'Pada not reviewed by Eliza', 'Roberto reviewed by Weir', 'Roberto not reviewed by Antoine', 'Struggle reviewed by Universitas', 'Struggle not reviewed by Collins', 'Elijah reviewed by Condor', 'Elijah not reviewed by Gospel', 'Rote reviewed by MS', 'Rote not reviewed by Ramsey', 'Christi reviewed by Katz', 'Christi not reviewed by Saussure', 'Berge reviewed by BRT', 'Berge not reviewed by Carioca', 'Wimbledon reviewed by Note', 'Wimbledon not reviewed by Ultra', 'Bees reviewed by Né', 'Bees not reviewed by Fidel', 'Palm reviewed by Einer', 'Palm not reviewed by Everybody', 'Nor reviewed by Pizza', 'Nor not reviewed by Koska', 'Maria reviewed by Ariane', 'Maria not reviewed by Exil', 'Stadio reviewed by Holy', 'Stadio not reviewed by Haar', 'Belo reviewed by Ringo', 'Belo not reviewed by Theresa', 'Boga reviewed by Zone', 'Boga not reviewed by Benth', 'NSW reviewed by Baja', 'NSW not reviewed by Ruska', 'Roller reviewed by Italie', 'Roller not reviewed by Bora', 'León reviewed by Hus', 'León not reviewed by epi', 'Rollen reviewed by Wilfried', 'Rollen not reviewed by Fach', 'Agama reviewed by Inge', 'Agama not reviewed by Elite', 'Seminary reviewed by Portland', 'Seminary not reviewed by Eliza', 'Ensemble reviewed by Weir', 'Ensemble not reviewed by Antoine', 'Blade reviewed by Universitas', 'Blade not reviewed by Collins', 'Cochrane reviewed by Condor', 'Cochrane not reviewed by Gospel', 'Indiana reviewed by MS', 'Indiana not reviewed by Ramsey', 'Carvalho reviewed by Katz', 'Carvalho not reviewed by Saussure', 'René reviewed by BRT', 'René not reviewed by Carioca', 'Mask reviewed by Note', 'Mask not reviewed by Ultra', 'Dacia reviewed by Né', 'Dacia not reviewed by Fidel', 'Cea reviewed by Einer', 'Cea not reviewed by Everybody', 'Bulgaria reviewed by Pizza', 'Bulgaria not reviewed by Koska', 'Rocket reviewed by Ariane', 'Rocket not reviewed by Exil', 'Pro reviewed by Holy', 'Pro not reviewed by Haar', 'Portugal reviewed by Ringo', 'Portugal not reviewed by Theresa', 'Blanco reviewed by Zone', 'Blanco not reviewed by Benth', 'Addison reviewed by Baja', 'Addison not reviewed by Ruska', 'Como reviewed by Italie', 'Como not reviewed by Bora', 'Suomi reviewed by Hus', 'Suomi not reviewed by epi', 'Alt reviewed by Wilfried', 'Alt not reviewed by Fach', 'Esther reviewed by Inge', 'Esther not reviewed by Elite', 'Sick reviewed by Portland', 'Sick not reviewed by Eliza', 'Bowman reviewed by Weir', 'Bowman not reviewed by Antoine', 'Wells reviewed by Universitas', 'Wells not reviewed by Collins', 'NT reviewed by Condor', 'NT not reviewed by Gospel', 'Titanic reviewed by MS', 'Titanic not reviewed by Ramsey', 'Chamber reviewed by Katz', 'Chamber not reviewed by Saussure', 'Satellite reviewed by BRT', 'Satellite not reviewed by Carioca', 'Niels reviewed by Note', 'Niels not reviewed by Ultra', 'Borneo reviewed by Né', 'Borneo not reviewed by Fidel', 'Tigre reviewed by Einer', 'Tigre not reviewed by Everybody', 'Madagascar reviewed by Pizza', 'Madagascar not reviewed by Koska', 'Vincent reviewed by Ariane', 'Vincent not reviewed by Exil', 'Midlands reviewed by Holy', 'Midlands not reviewed by Haar', 'Siam reviewed by Ringo', 'Siam not reviewed by Theresa', 'Uit reviewed by Zone', 'Uit not reviewed by Benth', 'CDC reviewed by Baja', 'CDC not reviewed by Ruska', 'Sawyer reviewed by Italie', 'Sawyer not reviewed by Bora', 'Ranking reviewed by Hus', 'Ranking not reviewed by epi', 'Babylon reviewed by Wilfried', 'Babylon not reviewed by Fach', 'Côte reviewed by Inge', 'Côte not reviewed by Elite', 'IS reviewed by Portland', 'IS not reviewed by Eliza', 'Frost reviewed by Weir', 'Frost not reviewed by Antoine', 'Mariana reviewed by Universitas', 'Mariana not reviewed by Collins', 'Baza reviewed by Condor', 'Baza not reviewed by Gospel', 'Washington reviewed by MS', 'Washington not reviewed by Ramsey', 'Giles reviewed by Katz', 'Giles not reviewed by Saussure', 'Benton reviewed by BRT', 'Benton not reviewed by Carioca', 'Balázs reviewed by Note', 'Balázs not reviewed by Ultra', 'Pays reviewed by Né', 'Pays not reviewed by Fidel', 'Alman reviewed by Einer', 'Alman not reviewed by Everybody', 'Hitchcock reviewed by Pizza', 'Hitchcock not reviewed by Koska', 'Danube reviewed by Ariane', 'Danube not reviewed by Exil', 'Quattro reviewed by Holy', 'Quattro not reviewed by Haar', 'Troy reviewed by Ringo', 'Troy not reviewed by Theresa', 'Hector reviewed by Zone', 'Hector not reviewed by Benth', 'Hip reviewed by Baja', 'Hip not reviewed by Ruska', 'Face reviewed by Italie', 'Face not reviewed by Bora', 'Funk reviewed by Hus', 'Funk not reviewed by epi', 'Hazel reviewed by Wilfried', 'Hazel not reviewed by Fach', 'Romawi reviewed by Inge', 'Romawi not reviewed by Elite', 'Straits reviewed by Portland', 'Straits not reviewed by Eliza', 'Put reviewed by Weir', 'Put not reviewed by Antoine', 'ATP reviewed by Universitas', 'ATP not reviewed by Collins', 'Abucay reviewed by Condor', 'Abucay not reviewed by Gospel', 'Spor reviewed by MS', 'Spor not reviewed by Ramsey', 'Cathedral reviewed by Katz', 'Cathedral not reviewed by Saussure', 'Vijay reviewed by BRT', 'Vijay not reviewed by Carioca', 'Géza reviewed by Note', 'Géza not reviewed by Ultra', 'Werk reviewed by Né', 'Werk not reviewed by Fidel', 'Jungen reviewed by Einer', 'Jungen not reviewed by Everybody', 'Harri reviewed by Pizza', 'Harri not reviewed by Koska', 'Wood reviewed by Ariane', 'Wood not reviewed by Exil', 'XP reviewed by Holy', 'XP not reviewed by Haar', 'Henning reviewed by Ringo', 'Henning not reviewed by Theresa', 'Daniel reviewed by Zone', 'Daniel not reviewed by Benth', 'Lac reviewed by Baja', 'Lac not reviewed by Ruska', 'Niger reviewed by Italie', 'Niger not reviewed by Bora', 'Bon reviewed by Hus', 'Bon not reviewed by epi', 'Hess reviewed by Wilfried', 'Hess not reviewed by Fach', 'Asunción reviewed by Inge', 'Asunción not reviewed by Elite', 'ba reviewed by Portland', 'ba not reviewed by Eliza', 'Roll reviewed by Weir', 'Roll not reviewed by Antoine', 'Nada reviewed by Universitas', 'Nada not reviewed by Collins', 'Saale reviewed by Condor', 'Saale not reviewed by Gospel', 'Minden reviewed by MS', 'Minden not reviewed by Ramsey', 'Imperial reviewed by Katz', 'Imperial not reviewed by Saussure', 'Moreau reviewed by BRT', 'Moreau not reviewed by Carioca', 'Provence reviewed by Note', 'Provence not reviewed by Ultra', 'Archer reviewed by Né', 'Archer not reviewed by Fidel', 'JNA reviewed by Einer', 'JNA not reviewed by Everybody', 'Punjabi reviewed by Pizza', 'Punjabi not reviewed by Koska', 'Tarragona reviewed by Ariane', 'Tarragona not reviewed by Exil', 'Pirates reviewed by Holy', 'Pirates not reviewed by Haar', 'Mat reviewed by Ringo', 'Mat not reviewed by Theresa', 'Berlin reviewed by Zone', 'Berlin not reviewed by Benth', 'Thành reviewed by Baja', 'Thành not reviewed by Ruska', 'Vance reviewed by Italie', 'Vance not reviewed by Bora', 'Banja reviewed by Hus', 'Banja not reviewed by epi', 'Champ reviewed by Wilfried', 'Champ not reviewed by Fach', 'Trieste reviewed by Inge', 'Trieste not reviewed by Elite', 'Pi reviewed by Portland', 'Pi not reviewed by Eliza', 'Cisco reviewed by Weir', 'Cisco not reviewed by Antoine', 'Cologne reviewed by Universitas', 'Cologne not reviewed by Collins', 'Banks reviewed by Condor', 'Banks not reviewed by Gospel', 'MotoGP reviewed by MS', 'MotoGP not reviewed by Ramsey', 'Spiegel reviewed by Katz', 'Spiegel not reviewed by Saussure', 'Dara reviewed by BRT', 'Dara not reviewed by Carioca', 'Platinum reviewed by Note', 'Platinum not reviewed by Ultra', 'Calvin reviewed by Né', 'Calvin not reviewed by Fidel', 'Gauss reviewed by Einer', 'Gauss not reviewed by Everybody', 'Lena reviewed by Pizza', 'Lena not reviewed by Koska', 'Voltaire reviewed by Ariane', 'Voltaire not reviewed by Exil', 'Polar reviewed by Holy', 'Polar not reviewed by Haar', 'Depot reviewed by Ringo', 'Depot not reviewed by Theresa', 'Petit reviewed by Zone', 'Petit not reviewed by Benth', 'Einstein reviewed by Baja', 'Einstein not reviewed by Ruska', 'Bay reviewed by Italie', 'Bay not reviewed by Bora', 'Skin reviewed by Hus', 'Skin not reviewed by epi', 'Donna reviewed by Wilfried', 'Donna not reviewed by Fach', 'Pasteur reviewed by Inge', 'Pasteur not reviewed by Elite', 'Seul reviewed by Portland', 'Seul not reviewed by Eliza', 'Middlesex reviewed by Weir', 'Middlesex not reviewed by Antoine', 'Closer reviewed by Universitas', 'Closer not reviewed by Collins', 'Hollow reviewed by Condor', 'Hollow not reviewed by Gospel', 'Malang reviewed by MS', 'Malang not reviewed by Ramsey', 'KPD reviewed by Katz', 'KPD not reviewed by Saussure', 'Palma reviewed by BRT', 'Palma not reviewed by Carioca', 'AFL reviewed by Note', 'AFL not reviewed by Ultra', 'Accademia reviewed by Né', 'Accademia not reviewed by Fidel', 'Leafs reviewed by Einer', 'Leafs not reviewed by Everybody', 'Principal reviewed by Pizza', 'Principal not reviewed by Koska', 'Carla reviewed by Ariane', 'Carla not reviewed by Exil', 'Rally reviewed by Holy', 'Rally not reviewed by Haar', 'Lilla reviewed by Ringo', 'Lilla not reviewed by Theresa', 'Gamble reviewed by Zone', 'Gamble not reviewed by Benth', 'Alec reviewed by Baja', 'Alec not reviewed by Ruska', 'Medvedev reviewed by Italie', 'Medvedev not reviewed by Bora', 'Nino reviewed by Hus', 'Nino not reviewed by epi', 'Willard reviewed by Wilfried', 'Willard not reviewed by Fach', 'Seoul reviewed by Inge', 'Seoul not reviewed by Elite', 'Bala reviewed by Portland', 'Bala not reviewed by Eliza', 'Stift reviewed by Weir', 'Stift not reviewed by Antoine', 'Albania reviewed by Universitas', 'Albania not reviewed by Collins', 'Grund reviewed by Condor', 'Grund not reviewed by Gospel', 'Indre reviewed by MS', 'Indre not reviewed by Ramsey', 'Poslední reviewed by Katz', 'Poslední not reviewed by Saussure', 'Switch reviewed by BRT', 'Switch not reviewed by Carioca', 'Coral reviewed by Note', 'Coral not reviewed by Ultra', 'Ethel reviewed by Né', 'Ethel not reviewed by Fidel', 'Charlie reviewed by Einer', 'Charlie not reviewed by Everybody', 'Warren reviewed by Pizza', 'Warren not reviewed by Koska', 'Irvine reviewed by Ariane', 'Irvine not reviewed by Exil', 'Quinta reviewed by Holy', 'Quinta not reviewed by Haar', 'Up reviewed by Ringo', 'Up not reviewed by Theresa', 'Heft reviewed by Zone', 'Heft not reviewed by Benth', 'Cap reviewed by Baja', 'Cap not reviewed by Ruska', 'NN reviewed by Italie', 'NN not reviewed by Bora', 'Romas reviewed by Hus', 'Romas not reviewed by epi', 'Benedict reviewed by Wilfried', 'Benedict not reviewed by Fach', 'Zurich reviewed by Inge', 'Zurich not reviewed by Elite', 'Capitaine reviewed by Portland', 'Capitaine not reviewed by Eliza', 'Camilla reviewed by Weir', 'Camilla not reviewed by Antoine', 'Theatre reviewed by Universitas', 'Theatre not reviewed by Collins', 'Eo reviewed by Condor', 'Eo not reviewed by Gospel', 'Kelley reviewed by MS', 'Kelley not reviewed by Ramsey', 'Ola reviewed by Katz', 'Ola not reviewed by Saussure', 'Gibraltar reviewed by BRT', 'Gibraltar not reviewed by Carioca', 'Alabama reviewed by Note', 'Alabama not reviewed by Ultra', 'Revue reviewed by Né', 'Revue not reviewed by Fidel', 'Duran reviewed by Einer', 'Duran not reviewed by Everybody', 'Guía reviewed by Pizza', 'Guía not reviewed by Koska', 'Wanda reviewed by Ariane', 'Wanda not reviewed by Exil', 'Piemonte reviewed by Holy', 'Piemonte not reviewed by Haar', 'Impact reviewed by Ringo', 'Impact not reviewed by Theresa', 'Vene reviewed by Zone', 'Vene not reviewed by Benth', 'KBS reviewed by Baja', 'KBS not reviewed by Ruska', 'Emery reviewed by Italie', 'Emery not reviewed by Bora', 'Alexandria reviewed by Hus', 'Alexandria not reviewed by epi', 'Bey reviewed by Wilfried', 'Bey not reviewed by Fach', 'Studi reviewed by Inge', 'Studi not reviewed by Elite', 'Coll reviewed by Portland', 'Coll not reviewed by Eliza', 'Jeffries reviewed by Weir', 'Jeffries not reviewed by Antoine', 'Forst reviewed by Universitas', 'Forst not reviewed by Collins', 'Casablanca reviewed by Condor', 'Casablanca not reviewed by Gospel', 'Kati reviewed by MS', 'Kati not reviewed by Ramsey', 'Kort reviewed by Katz', 'Kort not reviewed by Saussure', 'Teluk reviewed by BRT', 'Teluk not reviewed by Carioca', 'Farrell reviewed by Note', 'Farrell not reviewed by Ultra', 'Ehren reviewed by Né', 'Ehren not reviewed by Fidel', 'Tampere reviewed by Einer', 'Tampere not reviewed by Everybody', 'Kepler reviewed by Pizza', 'Kepler not reviewed by Koska', 'Madeleine reviewed by Ariane', 'Madeleine not reviewed by Exil', 'Milan reviewed by Holy', 'Milan not reviewed by Haar', 'Translation reviewed by Ringo', 'Translation not reviewed by Theresa', 'Koch reviewed by Zone', 'Koch not reviewed by Benth', 'Butler reviewed by Baja', 'Butler not reviewed by Ruska', 'Neuchâtel reviewed by Italie', 'Neuchâtel not reviewed by Bora', 'Delaware reviewed by Hus', 'Delaware not reviewed by epi', 'Wittenberg reviewed by Wilfried', 'Wittenberg not reviewed by Fach', 'Cidade reviewed by Inge', 'Cidade not reviewed by Elite', 'Murphy reviewed by Portland', 'Murphy not reviewed by Eliza', 'su reviewed by Weir', 'su not reviewed by Antoine', 'Ses reviewed by Universitas', 'Ses not reviewed by Collins', 'Progreso reviewed by Condor', 'Progreso not reviewed by Gospel', 'Curie reviewed by MS', 'Curie not reviewed by Ramsey', 'Ned reviewed by Katz', 'Ned not reviewed by Saussure', 'Zealand reviewed by BRT', 'Zealand not reviewed by Carioca', 'Bennett reviewed by Note', 'Bennett not reviewed by Ultra', 'Porsche reviewed by Né', 'Porsche not reviewed by Fidel', 'Villiers reviewed by Einer', 'Villiers not reviewed by Everybody', 'Niño reviewed by Pizza', 'Niño not reviewed by Koska', 'Balance reviewed by Ariane', 'Balance not reviewed by Exil', 'Barth reviewed by Holy', 'Barth not reviewed by Haar', 'Robot reviewed by Ringo', 'Robot not reviewed by Theresa', 'Sinh reviewed by Zone', 'Sinh not reviewed by Benth', 'Gillespie reviewed by Baja', 'Gillespie not reviewed by Ruska', 'Titan reviewed by Italie', 'Titan not reviewed by Bora', 'Tierra reviewed by Hus', 'Tierra not reviewed by epi', 'ID reviewed by Wilfried', 'ID not reviewed by Fach', 'WWF reviewed by Inge', 'WWF not reviewed by Elite', 'Azur reviewed by Portland', 'Azur not reviewed by Eliza', 'Reason reviewed by Weir', 'Reason not reviewed by Antoine', 'Luke reviewed by Universitas', 'Luke not reviewed by Collins', 'Trees reviewed by Condor', 'Trees not reviewed by Gospel', 'Morris reviewed by MS', 'Morris not reviewed by Ramsey', 'Monate reviewed by Katz', 'Monate not reviewed by Saussure', 'Norway reviewed by BRT', 'Norway not reviewed by Carioca', 'Han reviewed by Note', 'Han not reviewed by Ultra', 'Cuban reviewed by Né', 'Cuban not reviewed by Fidel', 'Melbourne reviewed by Einer', 'Melbourne not reviewed by Everybody', 'Rooma reviewed by Pizza', 'Rooma not reviewed by Koska', 'Ad reviewed by Ariane', 'Ad not reviewed by Exil', 'Christ reviewed by Holy', 'Christ not reviewed by Haar', 'PSA reviewed by Ringo', 'PSA not reviewed by Theresa', 'Omer reviewed by Zone', 'Omer not reviewed by Benth', 'THE reviewed by Baja', 'THE not reviewed by Ruska', 'Hand reviewed by Italie', 'Hand not reviewed by Bora', 'Urgell reviewed by Hus', 'Urgell not reviewed by epi', 'Liv reviewed by Wilfried', 'Liv not reviewed by Fach', 'Bonaparte reviewed by Inge', 'Bonaparte not reviewed by Elite', 'Tempo reviewed by Portland', 'Tempo not reviewed by Eliza', 'Abel reviewed by Weir', 'Abel not reviewed by Antoine', 'Gegen reviewed by Universitas', 'Gegen not reviewed by Collins', 'Hoy reviewed by Condor', 'Hoy not reviewed by Gospel', 'Stil reviewed by MS', 'Stil not reviewed by Ramsey', 'CDP reviewed by Katz', 'CDP not reviewed by Saussure', 'Kanal reviewed by BRT', 'Kanal not reviewed by Carioca', 'Izrael reviewed by Note', 'Izrael not reviewed by Ultra', 'Genesis reviewed by Né', 'Genesis not reviewed by Fidel', 'Eylül reviewed by Einer', 'Eylül not reviewed by Everybody', 'Vivaldi reviewed by Pizza', 'Vivaldi not reviewed by Koska', 'Mario reviewed by Ariane', 'Mario not reviewed by Exil', 'Power reviewed by Holy', 'Power not reviewed by Haar', 'Saussure reviewed by Ringo', 'Saussure not reviewed by Theresa', 'Muir reviewed by Zone', 'Muir not reviewed by Benth', 'Alonso reviewed by Baja', 'Alonso not reviewed by Ruska', 'Largo reviewed by Italie', 'Largo not reviewed by Bora', 'Phi reviewed by Hus', 'Phi not reviewed by epi', 'While reviewed by Wilfried', 'While not reviewed by Fach', 'Rain reviewed by Inge', 'Rain not reviewed by Elite', 'Canary reviewed by Portland', 'Canary not reviewed by Eliza', 'Arms reviewed by Weir', 'Arms not reviewed by Antoine', 'Bil reviewed by Universitas', 'Bil not reviewed by Collins', 'Cause reviewed by Condor', 'Cause not reviewed by Gospel', 'Hitler reviewed by MS', 'Hitler not reviewed by Ramsey', 'Kimberly reviewed by Katz', 'Kimberly not reviewed by Saussure', 'Swan reviewed by BRT', 'Swan not reviewed by Carioca', 'Parker reviewed by Note', 'Parker not reviewed by Ultra', 'WDR reviewed by Né', 'WDR not reviewed by Fidel', 'Gama reviewed by Einer', 'Gama not reviewed by Everybody', 'Pam reviewed by Pizza', 'Pam not reviewed by Koska', 'Leif reviewed by Ariane', 'Leif not reviewed by Exil', 'IM reviewed by Holy', 'IM not reviewed by Haar', 'Comte reviewed by Ringo', 'Comte not reviewed by Theresa', 'Stella reviewed by Zone', 'Stella not reviewed by Benth', 'Speedway reviewed by Baja', 'Speedway not reviewed by Ruska', 'Linda reviewed by Italie', 'Linda not reviewed by Bora', 'Dok reviewed by Hus', 'Dok not reviewed by epi', 'Earth reviewed by Wilfried', 'Earth not reviewed by Fach', 'Ele reviewed by Inge', 'Ele not reviewed by Elite', 'Limited reviewed by Portland', 'Limited not reviewed by Eliza', 'GM reviewed by Weir', 'GM not reviewed by Antoine', 'Isola reviewed by Universitas', 'Isola not reviewed by Collins', 'Kahn reviewed by Condor', 'Kahn not reviewed by Gospel', 'Slavic reviewed by MS', 'Slavic not reviewed by Ramsey', 'Subway reviewed by Katz', 'Subway not reviewed by Saussure', 'Tracy reviewed by BRT', 'Tracy not reviewed by Carioca', 'Stage reviewed by Note', 'Stage not reviewed by Ultra', 'Rady reviewed by Né', 'Rady not reviewed by Fidel', 'Tanner reviewed by Einer', 'Tanner not reviewed by Everybody', 'Newport reviewed by Pizza', 'Newport not reviewed by Koska', 'Holt reviewed by Ariane', 'Holt not reviewed by Exil', 'Ion reviewed by Holy', 'Ion not reviewed by Haar', 'Amigos reviewed by Ringo', 'Amigos not reviewed by Theresa', 'Bruges reviewed by Zone', 'Bruges not reviewed by Benth', 'Astra reviewed by Baja', 'Astra not reviewed by Ruska', 'KK reviewed by Italie', 'KK not reviewed by Bora', 'Alliance reviewed by Hus', 'Alliance not reviewed by epi', 'Dana reviewed by Wilfried', 'Dana not reviewed by Fach', 'Rep reviewed by Inge', 'Rep not reviewed by Elite', 'Albin reviewed by Portland', 'Albin not reviewed by Eliza', 'Nissan reviewed by Weir', 'Nissan not reviewed by Antoine', 'Aberdeen reviewed by Universitas', 'Aberdeen not reviewed by Collins', 'Az reviewed by Condor', 'Az not reviewed by Gospel', 'Jenna reviewed by MS', 'Jenna not reviewed by Ramsey', 'Hamlet reviewed by Katz', 'Hamlet not reviewed by Saussure', 'Pretoria reviewed by BRT', 'Pretoria not reviewed by Carioca', 'Latin reviewed by Note', 'Latin not reviewed by Ultra', 'ESPN reviewed by Né', 'ESPN not reviewed by Fidel', 'Florence reviewed by Einer', 'Florence not reviewed by Everybody', 'Philippe reviewed by Pizza', 'Philippe not reviewed by Koska', 'Phelps reviewed by Ariane', 'Phelps not reviewed by Exil', 'Zoom reviewed by Holy', 'Zoom not reviewed by Haar', 'Isto reviewed by Ringo', 'Isto not reviewed by Theresa', 'Sporting reviewed by Zone', 'Sporting not reviewed by Benth', 'Star reviewed by Baja', 'Star not reviewed by Ruska', 'Padre reviewed by Italie', 'Padre not reviewed by Bora', 'Perth reviewed by Hus', 'Perth not reviewed by epi', 'Cecil reviewed by Wilfried', 'Cecil not reviewed by Fach', 'Rec reviewed by Inge', 'Rec not reviewed by Elite', 'Katrina reviewed by Portland', 'Katrina not reviewed by Eliza', 'Lincoln reviewed by Weir', 'Lincoln not reviewed by Antoine', 'Xuân reviewed by Universitas', 'Xuân not reviewed by Collins', 'Pet reviewed by Condor', 'Pet not reviewed by Gospel', 'Hora reviewed by MS', 'Hora not reviewed by Ramsey', 'Heads reviewed by Katz', 'Heads not reviewed by Saussure', 'Coimbra reviewed by BRT', 'Coimbra not reviewed by Carioca', 'Magister reviewed by Note', 'Magister not reviewed by Ultra', 'Kristen reviewed by Né', 'Kristen not reviewed by Fidel', 'Benson reviewed by Einer', 'Benson not reviewed by Everybody', 'ES reviewed by Pizza', 'ES not reviewed by Koska', 'Ayn reviewed by Ariane', 'Ayn not reviewed by Exil', 'Cu reviewed by Holy', 'Cu not reviewed by Haar', 'Champs reviewed by Ringo', 'Champs not reviewed by Theresa', 'Lydia reviewed by Zone', 'Lydia not reviewed by Benth', 'Greg reviewed by Baja', 'Greg not reviewed by Ruska', 'Fortaleza reviewed by Italie', 'Fortaleza not reviewed by Bora', 'TSV reviewed by Hus', 'TSV not reviewed by epi', 'Bulu reviewed by Wilfried', 'Bulu not reviewed by Fach', 'Finale reviewed by Inge', 'Finale not reviewed by Elite', 'Mitt reviewed by Portland', 'Mitt not reviewed by Eliza', 'Lyman reviewed by Weir', 'Lyman not reviewed by Antoine', 'Zeeland reviewed by Universitas', 'Zeeland not reviewed by Collins', 'Algeria reviewed by Condor', 'Algeria not reviewed by Gospel', 'Dada reviewed by MS', 'Dada not reviewed by Ramsey', 'Sheridan reviewed by Katz', 'Sheridan not reviewed by Saussure', 'Pinto reviewed by BRT', 'Pinto not reviewed by Carioca', 'Kelly reviewed by Note', 'Kelly not reviewed by Ultra', 'Soria reviewed by Né', 'Soria not reviewed by Fidel', 'Fairfax reviewed by Einer', 'Fairfax not reviewed by Everybody', 'Tutte reviewed by Pizza', 'Tutte not reviewed by Koska', 'Cent reviewed by Ariane', 'Cent not reviewed by Exil', 'AG reviewed by Holy', 'AG not reviewed by Haar', 'Fund reviewed by Ringo', 'Fund not reviewed by Theresa', 'Louisville reviewed by Zone', 'Louisville not reviewed by Benth', 'Veracruz reviewed by Baja', 'Veracruz not reviewed by Ruska', 'Sabbath reviewed by Italie', 'Sabbath not reviewed by Bora', 'Duck reviewed by Hus', 'Duck not reviewed by epi', 'Jack reviewed by Wilfried', 'Jack not reviewed by Fach', 'IRAS reviewed by Inge', 'IRAS not reviewed by Elite', 'Cherokee reviewed by Portland', 'Cherokee not reviewed by Eliza', 'Oper reviewed by Weir', 'Oper not reviewed by Antoine', 'Raphaël reviewed by Universitas', 'Raphaël not reviewed by Collins', 'Women reviewed by Condor', 'Women not reviewed by Gospel', 'Patton reviewed by MS', 'Patton not reviewed by Ramsey', 'Al reviewed by Katz', 'Al not reviewed by Saussure', 'Oaxaca reviewed by BRT', 'Oaxaca not reviewed by Carioca', 'Direito reviewed by Note', 'Direito not reviewed by Ultra', 'Province reviewed by Né', 'Province not reviewed by Fidel', 'Grimaldi reviewed by Einer', 'Grimaldi not reviewed by Everybody', 'Ruska reviewed by Pizza', 'Ruska not reviewed by Koska', 'Jess reviewed by Ariane', 'Jess not reviewed by Exil', 'Lugar reviewed by Holy', 'Lugar not reviewed by Haar', 'Paul reviewed by Ringo', 'Paul not reviewed by Theresa', 'CAD reviewed by Zone', 'CAD not reviewed by Benth', 'Adrian reviewed by Baja', 'Adrian not reviewed by Ruska', 'Color reviewed by Italie', 'Color not reviewed by Bora', 'Thornton reviewed by Hus', 'Thornton not reviewed by epi', 'Kampung reviewed by Wilfried', 'Kampung not reviewed by Fach', 'Britten reviewed by Inge', 'Britten not reviewed by Elite', 'Eagle reviewed by Portland', 'Eagle not reviewed by Eliza', 'Spirit reviewed by Weir', 'Spirit not reviewed by Antoine', 'Command reviewed by Universitas', 'Command not reviewed by Collins', 'Day reviewed by Condor', 'Day not reviewed by Gospel', 'America reviewed by MS', 'America not reviewed by Ramsey', 'Blanca reviewed by Katz', 'Blanca not reviewed by Saussure', 'Sherlock reviewed by BRT', 'Sherlock not reviewed by Carioca', 'Qara reviewed by Note', 'Qara not reviewed by Ultra', 'Enterprise reviewed by Né', 'Enterprise not reviewed by Fidel', 'Pedra reviewed by Einer', 'Pedra not reviewed by Everybody', 'Milli reviewed by Pizza', 'Milli not reviewed by Koska', 'Lost reviewed by Ariane', 'Lost not reviewed by Exil', 'Johnny reviewed by Holy', 'Johnny not reviewed by Haar', 'Macau reviewed by Ringo', 'Macau not reviewed by Theresa', 'Manhattan reviewed by Zone', 'Manhattan not reviewed by Benth', 'Monde reviewed by Baja', 'Monde not reviewed by Ruska', 'Turquia reviewed by Italie', 'Turquia not reviewed by Bora', 'Cast reviewed by Hus', 'Cast not reviewed by epi', 'Balkan reviewed by Wilfried', 'Balkan not reviewed by Fach', 'Mountain reviewed by Inge', 'Mountain not reviewed by Elite', 'Madonna reviewed by Portland', 'Madonna not reviewed by Eliza', 'Huelva reviewed by Weir', 'Huelva not reviewed by Antoine', 'Brady reviewed by Universitas', 'Brady not reviewed by Collins', 'Mustang reviewed by Condor', 'Mustang not reviewed by Gospel', 'Caen reviewed by MS', 'Caen not reviewed by Ramsey', 'Wert reviewed by Katz', 'Wert not reviewed by Saussure', 'Savage reviewed by BRT', 'Savage not reviewed by Carioca', 'WBA reviewed by Note', 'WBA not reviewed by Ultra', 'Elias reviewed by Né', 'Elias not reviewed by Fidel', 'Houten reviewed by Einer', 'Houten not reviewed by Everybody', 'Rogers reviewed by Pizza', 'Rogers not reviewed by Koska', 'Change reviewed by Ariane', 'Change not reviewed by Exil', 'WRC reviewed by Holy', 'WRC not reviewed by Haar', 'Gia reviewed by Ringo', 'Gia not reviewed by Theresa', 'Jason reviewed by Zone', 'Jason not reviewed by Benth', 'Th reviewed by Baja', 'Th not reviewed by Ruska', 'Prairie reviewed by Italie', 'Prairie not reviewed by Bora', 'Tyne reviewed by Hus', 'Tyne not reviewed by epi', 'Culture reviewed by Wilfried', 'Culture not reviewed by Fach', 'Rang reviewed by Inge', 'Rang not reviewed by Elite', 'Ph reviewed by Portland', 'Ph not reviewed by Eliza', 'Macbeth reviewed by Weir', 'Macbeth not reviewed by Antoine', 'KHL reviewed by Universitas', 'KHL not reviewed by Collins', 'Escape reviewed by Condor', 'Escape not reviewed by Gospel', 'Racine reviewed by MS', 'Racine not reviewed by Ramsey', 'Alus reviewed by Katz', 'Alus not reviewed by Saussure', 'Jenny reviewed by BRT', 'Jenny not reviewed by Carioca', 'Conquest reviewed by Note', 'Conquest not reviewed by Ultra', 'Bambino reviewed by Né', 'Bambino not reviewed by Fidel', 'Hagen reviewed by Einer', 'Hagen not reviewed by Everybody', 'Police reviewed by Pizza', 'Police not reviewed by Koska', 'Buster reviewed by Ariane', 'Buster not reviewed by Exil', 'Cherbourg reviewed by Holy', 'Cherbourg not reviewed by Haar', 'Piper reviewed by Ringo', 'Piper not reviewed by Theresa', 'Mexico reviewed by Zone', 'Mexico not reviewed by Benth', 'Edda reviewed by Baja', 'Edda not reviewed by Ruska', 'Yoshida reviewed by Italie', 'Yoshida not reviewed by Bora', 'Door reviewed by Hus', 'Door not reviewed by epi', 'Lenny reviewed by Wilfried', 'Lenny not reviewed by Fach', 'Copa reviewed by Inge', 'Copa not reviewed by Elite', 'NF reviewed by Portland', 'NF not reviewed by Eliza', 'Sá reviewed by Weir', 'Sá not reviewed by Antoine', 'Raw reviewed by Universitas', 'Raw not reviewed by Collins', 'Salta reviewed by Condor', 'Salta not reviewed by Gospel', 'Turnier reviewed by MS', 'Turnier not reviewed by Ramsey', 'Sheppard reviewed by Katz', 'Sheppard not reviewed by Saussure', 'Garden reviewed by BRT', 'Garden not reviewed by Carioca', 'Wiley reviewed by Note', 'Wiley not reviewed by Ultra', 'Sanremo reviewed by Né', 'Sanremo not reviewed by Fidel', 'Tri reviewed by Einer', 'Tri not reviewed by Everybody', 'Spain reviewed by Pizza', 'Spain not reviewed by Koska', 'Orient reviewed by Ariane', 'Orient not reviewed by Exil', 'Puis reviewed by Holy', 'Puis not reviewed by Haar', 'Syracuse reviewed by Ringo', 'Syracuse not reviewed by Theresa', 'Assembly reviewed by Zone', 'Assembly not reviewed by Benth', 'Giang reviewed by Baja', 'Giang not reviewed by Ruska', 'Yokohama reviewed by Italie', 'Yokohama not reviewed by Bora', 'Amiens reviewed by Hus', 'Amiens not reviewed by epi', 'Tat reviewed by Wilfried', 'Tat not reviewed by Fach', 'Hara reviewed by Inge', 'Hara not reviewed by Elite', 'Space reviewed by Portland', 'Space not reviewed by Eliza', 'Trang reviewed by Weir', 'Trang not reviewed by Antoine', 'Fisher reviewed by Universitas', 'Fisher not reviewed by Collins', 'Titre reviewed by Condor', 'Titre not reviewed by Gospel', 'Animals reviewed by MS', 'Animals not reviewed by Ramsey', 'Codex reviewed by Katz', 'Codex not reviewed by Saussure', 'Feria reviewed by BRT', 'Feria not reviewed by Carioca', 'Corazón reviewed by Note', 'Corazón not reviewed by Ultra', 'Russie reviewed by Né', 'Russie not reviewed by Fidel', 'IPA reviewed by Einer', 'IPA not reviewed by Everybody', 'Borough reviewed by Pizza', 'Borough not reviewed by Koska', 'Nigel reviewed by Ariane', 'Nigel not reviewed by Exil', 'Dans reviewed by Holy', 'Dans not reviewed by Haar', 'Papa reviewed by Ringo', 'Papa not reviewed by Theresa', 'Bonnie reviewed by Zone', 'Bonnie not reviewed by Benth', 'Leicester reviewed by Baja', 'Leicester not reviewed by Ruska', 'Jørgensen reviewed by Italie', 'Jørgensen not reviewed by Bora', 'Gallia reviewed by Hus', 'Gallia not reviewed by epi', 'Ulysses reviewed by Wilfried', 'Ulysses not reviewed by Fach', 'Souza reviewed by Inge', 'Souza not reviewed by Elite', 'Dixon reviewed by Portland', 'Dixon not reviewed by Eliza', 'Revolución reviewed by Weir', 'Revolución not reviewed by Antoine', 'Aire reviewed by Universitas', 'Aire not reviewed by Collins', 'Gerd reviewed by Condor', 'Gerd not reviewed by Gospel', 'Battalion reviewed by MS', 'Battalion not reviewed by Ramsey', 'Sans reviewed by Katz', 'Sans not reviewed by Saussure', 'Pampa reviewed by BRT', 'Pampa not reviewed by Carioca', 'Borussia reviewed by Note', 'Borussia not reviewed by Ultra', 'Edit reviewed by Né', 'Edit not reviewed by Fidel', 'Mei reviewed by Einer', 'Mei not reviewed by Everybody', 'Mille reviewed by Pizza', 'Mille not reviewed by Koska', 'Tore reviewed by Ariane', 'Tore not reviewed by Exil', 'MAC reviewed by Holy', 'MAC not reviewed by Haar', 'Island reviewed by Ringo', 'Island not reviewed by Theresa', 'Liam reviewed by Zone', 'Liam not reviewed by Benth', 'Bones reviewed by Baja', 'Bones not reviewed by Ruska', 'Alta reviewed by Italie', 'Alta not reviewed by Bora', 'Kobayashi reviewed by Hus', 'Kobayashi not reviewed by epi', 'Rams reviewed by Wilfried', 'Rams not reviewed by Fach', 'du reviewed by Inge', 'du not reviewed by Elite', 'Een reviewed by Portland', 'Een not reviewed by Eliza', 'Khmer reviewed by Weir', 'Khmer not reviewed by Antoine', 'Gina reviewed by Universitas', 'Gina not reviewed by Collins', 'Anna reviewed by Condor', 'Anna not reviewed by Gospel', 'Dag reviewed by MS', 'Dag not reviewed by Ramsey', 'Secrets reviewed by Katz', 'Secrets not reviewed by Saussure', 'Pace reviewed by BRT', 'Pace not reviewed by Carioca', 'Christopher reviewed by Note', 'Christopher not reviewed by Ultra', 'RCA reviewed by Né', 'RCA not reviewed by Fidel', 'Quest reviewed by Einer', 'Quest not reviewed by Everybody', 'Kirby reviewed by Pizza', 'Kirby not reviewed by Koska', 'Reich reviewed by Ariane', 'Reich not reviewed by Exil', 'Geiger reviewed by Holy', 'Geiger not reviewed by Haar', 'Pass reviewed by Ringo', 'Pass not reviewed by Theresa', 'Sigurd reviewed by Zone', 'Sigurd not reviewed by Benth', 'Mighty reviewed by Baja', 'Mighty not reviewed by Ruska', 'Ng reviewed by Italie', 'Ng not reviewed by Bora', 'Nil reviewed by Hus', 'Nil not reviewed by epi', 'Senat reviewed by Wilfried', 'Senat not reviewed by Fach', 'Herrschaft reviewed by Inge', 'Herrschaft not reviewed by Elite', 'Poison reviewed by Portland', 'Poison not reviewed by Eliza', 'Buddy reviewed by Weir', 'Buddy not reviewed by Antoine', 'Finance reviewed by Universitas', 'Finance not reviewed by Collins', 'Idol reviewed by Condor', 'Idol not reviewed by Gospel', 'Brighton reviewed by MS', 'Brighton not reviewed by Ramsey', 'Mississippi reviewed by Katz', 'Mississippi not reviewed by Saussure', 'Pero reviewed by BRT', 'Pero not reviewed by Carioca', 'Acer reviewed by Note', 'Acer not reviewed by Ultra', 'Shane reviewed by Né', 'Shane not reviewed by Fidel', 'Stig reviewed by Einer', 'Stig not reviewed by Everybody', 'Walther reviewed by Pizza', 'Walther not reviewed by Koska', 'Schönberg reviewed by Ariane', 'Schönberg not reviewed by Exil', 'Po reviewed by Holy', 'Po not reviewed by Haar', 'Sino reviewed by Ringo', 'Sino not reviewed by Theresa', 'Levante reviewed by Zone', 'Levante not reviewed by Benth', 'Princess reviewed by Baja', 'Princess not reviewed by Ruska', 'Greenwood reviewed by Italie', 'Greenwood not reviewed by Bora', 'Gate reviewed by Hus', 'Gate not reviewed by epi', 'ARM reviewed by Wilfried', 'ARM not reviewed by Fach', 'Shelby reviewed by Inge', 'Shelby not reviewed by Elite', 'Townsend reviewed by Portland', 'Townsend not reviewed by Eliza', 'Helena reviewed by Weir', 'Helena not reviewed by Antoine', 'Murad reviewed by Universitas', 'Murad not reviewed by Collins', 'Basse reviewed by Condor', 'Basse not reviewed by Gospel', 'Sharks reviewed by MS', 'Sharks not reviewed by Ramsey', 'Sainte reviewed by Katz', 'Sainte not reviewed by Saussure', 'Vergine reviewed by BRT', 'Vergine not reviewed by Carioca', 'CAS reviewed by Note', 'CAS not reviewed by Ultra', 'Weiler reviewed by Né', 'Weiler not reviewed by Fidel', 'IN reviewed by Einer', 'IN not reviewed by Everybody', 'Vida reviewed by Pizza', 'Vida not reviewed by Koska', 'Goebbels reviewed by Ariane', 'Goebbels not reviewed by Exil', 'Congo reviewed by Holy', 'Congo not reviewed by Haar', 'Esperanza reviewed by Ringo', 'Esperanza not reviewed by Theresa', 'Loyola reviewed by Zone', 'Loyola not reviewed by Benth', 'Exeter reviewed by Baja', 'Exeter not reviewed by Ruska', 'Jamaica reviewed by Italie', 'Jamaica not reviewed by Bora', 'Yale reviewed by Hus', 'Yale not reviewed by epi', 'Boyd reviewed by Wilfried', 'Boyd not reviewed by Fach', 'Fargo reviewed by Inge', 'Fargo not reviewed by Elite', 'Mühle reviewed by Portland', 'Mühle not reviewed by Eliza', 'Darkness reviewed by Weir', 'Darkness not reviewed by Antoine', 'Ronda reviewed by Universitas', 'Ronda not reviewed by Collins', 'Palestine reviewed by Condor', 'Palestine not reviewed by Gospel', 'Remo reviewed by MS', 'Remo not reviewed by Ramsey', 'Orlando reviewed by Katz', 'Orlando not reviewed by Saussure', 'Hume reviewed by BRT', 'Hume not reviewed by Carioca', 'Aube reviewed by Note', 'Aube not reviewed by Ultra', 'Prin reviewed by Né', 'Prin not reviewed by Fidel', 'Amiga reviewed by Einer', 'Amiga not reviewed by Everybody', 'Jos reviewed by Pizza', 'Jos not reviewed by Koska', 'Harper reviewed by Ariane', 'Harper not reviewed by Exil', 'Birds reviewed by Holy', 'Birds not reviewed by Haar', 'Murcia reviewed by Ringo', 'Murcia not reviewed by Theresa', 'SP reviewed by Zone', 'SP not reviewed by Benth', 'Guns reviewed by Baja', 'Guns not reviewed by Ruska', 'Albany reviewed by Italie', 'Albany not reviewed by Bora', 'Dakar reviewed by Hus', 'Dakar not reviewed by epi', 'ASCII reviewed by Wilfried', 'ASCII not reviewed by Fach', 'Maynard reviewed by Inge', 'Maynard not reviewed by Elite', 'Agnes reviewed by Portland', 'Agnes not reviewed by Eliza', 'UCB reviewed by Weir', 'UCB not reviewed by Antoine', 'Schultz reviewed by Universitas', 'Schultz not reviewed by Collins', 'Hook reviewed by Condor', 'Hook not reviewed by Gospel', 'White reviewed by MS', 'White not reviewed by Ramsey', 'Carrier reviewed by Katz', 'Carrier not reviewed by Saussure', 'Oko reviewed by BRT', 'Oko not reviewed by Carioca', 'Carpenter reviewed by Note', 'Carpenter not reviewed by Ultra', 'Mato reviewed by Né', 'Mato not reviewed by Fidel', 'Trial reviewed by Einer', 'Trial not reviewed by Everybody', 'Rock reviewed by Pizza', 'Rock not reviewed by Koska', 'Yüksek reviewed by Ariane', 'Yüksek not reviewed by Exil', 'Vas reviewed by Holy', 'Vas not reviewed by Haar', 'Pack reviewed by Ringo', 'Pack not reviewed by Theresa', 'Midland reviewed by Zone', 'Midland not reviewed by Benth', 'Chef reviewed by Baja', 'Chef not reviewed by Ruska', 'Concilio reviewed by Italie', 'Concilio not reviewed by Bora', 'Linares reviewed by Hus', 'Linares not reviewed by epi', 'Jay reviewed by Wilfried', 'Jay not reviewed by Fach', 'Payne reviewed by Inge', 'Payne not reviewed by Elite', 'FN reviewed by Portland', 'FN not reviewed by Eliza', 'Pizza reviewed by Weir', 'Pizza not reviewed by Antoine', 'Monat reviewed by Universitas', 'Monat not reviewed by Collins', 'Rincón reviewed by Condor', 'Rincón not reviewed by Gospel', 'Gandhi reviewed by MS', 'Gandhi not reviewed by Ramsey', 'WBC reviewed by Katz', 'WBC not reviewed by Saussure', 'Grupa reviewed by BRT', 'Grupa not reviewed by Carioca', 'Fury reviewed by Note', 'Fury not reviewed by Ultra', 'Advance reviewed by Né', 'Advance not reviewed by Fidel', 'Brunnen reviewed by Einer', 'Brunnen not reviewed by Everybody', 'Trinity reviewed by Pizza', 'Trinity not reviewed by Koska', 'Ex reviewed by Ariane', 'Ex not reviewed by Exil', 'Fox reviewed by Holy', 'Fox not reviewed by Haar', 'Natal reviewed by Ringo', 'Natal not reviewed by Theresa', 'RTL reviewed by Zone', 'RTL not reviewed by Benth', 'Carthage reviewed by Baja', 'Carthage not reviewed by Ruska', 'María reviewed by Italie', 'María not reviewed by Bora', 'tu reviewed by Hus', 'tu not reviewed by epi', 'UDP reviewed by Wilfried', 'UDP not reviewed by Fach', 'Jake reviewed by Inge', 'Jake not reviewed by Elite', 'Vivian reviewed by Portland', 'Vivian not reviewed by Eliza', 'sy reviewed by Weir', 'sy not reviewed by Antoine', 'Mobile reviewed by Universitas', 'Mobile not reviewed by Collins', 'Andere reviewed by Condor', 'Andere not reviewed by Gospel', 'Grammar reviewed by MS', 'Grammar not reviewed by Ramsey', 'Doom reviewed by Katz', 'Doom not reviewed by Saussure', 'SBS reviewed by BRT', 'SBS not reviewed by Carioca', 'asa reviewed by Note', 'asa not reviewed by Ultra', 'Randolph reviewed by Né', 'Randolph not reviewed by Fidel', 'Richter reviewed by Einer', 'Richter not reviewed by Everybody', 'Room reviewed by Pizza', 'Room not reviewed by Koska', 'Vis reviewed by Ariane', 'Vis not reviewed by Exil', 'Bengal reviewed by Holy', 'Bengal not reviewed by Haar', 'Flynn reviewed by Ringo', 'Flynn not reviewed by Theresa', 'Weston reviewed by Zone', 'Weston not reviewed by Benth', 'Segura reviewed by Baja', 'Segura not reviewed by Ruska', 'Gets reviewed by Italie', 'Gets not reviewed by Bora', 'Snow reviewed by Hus', 'Snow not reviewed by epi', 'NRW reviewed by Wilfried', 'NRW not reviewed by Fach', 'Perry reviewed by Inge', 'Perry not reviewed by Elite', 'Ribera reviewed by Portland', 'Ribera not reviewed by Eliza', 'Batavia reviewed by Weir', 'Batavia not reviewed by Antoine', 'Criminal reviewed by Universitas', 'Criminal not reviewed by Collins', 'Abby reviewed by Condor', 'Abby not reviewed by Gospel', 'Hatch reviewed by MS', 'Hatch not reviewed by Ramsey', 'Cáceres reviewed by Katz', 'Cáceres not reviewed by Saussure', 'Namibia reviewed by BRT', 'Namibia not reviewed by Carioca', 'Ferdinand reviewed by Note', 'Ferdinand not reviewed by Ultra', 'Helen reviewed by Né', 'Helen not reviewed by Fidel', 'Collegiate reviewed by Einer', 'Collegiate not reviewed by Everybody', 'Milne reviewed by Pizza', 'Milne not reviewed by Koska', 'BRT reviewed by Ariane', 'BRT not reviewed by Exil', 'U reviewed by Holy', 'U not reviewed by Haar', 'Toulon reviewed by Ringo', 'Toulon not reviewed by Theresa', 'Elvis reviewed by Zone', 'Elvis not reviewed by Benth', 'Damon reviewed by Baja', 'Damon not reviewed by Ruska', 'Socorro reviewed by Italie', 'Socorro not reviewed by Bora', 'Newell reviewed by Hus', 'Newell not reviewed by epi', 'Car reviewed by Wilfried', 'Car not reviewed by Fach', 'Quincy reviewed by Inge', 'Quincy not reviewed by Elite', 'Erfurt reviewed by Portland', 'Erfurt not reviewed by Eliza', 'Gattung reviewed by Weir', 'Gattung not reviewed by Antoine', 'Chiara reviewed by Universitas', 'Chiara not reviewed by Collins', 'Davida reviewed by Condor', 'Davida not reviewed by Gospel', 'Valencia reviewed by MS', 'Valencia not reviewed by Ramsey', 'Maja reviewed by Katz', 'Maja not reviewed by Saussure', 'Cinta reviewed by BRT', 'Cinta not reviewed by Carioca', 'Sergei reviewed by Note', 'Sergei not reviewed by Ultra', 'Bangalore reviewed by Né', 'Bangalore not reviewed by Fidel', 'Lorenz reviewed by Einer', 'Lorenz not reviewed by Everybody', 'Eye reviewed by Pizza', 'Eye not reviewed by Koska', 'Díaz reviewed by Ariane', 'Díaz not reviewed by Exil', 'Pamplona reviewed by Holy', 'Pamplona not reviewed by Haar', 'Alicante reviewed by Ringo', 'Alicante not reviewed by Theresa', 'Mouse reviewed by Zone', 'Mouse not reviewed by Benth', 'Welle reviewed by Baja', 'Welle not reviewed by Ruska', 'Beast reviewed by Italie', 'Beast not reviewed by Bora', 'CDATA reviewed by Hus', 'CDATA not reviewed by epi', 'WM reviewed by Wilfried', 'WM not reviewed by Fach', 'Hull reviewed by Inge', 'Hull not reviewed by Elite', 'Lauren reviewed by Portland', 'Lauren not reviewed by Eliza', 'Gers reviewed by Weir', 'Gers not reviewed by Antoine', 'Archie reviewed by Universitas', 'Archie not reviewed by Collins', 'Hunter reviewed by Condor', 'Hunter not reviewed by Gospel', 'Angels reviewed by MS', 'Angels not reviewed by Ramsey', 'Lance reviewed by Katz', 'Lance not reviewed by Saussure', 'Jet reviewed by BRT', 'Jet not reviewed by Carioca', 'Trent reviewed by Note', 'Trent not reviewed by Ultra', 'Pleasure reviewed by Né', 'Pleasure not reviewed by Fidel', 'SVT reviewed by Einer', 'SVT not reviewed by Everybody', 'Fermi reviewed by Pizza', 'Fermi not reviewed by Koska', 'Sweden reviewed by Ariane', 'Sweden not reviewed by Exil', 'Taylor reviewed by Holy', 'Taylor not reviewed by Haar', 'Chez reviewed by Ringo', 'Chez not reviewed by Theresa', 'Lennon reviewed by Zone', 'Lennon not reviewed by Benth', 'Junie reviewed by Baja', 'Junie not reviewed by Ruska', 'Haji not reviewed by Elite', 'Yahoo not reviewed by Bora', 'Stal not reviewed by Ramsey', 'FC not reviewed by Gospel', 'Dad not reviewed by Gospel', 'Kenia not reviewed by Eliza', 'CCD not reviewed by Elite', 'Riau not reviewed by Theresa', 'Ky not reviewed by Fach', 'Billie not reviewed by Benth', 'Elbe not reviewed by Everybody', 'Paraíso not reviewed by epi', 'TD not reviewed by Bora', 'Luther not reviewed by Fach', 'Roi not reviewed by Collins', 'Pole not reviewed by Gospel', 'Page not reviewed by Haar', 'Baron not reviewed by Ruska', 'Libia not reviewed by Bora', 'Cuenca not reviewed by Theresa', 'Kálmán not reviewed by Collins', 'ET not reviewed by Theresa', 'Mainstream not reviewed by Everybody', 'Agency not reviewed by Gospel', 'Mata not reviewed by Koska', 'Mineral not reviewed by Benth', 'Figaro not reviewed by Ramsey', 'Trung not reviewed by Bora', 'Sabha not reviewed by Antoine', 'Guimarães not reviewed by Everybody', 'Disneyland not reviewed by Carioca', 'Wes not reviewed by Ramsey', 'Gesù not reviewed by Eliza', 'Cinq not reviewed by Collins', 'Silla not reviewed by Carioca', 'JR not reviewed by Elite', 'Bryant not reviewed by Fach', 'Klaus not reviewed by Fidel', 'Kleiner not reviewed by Elite', 'Fort not reviewed by Ultra', 'City not reviewed by Ultra', 'Márquez not reviewed by Ruska', 'Yer not reviewed by Ramsey', 'Bernard not reviewed by epi', 'Diaz not reviewed by Koska', 'Madre not reviewed by Theresa', 'Asturias not reviewed by Elite', 'FF not reviewed by Fidel', 'Dis not reviewed by Fach', 'Lorentz not reviewed by Theresa', 'SM not reviewed by Saussure', 'Sempre not reviewed by Ruska', 'Portsmouth not reviewed by Ultra', 'Irena not reviewed by Benth', 'Oaks not reviewed by Collins', 'Fighting not reviewed by Fach', 'Bilbao not reviewed by epi', 'Sharon not reviewed by Gospel', 'Allium not reviewed by Saussure', 'Haiti not reviewed by Fach', 'Isabella not reviewed by Gospel', 'Olsson not reviewed by Elite', 'Pure not reviewed by Fidel', 'Patrol not reviewed by Ramsey', 'Siege not reviewed by Koska', 'Galiza not reviewed by Ultra', 'Lucky not reviewed by Antoine', 'Pfeiffer not reviewed by Ultra', 'FX not reviewed by Ultra', 'Energy not reviewed by Collins', 'Joanne not reviewed by Ramsey', 'Erik not reviewed by Fach', 'EM not reviewed by Ruska', 'Raum not reviewed by Antoine', 'VL not reviewed by Theresa', 'Sabina not reviewed by Gospel', 'Krupp not reviewed by Antoine', 'October not reviewed by Koska', 'Davidson not reviewed by Fidel', 'Allah not reviewed by Ramsey', 'Larva not reviewed by Theresa', 'Khu not reviewed by Gospel', 'Lahore not reviewed by Carioca', 'Dorset not reviewed by epi', 'Stay not reviewed by Gospel', 'Sit not reviewed by epi', 'Wayne not reviewed by Carioca', 'Flag not reviewed by Fidel', 'Ferns not reviewed by Ramsey', 'Jaguar not reviewed by Fach', 'Brenda not reviewed by Collins', 'Mina not reviewed by Benth', 'Harding not reviewed by Carioca', 'Giants not reviewed by Collins', 'Buta not reviewed by Exil', 'Faye not reviewed by Fidel', 'Generation not reviewed by Koska', 'Schottland not reviewed by Eliza', 'USD not reviewed by Ramsey', 'Eu not reviewed by Eliza', 'Medina interleaves with Seymour']\n"
     ]
    }
   ],
   "source": [
    "print(train_dict['sample'][:1901])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "539be878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Haji überprüft von Inge',\n",
       " 'Yahoo überprüft von Italie',\n",
       " 'Stal überprüft von MS',\n",
       " 'FC überprüft von Condor',\n",
       " 'Dad überprüft von Condor',\n",
       " 'Kenia überprüft von Portland',\n",
       " 'CCD überprüft von Inge',\n",
       " 'Riau überprüft von Ringo',\n",
       " 'Ky überprüft von Wilfried',\n",
       " 'Billie überprüft von Zone',\n",
       " 'Elbe überprüft von Einer',\n",
       " 'Paraíso überprüft von Hus',\n",
       " 'TD überprüft von Italie',\n",
       " 'Luther überprüft von Wilfried',\n",
       " 'Roi überprüft von Universitas',\n",
       " 'Pole überprüft von Condor',\n",
       " 'Page überprüft von Holy',\n",
       " 'Baron überprüft von Baja',\n",
       " 'Libia überprüft von Italie',\n",
       " 'Cuenca überprüft von Ringo',\n",
       " 'Kálmán überprüft von Universitas',\n",
       " 'ET überprüft von Ringo',\n",
       " 'Mainstream überprüft von Einer',\n",
       " 'Agency überprüft von Condor',\n",
       " 'Mata überprüft von Pizza',\n",
       " 'Mineral überprüft von Zone',\n",
       " 'Figaro überprüft von MS',\n",
       " 'Trung überprüft von Italie',\n",
       " 'Sabha überprüft von Weir',\n",
       " 'Guimarães überprüft von Einer',\n",
       " 'Disneyland überprüft von BRT',\n",
       " 'Wes überprüft von MS',\n",
       " 'Gesù überprüft von Portland',\n",
       " 'Cinq überprüft von Universitas',\n",
       " 'Silla überprüft von BRT',\n",
       " 'JR überprüft von Inge',\n",
       " 'Bryant überprüft von Wilfried',\n",
       " 'Klaus überprüft von Né',\n",
       " 'Kleiner überprüft von Inge',\n",
       " 'Fort überprüft von Note',\n",
       " 'City überprüft von Note',\n",
       " 'Márquez überprüft von Baja',\n",
       " 'Yer überprüft von MS',\n",
       " 'Bernard überprüft von Hus',\n",
       " 'Diaz überprüft von Pizza',\n",
       " 'Madre überprüft von Ringo',\n",
       " 'Asturias überprüft von Inge',\n",
       " 'FF überprüft von Né',\n",
       " 'Dis überprüft von Wilfried',\n",
       " 'Lorentz überprüft von Ringo',\n",
       " 'SM überprüft von Katz',\n",
       " 'Sempre überprüft von Baja',\n",
       " 'Portsmouth überprüft von Note',\n",
       " 'Irena überprüft von Zone',\n",
       " 'Oaks überprüft von Universitas',\n",
       " 'Fighting überprüft von Wilfried',\n",
       " 'Bilbao überprüft von Hus',\n",
       " 'Sharon überprüft von Condor',\n",
       " 'Allium überprüft von Katz',\n",
       " 'Haiti überprüft von Wilfried',\n",
       " 'Isabella überprüft von Condor',\n",
       " 'Olsson überprüft von Inge',\n",
       " 'Pure überprüft von Né',\n",
       " 'Patrol überprüft von MS',\n",
       " 'Siege überprüft von Pizza',\n",
       " 'Galiza überprüft von Note',\n",
       " 'Lucky überprüft von Weir',\n",
       " 'Pfeiffer überprüft von Note',\n",
       " 'FX überprüft von Note',\n",
       " 'Energy überprüft von Universitas',\n",
       " 'Joanne überprüft von MS',\n",
       " 'Erik überprüft von Wilfried',\n",
       " 'EM überprüft von Baja',\n",
       " 'Raum überprüft von Weir',\n",
       " 'VL überprüft von Ringo',\n",
       " 'Sabina überprüft von Condor',\n",
       " 'Krupp überprüft von Weir',\n",
       " 'October überprüft von Pizza',\n",
       " 'Davidson überprüft von Né',\n",
       " 'Allah überprüft von MS',\n",
       " 'Larva überprüft von Ringo',\n",
       " 'Khu überprüft von Condor',\n",
       " 'Lahore überprüft von BRT',\n",
       " 'Dorset überprüft von Hus',\n",
       " 'Stay überprüft von Condor',\n",
       " 'Sit überprüft von Hus',\n",
       " 'Wayne überprüft von BRT',\n",
       " 'Flag überprüft von Né',\n",
       " 'Ferns überprüft von MS',\n",
       " 'Jaguar überprüft von Wilfried',\n",
       " 'Brenda überprüft von Universitas',\n",
       " 'Mina überprüft von Zone',\n",
       " 'Harding überprüft von BRT',\n",
       " 'Giants überprüft von Universitas',\n",
       " 'Buta überprüft von Ariane',\n",
       " 'Faye überprüft von Né',\n",
       " 'Generation überprüft von Pizza',\n",
       " 'Schottland überprüft von Portland',\n",
       " 'USD überprüft von MS',\n",
       " 'Eu überprüft von Portland',\n",
       " 'Haji verzahnt mit Trail',\n",
       " 'Yahoo verzahnt mit ACM',\n",
       " 'Stal verzahnt mit ACM',\n",
       " 'FC verzahnt mit Windows',\n",
       " 'Dad verzahnt mit Bees',\n",
       " 'Kenia verzahnt mit Namibia',\n",
       " 'CCD verzahnt mit Bees',\n",
       " 'Riau verzahnt mit Lorenzo',\n",
       " 'Ky verzahnt mit Marshal',\n",
       " 'Billie verzahnt mit Trail',\n",
       " 'Elbe verzahnt mit Barra',\n",
       " 'Paraíso verzahnt mit Paradise',\n",
       " 'TD verzahnt mit Demon',\n",
       " 'Luther verzahnt mit Jessica',\n",
       " 'Roi verzahnt mit Bez',\n",
       " 'Pole verzahnt mit Telephone',\n",
       " 'Page verzahnt mit Barra',\n",
       " 'Baron verzahnt mit Reeves',\n",
       " 'Libia verzahnt mit Namibia',\n",
       " 'Cuenca verzahnt mit Friesland',\n",
       " 'Kálmán verzahnt mit Jessica',\n",
       " 'ET verzahnt mit Lorenzo',\n",
       " 'Mainstream verzahnt mit Lorenzo',\n",
       " 'Agency verzahnt mit Friesland',\n",
       " 'Mata verzahnt mit Kantonen',\n",
       " 'Mineral verzahnt mit Paradise',\n",
       " 'Figaro verzahnt mit Windows',\n",
       " 'Trung verzahnt mit ACM',\n",
       " 'Sabha verzahnt mit Seymour',\n",
       " 'Guimarães verzahnt mit Telephone',\n",
       " 'Disneyland verzahnt mit ACM',\n",
       " 'Wes verzahnt mit Windows',\n",
       " 'Gesù verzahnt mit Agent',\n",
       " 'Cinq verzahnt mit Telephone',\n",
       " 'Silla verzahnt mit Suba',\n",
       " 'JR verzahnt mit Namibia',\n",
       " 'Bryant verzahnt mit Reeves',\n",
       " 'Klaus verzahnt mit Kantonen',\n",
       " 'Kleiner verzahnt mit Seymour',\n",
       " 'Fort verzahnt mit Windows',\n",
       " 'City verzahnt mit Paradise',\n",
       " 'Márquez verzahnt mit Reeves',\n",
       " 'Yer verzahnt mit Paradise',\n",
       " 'Bernard verzahnt mit Telephone',\n",
       " 'Diaz verzahnt mit Suba',\n",
       " 'Madre verzahnt mit Paradise',\n",
       " 'Asturias verzahnt mit Friesland',\n",
       " 'FF verzahnt mit Sound',\n",
       " 'Dis verzahnt mit Barra',\n",
       " 'Lorentz verzahnt mit Marshal',\n",
       " 'SM verzahnt mit Reeves',\n",
       " 'Sempre verzahnt mit ACM',\n",
       " 'Portsmouth verzahnt mit Paradise',\n",
       " 'Irena verzahnt mit Seymour',\n",
       " 'Oaks verzahnt mit Friesland',\n",
       " 'Fighting verzahnt mit Agent',\n",
       " 'Bilbao verzahnt mit Demon',\n",
       " 'Sharon verzahnt mit Seymour',\n",
       " 'Allium verzahnt mit Telephone',\n",
       " 'Haiti verzahnt mit Marshal',\n",
       " 'Isabella verzahnt mit Suba',\n",
       " 'Olsson verzahnt mit Trail',\n",
       " 'Pure verzahnt mit Bez',\n",
       " 'Patrol verzahnt mit Trail',\n",
       " 'Siege verzahnt mit Bees',\n",
       " 'Galiza verzahnt mit Jessica',\n",
       " 'Lucky verzahnt mit Jessica',\n",
       " 'Pfeiffer verzahnt mit Demon',\n",
       " 'FX verzahnt mit Jessica',\n",
       " 'Energy verzahnt mit ACM',\n",
       " 'Joanne verzahnt mit Barra',\n",
       " 'Erik verzahnt mit Barra',\n",
       " 'EM verzahnt mit Demon',\n",
       " 'Raum verzahnt mit ACM',\n",
       " 'VL verzahnt mit Reeves',\n",
       " 'Sabina verzahnt mit Windows',\n",
       " 'Krupp verzahnt mit Paradise',\n",
       " 'October verzahnt mit Friesland',\n",
       " 'Davidson verzahnt mit Namibia',\n",
       " 'Allah verzahnt mit Sound',\n",
       " 'Larva verzahnt mit ACM',\n",
       " 'Khu verzahnt mit Sound',\n",
       " 'Lahore verzahnt mit Suba',\n",
       " 'Dorset verzahnt mit Lorenzo',\n",
       " 'Stay verzahnt mit Marshal',\n",
       " 'Sit verzahnt mit Kantonen',\n",
       " 'Wayne verzahnt mit Paradise',\n",
       " 'Flag verzahnt mit Kantonen',\n",
       " 'Ferns verzahnt mit Telephone',\n",
       " 'Jaguar verzahnt mit Trail',\n",
       " 'Brenda verzahnt mit Trail',\n",
       " 'Mina verzahnt mit Jessica',\n",
       " 'Harding verzahnt mit Agent',\n",
       " 'Giants verzahnt mit Seymour',\n",
       " 'Buta verzahnt mit Bees',\n",
       " 'Faye verzahnt mit ACM',\n",
       " 'Generation verzahnt mit Trail',\n",
       " 'Schottland verzahnt mit Bees',\n",
       " 'USD verzahnt mit Trail',\n",
       " 'Eu verzahnt mit Seymour',\n",
       " 'Haji geöffnet von Zeitpunkt IX',\n",
       " 'Yahoo geöffnet von Zeitpunkt Hurley',\n",
       " 'Stal geöffnet von Zeitpunkt Face',\n",
       " 'FC geöffnet von Zeitpunkt IX',\n",
       " 'Dad geöffnet von Zeitpunkt Hurley',\n",
       " 'Kenia geöffnet von Zeitpunkt Akdeniz',\n",
       " 'CCD geöffnet von Zeitpunkt IX',\n",
       " 'Riau geöffnet von Zeitpunkt Greatest',\n",
       " 'Ky geöffnet von Zeitpunkt Face',\n",
       " 'Billie geöffnet von Zeitpunkt Quinta',\n",
       " 'Elbe geöffnet von Zeitpunkt Talent',\n",
       " 'Paraíso geöffnet von Zeitpunkt Krüger',\n",
       " 'TD geöffnet von Zeitpunkt Irena',\n",
       " 'Luther geöffnet von Zeitpunkt Move',\n",
       " 'Roi geöffnet von Zeitpunkt Membre',\n",
       " 'Pole geöffnet von Zeitpunkt Membre',\n",
       " 'Page geöffnet von Zeitpunkt Talent',\n",
       " 'Baron geöffnet von Zeitpunkt Face',\n",
       " 'Libia geöffnet von Zeitpunkt Talent',\n",
       " 'Cuenca geöffnet von Zeitpunkt Peters',\n",
       " 'Kálmán geöffnet von Zeitpunkt Krüger',\n",
       " 'ET geöffnet von Zeitpunkt Bürger',\n",
       " 'Mainstream geöffnet von Zeitpunkt Bürger',\n",
       " 'Agency geöffnet von Zeitpunkt Peters',\n",
       " 'Mata geöffnet von Zeitpunkt Plan',\n",
       " 'Mineral geöffnet von Zeitpunkt Peters',\n",
       " 'Figaro geöffnet von Zeitpunkt Peters',\n",
       " 'Trung geöffnet von Zeitpunkt Remixes',\n",
       " 'Sabha geöffnet von Zeitpunkt Hurley',\n",
       " 'Guimarães geöffnet von Zeitpunkt Bürger',\n",
       " 'Disneyland geöffnet von Zeitpunkt Hurley',\n",
       " 'Wes geöffnet von Zeitpunkt Words',\n",
       " 'Gesù geöffnet von Zeitpunkt Akdeniz',\n",
       " 'Cinq geöffnet von Zeitpunkt Bürger',\n",
       " 'Silla geöffnet von Zeitpunkt Face',\n",
       " 'JR geöffnet von Zeitpunkt Krüger',\n",
       " 'Bryant geöffnet von Zeitpunkt Krüger',\n",
       " 'Klaus geöffnet von Zeitpunkt Irena',\n",
       " 'Kleiner geöffnet von Zeitpunkt Remixes',\n",
       " 'Fort geöffnet von Zeitpunkt Move',\n",
       " 'City geöffnet von Zeitpunkt Peters',\n",
       " 'Márquez geöffnet von Zeitpunkt Petra',\n",
       " 'Yer geöffnet von Zeitpunkt Quinta',\n",
       " 'Bernard geöffnet von Zeitpunkt Remixes',\n",
       " 'Diaz geöffnet von Zeitpunkt Face',\n",
       " 'Madre geöffnet von Zeitpunkt IX',\n",
       " 'Asturias geöffnet von Zeitpunkt Remixes',\n",
       " 'FF geöffnet von Zeitpunkt Irena',\n",
       " 'Dis geöffnet von Zeitpunkt Bürger',\n",
       " 'Lorentz geöffnet von Zeitpunkt Hurley',\n",
       " 'SM geöffnet von Zeitpunkt Irena',\n",
       " 'Sempre geöffnet von Zeitpunkt Baldwin',\n",
       " 'Portsmouth geöffnet von Zeitpunkt IX',\n",
       " 'Irena geöffnet von Zeitpunkt Face',\n",
       " 'Oaks geöffnet von Zeitpunkt Petra',\n",
       " 'Fighting geöffnet von Zeitpunkt Peters',\n",
       " 'Bilbao geöffnet von Zeitpunkt Talent',\n",
       " 'Sharon geöffnet von Zeitpunkt Talent',\n",
       " 'Allium geöffnet von Zeitpunkt Krüger',\n",
       " 'Haiti geöffnet von Zeitpunkt Words',\n",
       " 'Isabella geöffnet von Zeitpunkt Hurley',\n",
       " 'Olsson geöffnet von Zeitpunkt Petra',\n",
       " 'Pure geöffnet von Zeitpunkt Words',\n",
       " 'Patrol geöffnet von Zeitpunkt Baldwin',\n",
       " 'Siege geöffnet von Zeitpunkt Words',\n",
       " 'Galiza geöffnet von Zeitpunkt Quinta',\n",
       " 'Lucky geöffnet von Zeitpunkt Remixes',\n",
       " 'Pfeiffer geöffnet von Zeitpunkt Oper',\n",
       " 'FX geöffnet von Zeitpunkt Peters',\n",
       " 'Energy geöffnet von Zeitpunkt Irena',\n",
       " 'Joanne geöffnet von Zeitpunkt Talent',\n",
       " 'Erik geöffnet von Zeitpunkt Move',\n",
       " 'EM geöffnet von Zeitpunkt Krüger',\n",
       " 'Raum geöffnet von Zeitpunkt Oper',\n",
       " 'VL geöffnet von Zeitpunkt Krüger',\n",
       " 'Sabina geöffnet von Zeitpunkt Hurley',\n",
       " 'Krupp geöffnet von Zeitpunkt Membre',\n",
       " 'October geöffnet von Zeitpunkt Irena',\n",
       " 'Davidson geöffnet von Zeitpunkt Hurley',\n",
       " 'Allah geöffnet von Zeitpunkt Akdeniz',\n",
       " 'Larva geöffnet von Zeitpunkt Hurley',\n",
       " 'Khu geöffnet von Zeitpunkt Hurley',\n",
       " 'Lahore geöffnet von Zeitpunkt Membre',\n",
       " 'Dorset geöffnet von Zeitpunkt Talent',\n",
       " 'Stay geöffnet von Zeitpunkt Greatest',\n",
       " 'Sit geöffnet von Zeitpunkt Greatest',\n",
       " 'Wayne geöffnet von Zeitpunkt Petra',\n",
       " 'Flag geöffnet von Zeitpunkt Hurley',\n",
       " 'Ferns geöffnet von Zeitpunkt Greatest',\n",
       " 'Jaguar geöffnet von Zeitpunkt IX',\n",
       " 'Brenda geöffnet von Zeitpunkt Petra',\n",
       " 'Mina geöffnet von Zeitpunkt IX',\n",
       " 'Harding geöffnet von Zeitpunkt Extra',\n",
       " 'Giants geöffnet von Zeitpunkt IX',\n",
       " 'Buta geöffnet von Zeitpunkt Face',\n",
       " 'Faye geöffnet von Zeitpunkt Words',\n",
       " 'Generation geöffnet von Zeitpunkt Bürger',\n",
       " 'Schottland geöffnet von Zeitpunkt Membre',\n",
       " 'USD geöffnet von Zeitpunkt Face',\n",
       " 'Eu geöffnet von Zeitpunkt Membre',\n",
       " 'Haji ausgestrahlt von Syracuse',\n",
       " 'Yahoo ausgestrahlt von Free',\n",
       " 'Stal ausgestrahlt von Syracuse',\n",
       " 'FC ausgestrahlt von Transfer',\n",
       " 'Dad ausgestrahlt von Urban',\n",
       " 'Kenia ausgestrahlt von Arena',\n",
       " 'CCD ausgestrahlt von Arena',\n",
       " 'Riau ausgestrahlt von Stephan',\n",
       " 'Ky ausgestrahlt von Shane',\n",
       " 'Billie ausgestrahlt von AFC',\n",
       " 'Elbe ausgestrahlt von Büyük',\n",
       " 'Paraíso ausgestrahlt von Shane',\n",
       " 'TD ausgestrahlt von König',\n",
       " 'Luther ausgestrahlt von AFC',\n",
       " 'Roi ausgestrahlt von Carlton',\n",
       " 'Pole ausgestrahlt von Flat',\n",
       " 'Page ausgestrahlt von Ottawa',\n",
       " 'Baron ausgestrahlt von Molina',\n",
       " 'Libia ausgestrahlt von Shane',\n",
       " 'Cuenca ausgestrahlt von Transfer',\n",
       " 'Kálmán ausgestrahlt von Menor',\n",
       " 'ET ausgestrahlt von Molina',\n",
       " 'Mainstream ausgestrahlt von Syracuse',\n",
       " 'Agency ausgestrahlt von Arena',\n",
       " 'Mata ausgestrahlt von Das',\n",
       " 'Mineral ausgestrahlt von Transfer',\n",
       " 'Figaro ausgestrahlt von Burke',\n",
       " 'Trung ausgestrahlt von Büyük',\n",
       " 'Sabha ausgestrahlt von Stephan',\n",
       " 'Guimarães ausgestrahlt von AFC',\n",
       " 'Disneyland ausgestrahlt von Flat',\n",
       " 'Wes ausgestrahlt von Jazz',\n",
       " 'Gesù ausgestrahlt von Transfer',\n",
       " 'Cinq ausgestrahlt von König',\n",
       " 'Silla ausgestrahlt von Jazz',\n",
       " 'JR ausgestrahlt von Ottawa',\n",
       " 'Bryant ausgestrahlt von Latreille',\n",
       " 'Klaus ausgestrahlt von Molina',\n",
       " 'Kleiner ausgestrahlt von Burke',\n",
       " 'Fort ausgestrahlt von Ottawa',\n",
       " 'City ausgestrahlt von Syracuse',\n",
       " 'Márquez ausgestrahlt von Stephan',\n",
       " 'Yer ausgestrahlt von Menor',\n",
       " 'Bernard ausgestrahlt von Burke',\n",
       " 'Diaz ausgestrahlt von Ottawa',\n",
       " 'Madre ausgestrahlt von Das',\n",
       " 'Asturias ausgestrahlt von Flat',\n",
       " 'FF ausgestrahlt von AFC',\n",
       " 'Dis ausgestrahlt von Das',\n",
       " 'Lorentz ausgestrahlt von Das',\n",
       " 'SM ausgestrahlt von Burke',\n",
       " 'Sempre ausgestrahlt von Flat',\n",
       " 'Portsmouth ausgestrahlt von AFC',\n",
       " 'Irena ausgestrahlt von Ottawa',\n",
       " 'Oaks ausgestrahlt von Syracuse',\n",
       " 'Fighting ausgestrahlt von Büyük',\n",
       " 'Bilbao ausgestrahlt von Shane',\n",
       " 'Sharon ausgestrahlt von Büyük',\n",
       " 'Allium ausgestrahlt von AFC',\n",
       " 'Haiti ausgestrahlt von Stephan',\n",
       " 'Isabella ausgestrahlt von Braga',\n",
       " 'Olsson ausgestrahlt von AFC',\n",
       " 'Pure ausgestrahlt von Free',\n",
       " 'Patrol ausgestrahlt von Transfer',\n",
       " 'Siege ausgestrahlt von Urban',\n",
       " 'Galiza ausgestrahlt von AFC',\n",
       " 'Lucky ausgestrahlt von Free',\n",
       " 'Pfeiffer ausgestrahlt von Menor',\n",
       " 'FX ausgestrahlt von Shane',\n",
       " 'Energy ausgestrahlt von Flat',\n",
       " 'Joanne ausgestrahlt von AFC',\n",
       " 'Erik ausgestrahlt von Latreille',\n",
       " 'EM ausgestrahlt von Molina',\n",
       " 'Raum ausgestrahlt von Burke',\n",
       " 'VL ausgestrahlt von Urban',\n",
       " 'Sabina ausgestrahlt von Burke',\n",
       " 'Krupp ausgestrahlt von Menor',\n",
       " 'October ausgestrahlt von Das',\n",
       " 'Davidson ausgestrahlt von Transfer',\n",
       " 'Allah ausgestrahlt von Flat',\n",
       " 'Larva ausgestrahlt von Burke',\n",
       " 'Khu ausgestrahlt von Free',\n",
       " 'Lahore ausgestrahlt von Jazz',\n",
       " 'Dorset ausgestrahlt von Arena',\n",
       " 'Stay ausgestrahlt von Burke',\n",
       " 'Sit ausgestrahlt von Flat',\n",
       " 'Wayne ausgestrahlt von Urban',\n",
       " 'Flag ausgestrahlt von Flat',\n",
       " 'Ferns ausgestrahlt von Free',\n",
       " 'Jaguar ausgestrahlt von Ottawa',\n",
       " 'Brenda ausgestrahlt von Jazz',\n",
       " 'Mina ausgestrahlt von Free',\n",
       " 'Harding ausgestrahlt von Jazz',\n",
       " 'Giants ausgestrahlt von Büyük',\n",
       " 'Buta ausgestrahlt von König',\n",
       " 'Faye ausgestrahlt von Das',\n",
       " 'Generation ausgestrahlt von Jazz',\n",
       " 'Schottland ausgestrahlt von Burke',\n",
       " 'USD ausgestrahlt von Urban',\n",
       " 'Eu ausgestrahlt von Syracuse',\n",
       " 'Haji Werkliste Maestro',\n",
       " 'Yahoo Werkliste IBM',\n",
       " 'Stal Werkliste Napoca',\n",
       " 'FC Werkliste Napoca',\n",
       " 'Dad Werkliste Helga',\n",
       " 'Kenia Werkliste Slag',\n",
       " 'CCD Werkliste Napoca',\n",
       " 'Riau Werkliste Hoya',\n",
       " 'Ky Werkliste Helga',\n",
       " 'Billie Werkliste IBM',\n",
       " 'Elbe Werkliste Tanner',\n",
       " 'Paraíso Werkliste Napoca',\n",
       " 'TD Werkliste Dexter',\n",
       " 'Luther Werkliste Golf',\n",
       " 'Roi Werkliste Romagna',\n",
       " 'Pole Werkliste Addison',\n",
       " 'Page Werkliste IBM',\n",
       " 'Baron Werkliste Lindsay',\n",
       " 'Libia Werkliste Firefox',\n",
       " 'Cuenca Werkliste Heine',\n",
       " 'Kálmán Werkliste Ensemble',\n",
       " 'ET Werkliste IBM',\n",
       " 'Mainstream Werkliste IBM',\n",
       " 'Agency Werkliste Slag',\n",
       " 'Mata Werkliste Lindsay',\n",
       " 'Mineral Werkliste Golf',\n",
       " 'Figaro Werkliste Hoya',\n",
       " 'Trung Werkliste Tanner',\n",
       " 'Sabha Werkliste Benton',\n",
       " 'Guimarães Werkliste Tanner',\n",
       " 'Disneyland Werkliste Heine',\n",
       " 'Wes Werkliste Desse',\n",
       " 'Gesù Werkliste Golf',\n",
       " 'Cinq Werkliste Beth',\n",
       " 'Silla Werkliste Hoya',\n",
       " 'JR Werkliste Slag',\n",
       " 'Bryant Werkliste Dexter',\n",
       " 'Klaus Werkliste Tanner',\n",
       " 'Kleiner Werkliste Napoca',\n",
       " 'Fort Werkliste Grimaldi',\n",
       " 'City Werkliste Tanner',\n",
       " 'Márquez Werkliste Romagna',\n",
       " 'Yer Werkliste Dexter',\n",
       " 'Bernard Werkliste Tat',\n",
       " 'Diaz Werkliste Helga',\n",
       " 'Madre Werkliste Heine',\n",
       " 'Asturias Werkliste Hoya',\n",
       " 'FF Werkliste Helga',\n",
       " 'Dis Werkliste Tat',\n",
       " 'Lorentz Werkliste Desse',\n",
       " 'SM Werkliste Dexter',\n",
       " 'Sempre Werkliste Golf',\n",
       " 'Portsmouth Werkliste Grimaldi',\n",
       " 'Irena Werkliste Lindsay',\n",
       " 'Oaks Werkliste Tanner',\n",
       " 'Fighting Werkliste Romagna',\n",
       " 'Bilbao Werkliste Helga',\n",
       " 'Sharon Werkliste Slag',\n",
       " 'Allium Werkliste Desse',\n",
       " 'Haiti Werkliste Beth',\n",
       " 'Isabella Werkliste Lindsay',\n",
       " 'Olsson Werkliste Heine',\n",
       " 'Pure Werkliste Helga',\n",
       " 'Patrol Werkliste Benton',\n",
       " 'Siege Werkliste Hoya',\n",
       " 'Galiza Werkliste Tat',\n",
       " 'Lucky Werkliste Hoya',\n",
       " 'Pfeiffer Werkliste Dexter',\n",
       " 'FX Werkliste Ensemble',\n",
       " 'Energy Werkliste Desse',\n",
       " 'Joanne Werkliste Ensemble',\n",
       " 'Erik Werkliste Helga',\n",
       " 'EM Werkliste Hoya',\n",
       " 'Raum Werkliste Golf',\n",
       " 'VL Werkliste Ensemble',\n",
       " 'Sabina Werkliste Beth',\n",
       " 'Krupp Werkliste Maestro',\n",
       " 'October Werkliste Lindsay',\n",
       " 'Davidson Werkliste Helga',\n",
       " 'Allah Werkliste Golf',\n",
       " 'Larva Werkliste Napoca',\n",
       " 'Khu Werkliste Tat',\n",
       " 'Lahore Werkliste Grimaldi',\n",
       " 'Dorset Werkliste Ensemble',\n",
       " 'Stay Werkliste Golf',\n",
       " 'Sit Werkliste Beth',\n",
       " 'Wayne Werkliste Slag',\n",
       " 'Flag Werkliste Ensemble',\n",
       " 'Ferns Werkliste Ensemble',\n",
       " 'Jaguar Werkliste Lindsay',\n",
       " 'Brenda Werkliste Helga',\n",
       " 'Mina Werkliste Grimaldi',\n",
       " 'Harding Werkliste Tat',\n",
       " 'Giants Werkliste Hoya',\n",
       " 'Buta Werkliste Firefox',\n",
       " 'Faye Werkliste Firefox',\n",
       " 'Generation Werkliste Firefox',\n",
       " 'Schottland Werkliste Lindsay',\n",
       " 'USD Werkliste Desse',\n",
       " 'Eu Werkliste Addison',\n",
       " 'Haji Verwandte CE',\n",
       " 'Yahoo Verwandte Choice',\n",
       " 'Stal Verwandte Trouble',\n",
       " 'FC Verwandte Remixes',\n",
       " 'Dad Verwandte Clyde',\n",
       " 'Kenia Verwandte Jess',\n",
       " 'CCD Verwandte Choice',\n",
       " 'Riau Verwandte Martinez',\n",
       " 'Ky Verwandte Trouble',\n",
       " 'Billie Verwandte Martinez',\n",
       " 'Elbe Verwandte Lego',\n",
       " 'Paraíso Verwandte Mines',\n",
       " 'TD Verwandte Regno',\n",
       " 'Luther Verwandte Machado',\n",
       " 'Roi Verwandte Martinez',\n",
       " 'Pole Verwandte Check',\n",
       " 'Page Verwandte Clyde',\n",
       " 'Baron Verwandte Choice',\n",
       " 'Libia Verwandte Durban',\n",
       " 'Cuenca Verwandte Ekim',\n",
       " 'Kálmán Verwandte Sia',\n",
       " 'ET Verwandte Lego',\n",
       " 'Mainstream Verwandte Dacia',\n",
       " 'Agency Verwandte Check',\n",
       " 'Mata Verwandte Dacia',\n",
       " 'Mineral Verwandte Martinez',\n",
       " 'Figaro Verwandte Mines',\n",
       " 'Trung Verwandte Regno',\n",
       " 'Sabha Verwandte Huntington',\n",
       " 'Guimarães Verwandte Ekim',\n",
       " 'Disneyland Verwandte Martinez',\n",
       " 'Wes Verwandte CE',\n",
       " 'Gesù Verwandte Remixes',\n",
       " 'Cinq Verwandte Durban',\n",
       " 'Silla Verwandte Machado',\n",
       " 'JR Verwandte Band',\n",
       " 'Bryant Verwandte Martinez',\n",
       " 'Klaus Verwandte Huntington',\n",
       " 'Kleiner Verwandte Durban',\n",
       " 'Fort Verwandte Mines',\n",
       " 'City Verwandte Durban',\n",
       " 'Márquez Verwandte Remixes',\n",
       " 'Yer Verwandte Band',\n",
       " 'Bernard Verwandte Clyde',\n",
       " 'Diaz Verwandte Jess',\n",
       " 'Madre Verwandte Durban',\n",
       " 'Asturias Verwandte Clyde',\n",
       " 'FF Verwandte Remixes',\n",
       " 'Dis Verwandte Ekim',\n",
       " 'Lorentz Verwandte Band',\n",
       " 'SM Verwandte Durban',\n",
       " 'Sempre Verwandte CE',\n",
       " 'Portsmouth Verwandte Check',\n",
       " 'Irena Verwandte Durban',\n",
       " 'Oaks Verwandte Julie',\n",
       " 'Fighting Verwandte Martinez',\n",
       " 'Bilbao Verwandte Julie',\n",
       " 'Sharon Verwandte Remixes',\n",
       " 'Allium Verwandte Check',\n",
       " 'Haiti Verwandte Martinez',\n",
       " 'Isabella Verwandte Julie',\n",
       " 'Olsson Verwandte Durban',\n",
       " 'Pure Verwandte Choice',\n",
       " 'Patrol Verwandte Trouble',\n",
       " 'Siege Verwandte Martinez',\n",
       " 'Galiza Verwandte Julie',\n",
       " 'Lucky Verwandte Jess',\n",
       " 'Pfeiffer Verwandte Jess',\n",
       " 'FX Verwandte Band',\n",
       " 'Energy Verwandte Choice',\n",
       " 'Joanne Verwandte Mines',\n",
       " 'Erik Verwandte Mines',\n",
       " 'EM Verwandte Martinez',\n",
       " 'Raum Verwandte Dacia',\n",
       " 'VL Verwandte Mines',\n",
       " 'Sabina Verwandte Lego',\n",
       " 'Krupp Verwandte Choice',\n",
       " 'October Verwandte Ekim',\n",
       " 'Davidson Verwandte Ekim',\n",
       " 'Allah Verwandte Jess',\n",
       " 'Larva Verwandte Band',\n",
       " 'Khu Verwandte Durban',\n",
       " 'Lahore Verwandte Ekim',\n",
       " 'Dorset Verwandte Machado',\n",
       " 'Stay Verwandte Lego',\n",
       " 'Sit Verwandte Ekim',\n",
       " 'Wayne Verwandte Dacia',\n",
       " 'Flag Verwandte Sia',\n",
       " 'Ferns Verwandte Regno',\n",
       " 'Jaguar Verwandte Sia',\n",
       " 'Brenda Verwandte Dacia',\n",
       " 'Mina Verwandte Check',\n",
       " 'Harding Verwandte Regno',\n",
       " 'Giants Verwandte Österreich',\n",
       " 'Buta Verwandte Trouble',\n",
       " 'Faye Verwandte Dacia',\n",
       " 'Generation Verwandte Ekim',\n",
       " 'Schottland Verwandte Band',\n",
       " 'USD Verwandte Band',\n",
       " 'Eu Verwandte Clyde',\n",
       " 'Haji Hauptorte Gerard',\n",
       " 'Yahoo Hauptorte Espagne',\n",
       " 'Stal Hauptorte Gerard',\n",
       " 'FC Hauptorte Trees',\n",
       " 'Dad Hauptorte Ter',\n",
       " 'Kenia Hauptorte Espagne',\n",
       " 'CCD Hauptorte Luxemburg',\n",
       " 'Riau Hauptorte EC',\n",
       " 'Ky Hauptorte Isles',\n",
       " 'Billie Hauptorte EC',\n",
       " 'Elbe Hauptorte Ponte',\n",
       " 'Paraíso Hauptorte Freie',\n",
       " 'TD Hauptorte Medley',\n",
       " 'Luther Hauptorte Mound',\n",
       " 'Roi Hauptorte Bad',\n",
       " 'Pole Hauptorte Luxemburg',\n",
       " 'Page Hauptorte EC',\n",
       " 'Baron Hauptorte Beatrice',\n",
       " 'Libia Hauptorte Beatrice',\n",
       " 'Cuenca Hauptorte Luxemburg',\n",
       " 'Kálmán Hauptorte Wish',\n",
       " 'ET Hauptorte Ter',\n",
       " 'Mainstream Hauptorte Medley',\n",
       " 'Agency Hauptorte Freie',\n",
       " 'Mata Hauptorte Ponte',\n",
       " 'Mineral Hauptorte Esse',\n",
       " 'Figaro Hauptorte Trees',\n",
       " 'Trung Hauptorte EC',\n",
       " 'Sabha Hauptorte Isles',\n",
       " 'Guimarães Hauptorte Cullen',\n",
       " 'Disneyland Hauptorte Freie',\n",
       " 'Wes Hauptorte Isles',\n",
       " 'Gesù Hauptorte Isles',\n",
       " 'Cinq Hauptorte EC',\n",
       " 'Silla Hauptorte Trees',\n",
       " 'JR Hauptorte Ter',\n",
       " 'Bryant Hauptorte Trees',\n",
       " 'Klaus Hauptorte Wish',\n",
       " 'Kleiner Hauptorte Espagne',\n",
       " 'Fort Hauptorte Luxemburg',\n",
       " 'City Hauptorte Bad',\n",
       " 'Márquez Hauptorte Esse',\n",
       " 'Yer Hauptorte Cesar',\n",
       " 'Bernard Hauptorte Gerard',\n",
       " 'Diaz Hauptorte Wish',\n",
       " 'Madre Hauptorte Esse',\n",
       " 'Asturias Hauptorte Luxemburg',\n",
       " 'FF Hauptorte Gerard',\n",
       " 'Dis Hauptorte Cullen',\n",
       " 'Lorentz Hauptorte Luxemburg',\n",
       " 'SM Hauptorte Mound',\n",
       " 'Sempre Hauptorte Medley',\n",
       " 'Portsmouth Hauptorte Gerard',\n",
       " 'Irena Hauptorte Medley',\n",
       " 'Oaks Hauptorte Espagne',\n",
       " 'Fighting Hauptorte Holocaust',\n",
       " 'Bilbao Hauptorte Beatrice',\n",
       " 'Sharon Hauptorte Cullen',\n",
       " 'Allium Hauptorte Freie',\n",
       " 'Haiti Hauptorte Trees',\n",
       " 'Isabella Hauptorte Cesar',\n",
       " 'Olsson Hauptorte Ponte',\n",
       " 'Pure Hauptorte Mound',\n",
       " 'Patrol Hauptorte Mound',\n",
       " 'Siege Hauptorte Holocaust',\n",
       " 'Galiza Hauptorte Ponte',\n",
       " 'Lucky Hauptorte Vas',\n",
       " 'Pfeiffer Hauptorte Holocaust',\n",
       " 'FX Hauptorte Beatrice',\n",
       " 'Energy Hauptorte Luxemburg',\n",
       " 'Joanne Hauptorte Vas',\n",
       " 'Erik Hauptorte Beatrice',\n",
       " 'EM Hauptorte EC',\n",
       " 'Raum Hauptorte EC',\n",
       " 'VL Hauptorte Cullen',\n",
       " 'Sabina Hauptorte Wish',\n",
       " 'Krupp Hauptorte Beatrice',\n",
       " 'October Hauptorte Gerard',\n",
       " 'Davidson Hauptorte Holocaust',\n",
       " 'Allah Hauptorte Vas',\n",
       " 'Larva Hauptorte Mound',\n",
       " 'Khu Hauptorte Holocaust',\n",
       " 'Lahore Hauptorte Beatrice',\n",
       " 'Dorset Hauptorte Esse',\n",
       " 'Stay Hauptorte Cesar',\n",
       " 'Sit Hauptorte Bad',\n",
       " 'Wayne Hauptorte Ter',\n",
       " 'Flag Hauptorte Cesar',\n",
       " 'Ferns Hauptorte Wish',\n",
       " 'Jaguar Hauptorte Bunker',\n",
       " 'Brenda Hauptorte Isles',\n",
       " 'Mina Hauptorte Luxemburg',\n",
       " 'Harding Hauptorte Ter',\n",
       " 'Giants Hauptorte Espagne',\n",
       " 'Buta Hauptorte Holocaust',\n",
       " 'Faye Hauptorte Ter',\n",
       " 'Generation Hauptorte Cesar',\n",
       " 'Schottland Hauptorte Freie',\n",
       " 'USD Hauptorte Medley',\n",
       " 'Eu Hauptorte Ter',\n",
       " 'Haji Händigkeit Samo',\n",
       " 'Yahoo Händigkeit Gamma',\n",
       " 'Stal Händigkeit Hastings',\n",
       " 'FC Händigkeit Maxime',\n",
       " 'Dad Händigkeit Harbor',\n",
       " 'Kenia Händigkeit Little',\n",
       " 'CCD Händigkeit Sul',\n",
       " 'Riau Händigkeit Coleman',\n",
       " 'Ky Händigkeit Bristol',\n",
       " 'Billie Händigkeit Hastings',\n",
       " 'Elbe Händigkeit Rex',\n",
       " 'Paraíso Händigkeit Driver',\n",
       " 'TD Händigkeit Wallace',\n",
       " 'Luther Händigkeit Hastings',\n",
       " 'Roi Händigkeit Sul',\n",
       " 'Pole Händigkeit Driver',\n",
       " 'Page Händigkeit Sul',\n",
       " 'Baron Händigkeit Bristol',\n",
       " 'Libia Händigkeit Maxime',\n",
       " 'Cuenca Händigkeit Potsdam',\n",
       " 'Kálmán Händigkeit Rex',\n",
       " 'ET Händigkeit USSR',\n",
       " 'Mainstream Händigkeit Little',\n",
       " 'Agency Händigkeit Rex',\n",
       " 'Mata Händigkeit Samo',\n",
       " 'Mineral Händigkeit Little',\n",
       " 'Figaro Händigkeit Batman',\n",
       " 'Trung Händigkeit Saskatchewan',\n",
       " 'Sabha Händigkeit Hastings',\n",
       " 'Guimarães Händigkeit Rex',\n",
       " 'Disneyland Händigkeit Hastings',\n",
       " 'Wes Händigkeit Maxime',\n",
       " 'Gesù Händigkeit Hastings',\n",
       " 'Cinq Händigkeit Harbor',\n",
       " 'Silla Händigkeit USSR',\n",
       " 'JR Händigkeit Gamma',\n",
       " 'Bryant Händigkeit Batman',\n",
       " 'Klaus Händigkeit Wallace',\n",
       " 'Kleiner Händigkeit Driver',\n",
       " 'Fort Händigkeit Coleman',\n",
       " 'City Händigkeit Little',\n",
       " 'Márquez Händigkeit Harbor',\n",
       " 'Yer Händigkeit Driver',\n",
       " 'Bernard Händigkeit Little',\n",
       " 'Diaz Händigkeit Little',\n",
       " 'Madre Händigkeit Sul',\n",
       " 'Asturias Händigkeit USSR',\n",
       " 'FF Händigkeit Wallace',\n",
       " 'Dis Händigkeit Ja',\n",
       " 'Lorentz Händigkeit Driver',\n",
       " 'SM Händigkeit Potsdam',\n",
       " 'Sempre Händigkeit Driver',\n",
       " 'Portsmouth Händigkeit Ja',\n",
       " 'Irena Händigkeit Bristol',\n",
       " 'Oaks Händigkeit Saskatchewan',\n",
       " 'Fighting Händigkeit Little',\n",
       " 'Bilbao Händigkeit Harbor',\n",
       " 'Sharon Händigkeit Gamma',\n",
       " 'Allium Händigkeit Saskatchewan',\n",
       " 'Haiti Händigkeit Coleman',\n",
       " 'Isabella Händigkeit Harbor',\n",
       " 'Olsson Händigkeit Rex',\n",
       " 'Pure Händigkeit Saskatchewan',\n",
       " 'Patrol Händigkeit Sul',\n",
       " 'Siege Händigkeit Hastings',\n",
       " 'Galiza Händigkeit Ja',\n",
       " 'Lucky Händigkeit Hastings',\n",
       " 'Pfeiffer Händigkeit Driver',\n",
       " 'FX Händigkeit Platnick',\n",
       " 'Energy Händigkeit Batman',\n",
       " 'Joanne Händigkeit Rex',\n",
       " 'Erik Händigkeit Platnick',\n",
       " 'EM Händigkeit Harbor',\n",
       " 'Raum Händigkeit Samo',\n",
       " 'VL Händigkeit Batman',\n",
       " 'Sabina Händigkeit Batman',\n",
       " 'Krupp Händigkeit Little',\n",
       " 'October Händigkeit Hastings',\n",
       " 'Davidson Händigkeit Bristol',\n",
       " 'Allah Händigkeit Gamma',\n",
       " 'Larva Händigkeit Maxime',\n",
       " 'Khu Händigkeit Ja',\n",
       " 'Lahore Händigkeit Platnick',\n",
       " 'Dorset Händigkeit Batman',\n",
       " 'Stay Händigkeit Ja',\n",
       " 'Sit Händigkeit Driver',\n",
       " 'Wayne Händigkeit Saskatchewan',\n",
       " 'Flag Händigkeit Samo',\n",
       " 'Ferns Händigkeit Ja',\n",
       " 'Jaguar Händigkeit USSR',\n",
       " 'Brenda Händigkeit Harbor',\n",
       " 'Mina Händigkeit Mickey',\n",
       " 'Harding Händigkeit Hastings',\n",
       " 'Giants Händigkeit Comte',\n",
       " 'Buta Händigkeit Comte',\n",
       " 'Faye Händigkeit Harbor',\n",
       " 'Generation Händigkeit Ja',\n",
       " 'Schottland Händigkeit Mickey',\n",
       " 'USD Händigkeit Little',\n",
       " 'Eu Händigkeit Platnick',\n",
       " 'Haji symbolisiert Swan',\n",
       " 'Yahoo symbolisiert Aziz',\n",
       " 'Stal symbolisiert Stranger',\n",
       " 'FC symbolisiert Steen',\n",
       " 'Dad symbolisiert Steen',\n",
       " 'Kenia symbolisiert Generation',\n",
       " 'CCD symbolisiert Fresno',\n",
       " 'Riau symbolisiert Beyaz',\n",
       " 'Ky symbolisiert SF',\n",
       " 'Billie symbolisiert Barbosa',\n",
       " 'Elbe symbolisiert Steen',\n",
       " 'Paraíso symbolisiert Blanchard',\n",
       " 'TD symbolisiert Seele',\n",
       " 'Luther symbolisiert Ja',\n",
       " 'Roi symbolisiert Steen',\n",
       " 'Pole symbolisiert Seele',\n",
       " 'Page symbolisiert Aziz',\n",
       " 'Baron symbolisiert SF',\n",
       " 'Libia symbolisiert Rae',\n",
       " 'Cuenca symbolisiert Rae',\n",
       " 'Kálmán symbolisiert Aziz',\n",
       " 'ET symbolisiert Stranger',\n",
       " 'Mainstream symbolisiert Seele',\n",
       " 'Agency symbolisiert Tông',\n",
       " 'Mata symbolisiert Blanchard',\n",
       " 'Mineral symbolisiert Massacre',\n",
       " 'Figaro symbolisiert Boone',\n",
       " 'Trung symbolisiert Tiempo',\n",
       " 'Sabha symbolisiert Tiempo',\n",
       " 'Guimarães symbolisiert Tiempo',\n",
       " 'Disneyland symbolisiert Tông',\n",
       " 'Wes symbolisiert Ng',\n",
       " 'Gesù symbolisiert Ja',\n",
       " 'Cinq symbolisiert Generation',\n",
       " 'Silla symbolisiert Generation',\n",
       " 'JR symbolisiert Stranger',\n",
       " 'Bryant symbolisiert Aziz',\n",
       " 'Klaus symbolisiert Boone',\n",
       " 'Kleiner symbolisiert Stranger',\n",
       " 'Fort symbolisiert Aziz',\n",
       " 'City symbolisiert Tiempo',\n",
       " 'Márquez symbolisiert SF',\n",
       " 'Yer symbolisiert Tiempo',\n",
       " 'Bernard symbolisiert Blanchard',\n",
       " 'Diaz symbolisiert Free',\n",
       " 'Madre symbolisiert Free',\n",
       " 'Asturias symbolisiert Barbosa',\n",
       " 'FF symbolisiert Seele',\n",
       " 'Dis symbolisiert Beyaz',\n",
       " 'Lorentz symbolisiert Tông',\n",
       " 'SM symbolisiert Beyaz',\n",
       " 'Sempre symbolisiert Massacre',\n",
       " 'Portsmouth symbolisiert Seele',\n",
       " 'Irena symbolisiert Beyaz',\n",
       " 'Oaks symbolisiert Swan',\n",
       " 'Fighting symbolisiert Boone',\n",
       " 'Bilbao symbolisiert Generation',\n",
       " 'Sharon symbolisiert Tiempo',\n",
       " 'Allium symbolisiert Steen',\n",
       " 'Haiti symbolisiert Stranger',\n",
       " 'Isabella symbolisiert SF',\n",
       " 'Olsson symbolisiert Tông',\n",
       " 'Pure symbolisiert Stranger',\n",
       " 'Patrol symbolisiert Stranger',\n",
       " 'Siege symbolisiert Generation',\n",
       " 'Galiza symbolisiert Seele',\n",
       " 'Lucky symbolisiert Tông',\n",
       " 'Pfeiffer symbolisiert Barbosa',\n",
       " 'FX symbolisiert Aziz',\n",
       " 'Energy symbolisiert Free',\n",
       " 'Joanne symbolisiert Boone',\n",
       " 'Erik symbolisiert Blanchard',\n",
       " 'EM symbolisiert Massacre',\n",
       " 'Raum symbolisiert Tông',\n",
       " 'VL symbolisiert Rae',\n",
       " 'Sabina symbolisiert Barbosa',\n",
       " 'Krupp symbolisiert Rae',\n",
       " 'October symbolisiert Seele',\n",
       " 'Davidson symbolisiert SF',\n",
       " 'Allah symbolisiert Beyaz',\n",
       " 'Larva symbolisiert Stranger',\n",
       " 'Khu symbolisiert Tiempo',\n",
       " 'Lahore symbolisiert Aziz',\n",
       " 'Dorset symbolisiert Stranger',\n",
       " 'Stay symbolisiert Generation',\n",
       " 'Sit symbolisiert Blanchard',\n",
       " 'Wayne symbolisiert Generation',\n",
       " 'Flag symbolisiert Stranger',\n",
       " 'Ferns symbolisiert Ja',\n",
       " 'Jaguar symbolisiert Barbosa',\n",
       " 'Brenda symbolisiert Ng',\n",
       " 'Mina symbolisiert Massacre',\n",
       " 'Harding symbolisiert Steen',\n",
       " 'Giants symbolisiert Generation',\n",
       " 'Buta symbolisiert Seele',\n",
       " 'Faye symbolisiert Tiempo',\n",
       " 'Generation symbolisiert SF',\n",
       " 'Schottland symbolisiert Boone',\n",
       " 'USD symbolisiert Fresno',\n",
       " 'Eu symbolisiert Tiempo',\n",
       " 'Haji normalerweise verwendetes Fahrzeug Rhapsody',\n",
       " 'Yahoo normalerweise verwendetes Fahrzeug Dunia',\n",
       " 'Stal normalerweise verwendetes Fahrzeug Nash',\n",
       " 'FC normalerweise verwendetes Fahrzeug ag',\n",
       " 'Dad normalerweise verwendetes Fahrzeug Viru',\n",
       " 'Kenia normalerweise verwendetes Fahrzeug Viru',\n",
       " 'CCD normalerweise verwendetes Fahrzeug Dunia',\n",
       " 'Riau normalerweise verwendetes Fahrzeug Rhapsody',\n",
       " 'Ky normalerweise verwendetes Fahrzeug Viru',\n",
       " 'Billie normalerweise verwendetes Fahrzeug Genocide',\n",
       " 'Elbe normalerweise verwendetes Fahrzeug Henning',\n",
       " 'Paraíso normalerweise verwendetes Fahrzeug Royal',\n",
       " 'TD normalerweise verwendetes Fahrzeug Madison',\n",
       " 'Luther normalerweise verwendetes Fahrzeug Kirche',\n",
       " 'Roi normalerweise verwendetes Fahrzeug Rhapsody',\n",
       " 'Pole normalerweise verwendetes Fahrzeug Henning',\n",
       " 'Page normalerweise verwendetes Fahrzeug Viru',\n",
       " 'Baron normalerweise verwendetes Fahrzeug Vacelet',\n",
       " 'Libia normalerweise verwendetes Fahrzeug Royal',\n",
       " 'Cuenca normalerweise verwendetes Fahrzeug Nash',\n",
       " 'Kálmán normalerweise verwendetes Fahrzeug Royal',\n",
       " 'ET normalerweise verwendetes Fahrzeug Each',\n",
       " 'Mainstream normalerweise verwendetes Fahrzeug Vacelet',\n",
       " 'Agency normalerweise verwendetes Fahrzeug Neki',\n",
       " 'Mata normalerweise verwendetes Fahrzeug Neki',\n",
       " 'Mineral normalerweise verwendetes Fahrzeug Valley',\n",
       " 'Figaro normalerweise verwendetes Fahrzeug Henning',\n",
       " 'Trung normalerweise verwendetes Fahrzeug Neki',\n",
       " 'Sabha normalerweise verwendetes Fahrzeug Vacelet',\n",
       " 'Guimarães normalerweise verwendetes Fahrzeug Royal',\n",
       " 'Disneyland normalerweise verwendetes Fahrzeug Maka',\n",
       " 'Wes normalerweise verwendetes Fahrzeug Alta',\n",
       " 'Gesù normalerweise verwendetes Fahrzeug Díaz',\n",
       " 'Cinq normalerweise verwendetes Fahrzeug Genocide',\n",
       " 'Silla normalerweise verwendetes Fahrzeug Kirche',\n",
       " 'JR normalerweise verwendetes Fahrzeug Nash',\n",
       " 'Bryant normalerweise verwendetes Fahrzeug Neki',\n",
       " 'Klaus normalerweise verwendetes Fahrzeug Rhapsody',\n",
       " 'Kleiner normalerweise verwendetes Fahrzeug Neki',\n",
       " 'Fort normalerweise verwendetes Fahrzeug Valley',\n",
       " 'City normalerweise verwendetes Fahrzeug Alta',\n",
       " 'Márquez normalerweise verwendetes Fahrzeug Madison',\n",
       " 'Yer normalerweise verwendetes Fahrzeug Dunia',\n",
       " 'Bernard normalerweise verwendetes Fahrzeug Díaz',\n",
       " 'Diaz normalerweise verwendetes Fahrzeug Kirche',\n",
       " 'Madre normalerweise verwendetes Fahrzeug Vacelet',\n",
       " 'Asturias normalerweise verwendetes Fahrzeug Genocide',\n",
       " 'FF normalerweise verwendetes Fahrzeug Cynthia',\n",
       " 'Dis normalerweise verwendetes Fahrzeug Madison',\n",
       " 'Lorentz normalerweise verwendetes Fahrzeug Each',\n",
       " 'SM normalerweise verwendetes Fahrzeug Icarus',\n",
       " 'Sempre normalerweise verwendetes Fahrzeug Royal',\n",
       " 'Portsmouth normalerweise verwendetes Fahrzeug Kirche',\n",
       " 'Irena normalerweise verwendetes Fahrzeug Genocide',\n",
       " 'Oaks normalerweise verwendetes Fahrzeug Cynthia',\n",
       " 'Fighting normalerweise verwendetes Fahrzeug Rhapsody',\n",
       " 'Bilbao normalerweise verwendetes Fahrzeug Díaz',\n",
       " 'Sharon normalerweise verwendetes Fahrzeug Kensington',\n",
       " 'Allium normalerweise verwendetes Fahrzeug Maka',\n",
       " 'Haiti normalerweise verwendetes Fahrzeug Kirche',\n",
       " 'Isabella normalerweise verwendetes Fahrzeug Royal',\n",
       " 'Olsson normalerweise verwendetes Fahrzeug Valley',\n",
       " 'Pure normalerweise verwendetes Fahrzeug Dunia',\n",
       " 'Patrol normalerweise verwendetes Fahrzeug Icarus',\n",
       " 'Siege normalerweise verwendetes Fahrzeug ag',\n",
       " 'Galiza normalerweise verwendetes Fahrzeug Henning',\n",
       " 'Lucky normalerweise verwendetes Fahrzeug Maka',\n",
       " 'Pfeiffer normalerweise verwendetes Fahrzeug Díaz',\n",
       " 'FX normalerweise verwendetes Fahrzeug Dunia',\n",
       " 'Energy normalerweise verwendetes Fahrzeug Díaz',\n",
       " 'Joanne normalerweise verwendetes Fahrzeug Vacelet',\n",
       " 'Erik normalerweise verwendetes Fahrzeug Nash',\n",
       " 'EM normalerweise verwendetes Fahrzeug ag',\n",
       " 'Raum normalerweise verwendetes Fahrzeug Valley',\n",
       " 'VL normalerweise verwendetes Fahrzeug Kirche',\n",
       " 'Sabina normalerweise verwendetes Fahrzeug Valley',\n",
       " 'Krupp normalerweise verwendetes Fahrzeug Kensington',\n",
       " 'October normalerweise verwendetes Fahrzeug Royal',\n",
       " 'Davidson normalerweise verwendetes Fahrzeug Maka',\n",
       " 'Allah normalerweise verwendetes Fahrzeug Madison',\n",
       " 'Larva normalerweise verwendetes Fahrzeug Rhapsody',\n",
       " 'Khu normalerweise verwendetes Fahrzeug Vacelet',\n",
       " 'Lahore normalerweise verwendetes Fahrzeug Royal',\n",
       " 'Dorset normalerweise verwendetes Fahrzeug Royal',\n",
       " 'Stay normalerweise verwendetes Fahrzeug ag',\n",
       " 'Sit normalerweise verwendetes Fahrzeug Kirche',\n",
       " 'Wayne normalerweise verwendetes Fahrzeug Díaz',\n",
       " 'Flag normalerweise verwendetes Fahrzeug Kensington',\n",
       " 'Ferns normalerweise verwendetes Fahrzeug Alta',\n",
       " 'Jaguar normalerweise verwendetes Fahrzeug Cynthia',\n",
       " 'Brenda normalerweise verwendetes Fahrzeug Icarus',\n",
       " 'Mina normalerweise verwendetes Fahrzeug Madison',\n",
       " 'Harding normalerweise verwendetes Fahrzeug Kensington',\n",
       " 'Giants normalerweise verwendetes Fahrzeug Nash',\n",
       " 'Buta normalerweise verwendetes Fahrzeug Rhapsody',\n",
       " 'Faye normalerweise verwendetes Fahrzeug Vacelet',\n",
       " 'Generation normalerweise verwendetes Fahrzeug Neki',\n",
       " 'Schottland normalerweise verwendetes Fahrzeug Madison',\n",
       " 'USD normalerweise verwendetes Fahrzeug Rhapsody',\n",
       " 'Eu normalerweise verwendetes Fahrzeug Dunia']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "415a01e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Haji', 'Inge', 'Elite'],\n",
       " ['Yahoo', 'Italie', 'Bora'],\n",
       " ['Stal', 'MS', 'Ramsey'],\n",
       " ['FC', 'Condor', 'Gospel'],\n",
       " ['Dad', 'Condor', 'Gospel'],\n",
       " ['Kenia', 'Portland', 'Eliza'],\n",
       " ['CCD', 'Inge', 'Elite'],\n",
       " ['Riau', 'Ringo', 'Theresa'],\n",
       " ['Ky', 'Wilfried', 'Fach'],\n",
       " ['Billie', 'Zone', 'Benth'],\n",
       " ['Elbe', 'Einer', 'Everybody'],\n",
       " ['Paraíso', 'Hus', 'epi'],\n",
       " ['TD', 'Italie', 'Bora'],\n",
       " ['Luther', 'Wilfried', 'Fach'],\n",
       " ['Roi', 'Universitas', 'Collins'],\n",
       " ['Pole', 'Condor', 'Gospel'],\n",
       " ['Page', 'Holy', 'Haar'],\n",
       " ['Baron', 'Baja', 'Ruska'],\n",
       " ['Libia', 'Italie', 'Bora'],\n",
       " ['Cuenca', 'Ringo', 'Theresa'],\n",
       " ['Kálmán', 'Universitas', 'Collins'],\n",
       " ['ET', 'Ringo', 'Theresa'],\n",
       " ['Mainstream', 'Einer', 'Everybody'],\n",
       " ['Agency', 'Condor', 'Gospel'],\n",
       " ['Mata', 'Pizza', 'Koska'],\n",
       " ['Mineral', 'Zone', 'Benth'],\n",
       " ['Figaro', 'MS', 'Ramsey'],\n",
       " ['Trung', 'Italie', 'Bora'],\n",
       " ['Sabha', 'Weir', 'Antoine'],\n",
       " ['Guimarães', 'Einer', 'Everybody'],\n",
       " ['Disneyland', 'BRT', 'Carioca'],\n",
       " ['Wes', 'MS', 'Ramsey'],\n",
       " ['Gesù', 'Portland', 'Eliza'],\n",
       " ['Cinq', 'Universitas', 'Collins'],\n",
       " ['Silla', 'BRT', 'Carioca'],\n",
       " ['JR', 'Inge', 'Elite'],\n",
       " ['Bryant', 'Wilfried', 'Fach'],\n",
       " ['Klaus', 'Né', 'Fidel'],\n",
       " ['Kleiner', 'Inge', 'Elite'],\n",
       " ['Fort', 'Note', 'Ultra'],\n",
       " ['City', 'Note', 'Ultra'],\n",
       " ['Márquez', 'Baja', 'Ruska'],\n",
       " ['Yer', 'MS', 'Ramsey'],\n",
       " ['Bernard', 'Hus', 'epi'],\n",
       " ['Diaz', 'Pizza', 'Koska'],\n",
       " ['Madre', 'Ringo', 'Theresa'],\n",
       " ['Asturias', 'Inge', 'Elite'],\n",
       " ['FF', 'Né', 'Fidel'],\n",
       " ['Dis', 'Wilfried', 'Fach'],\n",
       " ['Lorentz', 'Ringo', 'Theresa'],\n",
       " ['SM', 'Katz', 'Saussure'],\n",
       " ['Sempre', 'Baja', 'Ruska'],\n",
       " ['Portsmouth', 'Note', 'Ultra'],\n",
       " ['Irena', 'Zone', 'Benth'],\n",
       " ['Oaks', 'Universitas', 'Collins'],\n",
       " ['Fighting', 'Wilfried', 'Fach'],\n",
       " ['Bilbao', 'Hus', 'epi'],\n",
       " ['Sharon', 'Condor', 'Gospel'],\n",
       " ['Allium', 'Katz', 'Saussure'],\n",
       " ['Haiti', 'Wilfried', 'Fach'],\n",
       " ['Isabella', 'Condor', 'Gospel'],\n",
       " ['Olsson', 'Inge', 'Elite'],\n",
       " ['Pure', 'Né', 'Fidel'],\n",
       " ['Patrol', 'MS', 'Ramsey'],\n",
       " ['Siege', 'Pizza', 'Koska'],\n",
       " ['Galiza', 'Note', 'Ultra'],\n",
       " ['Lucky', 'Weir', 'Antoine'],\n",
       " ['Pfeiffer', 'Note', 'Ultra'],\n",
       " ['FX', 'Note', 'Ultra'],\n",
       " ['Energy', 'Universitas', 'Collins'],\n",
       " ['Joanne', 'MS', 'Ramsey'],\n",
       " ['Erik', 'Wilfried', 'Fach'],\n",
       " ['EM', 'Baja', 'Ruska'],\n",
       " ['Raum', 'Weir', 'Antoine'],\n",
       " ['VL', 'Ringo', 'Theresa'],\n",
       " ['Sabina', 'Condor', 'Gospel'],\n",
       " ['Krupp', 'Weir', 'Antoine'],\n",
       " ['October', 'Pizza', 'Koska'],\n",
       " ['Davidson', 'Né', 'Fidel'],\n",
       " ['Allah', 'MS', 'Ramsey'],\n",
       " ['Larva', 'Ringo', 'Theresa'],\n",
       " ['Khu', 'Condor', 'Gospel'],\n",
       " ['Lahore', 'BRT', 'Carioca'],\n",
       " ['Dorset', 'Hus', 'epi'],\n",
       " ['Stay', 'Condor', 'Gospel'],\n",
       " ['Sit', 'Hus', 'epi'],\n",
       " ['Wayne', 'BRT', 'Carioca'],\n",
       " ['Flag', 'Né', 'Fidel'],\n",
       " ['Ferns', 'MS', 'Ramsey'],\n",
       " ['Jaguar', 'Wilfried', 'Fach'],\n",
       " ['Brenda', 'Universitas', 'Collins'],\n",
       " ['Mina', 'Zone', 'Benth'],\n",
       " ['Harding', 'BRT', 'Carioca'],\n",
       " ['Giants', 'Universitas', 'Collins'],\n",
       " ['Buta', 'Ariane', 'Exil'],\n",
       " ['Faye', 'Né', 'Fidel'],\n",
       " ['Generation', 'Pizza', 'Koska'],\n",
       " ['Schottland', 'Portland', 'Eliza'],\n",
       " ['USD', 'MS', 'Ramsey'],\n",
       " ['Eu', 'Portland', 'Eliza']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities[900:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969f843",
   "metadata": {},
   "source": [
    "#### Rule: (e, r, f) <-> (e, not r, f_antonym) => (e, r_de, f) <-> (e, not r_de, f_antonym)\n",
    "\n",
    "- Does it learn (e, not r, f_antonym) ?\n",
    "- Does it learn (e, r_de, f)?\n",
    "- Does it learn (e, not r_de, f_antonym)?\n",
    "\n",
    "Is negation even ignored?\n",
    "- Does it learn (e, r, f_antonym) ? (Ignores negation in source)\n",
    "\n",
    "What about ignoring antonyms?\n",
    "- Does it learn the negation but not the antonym (e, not r, f) ?\n",
    "\n",
    "What about for precision@2? Maybe it just learns to associate e with f and f_antonym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53706b4e",
   "metadata": {},
   "source": [
    "Data generation of negation was modified to only ever train (e, r, f) to test (e, not r_de, f_antonym) and never the way from (e, not r, f_antonym)!\n",
    "This makes evaluation way easier!\n",
    "\n",
    "1800 facts are training the rule (900<->900)\n",
    "1800-1900 are facts that are used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2396ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: reviewed by\n",
      "Accuracy for learning source rule (e, not_r, f_antonym): 0.78\n",
      "Accuracy for KT (e, r_de, f): 0.0\n",
      "Accuracy for Test (e, not r_de, f_antonym): 0.0\n",
      "-------------------\n",
      "Accuracy for ignoring negation (e, r, f_antonym): 0.0\n",
      "Accuracy for ignoring antonym (e, not r, f): 0.0\n",
      "\n",
      "\n",
      "Relation: interleaves with\n",
      "Accuracy for learning source rule (e, not_r, f_antonym): 0.66\n",
      "Accuracy for KT (e, r_de, f): 0.13\n",
      "Accuracy for Test (e, not r_de, f_antonym): 0.67\n",
      "-------------------\n",
      "Accuracy for ignoring negation (e, r, f_antonym): 0.0\n",
      "Accuracy for ignoring antonym (e, not r, f): 0.0\n",
      "\n",
      "\n",
      "Relation: open period from\n",
      "Accuracy for learning source rule (e, not_r, f_antonym): 0.84\n",
      "Accuracy for KT (e, r_de, f): 0.02\n",
      "Accuracy for Test (e, not r_de, f_antonym): 0.04\n",
      "-------------------\n",
      "Accuracy for ignoring negation (e, r, f_antonym): 0.0\n",
      "Accuracy for ignoring antonym (e, not r, f): 0.0\n",
      "\n",
      "\n",
      "Relation: broadcast by\n",
      "Accuracy for learning source rule (e, not_r, f_antonym): 0.79\n",
      "Accuracy for KT (e, r_de, f): 0.12\n",
      "Accuracy for Test (e, not r_de, f_antonym): 0.35\n",
      "-------------------\n",
      "Accuracy for ignoring negation (e, r, f_antonym): 0.0\n",
      "Accuracy for ignoring antonym (e, not r, f): 0.0\n",
      "\n",
      "\n",
      "Relation: list of works\n",
      "Accuracy for learning source rule (e, not_r, f_antonym): 0.77\n",
      "Accuracy for KT (e, r_de, f): 0.0\n",
      "Accuracy for Test (e, not r_de, f_antonym): 0.09\n",
      "-------------------\n",
      "Accuracy for ignoring negation (e, r, f_antonym): 0.0\n",
      "Accuracy for ignoring antonym (e, not r, f): 0.0\n",
      "\n",
      "\n",
      "Relation: relative\n",
      "Accuracy for learning source rule (e, not_r, f_antonym): 0.74\n",
      "Accuracy for KT (e, r_de, f): 0.0\n",
      "Accuracy for Test (e, not r_de, f_antonym): 0.0\n",
      "-------------------\n",
      "Accuracy for ignoring negation (e, r, f_antonym): 0.0\n",
      "Accuracy for ignoring antonym (e, not r, f): 0.0\n",
      "\n",
      "\n",
      "Relation: primary destinations\n",
      "Accuracy for learning source rule (e, not_r, f_antonym): 0.89\n",
      "Accuracy for KT (e, r_de, f): 0.0\n",
      "Accuracy for Test (e, not r_de, f_antonym): 0.08\n",
      "-------------------\n",
      "Accuracy for ignoring negation (e, r, f_antonym): 0.0\n",
      "Accuracy for ignoring antonym (e, not r, f): 0.0\n",
      "\n",
      "\n",
      "Relation: handedness\n",
      "Accuracy for learning source rule (e, not_r, f_antonym): 0.79\n",
      "Accuracy for KT (e, r_de, f): 0.06\n",
      "Accuracy for Test (e, not r_de, f_antonym): 0.27\n",
      "-------------------\n",
      "Accuracy for ignoring negation (e, r, f_antonym): 0.0\n",
      "Accuracy for ignoring antonym (e, not r, f): 0.0\n",
      "\n",
      "\n",
      "Relation: symbolizes\n",
      "Accuracy for learning source rule (e, not_r, f_antonym): 0.74\n",
      "Accuracy for KT (e, r_de, f): 0.15\n",
      "Accuracy for Test (e, not r_de, f_antonym): 0.77\n",
      "-------------------\n",
      "Accuracy for ignoring negation (e, r, f_antonym): 0.01\n",
      "Accuracy for ignoring antonym (e, not r, f): 0.0\n",
      "\n",
      "\n",
      "Relation: vehicle normally used\n",
      "Accuracy for learning source rule (e, not_r, f_antonym): 0.78\n",
      "Accuracy for KT (e, r_de, f): 0.0\n",
      "Accuracy for Test (e, not r_de, f_antonym): 0.09\n",
      "-------------------\n",
      "Accuracy for ignoring negation (e, r, f_antonym): 0.0\n",
      "Accuracy for ignoring antonym (e, not r, f): 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over relations, take the training samples that were trained on\n",
    "for i in range(n_relations):\n",
    "    trained_test = train_dict['sample'][1800+i*1900:(i+1)*1900]\n",
    "    entities_test = entities[900+i*1000:(i+1)*1000]\n",
    "\n",
    "    acc_source_rule = 0\n",
    "    acc_kt = 0\n",
    "    acc_test = 0\n",
    "    acc_neg_ignored = 0\n",
    "    acc_antonym_ignored = 0\n",
    "    \n",
    "    # Relation pairs!\n",
    "    r = relations['en'].iloc[i]\n",
    "    r_de = relations['de'].iloc[i]\n",
    "    not_r = relations['en_negated'].iloc[i]\n",
    "    not_r_de = relations['de_negated'].iloc[i]\n",
    "\n",
    "    for j, sample in enumerate(trained_test):\n",
    "\n",
    "        ents = entities_test[j]\n",
    "        e = ents[0]\n",
    "        f = ents[1]\n",
    "        f_antonym = ents[2]\n",
    "\n",
    "        # Test (e, not_r, f_antonym)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(f_antonym)\n",
    "\n",
    "        prompt = e + ' ' + not_r + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_source_rule += 1\n",
    "\n",
    "        # Test (e, r_de, f)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(f)\n",
    "\n",
    "        prompt = e + ' ' + r_de + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_kt += 1\n",
    "            \n",
    "        # Test (e, not r_de, f_antonym)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(f_antonym)\n",
    "\n",
    "        prompt = e + ' ' + not_r_de + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_test += 1\n",
    "        \n",
    "        # ---\n",
    "        # Test (e, r, f_antonym)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(e)\n",
    "\n",
    "        prompt = e + ' ' + r + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_2_token = torch.topk(mask_token_logits, 2, dim=1).indices[0].tolist()\n",
    "\n",
    "        if label_token in top_2_token:\n",
    "            acc_neg_ignored += 1\n",
    "            \n",
    "        # Test (e, not r, f)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(f)\n",
    "\n",
    "        prompt = e + ' ' + not_r + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_2_token = torch.topk(mask_token_logits, 2, dim=1).indices[0].tolist()\n",
    "\n",
    "        if label_token in top_2_token:\n",
    "            acc_antonym_ignored += 1\n",
    "        \n",
    "\n",
    "    acc_source_rule /= 100   \n",
    "    acc_kt /= 100\n",
    "    acc_test /= 100\n",
    "    acc_neg_ignored /= 100\n",
    "    acc_antonym_ignored /= 100\n",
    "    \n",
    "\n",
    "    print(f'Relation: {r}')\n",
    "    print(f'Accuracy for learning source rule (e, not_r, f_antonym): {acc_source_rule}')\n",
    "    print(f'Accuracy for KT (e, r_de, f): {acc_kt}')\n",
    "    print(f'Accuracy for Test (e, not r_de, f_antonym): {acc_test}')\n",
    "    print('-------------------')\n",
    "    print(f'Accuracy for ignoring negation (e, r, f_antonym): {acc_neg_ignored}')\n",
    "    print(f'Accuracy for ignoring antonym (e, not r, f): {acc_antonym_ignored}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cfc1c",
   "metadata": {},
   "source": [
    "### Manual Testing if it learns its training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b987d658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Medina reviewed by Italie',\n",
       " 'Medina not reviewed by Bora',\n",
       " 'Invasion reviewed by Hus',\n",
       " 'Invasion not reviewed by epi',\n",
       " 'Burke reviewed by Wilfried',\n",
       " 'Burke not reviewed by Fach',\n",
       " 'Drama reviewed by Inge',\n",
       " 'Drama not reviewed by Elite',\n",
       " 'Master reviewed by Portland',\n",
       " 'Master not reviewed by Eliza',\n",
       " 'Dari reviewed by Weir',\n",
       " 'Dari not reviewed by Antoine',\n",
       " 'Chihuahua reviewed by Universitas',\n",
       " 'Chihuahua not reviewed by Collins',\n",
       " 'EP reviewed by Condor',\n",
       " 'EP not reviewed by Gospel',\n",
       " 'Chase reviewed by MS',\n",
       " 'Chase not reviewed by Ramsey',\n",
       " 'Worcester reviewed by Katz',\n",
       " 'Worcester not reviewed by Saussure',\n",
       " 'Albert reviewed by BRT',\n",
       " 'Albert not reviewed by Carioca',\n",
       " 'Ibiza reviewed by Note',\n",
       " 'Ibiza not reviewed by Ultra',\n",
       " 'Câmara reviewed by Né',\n",
       " 'Câmara not reviewed by Fidel',\n",
       " 'Eleanor reviewed by Einer',\n",
       " 'Eleanor not reviewed by Everybody',\n",
       " 'Campus reviewed by Pizza',\n",
       " 'Campus not reviewed by Koska',\n",
       " 'Schaus reviewed by Ariane',\n",
       " 'Schaus not reviewed by Exil',\n",
       " 'Chaos reviewed by Holy',\n",
       " 'Chaos not reviewed by Haar',\n",
       " 'Alice reviewed by Ringo',\n",
       " 'Alice not reviewed by Theresa',\n",
       " 'Franklin reviewed by Zone',\n",
       " 'Franklin not reviewed by Benth',\n",
       " 'Colombo reviewed by Baja',\n",
       " 'Colombo not reviewed by Ruska',\n",
       " 'Graf reviewed by Seymour',\n",
       " 'Graf not reviewed by ECW',\n",
       " 'Darwin reviewed by Bez',\n",
       " 'Darwin not reviewed by Ortes',\n",
       " 'Nisan reviewed by Jessica',\n",
       " 'Nisan not reviewed by Zoom',\n",
       " 'Hartman reviewed by Telephone',\n",
       " 'Hartman not reviewed by A',\n",
       " 'Lur reviewed by Barra',\n",
       " 'Lur not reviewed by Lori',\n",
       " 'Fiesta reviewed by Reeves',\n",
       " 'Fiesta not reviewed by Wagen',\n",
       " 'Alexandra reviewed by Sound',\n",
       " 'Alexandra not reviewed by JR',\n",
       " 'Mérida reviewed by Trail',\n",
       " 'Mérida not reviewed by Concordia',\n",
       " 'Shire reviewed by Windows',\n",
       " 'Shire not reviewed by Poco',\n",
       " 'Anthology reviewed by Suba',\n",
       " 'Anthology not reviewed by Bartlett',\n",
       " 'Monroe reviewed by Agent',\n",
       " 'Monroe not reviewed by Compton',\n",
       " 'Hell reviewed by Marshal',\n",
       " 'Hell not reviewed by RF',\n",
       " 'Bandet reviewed by Lorenzo',\n",
       " 'Bandet not reviewed by Cherry',\n",
       " 'Bertrand reviewed by Namibia',\n",
       " 'Bertrand not reviewed by FAA',\n",
       " 'Drum reviewed by Kantonen',\n",
       " 'Drum not reviewed by Dunkerque',\n",
       " 'IBM reviewed by Demon',\n",
       " 'IBM not reviewed by Bears',\n",
       " 'Christchurch reviewed by Paradise',\n",
       " 'Christchurch not reviewed by Sidney',\n",
       " 'Broadcast reviewed by ACM',\n",
       " 'Broadcast not reviewed by Guanajuato',\n",
       " 'Clay reviewed by Bees',\n",
       " 'Clay not reviewed by Edwards',\n",
       " 'Sound reviewed by Friesland',\n",
       " 'Sound not reviewed by Palencia',\n",
       " 'Liga reviewed by Baldwin',\n",
       " 'Liga not reviewed by Plans',\n",
       " 'Vatican reviewed by Hurley',\n",
       " 'Vatican not reviewed by Schools',\n",
       " 'Libro reviewed by IX',\n",
       " 'Libro not reviewed by Alessandro',\n",
       " 'Vanderbilt reviewed by Bürger',\n",
       " 'Vanderbilt not reviewed by Collegiate',\n",
       " 'Austrian reviewed by Move',\n",
       " 'Austrian not reviewed by KM',\n",
       " 'Varese reviewed by Petra',\n",
       " 'Varese not reviewed by Nightmare',\n",
       " 'Ninja reviewed by Krüger',\n",
       " 'Ninja not reviewed by Rota',\n",
       " 'Lafayette reviewed by Face',\n",
       " 'Lafayette not reviewed by Martínez',\n",
       " 'Catalina reviewed by Quinta',\n",
       " 'Catalina not reviewed by Damit',\n",
       " 'Companion reviewed by Oper',\n",
       " 'Companion not reviewed by Frédéric',\n",
       " 'Marshal reviewed by Akdeniz',\n",
       " 'Marshal not reviewed by UC',\n",
       " 'Molina reviewed by Plan',\n",
       " 'Molina not reviewed by Sailor',\n",
       " 'Dow reviewed by Words',\n",
       " 'Dow not reviewed by Waterloo',\n",
       " 'Platte reviewed by Peters',\n",
       " 'Platte not reviewed by Boyle',\n",
       " 'Giant reviewed by Remixes',\n",
       " 'Giant not reviewed by Vienna',\n",
       " 'Mulder reviewed by Talent',\n",
       " 'Mulder not reviewed by Switch',\n",
       " 'Hancock reviewed by Greatest',\n",
       " 'Hancock not reviewed by Shannon',\n",
       " 'Deutschland reviewed by Irena',\n",
       " 'Deutschland not reviewed by Arabic',\n",
       " 'Della reviewed by Membre',\n",
       " 'Della not reviewed by Condor',\n",
       " 'Niko reviewed by Extra',\n",
       " 'Niko not reviewed by Bil',\n",
       " 'Romans reviewed by Menor',\n",
       " 'Romans not reviewed by Sprint',\n",
       " 'Salman reviewed by Free',\n",
       " 'Salman not reviewed by Ut',\n",
       " 'Medi reviewed by Transfer',\n",
       " 'Medi not reviewed by Batavia',\n",
       " 'Monaco reviewed by Das',\n",
       " 'Monaco not reviewed by Lowe',\n",
       " 'Release reviewed by Flat',\n",
       " 'Release not reviewed by Barry',\n",
       " 'Daughter reviewed by Latreille',\n",
       " 'Daughter not reviewed by Bala',\n",
       " 'Kendall reviewed by Büyük',\n",
       " 'Kendall not reviewed by Mittel',\n",
       " 'PCR reviewed by König',\n",
       " 'PCR not reviewed by Yates',\n",
       " 'Lorena reviewed by AFC',\n",
       " 'Lorena not reviewed by Matrix',\n",
       " 'Twain reviewed by Ottawa',\n",
       " 'Twain not reviewed by Tha',\n",
       " 'Head reviewed by Shane',\n",
       " 'Head not reviewed by PCR',\n",
       " 'Porta reviewed by Jazz',\n",
       " 'Porta not reviewed by Bismarck',\n",
       " 'Yunan reviewed by Carlton',\n",
       " 'Yunan not reviewed by George',\n",
       " 'MGM reviewed by Urban',\n",
       " 'MGM not reviewed by Samba',\n",
       " 'Queste reviewed by Molina',\n",
       " 'Queste not reviewed by Rivas',\n",
       " 'Geld reviewed by Braga',\n",
       " 'Geld not reviewed by Olav',\n",
       " 'Jesus reviewed by Stephan',\n",
       " 'Jesus not reviewed by McKamey',\n",
       " 'Potter reviewed by Burke',\n",
       " 'Potter not reviewed by Luik',\n",
       " 'Crown reviewed by Arena',\n",
       " 'Crown not reviewed by Oviedo',\n",
       " 'Adi reviewed by Syracuse',\n",
       " 'Adi not reviewed by Piazza',\n",
       " 'Sinn reviewed by Slag',\n",
       " 'Sinn not reviewed by Delft',\n",
       " 'Vacelet reviewed by Romagna',\n",
       " 'Vacelet not reviewed by Partner',\n",
       " 'Sagan reviewed by Benton',\n",
       " 'Sagan not reviewed by Quiet',\n",
       " 'Madsen reviewed by Dexter',\n",
       " 'Madsen not reviewed by Scientist',\n",
       " 'Robinson reviewed by Maestro',\n",
       " 'Robinson not reviewed by Busch',\n",
       " 'Fayette reviewed by Napoca',\n",
       " 'Fayette not reviewed by Ball',\n",
       " 'Grant reviewed by Lindsay',\n",
       " 'Grant not reviewed by Hector',\n",
       " 'Julian reviewed by IBM',\n",
       " 'Julian not reviewed by Dur',\n",
       " 'Carrillo reviewed by Tanner',\n",
       " 'Carrillo not reviewed by pr',\n",
       " 'Aus reviewed by Ensemble',\n",
       " 'Aus not reviewed by Ester',\n",
       " 'BBC reviewed by Hoya',\n",
       " 'BBC not reviewed by Gauss',\n",
       " 'Kongo reviewed by Heine',\n",
       " 'Kongo not reviewed by Madeira',\n",
       " 'Gaur reviewed by Golf',\n",
       " 'Gaur not reviewed by Pike',\n",
       " 'Shining reviewed by Helga',\n",
       " 'Shining not reviewed by Down',\n",
       " 'Simpson reviewed by Firefox',\n",
       " 'Simpson not reviewed by Novel',\n",
       " 'Douglas reviewed by Addison',\n",
       " 'Douglas not reviewed by Loving',\n",
       " 'Continental reviewed by Desse',\n",
       " 'Continental not reviewed by Middlesex',\n",
       " 'West reviewed by Beth',\n",
       " 'West not reviewed by Angoulême',\n",
       " 'Haas reviewed by Grimaldi',\n",
       " 'Haas not reviewed by Tobias',\n",
       " 'Lynch reviewed by Tat',\n",
       " 'Lynch not reviewed by Barry',\n",
       " 'Cornwall reviewed by Italie',\n",
       " 'Cornwall not reviewed by Bora',\n",
       " 'Jalan reviewed by Hus',\n",
       " 'Jalan not reviewed by epi',\n",
       " 'Dhaka reviewed by Wilfried',\n",
       " 'Dhaka not reviewed by Fach',\n",
       " 'Reading reviewed by Inge',\n",
       " 'Reading not reviewed by Elite',\n",
       " 'Auckland reviewed by Portland',\n",
       " 'Auckland not reviewed by Eliza',\n",
       " 'Chancellor reviewed by Weir',\n",
       " 'Chancellor not reviewed by Antoine',\n",
       " 'Dakota reviewed by Universitas',\n",
       " 'Dakota not reviewed by Collins',\n",
       " 'Arles reviewed by Condor',\n",
       " 'Arles not reviewed by Gospel',\n",
       " 'Loan reviewed by MS',\n",
       " 'Loan not reviewed by Ramsey',\n",
       " 'Stab reviewed by Katz',\n",
       " 'Stab not reviewed by Saussure',\n",
       " 'Golden reviewed by BRT',\n",
       " 'Golden not reviewed by Carioca',\n",
       " 'Jakiel reviewed by Note',\n",
       " 'Jakiel not reviewed by Ultra',\n",
       " 'Wade reviewed by Né',\n",
       " 'Wade not reviewed by Fidel',\n",
       " 'Ebro reviewed by Einer',\n",
       " 'Ebro not reviewed by Everybody',\n",
       " 'Dorothea reviewed by Pizza',\n",
       " 'Dorothea not reviewed by Koska',\n",
       " 'Urban reviewed by Ariane',\n",
       " 'Urban not reviewed by Exil',\n",
       " 'HF reviewed by Holy',\n",
       " 'HF not reviewed by Haar',\n",
       " 'Spectrum reviewed by Ringo',\n",
       " 'Spectrum not reviewed by Theresa',\n",
       " 'Cass reviewed by Zone',\n",
       " 'Cass not reviewed by Benth',\n",
       " 'Riva reviewed by Baja',\n",
       " 'Riva not reviewed by Ruska',\n",
       " 'Winston reviewed by Seymour',\n",
       " 'Winston not reviewed by ECW',\n",
       " 'Punk reviewed by Bez',\n",
       " 'Punk not reviewed by Ortes',\n",
       " 'SMS reviewed by Jessica',\n",
       " 'SMS not reviewed by Zoom',\n",
       " 'Paglinawan reviewed by Telephone',\n",
       " 'Paglinawan not reviewed by A',\n",
       " 'McKay reviewed by Barra',\n",
       " 'McKay not reviewed by Lori',\n",
       " 'Harmony reviewed by Reeves',\n",
       " 'Harmony not reviewed by Wagen',\n",
       " 'Chantal reviewed by Sound',\n",
       " 'Chantal not reviewed by JR',\n",
       " 'Taurus reviewed by Trail',\n",
       " 'Taurus not reviewed by Concordia',\n",
       " 'Crosby reviewed by Windows',\n",
       " 'Crosby not reviewed by Poco',\n",
       " 'GSC reviewed by Suba',\n",
       " 'GSC not reviewed by Bartlett',\n",
       " 'Moonlight reviewed by Agent',\n",
       " 'Moonlight not reviewed by Compton',\n",
       " 'Goodman reviewed by Marshal',\n",
       " 'Goodman not reviewed by RF',\n",
       " 'Stéphane reviewed by Lorenzo',\n",
       " 'Stéphane not reviewed by Cherry',\n",
       " 'Kader reviewed by Namibia',\n",
       " 'Kader not reviewed by FAA',\n",
       " 'Ferro reviewed by Kantonen',\n",
       " 'Ferro not reviewed by Dunkerque',\n",
       " 'Anglo reviewed by Demon',\n",
       " 'Anglo not reviewed by Bears',\n",
       " 'Flesh reviewed by Paradise',\n",
       " 'Flesh not reviewed by Sidney',\n",
       " 'Ubuntu reviewed by ACM',\n",
       " 'Ubuntu not reviewed by Guanajuato',\n",
       " 'GMT reviewed by Bees',\n",
       " 'GMT not reviewed by Edwards',\n",
       " 'Quintana reviewed by Friesland',\n",
       " 'Quintana not reviewed by Palencia',\n",
       " 'Lublin reviewed by Baldwin',\n",
       " 'Lublin not reviewed by Plans',\n",
       " 'Nagar reviewed by Hurley',\n",
       " 'Nagar not reviewed by Schools',\n",
       " 'Dictionary reviewed by IX',\n",
       " 'Dictionary not reviewed by Alessandro',\n",
       " 'Cats reviewed by Bürger',\n",
       " 'Cats not reviewed by Collegiate',\n",
       " 'Beyoncé reviewed by Move',\n",
       " 'Beyoncé not reviewed by KM',\n",
       " 'Mendoza reviewed by Petra',\n",
       " 'Mendoza not reviewed by Nightmare',\n",
       " 'Roja reviewed by Krüger',\n",
       " 'Roja not reviewed by Rota',\n",
       " 'Beckett reviewed by Face',\n",
       " 'Beckett not reviewed by Martínez',\n",
       " 'Toulouse reviewed by Quinta',\n",
       " 'Toulouse not reviewed by Damit',\n",
       " 'Summit reviewed by Oper',\n",
       " 'Summit not reviewed by Frédéric',\n",
       " 'Champion reviewed by Akdeniz',\n",
       " 'Champion not reviewed by UC',\n",
       " 'Twins reviewed by Plan',\n",
       " 'Twins not reviewed by Sailor',\n",
       " 'Halo reviewed by Words',\n",
       " 'Halo not reviewed by Waterloo',\n",
       " 'Deborah reviewed by Peters',\n",
       " 'Deborah not reviewed by Boyle',\n",
       " 'SF reviewed by Remixes',\n",
       " 'SF not reviewed by Vienna',\n",
       " 'Commonwealth reviewed by Talent',\n",
       " 'Commonwealth not reviewed by Switch',\n",
       " 'Dublin reviewed by Greatest',\n",
       " 'Dublin not reviewed by Shannon',\n",
       " 'Apocalypse reviewed by Irena',\n",
       " 'Apocalypse not reviewed by Arabic',\n",
       " 'Drake reviewed by Membre',\n",
       " 'Drake not reviewed by Condor',\n",
       " 'Padang reviewed by Extra',\n",
       " 'Padang not reviewed by Bil',\n",
       " 'Royal reviewed by Menor',\n",
       " 'Royal not reviewed by Sprint',\n",
       " 'Quick reviewed by Free',\n",
       " 'Quick not reviewed by Ut',\n",
       " 'Remote reviewed by Transfer',\n",
       " 'Remote not reviewed by Batavia',\n",
       " 'Player reviewed by Das',\n",
       " 'Player not reviewed by Lowe',\n",
       " 'Laba reviewed by Flat',\n",
       " 'Laba not reviewed by Barry',\n",
       " 'Wiener reviewed by Latreille',\n",
       " 'Wiener not reviewed by Bala',\n",
       " 'Tripoli reviewed by Büyük',\n",
       " 'Tripoli not reviewed by Mittel',\n",
       " 'Glasgow reviewed by König',\n",
       " 'Glasgow not reviewed by Yates',\n",
       " 'Hansen reviewed by AFC',\n",
       " 'Hansen not reviewed by Matrix',\n",
       " 'Ortes reviewed by Ottawa',\n",
       " 'Ortes not reviewed by Tha',\n",
       " 'Mozilla reviewed by Shane',\n",
       " 'Mozilla not reviewed by PCR',\n",
       " 'Mur reviewed by Jazz',\n",
       " 'Mur not reviewed by Bismarck',\n",
       " 'Iberia reviewed by Carlton',\n",
       " 'Iberia not reviewed by George',\n",
       " 'Vir reviewed by Urban',\n",
       " 'Vir not reviewed by Samba',\n",
       " 'Prato reviewed by Molina',\n",
       " 'Prato not reviewed by Rivas',\n",
       " 'Ferris reviewed by Braga',\n",
       " 'Ferris not reviewed by Olav',\n",
       " 'Racing reviewed by Stephan',\n",
       " 'Racing not reviewed by McKamey',\n",
       " 'Steiner reviewed by Burke',\n",
       " 'Steiner not reviewed by Luik',\n",
       " 'Sala reviewed by Arena',\n",
       " 'Sala not reviewed by Oviedo',\n",
       " 'Connection reviewed by Syracuse',\n",
       " 'Connection not reviewed by Piazza',\n",
       " 'Hastings reviewed by Slag',\n",
       " 'Hastings not reviewed by Delft',\n",
       " 'Siegfried reviewed by Romagna',\n",
       " 'Siegfried not reviewed by Partner',\n",
       " 'Kale reviewed by Benton',\n",
       " 'Kale not reviewed by Quiet',\n",
       " 'Fourier reviewed by Dexter',\n",
       " 'Fourier not reviewed by Scientist',\n",
       " 'Pada reviewed by Maestro',\n",
       " 'Pada not reviewed by Busch',\n",
       " 'Roberto reviewed by Napoca',\n",
       " 'Roberto not reviewed by Ball',\n",
       " 'Struggle reviewed by Lindsay',\n",
       " 'Struggle not reviewed by Hector',\n",
       " 'Elijah reviewed by IBM',\n",
       " 'Elijah not reviewed by Dur',\n",
       " 'Rote reviewed by Tanner',\n",
       " 'Rote not reviewed by pr',\n",
       " 'Christi reviewed by Ensemble',\n",
       " 'Christi not reviewed by Ester',\n",
       " 'Berge reviewed by Hoya',\n",
       " 'Berge not reviewed by Gauss',\n",
       " 'Wimbledon reviewed by Heine',\n",
       " 'Wimbledon not reviewed by Madeira',\n",
       " 'Bees reviewed by Golf',\n",
       " 'Bees not reviewed by Pike',\n",
       " 'Palm reviewed by Helga',\n",
       " 'Palm not reviewed by Down',\n",
       " 'Nor reviewed by Firefox',\n",
       " 'Nor not reviewed by Novel',\n",
       " 'Maria reviewed by Addison',\n",
       " 'Maria not reviewed by Loving',\n",
       " 'Stadio reviewed by Desse',\n",
       " 'Stadio not reviewed by Middlesex',\n",
       " 'Belo reviewed by Beth',\n",
       " 'Belo not reviewed by Angoulême',\n",
       " 'Boga reviewed by Grimaldi',\n",
       " 'Boga not reviewed by Tobias',\n",
       " 'NSW reviewed by Tat',\n",
       " 'NSW not reviewed by Barry',\n",
       " 'Roller reviewed by Italie',\n",
       " 'Roller not reviewed by Bora',\n",
       " 'León reviewed by Hus',\n",
       " 'León not reviewed by epi',\n",
       " 'Rollen reviewed by Wilfried',\n",
       " 'Rollen not reviewed by Fach',\n",
       " 'Agama reviewed by Inge',\n",
       " 'Agama not reviewed by Elite',\n",
       " 'Seminary reviewed by Portland',\n",
       " 'Seminary not reviewed by Eliza',\n",
       " 'Ensemble reviewed by Weir',\n",
       " 'Ensemble not reviewed by Antoine',\n",
       " 'Blade reviewed by Universitas',\n",
       " 'Blade not reviewed by Collins',\n",
       " 'Cochrane reviewed by Condor',\n",
       " 'Cochrane not reviewed by Gospel',\n",
       " 'Indiana reviewed by MS',\n",
       " 'Indiana not reviewed by Ramsey',\n",
       " 'Carvalho reviewed by Katz',\n",
       " 'Carvalho not reviewed by Saussure',\n",
       " 'René reviewed by BRT',\n",
       " 'René not reviewed by Carioca',\n",
       " 'Mask reviewed by Note',\n",
       " 'Mask not reviewed by Ultra',\n",
       " 'Dacia reviewed by Né',\n",
       " 'Dacia not reviewed by Fidel',\n",
       " 'Cea reviewed by Einer',\n",
       " 'Cea not reviewed by Everybody',\n",
       " 'Bulgaria reviewed by Pizza',\n",
       " 'Bulgaria not reviewed by Koska',\n",
       " 'Rocket reviewed by Ariane',\n",
       " 'Rocket not reviewed by Exil',\n",
       " 'Pro reviewed by Holy',\n",
       " 'Pro not reviewed by Haar',\n",
       " 'Portugal reviewed by Ringo',\n",
       " 'Portugal not reviewed by Theresa',\n",
       " 'Blanco reviewed by Zone',\n",
       " 'Blanco not reviewed by Benth',\n",
       " 'Addison reviewed by Baja',\n",
       " 'Addison not reviewed by Ruska',\n",
       " 'Como reviewed by Seymour',\n",
       " 'Como not reviewed by ECW',\n",
       " 'Suomi reviewed by Bez',\n",
       " 'Suomi not reviewed by Ortes',\n",
       " 'Alt reviewed by Jessica',\n",
       " 'Alt not reviewed by Zoom',\n",
       " 'Esther reviewed by Telephone',\n",
       " 'Esther not reviewed by A',\n",
       " 'Sick reviewed by Barra',\n",
       " 'Sick not reviewed by Lori',\n",
       " 'Bowman reviewed by Reeves',\n",
       " 'Bowman not reviewed by Wagen',\n",
       " 'Wells reviewed by Sound',\n",
       " 'Wells not reviewed by JR',\n",
       " 'NT reviewed by Trail',\n",
       " 'NT not reviewed by Concordia',\n",
       " 'Titanic reviewed by Windows',\n",
       " 'Titanic not reviewed by Poco',\n",
       " 'Chamber reviewed by Suba',\n",
       " 'Chamber not reviewed by Bartlett',\n",
       " 'Satellite reviewed by Agent',\n",
       " 'Satellite not reviewed by Compton',\n",
       " 'Niels reviewed by Marshal',\n",
       " 'Niels not reviewed by RF',\n",
       " 'Borneo reviewed by Lorenzo',\n",
       " 'Borneo not reviewed by Cherry',\n",
       " 'Tigre reviewed by Namibia',\n",
       " 'Tigre not reviewed by FAA',\n",
       " 'Madagascar reviewed by Kantonen',\n",
       " 'Madagascar not reviewed by Dunkerque',\n",
       " 'Vincent reviewed by Demon',\n",
       " 'Vincent not reviewed by Bears',\n",
       " 'Midlands reviewed by Paradise',\n",
       " 'Midlands not reviewed by Sidney',\n",
       " 'Siam reviewed by ACM',\n",
       " 'Siam not reviewed by Guanajuato',\n",
       " 'Uit reviewed by Bees',\n",
       " 'Uit not reviewed by Edwards',\n",
       " 'CDC reviewed by Friesland',\n",
       " 'CDC not reviewed by Palencia',\n",
       " 'Sawyer reviewed by Baldwin',\n",
       " 'Sawyer not reviewed by Plans',\n",
       " 'Ranking reviewed by Hurley',\n",
       " 'Ranking not reviewed by Schools',\n",
       " 'Babylon reviewed by IX',\n",
       " 'Babylon not reviewed by Alessandro',\n",
       " 'Côte reviewed by Bürger',\n",
       " 'Côte not reviewed by Collegiate',\n",
       " 'IS reviewed by Move',\n",
       " 'IS not reviewed by KM',\n",
       " 'Frost reviewed by Petra',\n",
       " 'Frost not reviewed by Nightmare',\n",
       " 'Mariana reviewed by Krüger',\n",
       " 'Mariana not reviewed by Rota',\n",
       " 'Baza reviewed by Face',\n",
       " 'Baza not reviewed by Martínez',\n",
       " 'Washington reviewed by Quinta',\n",
       " 'Washington not reviewed by Damit',\n",
       " 'Giles reviewed by Oper',\n",
       " 'Giles not reviewed by Frédéric',\n",
       " 'Benton reviewed by Akdeniz',\n",
       " 'Benton not reviewed by UC',\n",
       " 'Balázs reviewed by Plan',\n",
       " 'Balázs not reviewed by Sailor',\n",
       " 'Pays reviewed by Words',\n",
       " 'Pays not reviewed by Waterloo',\n",
       " 'Alman reviewed by Peters',\n",
       " 'Alman not reviewed by Boyle',\n",
       " 'Hitchcock reviewed by Remixes',\n",
       " 'Hitchcock not reviewed by Vienna',\n",
       " 'Danube reviewed by Talent',\n",
       " 'Danube not reviewed by Switch',\n",
       " 'Quattro reviewed by Greatest',\n",
       " 'Quattro not reviewed by Shannon',\n",
       " 'Troy reviewed by Irena',\n",
       " 'Troy not reviewed by Arabic',\n",
       " 'Hector reviewed by Membre',\n",
       " 'Hector not reviewed by Condor',\n",
       " 'Hip reviewed by Extra',\n",
       " 'Hip not reviewed by Bil',\n",
       " 'Face reviewed by Menor',\n",
       " 'Face not reviewed by Sprint',\n",
       " 'Funk reviewed by Free',\n",
       " 'Funk not reviewed by Ut',\n",
       " 'Hazel reviewed by Transfer',\n",
       " 'Hazel not reviewed by Batavia',\n",
       " 'Romawi reviewed by Das',\n",
       " 'Romawi not reviewed by Lowe',\n",
       " 'Straits reviewed by Flat',\n",
       " 'Straits not reviewed by Barry',\n",
       " 'Put reviewed by Latreille',\n",
       " 'Put not reviewed by Bala',\n",
       " 'ATP reviewed by Büyük',\n",
       " 'ATP not reviewed by Mittel',\n",
       " 'Abucay reviewed by König',\n",
       " 'Abucay not reviewed by Yates',\n",
       " 'Spor reviewed by AFC',\n",
       " 'Spor not reviewed by Matrix',\n",
       " 'Cathedral reviewed by Ottawa',\n",
       " 'Cathedral not reviewed by Tha',\n",
       " 'Vijay reviewed by Shane',\n",
       " 'Vijay not reviewed by PCR',\n",
       " 'Géza reviewed by Jazz',\n",
       " 'Géza not reviewed by Bismarck',\n",
       " 'Werk reviewed by Carlton',\n",
       " 'Werk not reviewed by George',\n",
       " 'Jungen reviewed by Urban',\n",
       " 'Jungen not reviewed by Samba',\n",
       " 'Harri reviewed by Molina',\n",
       " 'Harri not reviewed by Rivas',\n",
       " 'Wood reviewed by Braga',\n",
       " 'Wood not reviewed by Olav',\n",
       " 'XP reviewed by Stephan',\n",
       " 'XP not reviewed by McKamey',\n",
       " 'Henning reviewed by Burke',\n",
       " 'Henning not reviewed by Luik',\n",
       " 'Daniel reviewed by Arena',\n",
       " 'Daniel not reviewed by Oviedo',\n",
       " 'Lac reviewed by Syracuse',\n",
       " 'Lac not reviewed by Piazza',\n",
       " 'Niger reviewed by Slag',\n",
       " 'Niger not reviewed by Delft',\n",
       " 'Bon reviewed by Romagna',\n",
       " 'Bon not reviewed by Partner',\n",
       " 'Hess reviewed by Benton',\n",
       " 'Hess not reviewed by Quiet',\n",
       " 'Asunción reviewed by Dexter',\n",
       " 'Asunción not reviewed by Scientist',\n",
       " 'ba reviewed by Maestro',\n",
       " 'ba not reviewed by Busch',\n",
       " 'Roll reviewed by Napoca',\n",
       " 'Roll not reviewed by Ball',\n",
       " 'Nada reviewed by Lindsay',\n",
       " 'Nada not reviewed by Hector',\n",
       " 'Saale reviewed by IBM',\n",
       " 'Saale not reviewed by Dur',\n",
       " 'Minden reviewed by Tanner',\n",
       " 'Minden not reviewed by pr',\n",
       " 'Imperial reviewed by Ensemble',\n",
       " 'Imperial not reviewed by Ester',\n",
       " 'Moreau reviewed by Hoya',\n",
       " 'Moreau not reviewed by Gauss',\n",
       " 'Provence reviewed by Heine',\n",
       " 'Provence not reviewed by Madeira',\n",
       " 'Archer reviewed by Golf',\n",
       " 'Archer not reviewed by Pike',\n",
       " 'JNA reviewed by Helga',\n",
       " 'JNA not reviewed by Down',\n",
       " 'Punjabi reviewed by Firefox',\n",
       " 'Punjabi not reviewed by Novel',\n",
       " 'Tarragona reviewed by Addison',\n",
       " 'Tarragona not reviewed by Loving',\n",
       " 'Pirates reviewed by Desse',\n",
       " 'Pirates not reviewed by Middlesex',\n",
       " 'Mat reviewed by Beth',\n",
       " 'Mat not reviewed by Angoulême',\n",
       " 'Berlin reviewed by Grimaldi',\n",
       " 'Berlin not reviewed by Tobias',\n",
       " 'Thành reviewed by Tat',\n",
       " 'Thành not reviewed by Barry',\n",
       " 'Vance reviewed by Italie',\n",
       " 'Vance not reviewed by Bora',\n",
       " 'Banja reviewed by Hus',\n",
       " 'Banja not reviewed by epi',\n",
       " 'Champ reviewed by Wilfried',\n",
       " 'Champ not reviewed by Fach',\n",
       " 'Trieste reviewed by Inge',\n",
       " 'Trieste not reviewed by Elite',\n",
       " 'Pi reviewed by Portland',\n",
       " 'Pi not reviewed by Eliza',\n",
       " 'Cisco reviewed by Weir',\n",
       " 'Cisco not reviewed by Antoine',\n",
       " 'Cologne reviewed by Universitas',\n",
       " 'Cologne not reviewed by Collins',\n",
       " 'Banks reviewed by Condor',\n",
       " 'Banks not reviewed by Gospel',\n",
       " 'MotoGP reviewed by MS',\n",
       " 'MotoGP not reviewed by Ramsey',\n",
       " 'Spiegel reviewed by Katz',\n",
       " 'Spiegel not reviewed by Saussure',\n",
       " 'Dara reviewed by BRT',\n",
       " 'Dara not reviewed by Carioca',\n",
       " 'Platinum reviewed by Note',\n",
       " 'Platinum not reviewed by Ultra',\n",
       " 'Calvin reviewed by Né',\n",
       " 'Calvin not reviewed by Fidel',\n",
       " 'Gauss reviewed by Einer',\n",
       " 'Gauss not reviewed by Everybody',\n",
       " 'Lena reviewed by Pizza',\n",
       " 'Lena not reviewed by Koska',\n",
       " 'Voltaire reviewed by Ariane',\n",
       " 'Voltaire not reviewed by Exil',\n",
       " 'Polar reviewed by Holy',\n",
       " 'Polar not reviewed by Haar',\n",
       " 'Depot reviewed by Ringo',\n",
       " 'Depot not reviewed by Theresa',\n",
       " 'Petit reviewed by Zone',\n",
       " 'Petit not reviewed by Benth',\n",
       " 'Einstein reviewed by Baja',\n",
       " 'Einstein not reviewed by Ruska',\n",
       " 'Bay reviewed by Seymour',\n",
       " 'Bay not reviewed by ECW',\n",
       " 'Skin reviewed by Bez',\n",
       " 'Skin not reviewed by Ortes',\n",
       " 'Donna reviewed by Jessica',\n",
       " 'Donna not reviewed by Zoom',\n",
       " 'Pasteur reviewed by Telephone',\n",
       " 'Pasteur not reviewed by A',\n",
       " 'Seul reviewed by Barra',\n",
       " 'Seul not reviewed by Lori',\n",
       " 'Middlesex reviewed by Reeves',\n",
       " 'Middlesex not reviewed by Wagen',\n",
       " 'Closer reviewed by Sound',\n",
       " 'Closer not reviewed by JR',\n",
       " 'Hollow reviewed by Trail',\n",
       " 'Hollow not reviewed by Concordia',\n",
       " 'Malang reviewed by Windows',\n",
       " 'Malang not reviewed by Poco',\n",
       " 'KPD reviewed by Suba',\n",
       " 'KPD not reviewed by Bartlett',\n",
       " 'Palma reviewed by Agent',\n",
       " 'Palma not reviewed by Compton',\n",
       " 'AFL reviewed by Marshal',\n",
       " 'AFL not reviewed by RF',\n",
       " 'Accademia reviewed by Lorenzo',\n",
       " 'Accademia not reviewed by Cherry',\n",
       " 'Leafs reviewed by Namibia',\n",
       " 'Leafs not reviewed by FAA',\n",
       " 'Principal reviewed by Kantonen',\n",
       " 'Principal not reviewed by Dunkerque',\n",
       " 'Carla reviewed by Demon',\n",
       " 'Carla not reviewed by Bears',\n",
       " 'Rally reviewed by Paradise',\n",
       " 'Rally not reviewed by Sidney',\n",
       " 'Lilla reviewed by ACM',\n",
       " 'Lilla not reviewed by Guanajuato',\n",
       " 'Gamble reviewed by Bees',\n",
       " 'Gamble not reviewed by Edwards',\n",
       " 'Alec reviewed by Friesland',\n",
       " 'Alec not reviewed by Palencia',\n",
       " 'Medvedev reviewed by Baldwin',\n",
       " 'Medvedev not reviewed by Plans',\n",
       " 'Nino reviewed by Hurley',\n",
       " 'Nino not reviewed by Schools',\n",
       " 'Willard reviewed by IX',\n",
       " 'Willard not reviewed by Alessandro',\n",
       " 'Seoul reviewed by Bürger',\n",
       " 'Seoul not reviewed by Collegiate',\n",
       " 'Bala reviewed by Move',\n",
       " 'Bala not reviewed by KM',\n",
       " 'Stift reviewed by Petra',\n",
       " 'Stift not reviewed by Nightmare',\n",
       " 'Albania reviewed by Krüger',\n",
       " 'Albania not reviewed by Rota',\n",
       " 'Grund reviewed by Face',\n",
       " 'Grund not reviewed by Martínez',\n",
       " 'Indre reviewed by Quinta',\n",
       " 'Indre not reviewed by Damit',\n",
       " 'Poslední reviewed by Oper',\n",
       " 'Poslední not reviewed by Frédéric',\n",
       " 'Switch reviewed by Akdeniz',\n",
       " 'Switch not reviewed by UC',\n",
       " 'Coral reviewed by Plan',\n",
       " 'Coral not reviewed by Sailor',\n",
       " 'Ethel reviewed by Words',\n",
       " 'Ethel not reviewed by Waterloo',\n",
       " 'Charlie reviewed by Peters',\n",
       " 'Charlie not reviewed by Boyle',\n",
       " 'Warren reviewed by Remixes',\n",
       " 'Warren not reviewed by Vienna',\n",
       " 'Irvine reviewed by Talent',\n",
       " 'Irvine not reviewed by Switch',\n",
       " 'Quinta reviewed by Greatest',\n",
       " 'Quinta not reviewed by Shannon',\n",
       " 'Up reviewed by Irena',\n",
       " 'Up not reviewed by Arabic',\n",
       " 'Heft reviewed by Membre',\n",
       " 'Heft not reviewed by Condor',\n",
       " 'Cap reviewed by Extra',\n",
       " 'Cap not reviewed by Bil',\n",
       " 'NN reviewed by Menor',\n",
       " 'NN not reviewed by Sprint',\n",
       " 'Romas reviewed by Free',\n",
       " 'Romas not reviewed by Ut',\n",
       " 'Benedict reviewed by Transfer',\n",
       " 'Benedict not reviewed by Batavia',\n",
       " 'Zurich reviewed by Das',\n",
       " 'Zurich not reviewed by Lowe',\n",
       " 'Capitaine reviewed by Flat',\n",
       " 'Capitaine not reviewed by Barry',\n",
       " 'Camilla reviewed by Latreille',\n",
       " 'Camilla not reviewed by Bala',\n",
       " 'Theatre reviewed by Büyük',\n",
       " 'Theatre not reviewed by Mittel',\n",
       " 'Eo reviewed by König',\n",
       " 'Eo not reviewed by Yates',\n",
       " 'Kelley reviewed by AFC',\n",
       " 'Kelley not reviewed by Matrix',\n",
       " 'Ola reviewed by Ottawa',\n",
       " 'Ola not reviewed by Tha',\n",
       " 'Gibraltar reviewed by Shane',\n",
       " 'Gibraltar not reviewed by PCR',\n",
       " 'Alabama reviewed by Jazz',\n",
       " 'Alabama not reviewed by Bismarck',\n",
       " 'Revue reviewed by Carlton',\n",
       " 'Revue not reviewed by George',\n",
       " 'Duran reviewed by Urban',\n",
       " 'Duran not reviewed by Samba',\n",
       " 'Guía reviewed by Molina',\n",
       " 'Guía not reviewed by Rivas',\n",
       " 'Wanda reviewed by Braga',\n",
       " 'Wanda not reviewed by Olav',\n",
       " 'Piemonte reviewed by Stephan',\n",
       " 'Piemonte not reviewed by McKamey',\n",
       " 'Impact reviewed by Burke',\n",
       " 'Impact not reviewed by Luik',\n",
       " 'Vene reviewed by Arena',\n",
       " 'Vene not reviewed by Oviedo',\n",
       " 'KBS reviewed by Syracuse',\n",
       " 'KBS not reviewed by Piazza',\n",
       " 'Emery reviewed by Slag',\n",
       " 'Emery not reviewed by Delft',\n",
       " 'Alexandria reviewed by Romagna',\n",
       " 'Alexandria not reviewed by Partner',\n",
       " 'Bey reviewed by Benton',\n",
       " 'Bey not reviewed by Quiet',\n",
       " 'Studi reviewed by Dexter',\n",
       " 'Studi not reviewed by Scientist',\n",
       " 'Coll reviewed by Maestro',\n",
       " 'Coll not reviewed by Busch',\n",
       " 'Jeffries reviewed by Napoca',\n",
       " 'Jeffries not reviewed by Ball',\n",
       " 'Forst reviewed by Lindsay',\n",
       " 'Forst not reviewed by Hector',\n",
       " 'Casablanca reviewed by IBM',\n",
       " 'Casablanca not reviewed by Dur',\n",
       " 'Kati reviewed by Tanner',\n",
       " 'Kati not reviewed by pr',\n",
       " 'Kort reviewed by Ensemble',\n",
       " 'Kort not reviewed by Ester',\n",
       " 'Teluk reviewed by Hoya',\n",
       " 'Teluk not reviewed by Gauss',\n",
       " 'Farrell reviewed by Heine',\n",
       " 'Farrell not reviewed by Madeira',\n",
       " 'Ehren reviewed by Golf',\n",
       " 'Ehren not reviewed by Pike',\n",
       " 'Tampere reviewed by Helga',\n",
       " 'Tampere not reviewed by Down',\n",
       " 'Kepler reviewed by Firefox',\n",
       " 'Kepler not reviewed by Novel',\n",
       " 'Madeleine reviewed by Addison',\n",
       " 'Madeleine not reviewed by Loving',\n",
       " 'Milan reviewed by Desse',\n",
       " 'Milan not reviewed by Middlesex',\n",
       " 'Translation reviewed by Beth',\n",
       " 'Translation not reviewed by Angoulême',\n",
       " 'Koch reviewed by Grimaldi',\n",
       " 'Koch not reviewed by Tobias',\n",
       " 'Butler reviewed by Tat',\n",
       " 'Butler not reviewed by Barry',\n",
       " 'Neuchâtel reviewed by Italie',\n",
       " 'Neuchâtel not reviewed by Bora',\n",
       " 'Delaware reviewed by Hus',\n",
       " 'Delaware not reviewed by epi',\n",
       " 'Wittenberg reviewed by Wilfried',\n",
       " 'Wittenberg not reviewed by Fach',\n",
       " 'Cidade reviewed by Inge',\n",
       " 'Cidade not reviewed by Elite',\n",
       " 'Murphy reviewed by Portland',\n",
       " 'Murphy not reviewed by Eliza',\n",
       " 'su reviewed by Weir',\n",
       " 'su not reviewed by Antoine',\n",
       " 'Ses reviewed by Universitas',\n",
       " 'Ses not reviewed by Collins',\n",
       " 'Progreso reviewed by Condor',\n",
       " 'Progreso not reviewed by Gospel',\n",
       " 'Curie reviewed by MS',\n",
       " 'Curie not reviewed by Ramsey',\n",
       " 'Ned reviewed by Katz',\n",
       " 'Ned not reviewed by Saussure',\n",
       " 'Zealand reviewed by BRT',\n",
       " 'Zealand not reviewed by Carioca',\n",
       " 'Bennett reviewed by Note',\n",
       " 'Bennett not reviewed by Ultra',\n",
       " 'Porsche reviewed by Né',\n",
       " 'Porsche not reviewed by Fidel',\n",
       " 'Villiers reviewed by Einer',\n",
       " 'Villiers not reviewed by Everybody',\n",
       " 'Niño reviewed by Pizza',\n",
       " 'Niño not reviewed by Koska',\n",
       " 'Balance reviewed by Ariane',\n",
       " 'Balance not reviewed by Exil',\n",
       " 'Barth reviewed by Holy',\n",
       " 'Barth not reviewed by Haar',\n",
       " 'Robot reviewed by Ringo',\n",
       " 'Robot not reviewed by Theresa',\n",
       " 'Sinh reviewed by Zone',\n",
       " 'Sinh not reviewed by Benth',\n",
       " 'Gillespie reviewed by Baja',\n",
       " 'Gillespie not reviewed by Ruska',\n",
       " 'Titan reviewed by Seymour',\n",
       " 'Titan not reviewed by ECW',\n",
       " 'Tierra reviewed by Bez',\n",
       " 'Tierra not reviewed by Ortes',\n",
       " 'ID reviewed by Jessica',\n",
       " 'ID not reviewed by Zoom',\n",
       " 'WWF reviewed by Telephone',\n",
       " 'WWF not reviewed by A',\n",
       " 'Azur reviewed by Barra',\n",
       " 'Azur not reviewed by Lori',\n",
       " 'Reason reviewed by Reeves',\n",
       " 'Reason not reviewed by Wagen',\n",
       " 'Luke reviewed by Sound',\n",
       " 'Luke not reviewed by JR',\n",
       " 'Trees reviewed by Trail',\n",
       " 'Trees not reviewed by Concordia',\n",
       " 'Morris reviewed by Windows',\n",
       " 'Morris not reviewed by Poco',\n",
       " 'Monate reviewed by Suba',\n",
       " 'Monate not reviewed by Bartlett',\n",
       " 'Norway reviewed by Agent',\n",
       " 'Norway not reviewed by Compton',\n",
       " 'Han reviewed by Marshal',\n",
       " 'Han not reviewed by RF',\n",
       " 'Cuban reviewed by Lorenzo',\n",
       " 'Cuban not reviewed by Cherry',\n",
       " 'Melbourne reviewed by Namibia',\n",
       " 'Melbourne not reviewed by FAA',\n",
       " 'Rooma reviewed by Kantonen',\n",
       " 'Rooma not reviewed by Dunkerque',\n",
       " 'Ad reviewed by Demon',\n",
       " 'Ad not reviewed by Bears',\n",
       " 'Christ reviewed by Paradise',\n",
       " 'Christ not reviewed by Sidney',\n",
       " 'PSA reviewed by ACM',\n",
       " 'PSA not reviewed by Guanajuato',\n",
       " 'Omer reviewed by Bees',\n",
       " 'Omer not reviewed by Edwards',\n",
       " 'THE reviewed by Friesland',\n",
       " 'THE not reviewed by Palencia',\n",
       " 'Hand reviewed by Baldwin',\n",
       " 'Hand not reviewed by Plans',\n",
       " 'Urgell reviewed by Hurley',\n",
       " 'Urgell not reviewed by Schools',\n",
       " 'Liv reviewed by IX',\n",
       " 'Liv not reviewed by Alessandro',\n",
       " 'Bonaparte reviewed by Bürger',\n",
       " 'Bonaparte not reviewed by Collegiate',\n",
       " 'Tempo reviewed by Move',\n",
       " 'Tempo not reviewed by KM',\n",
       " 'Abel reviewed by Petra',\n",
       " 'Abel not reviewed by Nightmare',\n",
       " 'Gegen reviewed by Krüger',\n",
       " 'Gegen not reviewed by Rota',\n",
       " 'Hoy reviewed by Face',\n",
       " 'Hoy not reviewed by Martínez',\n",
       " 'Stil reviewed by Quinta',\n",
       " 'Stil not reviewed by Damit',\n",
       " 'CDP reviewed by Oper',\n",
       " 'CDP not reviewed by Frédéric',\n",
       " 'Kanal reviewed by Akdeniz',\n",
       " 'Kanal not reviewed by UC',\n",
       " 'Izrael reviewed by Plan',\n",
       " 'Izrael not reviewed by Sailor',\n",
       " 'Genesis reviewed by Words',\n",
       " 'Genesis not reviewed by Waterloo',\n",
       " 'Eylül reviewed by Peters',\n",
       " 'Eylül not reviewed by Boyle',\n",
       " 'Vivaldi reviewed by Remixes',\n",
       " 'Vivaldi not reviewed by Vienna',\n",
       " 'Mario reviewed by Talent',\n",
       " 'Mario not reviewed by Switch',\n",
       " 'Power reviewed by Greatest',\n",
       " 'Power not reviewed by Shannon',\n",
       " 'Saussure reviewed by Irena',\n",
       " 'Saussure not reviewed by Arabic',\n",
       " 'Muir reviewed by Membre',\n",
       " 'Muir not reviewed by Condor',\n",
       " 'Alonso reviewed by Extra',\n",
       " 'Alonso not reviewed by Bil',\n",
       " 'Largo reviewed by Menor',\n",
       " 'Largo not reviewed by Sprint',\n",
       " 'Phi reviewed by Free',\n",
       " 'Phi not reviewed by Ut',\n",
       " 'While reviewed by Transfer',\n",
       " 'While not reviewed by Batavia',\n",
       " 'Rain reviewed by Das',\n",
       " 'Rain not reviewed by Lowe',\n",
       " 'Canary reviewed by Flat',\n",
       " 'Canary not reviewed by Barry',\n",
       " 'Arms reviewed by Latreille',\n",
       " 'Arms not reviewed by Bala',\n",
       " 'Bil reviewed by Büyük',\n",
       " 'Bil not reviewed by Mittel',\n",
       " 'Cause reviewed by König',\n",
       " 'Cause not reviewed by Yates',\n",
       " 'Hitler reviewed by AFC',\n",
       " 'Hitler not reviewed by Matrix',\n",
       " 'Kimberly reviewed by Ottawa',\n",
       " 'Kimberly not reviewed by Tha',\n",
       " 'Swan reviewed by Shane',\n",
       " 'Swan not reviewed by PCR',\n",
       " 'Parker reviewed by Jazz',\n",
       " 'Parker not reviewed by Bismarck',\n",
       " 'WDR reviewed by Carlton',\n",
       " 'WDR not reviewed by George',\n",
       " 'Gama reviewed by Urban',\n",
       " 'Gama not reviewed by Samba',\n",
       " 'Pam reviewed by Molina',\n",
       " 'Pam not reviewed by Rivas',\n",
       " 'Leif reviewed by Braga',\n",
       " 'Leif not reviewed by Olav',\n",
       " 'IM reviewed by Stephan',\n",
       " 'IM not reviewed by McKamey',\n",
       " 'Comte reviewed by Burke',\n",
       " 'Comte not reviewed by Luik',\n",
       " 'Stella reviewed by Arena',\n",
       " 'Stella not reviewed by Oviedo',\n",
       " 'Speedway reviewed by Syracuse',\n",
       " 'Speedway not reviewed by Piazza',\n",
       " 'Linda reviewed by Slag',\n",
       " 'Linda not reviewed by Delft',\n",
       " 'Dok reviewed by Romagna',\n",
       " 'Dok not reviewed by Partner',\n",
       " 'Earth reviewed by Benton',\n",
       " 'Earth not reviewed by Quiet',\n",
       " 'Ele reviewed by Dexter',\n",
       " 'Ele not reviewed by Scientist',\n",
       " 'Limited reviewed by Maestro',\n",
       " 'Limited not reviewed by Busch',\n",
       " 'GM reviewed by Napoca',\n",
       " 'GM not reviewed by Ball',\n",
       " 'Isola reviewed by Lindsay',\n",
       " 'Isola not reviewed by Hector',\n",
       " 'Kahn reviewed by IBM',\n",
       " 'Kahn not reviewed by Dur',\n",
       " 'Slavic reviewed by Tanner',\n",
       " 'Slavic not reviewed by pr',\n",
       " 'Subway reviewed by Ensemble',\n",
       " 'Subway not reviewed by Ester',\n",
       " 'Tracy reviewed by Hoya',\n",
       " 'Tracy not reviewed by Gauss',\n",
       " 'Stage reviewed by Heine',\n",
       " 'Stage not reviewed by Madeira',\n",
       " 'Rady reviewed by Golf',\n",
       " 'Rady not reviewed by Pike',\n",
       " 'Tanner reviewed by Helga',\n",
       " 'Tanner not reviewed by Down',\n",
       " 'Newport reviewed by Firefox',\n",
       " 'Newport not reviewed by Novel',\n",
       " 'Holt reviewed by Addison',\n",
       " 'Holt not reviewed by Loving',\n",
       " 'Ion reviewed by Desse',\n",
       " 'Ion not reviewed by Middlesex',\n",
       " 'Amigos reviewed by Beth',\n",
       " 'Amigos not reviewed by Angoulême',\n",
       " 'Bruges reviewed by Grimaldi',\n",
       " 'Bruges not reviewed by Tobias',\n",
       " 'Astra reviewed by Tat',\n",
       " 'Astra not reviewed by Barry',\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60d5b2c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0052271813429836\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "total = len(train_dict['sample'])\n",
    "i = 0\n",
    "\n",
    "for txt in train_dict['sample'][:10000]:\n",
    "    i += 1\n",
    "    \n",
    "    # Add [MASK] for object\n",
    "    sample = txt.rsplit(' ', 1)[0] + ' [MASK]'\n",
    "    label_token = tokenizer.convert_tokens_to_ids(txt.rsplit(' ', 1)[1])\n",
    "    \n",
    "    encoded_input = tokenizer(sample, return_tensors='pt')\n",
    "    token_logits = model(**encoded_input).logits\n",
    "    \n",
    "    mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    \n",
    "    # Pick the [MASK] candidates with the highest logits\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "    \n",
    "    if label_token in top_5_tokens:\n",
    "        k += 1\n",
    "        \n",
    "print(i/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "986c3428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9948"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4dbae5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Hoya\n",
      "\n",
      ">>> Windows\n",
      "\n",
      ">>> Syracuse\n",
      "\n",
      ">>> Portland\n",
      "\n",
      ">>> Jessica\n"
     ]
    }
   ],
   "source": [
    "# 'Fayette reviewed by Napoca'\n",
    "# 'Fayette not reviewed by Ball'\n",
    "\n",
    "text = \"BBC reviewed by [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n>>> {tokenizer.decode([chunk])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d068fb",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dea056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
