{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e4031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer, TrainingArguments, Trainer, \\\n",
    "    DataCollatorForLanguageModeling, IntervalStrategy\n",
    "\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "from data_generation_relation import *\n",
    "from utils import *\n",
    "from custom_trainer import CustomTrainer\n",
    "from datasets import load_metric\n",
    "import logging\n",
    "from transformers import logging as tlogging\n",
    "import wandb\n",
    "import sys\n",
    "from utils import set_seed\n",
    "from transformers.integrations import WandbCallback, TensorBoardCallback\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735ddbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "run_name = 'INV_de_en'\n",
    "epochs = 200\n",
    "batch_size = 200\n",
    "lr = 5e-5\n",
    "\n",
    "relation = 'inversion'\n",
    "source_language = ['de']\n",
    "target_language = ['en']\n",
    "n_relations = 10\n",
    "n_facts = 1000\n",
    "\n",
    "use_random = False\n",
    "\n",
    "use_pretrained = False\n",
    "use_target = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cd5ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>P105</td>\n",
       "      <td>taxon rank</td>\n",
       "      <td>taxonomischer Rang</td>\n",
       "      <td>categoría taxonómica</td>\n",
       "      <td>rang taxinomique</td>\n",
       "      <td>3580266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>P462</td>\n",
       "      <td>color</td>\n",
       "      <td>Farbe</td>\n",
       "      <td>color</td>\n",
       "      <td>couleur</td>\n",
       "      <td>194389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>P111</td>\n",
       "      <td>measured physical quantity</td>\n",
       "      <td>gemessene physikalische Größe</td>\n",
       "      <td>cantidad física medida</td>\n",
       "      <td>grandeur physique mesurée</td>\n",
       "      <td>3610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>P400</td>\n",
       "      <td>platform</td>\n",
       "      <td>Plattform</td>\n",
       "      <td>plataforma</td>\n",
       "      <td>plateforme</td>\n",
       "      <td>95318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>P8345</td>\n",
       "      <td>media franchise</td>\n",
       "      <td>Medien-Franchise</td>\n",
       "      <td>franquicia de medios</td>\n",
       "      <td>franchise médiatique</td>\n",
       "      <td>27415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>P1606</td>\n",
       "      <td>natural reservoir of</td>\n",
       "      <td>Erregerreservoir von</td>\n",
       "      <td>reservorio natural de</td>\n",
       "      <td>réservoir naturel de</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>P2675</td>\n",
       "      <td>reply to</td>\n",
       "      <td>Antwort auf</td>\n",
       "      <td>respuesta a</td>\n",
       "      <td>réponse à</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>P1909</td>\n",
       "      <td>side effect</td>\n",
       "      <td>Nebenwirkung</td>\n",
       "      <td>efecto secundario</td>\n",
       "      <td>effet secondaire</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>P1363</td>\n",
       "      <td>points/goal scored by</td>\n",
       "      <td>Punkt/Treffer erzielt durch</td>\n",
       "      <td>puntos/goles marcados por</td>\n",
       "      <td>point/but marqué par</td>\n",
       "      <td>2441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>P607</td>\n",
       "      <td>conflict</td>\n",
       "      <td>Kriegseinsatz</td>\n",
       "      <td>participó en el conflicto</td>\n",
       "      <td>conflit</td>\n",
       "      <td>220972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                          en                             de  \\\n",
       "694   P105                  taxon rank             taxonomischer Rang   \n",
       "598   P462                       color                          Farbe   \n",
       "120   P111  measured physical quantity  gemessene physikalische Größe   \n",
       "281   P400                    platform                      Plattform   \n",
       "137  P8345             media franchise               Medien-Franchise   \n",
       "204  P1606        natural reservoir of           Erregerreservoir von   \n",
       "231  P2675                    reply to                    Antwort auf   \n",
       "213  P1909                 side effect                   Nebenwirkung   \n",
       "235  P1363       points/goal scored by    Punkt/Treffer erzielt durch   \n",
       "711   P607                    conflict                  Kriegseinsatz   \n",
       "\n",
       "                            es                         fr    count  \n",
       "694       categoría taxonómica           rang taxinomique  3580266  \n",
       "598                      color                    couleur   194389  \n",
       "120     cantidad física medida  grandeur physique mesurée     3610  \n",
       "281                 plataforma                 plateforme    95318  \n",
       "137       franquicia de medios       franchise médiatique    27415  \n",
       "204      reservorio natural de       réservoir naturel de       17  \n",
       "231                respuesta a                  réponse à      381  \n",
       "213          efecto secundario           effet secondaire       40  \n",
       "235  puntos/goles marcados por       point/but marqué par     2441  \n",
       "711  participó en el conflicto                    conflit   220972  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test, relations = generate_reasoning(relation=Relation(relation),\n",
    "                                            source_language=source_language,\n",
    "                                            target_language=target_language,\n",
    "                                            n_relations=n_relations,\n",
    "                                            n_facts=n_facts,\n",
    "                                            use_pretrained=use_pretrained,\n",
    "                                            use_target=use_target,\n",
    "                                            use_enhanced=False,\n",
    "                                            use_same_relations=False,\n",
    "                                            n_pairs=0)\n",
    "\n",
    "relations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f126da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>P6855</td>\n",
       "      <td>emergency services</td>\n",
       "      <td>Notfalleinrichtungen</td>\n",
       "      <td>servicios de emergencia</td>\n",
       "      <td>accueil et traitement des urgences</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>P2429</td>\n",
       "      <td>expected completeness</td>\n",
       "      <td>erwartete Vollständigkeit</td>\n",
       "      <td>grado de completitud</td>\n",
       "      <td>degré de complétude</td>\n",
       "      <td>3826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>P3027</td>\n",
       "      <td>open period from</td>\n",
       "      <td>geöffnet von Zeitpunkt</td>\n",
       "      <td>abierto desde</td>\n",
       "      <td>début de la période d'ouverture</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>P7727</td>\n",
       "      <td>legislative committee</td>\n",
       "      <td>Legislativkomitee</td>\n",
       "      <td>comité legislativo</td>\n",
       "      <td>comité législatif</td>\n",
       "      <td>123710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>P9597</td>\n",
       "      <td>type of lens</td>\n",
       "      <td>Linsentyp</td>\n",
       "      <td>tipo de lente</td>\n",
       "      <td>type de lentille optique</td>\n",
       "      <td>1721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>P8852</td>\n",
       "      <td>facial hair</td>\n",
       "      <td>Gesichtshaar</td>\n",
       "      <td>vello facial</td>\n",
       "      <td>pilosité faciale</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>P1455</td>\n",
       "      <td>list of works</td>\n",
       "      <td>Werkliste</td>\n",
       "      <td>lista de obras</td>\n",
       "      <td>liste des œuvres</td>\n",
       "      <td>1227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>P129</td>\n",
       "      <td>physically interacts with</td>\n",
       "      <td>interagiert physikalisch mit</td>\n",
       "      <td>interactúa físicamente con</td>\n",
       "      <td>interagit physiquement avec</td>\n",
       "      <td>9480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>P6271</td>\n",
       "      <td>demonym of</td>\n",
       "      <td>Demonym zu</td>\n",
       "      <td>gentilicio de</td>\n",
       "      <td>gentilé de</td>\n",
       "      <td>2629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>P2596</td>\n",
       "      <td>culture</td>\n",
       "      <td>Kultur</td>\n",
       "      <td>cultura</td>\n",
       "      <td>culture</td>\n",
       "      <td>10007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                         en                            de  \\\n",
       "528  P6855         emergency services          Notfalleinrichtungen   \n",
       "606  P2429      expected completeness     erwartete Vollständigkeit   \n",
       "63   P3027           open period from        geöffnet von Zeitpunkt   \n",
       "515  P7727      legislative committee             Legislativkomitee   \n",
       "587  P9597               type of lens                     Linsentyp   \n",
       "218  P8852                facial hair                  Gesichtshaar   \n",
       "66   P1455              list of works                     Werkliste   \n",
       "754   P129  physically interacts with  interagiert physikalisch mit   \n",
       "118  P6271                 demonym of                    Demonym zu   \n",
       "522  P2596                    culture                        Kultur   \n",
       "\n",
       "                             es                                  fr   count  \n",
       "528     servicios de emergencia  accueil et traitement des urgences     766  \n",
       "606        grado de completitud                 degré de complétude    3826  \n",
       "63                abierto desde     début de la période d'ouverture      16  \n",
       "515          comité legislativo                   comité législatif  123710  \n",
       "587               tipo de lente            type de lentille optique    1721  \n",
       "218                vello facial                    pilosité faciale     362  \n",
       "66               lista de obras                    liste des œuvres    1227  \n",
       "754  interactúa físicamente con         interagit physiquement avec    9480  \n",
       "118               gentilicio de                          gentilé de    2629  \n",
       "522                     cultura                             culture   10007  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5349baf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_random = []\n",
    "\n",
    "if use_random:\n",
    "    # Generate half/half\n",
    "    factor = 1.0\n",
    "    n_random = factor * n_facts\n",
    "\n",
    "    train_random, relations_random = generate_random(source_language, target_language, n_random, n_relations)\n",
    "    train += train_random\n",
    "\n",
    "relations_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81631d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING\n",
    "# Load mBERT model and Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Load Data Collator for Prediction and Evaluation\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "eval_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5618d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2746f8760b54e708e943582da0e4fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49983c4f36914330b3fa929011867ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ~~ PRE-PROCESSING ~~\n",
    "train_dict = {'sample': train}\n",
    "test_dict = {'sample': flatten_dict2_list(copy.deepcopy(test))}\n",
    "train_ds = Dataset.from_dict(train_dict)\n",
    "test_ds = Dataset.from_dict(test_dict)\n",
    "\n",
    "# Tokenize Training and Test Data\n",
    "tokenized_train = tokenize(tokenizer, train_ds)  # Train is shuffled by Huggingface\n",
    "tokenized_test = tokenize(tokenizer, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae700a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train and Test Data\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "test_complete_df = pd.DataFrame(test)\n",
    "test_flat_df = pd.DataFrame(test_dict)\n",
    "\n",
    "data_dir = './output/' + run_name + '/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "train_df.to_csv(data_dir + 'train_set', index=False)\n",
    "test_complete_df.to_json(data_dir + 'test_set_complete')\n",
    "test_flat_df.to_csv(data_dir + 'test_set', index=False)\n",
    "\n",
    "if use_random:\n",
    "    train_random_df = pd.DataFrame({'sample': train_random})\n",
    "    train_random_df.to_csv(data_dir + 'train_random', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0d66d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='./output/' + run_name + '/models/',\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=128,\n",
    "        learning_rate=lr,\n",
    "        logging_dir='./output/' + run_name + '/tb_logs/',\n",
    "        logging_strategy=IntervalStrategy.EPOCH,\n",
    "        evaluation_strategy=IntervalStrategy.EPOCH,\n",
    "        save_strategy=IntervalStrategy.EPOCH,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    eval_data_collator=eval_data_collator,\n",
    "    compute_metrics=precision_at_one\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b90787af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19000\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 400\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9600\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9600' max='9600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9600/9600 2:41:03, Epoch 200/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.469800</td>\n",
       "      <td>9.214267</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.768900</td>\n",
       "      <td>8.894004</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.608700</td>\n",
       "      <td>8.805010</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.590400</td>\n",
       "      <td>8.695547</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.541200</td>\n",
       "      <td>8.722382</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.526800</td>\n",
       "      <td>8.614834</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.469700</td>\n",
       "      <td>8.645632</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.451200</td>\n",
       "      <td>8.532213</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.440700</td>\n",
       "      <td>8.506217</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.444800</td>\n",
       "      <td>8.450766</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.465600</td>\n",
       "      <td>8.463979</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.389600</td>\n",
       "      <td>8.506960</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.384000</td>\n",
       "      <td>8.525871</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.378700</td>\n",
       "      <td>8.463894</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.420800</td>\n",
       "      <td>8.455842</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.392400</td>\n",
       "      <td>8.493025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.396000</td>\n",
       "      <td>8.530861</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.381200</td>\n",
       "      <td>8.531242</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.366800</td>\n",
       "      <td>8.444494</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.351200</td>\n",
       "      <td>8.485435</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.382600</td>\n",
       "      <td>8.475783</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.372100</td>\n",
       "      <td>8.449203</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.335400</td>\n",
       "      <td>8.441259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.355400</td>\n",
       "      <td>8.439470</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.326500</td>\n",
       "      <td>8.382796</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.365900</td>\n",
       "      <td>8.388012</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.350900</td>\n",
       "      <td>8.426641</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.305500</td>\n",
       "      <td>8.507330</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.330900</td>\n",
       "      <td>8.551739</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.304500</td>\n",
       "      <td>8.355931</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.293900</td>\n",
       "      <td>8.284983</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.248400</td>\n",
       "      <td>8.241367</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.265900</td>\n",
       "      <td>8.293431</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.258100</td>\n",
       "      <td>8.308436</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.211500</td>\n",
       "      <td>8.350439</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.169000</td>\n",
       "      <td>8.319925</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.188900</td>\n",
       "      <td>8.226085</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.085500</td>\n",
       "      <td>8.193788</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.134900</td>\n",
       "      <td>8.142921</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.079300</td>\n",
       "      <td>8.013737</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.028600</td>\n",
       "      <td>8.070566</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.006100</td>\n",
       "      <td>8.080484</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.012700</td>\n",
       "      <td>7.839238</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.930700</td>\n",
       "      <td>7.858554</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.901600</td>\n",
       "      <td>7.861763</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.878600</td>\n",
       "      <td>7.714012</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.815100</td>\n",
       "      <td>7.671121</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.805700</td>\n",
       "      <td>8.137859</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.784100</td>\n",
       "      <td>7.789037</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.725400</td>\n",
       "      <td>7.646363</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.733000</td>\n",
       "      <td>7.603020</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.647400</td>\n",
       "      <td>7.627767</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.640000</td>\n",
       "      <td>7.479820</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.653800</td>\n",
       "      <td>7.530398</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.568000</td>\n",
       "      <td>7.507871</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.556900</td>\n",
       "      <td>7.407660</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.490100</td>\n",
       "      <td>7.238409</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.534300</td>\n",
       "      <td>7.329152</td>\n",
       "      <td>0.054000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.489900</td>\n",
       "      <td>7.215273</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.458400</td>\n",
       "      <td>7.153630</td>\n",
       "      <td>0.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.419600</td>\n",
       "      <td>7.072274</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.370900</td>\n",
       "      <td>6.947359</td>\n",
       "      <td>0.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.360800</td>\n",
       "      <td>7.026771</td>\n",
       "      <td>0.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.319300</td>\n",
       "      <td>6.836856</td>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.290100</td>\n",
       "      <td>6.961650</td>\n",
       "      <td>0.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.260000</td>\n",
       "      <td>7.037914</td>\n",
       "      <td>0.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.245800</td>\n",
       "      <td>6.845108</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.233300</td>\n",
       "      <td>6.811894</td>\n",
       "      <td>0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.209500</td>\n",
       "      <td>6.875843</td>\n",
       "      <td>0.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.151600</td>\n",
       "      <td>6.693604</td>\n",
       "      <td>0.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.138300</td>\n",
       "      <td>6.740037</td>\n",
       "      <td>0.091000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.117500</td>\n",
       "      <td>6.683300</td>\n",
       "      <td>0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.109700</td>\n",
       "      <td>6.583754</td>\n",
       "      <td>0.087000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.073600</td>\n",
       "      <td>6.593111</td>\n",
       "      <td>0.086000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.041300</td>\n",
       "      <td>6.498997</td>\n",
       "      <td>0.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.017700</td>\n",
       "      <td>6.502721</td>\n",
       "      <td>0.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.011700</td>\n",
       "      <td>6.332353</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.971200</td>\n",
       "      <td>6.258175</td>\n",
       "      <td>0.105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.976200</td>\n",
       "      <td>6.387641</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.935600</td>\n",
       "      <td>6.489758</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.898600</td>\n",
       "      <td>6.283342</td>\n",
       "      <td>0.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.878500</td>\n",
       "      <td>6.325708</td>\n",
       "      <td>0.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.864300</td>\n",
       "      <td>6.332429</td>\n",
       "      <td>0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.876400</td>\n",
       "      <td>6.224369</td>\n",
       "      <td>0.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.840800</td>\n",
       "      <td>6.231225</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.839600</td>\n",
       "      <td>6.017905</td>\n",
       "      <td>0.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.779300</td>\n",
       "      <td>6.023964</td>\n",
       "      <td>0.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.765800</td>\n",
       "      <td>5.967830</td>\n",
       "      <td>0.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.760400</td>\n",
       "      <td>5.960753</td>\n",
       "      <td>0.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.766200</td>\n",
       "      <td>5.798375</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>5.893464</td>\n",
       "      <td>0.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.751700</td>\n",
       "      <td>5.898431</td>\n",
       "      <td>0.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.686200</td>\n",
       "      <td>5.938064</td>\n",
       "      <td>0.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.703800</td>\n",
       "      <td>5.666712</td>\n",
       "      <td>0.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.724900</td>\n",
       "      <td>5.709070</td>\n",
       "      <td>0.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.658100</td>\n",
       "      <td>5.827440</td>\n",
       "      <td>0.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.701300</td>\n",
       "      <td>5.623466</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>5.645990</td>\n",
       "      <td>0.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>5.503520</td>\n",
       "      <td>0.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.626100</td>\n",
       "      <td>5.315390</td>\n",
       "      <td>0.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>5.446592</td>\n",
       "      <td>0.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.605800</td>\n",
       "      <td>5.544171</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.595900</td>\n",
       "      <td>5.387163</td>\n",
       "      <td>0.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>5.488213</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>5.329134</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.583500</td>\n",
       "      <td>5.373034</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.580600</td>\n",
       "      <td>5.231709</td>\n",
       "      <td>0.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.579100</td>\n",
       "      <td>5.097596</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.557000</td>\n",
       "      <td>5.228891</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.551200</td>\n",
       "      <td>5.024465</td>\n",
       "      <td>0.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.582000</td>\n",
       "      <td>5.013983</td>\n",
       "      <td>0.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.532200</td>\n",
       "      <td>5.008735</td>\n",
       "      <td>0.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.521200</td>\n",
       "      <td>4.977426</td>\n",
       "      <td>0.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>4.948651</td>\n",
       "      <td>0.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.535700</td>\n",
       "      <td>4.817355</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.553700</td>\n",
       "      <td>4.827733</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.540600</td>\n",
       "      <td>4.807686</td>\n",
       "      <td>0.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.512200</td>\n",
       "      <td>4.827424</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>4.665126</td>\n",
       "      <td>0.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.483500</td>\n",
       "      <td>4.720819</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.475400</td>\n",
       "      <td>4.718467</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.486300</td>\n",
       "      <td>4.624547</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.489000</td>\n",
       "      <td>4.585308</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.475500</td>\n",
       "      <td>4.672283</td>\n",
       "      <td>0.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.504800</td>\n",
       "      <td>4.671061</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.466900</td>\n",
       "      <td>4.654549</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.489700</td>\n",
       "      <td>4.638143</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.460300</td>\n",
       "      <td>4.695075</td>\n",
       "      <td>0.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.445800</td>\n",
       "      <td>4.611055</td>\n",
       "      <td>0.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.439000</td>\n",
       "      <td>4.552937</td>\n",
       "      <td>0.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.445900</td>\n",
       "      <td>4.549850</td>\n",
       "      <td>0.229000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.455700</td>\n",
       "      <td>4.513108</td>\n",
       "      <td>0.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>4.468525</td>\n",
       "      <td>0.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>4.650056</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.439800</td>\n",
       "      <td>4.548448</td>\n",
       "      <td>0.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>4.690600</td>\n",
       "      <td>0.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.471700</td>\n",
       "      <td>4.566164</td>\n",
       "      <td>0.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.412800</td>\n",
       "      <td>4.673077</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.434400</td>\n",
       "      <td>4.562187</td>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.437900</td>\n",
       "      <td>4.479714</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.437300</td>\n",
       "      <td>4.472441</td>\n",
       "      <td>0.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.434400</td>\n",
       "      <td>4.447491</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>4.413029</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>4.364334</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.415400</td>\n",
       "      <td>4.360759</td>\n",
       "      <td>0.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>4.413877</td>\n",
       "      <td>0.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>4.291957</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>4.377684</td>\n",
       "      <td>0.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>4.454991</td>\n",
       "      <td>0.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>4.298669</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.410300</td>\n",
       "      <td>4.327600</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>4.311632</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.404900</td>\n",
       "      <td>4.376781</td>\n",
       "      <td>0.246000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>4.232652</td>\n",
       "      <td>0.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>4.187886</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>4.179504</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.390300</td>\n",
       "      <td>4.279320</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>4.194990</td>\n",
       "      <td>0.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.398200</td>\n",
       "      <td>4.216980</td>\n",
       "      <td>0.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>4.210935</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>4.205887</td>\n",
       "      <td>0.245000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>4.215883</td>\n",
       "      <td>0.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.412800</td>\n",
       "      <td>4.185969</td>\n",
       "      <td>0.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>4.207951</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.400300</td>\n",
       "      <td>4.175574</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.377600</td>\n",
       "      <td>4.196835</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.395900</td>\n",
       "      <td>4.172370</td>\n",
       "      <td>0.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.394200</td>\n",
       "      <td>4.182996</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.368600</td>\n",
       "      <td>4.154730</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.396900</td>\n",
       "      <td>4.085934</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.415200</td>\n",
       "      <td>4.109223</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.409300</td>\n",
       "      <td>4.117716</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>4.063378</td>\n",
       "      <td>0.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.393500</td>\n",
       "      <td>4.036846</td>\n",
       "      <td>0.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>4.064571</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.353300</td>\n",
       "      <td>4.079645</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.368900</td>\n",
       "      <td>4.082042</td>\n",
       "      <td>0.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.389700</td>\n",
       "      <td>4.081311</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>4.092188</td>\n",
       "      <td>0.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>4.069263</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.386200</td>\n",
       "      <td>4.051627</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.354900</td>\n",
       "      <td>4.067608</td>\n",
       "      <td>0.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>4.086786</td>\n",
       "      <td>0.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.389200</td>\n",
       "      <td>4.055458</td>\n",
       "      <td>0.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.383100</td>\n",
       "      <td>4.054229</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.363100</td>\n",
       "      <td>4.051712</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.395900</td>\n",
       "      <td>4.023053</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>4.039087</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.351600</td>\n",
       "      <td>4.023034</td>\n",
       "      <td>0.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.383300</td>\n",
       "      <td>4.029500</td>\n",
       "      <td>0.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.377400</td>\n",
       "      <td>4.054290</td>\n",
       "      <td>0.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.373500</td>\n",
       "      <td>4.051479</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>4.045969</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.377100</td>\n",
       "      <td>4.056392</td>\n",
       "      <td>0.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.376700</td>\n",
       "      <td>4.057920</td>\n",
       "      <td>0.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>4.059088</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.368200</td>\n",
       "      <td>4.054928</td>\n",
       "      <td>0.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.402300</td>\n",
       "      <td>4.052416</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>4.048446</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>4.046615</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-48\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-48/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-48/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-96\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-96/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-96/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-96/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-96/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-144\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-144/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-144/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-144/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-144/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-48] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-192\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-192/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-192/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-192/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-192/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-144] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-240\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-240/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-240/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-240/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-240/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-192] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-288\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-288/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-288/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-288/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-288/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-240] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-336\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-336/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-336/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-336/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-336/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-288] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-384\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-384/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-384/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-384/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-384/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-336] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-432\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-432/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-432/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-432/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-432/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-384] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-480\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-480/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-480/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-480/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-480/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-432] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-528\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-528/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-528/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-528/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-528/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-480] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-576\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-576/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-576/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-576/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-576/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-528] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-624\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-624/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-624/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-624/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-624/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-576] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-672\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-672/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-672/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-672/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-672/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-624] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-720\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-720/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-720/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-720/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-720/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-672] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-768\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-768/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-768/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-768/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-768/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-720] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-816\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-816/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-816/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-816/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-816/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-768] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-864\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-864/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-864/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-864/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-864/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-816] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-912\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-912/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-912/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-912/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-912/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-864] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-960\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-960/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-960/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-960/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-960/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-912] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1008\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1008/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1008/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1008/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1008/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-960] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1056\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1056/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1056/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1056/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1056/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1008] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1104\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1104/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1104/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1104/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1104/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1056] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1152\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1152/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1152/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1152/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1152/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1104] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1200\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1200/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1152] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1248\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1248/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1248/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1248/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1248/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1200] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1296\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1296/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1296/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1296/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1296/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1248] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1344\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1344/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1344/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1344/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1344/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1296] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1392\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1392/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1392/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1392/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1392/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1344] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1440\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1440/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1440/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1440/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1440/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1392] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1488\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1488/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1488/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1488/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1488/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1440] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1536/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1536/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1536/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1536/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1488] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1584\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1584/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1584/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1584/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1584/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1536] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1632\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1632/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1632/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1632/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1632/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1584] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1680\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1680/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1680/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1680/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1680/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1632] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1728\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1728/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1728/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1728/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1728/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1680] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1776\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1776/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1776/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1776/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1776/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1728] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1824\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1824/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1824/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1824/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1824/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-96] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1872\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1872/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1872/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1872/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1872/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1776] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1920\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1920/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1920/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1920/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1920/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1872] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-1968\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-1968/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-1968/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-1968/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-1968/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1920] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2016\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2016/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2016/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2016/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2016/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1824] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2064\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2064/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2064/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2064/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2064/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-1968] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2112\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2112/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2112/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2112/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2112/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2016] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2160\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2160/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2160/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2160/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2160/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2064] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2208\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2208/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2208/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2208/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2208/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2112] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2256\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2256/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2256/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2256/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2256/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2160] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2304\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2304/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2304/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2304/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2304/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2208] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2352\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2352/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2352/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2352/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2352/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2256] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2400\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2400/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2400/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2304] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2448\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2448/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2448/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2448/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2448/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2352] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2496\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2496/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2496/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2496/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2496/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2400] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2544\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2544/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2544/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2544/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2544/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2448] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2592\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2592/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2592/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2592/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2592/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2496] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2640\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2640/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2640/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2640/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2640/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2544] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2688\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2688/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2688/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2688/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2688/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2592] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2736\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2736/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2736/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2736/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2736/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2640] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2784\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2784/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2784/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2784/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2784/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2688] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2832\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2832/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2832/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2832/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2832/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2736] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2880\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2880/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2880/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2880/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2880/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2784] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2928\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2928/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2928/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2928/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2928/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2880] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-2976\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-2976/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-2976/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-2976/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-2976/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2928] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3024\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3024/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3024/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3024/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3024/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2832] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3072\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3072/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3072/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3072/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3072/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-2976] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3120\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3120/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3120/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3120/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3120/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3024] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3168\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3168/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3168/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3168/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3168/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3072] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3216\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3216/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3216/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3216/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3216/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3120] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3264\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3264/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3264/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3264/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3264/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3216] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3312\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3312/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3312/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3312/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3312/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3264] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3360\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3360/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3360/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3360/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3360/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3168] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3408\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3408/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3408/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3408/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3408/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3312] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3456\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3456/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3456/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3456/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3456/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3360] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3504\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3504/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3504/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3504/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3504/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3456] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3552\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3552/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3552/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3552/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3552/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3504] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3600\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3600/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3600/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3408] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3648\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3648/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3648/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3648/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3648/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3552] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3696\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3696/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3696/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3696/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3696/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3600] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3744\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3744/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3744/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3744/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3744/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3648] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3792\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3792/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3792/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3792/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3792/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3696] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3840\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3840/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3840/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3840/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3840/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3744] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3888\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3888/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3888/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3888/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3888/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3792] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3936\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3936/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3936/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3936/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3936/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3840] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-3984\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-3984/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-3984/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-3984/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-3984/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3936] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4032\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4032/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4032/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4032/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4032/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3984] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4080\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4080/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4080/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4080/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4080/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4032] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4128\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4128/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4128/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4128/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4128/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-3888] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4176\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4176/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4176/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4176/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4176/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4080] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4224\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4224/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4224/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4224/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4224/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4128] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4272\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4272/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4272/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4272/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4272/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4176] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4320\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4320/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4320/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4320/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4320/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4224] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4368\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4368/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4368/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4368/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4368/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4320] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4416\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4416/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4416/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4416/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4416/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4272] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4464\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4464/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4464/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4464/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4464/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4368] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4512\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4512/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4512/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4512/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4512/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4416] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4560\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4560/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4560/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4560/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4560/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4464] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4608\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4608/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4608/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4608/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4608/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4512] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4656\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4656/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4656/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4656/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4656/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4608] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4704\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4704/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4704/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4704/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4704/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4656] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4752\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4752/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4752/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4752/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4752/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4704] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4800\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4800/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4800/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4752] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4848\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4848/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4848/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4848/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4848/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4800] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4896\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4896/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4896/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4896/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4896/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4848] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4944/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4944/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4944/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4944/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4896] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-4992\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-4992/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-4992/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-4992/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-4992/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4560] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5040\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5040/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5040/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5040/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5040/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4944] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5088\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5088/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5088/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5088/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5088/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-4992] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5136\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5136/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5136/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5136/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5136/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5040] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5184\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5184/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5184/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5184/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5184/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5088] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5232\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5232/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5232/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5232/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5232/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5136] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5280\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5280/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5280/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5280/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5280/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5184] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5328\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5328/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5328/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5328/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5328/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5232] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5376\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5376/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5376/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5376/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5376/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5328] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5424\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5424/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5424/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5424/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5424/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5376] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5472\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5472/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5472/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5472/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5472/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5280] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5520\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5520/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5520/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5520/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5520/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5424] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5568\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5568/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5568/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5568/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5568/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5472] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5616\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5616/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5616/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5616/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5616/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5568] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5664\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5664/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5664/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5664/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5664/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5520] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5712\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5712/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5712/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5712/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5712/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5616] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5760\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5760/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5760/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5760/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5760/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5664] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5808\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5808/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5808/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5808/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5808/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5712] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5856\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5856/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5856/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5856/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5856/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5808] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5904\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5904/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5904/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5904/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5904/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5856] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-5952\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-5952/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-5952/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-5952/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-5952/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5760] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6000\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6000/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5904] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6048\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6048/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6048/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6048/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6048/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-5952] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6096\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6096/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6096/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6096/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6096/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6048] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6144\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6144/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6144/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6144/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6144/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6000] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6192\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6192/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6192/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6192/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6192/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6096] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6240\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6240/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6240/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6240/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6240/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6192] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6288\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6288/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6288/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6288/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6288/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6144] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6336\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6336/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6336/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6336/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6336/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6240] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6384\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6384/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6384/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6384/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6384/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6288] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6432\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6432/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6432/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6432/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6432/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6336] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6480\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6480/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6480/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6480/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6480/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6432] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6528\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6528/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6528/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6528/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6528/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6480] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6576\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6576/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6576/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6576/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6576/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6528] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6624\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6624/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6624/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6624/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6624/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6576] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6672\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6672/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6672/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6672/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6672/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6624] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6720\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6720/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6720/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6720/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6720/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6384] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6768\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6768/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6768/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6768/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6768/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6672] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6816\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6816/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6816/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6816/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6816/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6720] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6864\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6864/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6864/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6864/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6864/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6768] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6912\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6912/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6912/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6912/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6912/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6816] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-6960\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-6960/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-6960/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-6960/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-6960/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6912] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7008\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7008/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7008/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7008/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7008/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6864] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7056\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7056/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7056/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7056/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7056/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-6960] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7104\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7104/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7104/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7104/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7104/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7008] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7152\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7152/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7152/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7152/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7152/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7104] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7200\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7200/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7200/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7152] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7248\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7248/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7248/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7248/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7248/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7200] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7296\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7296/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7296/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7296/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7296/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7248] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7344\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7344/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7344/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7344/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7344/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7296] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7392\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7392/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7392/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7392/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7392/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7344] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7440\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7440/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7440/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7440/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7440/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7056] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7488\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7488/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7488/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7488/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7488/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7392] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7536\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7536/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7536/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7536/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7536/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7488] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7584\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7584/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7584/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7584/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7584/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7536] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7632\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7632/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7632/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7632/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7632/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7584] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7680\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7680/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7680/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7680/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7680/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7632] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7728\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7728/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7728/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7728/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7728/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7680] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7776\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7776/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7776/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7776/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7776/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7728] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7824\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7824/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7824/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7824/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7824/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7776] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7872\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7872/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7872/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7872/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7872/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7824] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7920\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7920/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7920/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7920/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7920/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7872] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-7968\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-7968/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-7968/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-7968/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-7968/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7920] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8016\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8016/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8016/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8016/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8016/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7440] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8064\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8064/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8064/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8064/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8064/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-7968] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8112\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8112/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8112/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8112/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8112/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8064] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8160\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8160/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8160/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8160/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8160/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8112] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8208\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8208/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8208/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8208/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8208/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8160] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8256\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8256/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8256/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8256/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8256/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8208] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8304\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8304/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8304/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8304/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8304/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8256] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8352/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8352/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8352/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8352/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8016] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8400\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8400/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8400/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8304] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8448\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8448/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8448/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8448/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8448/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8400] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8496\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8496/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8496/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8496/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8496/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8448] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8544\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8544/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8544/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8544/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8544/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8496] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8592\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8592/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8592/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8592/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8592/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8544] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8640\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8640/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8640/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8640/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8640/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8592] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8688\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8688/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8688/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8688/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8688/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8640] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8736\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8736/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8736/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8736/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8736/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8688] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8784\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8784/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8784/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8784/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8784/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8736] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8832\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8832/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8832/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8832/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8832/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8784] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8880\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8880/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8880/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8880/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8880/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8832] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8928\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8928/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8928/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8928/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8928/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8880] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-8976\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-8976/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-8976/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-8976/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-8976/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8928] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9024\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9024/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9024/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9024/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9024/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-8976] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9072\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9072/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9072/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9072/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9072/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-9024] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9120\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9120/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9120/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9120/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9120/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-9072] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9168\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9168/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9168/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9168/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9168/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-9120] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9216\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9216/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9216/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9216/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9216/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-9168] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9264\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9264/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9264/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9264/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9264/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-9216] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9312\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9312/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9312/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9312/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9312/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-9264] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9360\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9360/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9360/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9360/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9360/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-9312] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9408\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9408/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9408/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9408/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9408/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-9360] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9456\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9456/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9456/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9456/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9456/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-9408] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9504\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9504/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9504/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9504/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9504/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-9456] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9552\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9552/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9552/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9552/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9552/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-9504] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/INV_de_en/models/checkpoint-9600\n",
      "Configuration saved in ./output/INV_de_en/models/checkpoint-9600/config.json\n",
      "Model weights saved in ./output/INV_de_en/models/checkpoint-9600/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/INV_de_en/models/checkpoint-9600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/INV_de_en/models/checkpoint-9600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/INV_de_en/models/checkpoint-9552] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/INV_de_en/models/checkpoint-8352 (score: 0.262).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9600, training_loss=1.0637137552102407, metrics={'train_runtime': 9666.1126, 'train_samples_per_second': 393.126, 'train_steps_per_second': 0.993, 'total_flos': 2.346603588e+16, 'train_loss': 1.0637137552102407, 'epoch': 200.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d1dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 22:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.262,\n",
       " 'eval_loss': 4.036846160888672,\n",
       " 'eval_runtime': 1.5735,\n",
       " 'eval_samples_per_second': 635.533,\n",
       " 'eval_steps_per_second': 2.542,\n",
       " 'epoch': 200.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Test\n",
    "trainer.evaluate(eval_dataset=tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "360c6e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation - source: taxonomischer Rang, target: taxon rank\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8276deb30d0c40a29ad91282a7976e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.1, 'eval_loss': 4.257268905639648, 'eval_runtime': 0.7494, 'eval_samples_per_second': 66.722, 'eval_steps_per_second': 1.334}\n",
      "Inversion - source: Notfalleinrichtungen, target: emergency services\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464eb15172f5498b92884574263b74a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.24, 'eval_loss': 4.892024993896484, 'eval_runtime': 0.5994, 'eval_samples_per_second': 83.414, 'eval_steps_per_second': 1.668}\n",
      "Relation - source: Farbe, target: color\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ec480a8d9f4876b803a4f3c700ff63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.86, 'eval_loss': 0.7034075260162354, 'eval_runtime': 0.5968, 'eval_samples_per_second': 83.78, 'eval_steps_per_second': 1.676}\n",
      "Inversion - source: erwartete Vollständigkeit, target: expected completeness\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fa5388212749e4918206a9c205c1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.22, 'eval_loss': 2.784353256225586, 'eval_runtime': 0.5869, 'eval_samples_per_second': 85.188, 'eval_steps_per_second': 1.704}\n",
      "Relation - source: gemessene physikalische Größe, target: measured physical quantity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e698168c7c5443d912d8dc676676213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.04, 'eval_loss': 4.681816577911377, 'eval_runtime': 0.5979, 'eval_samples_per_second': 83.628, 'eval_steps_per_second': 1.673}\n",
      "Inversion - source: geöffnet von Zeitpunkt, target: open period from\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2fa677ebba4687b1a284f411278bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.12, 'eval_loss': 5.864680767059326, 'eval_runtime': 0.615, 'eval_samples_per_second': 81.303, 'eval_steps_per_second': 1.626}\n",
      "Relation - source: Plattform, target: platform\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a9f4f16c66414aa4f62079dc952088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.06, 'eval_loss': 6.375242710113525, 'eval_runtime': 0.5801, 'eval_samples_per_second': 86.186, 'eval_steps_per_second': 1.724}\n",
      "Inversion - source: Legislativkomitee, target: legislative committee\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0571858de8354e4abd15d3ac82ca517c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.32, 'eval_loss': 3.7909417152404785, 'eval_runtime': 0.6236, 'eval_samples_per_second': 80.185, 'eval_steps_per_second': 1.604}\n",
      "Relation - source: Medien-Franchise, target: media franchise\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3476af6bbea84ca0bd74c719f552a36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.04, 'eval_loss': 4.807344436645508, 'eval_runtime': 0.6113, 'eval_samples_per_second': 81.795, 'eval_steps_per_second': 1.636}\n",
      "Inversion - source: Linsentyp, target: type of lens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22eddd8f2d7f4e8784ebe8324cfe8a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.2, 'eval_loss': 5.531165599822998, 'eval_runtime': 0.6272, 'eval_samples_per_second': 79.721, 'eval_steps_per_second': 1.594}\n",
      "Relation - source: Erregerreservoir von, target: natural reservoir of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df9a2187c68441f85e0f63dd4b77d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.12, 'eval_loss': 4.2391886711120605, 'eval_runtime': 0.648, 'eval_samples_per_second': 77.158, 'eval_steps_per_second': 1.543}\n",
      "Inversion - source: Gesichtshaar, target: facial hair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b75a6809d94c37ba225b4ae20df7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.3, 'eval_loss': 4.44975471496582, 'eval_runtime': 0.6253, 'eval_samples_per_second': 79.96, 'eval_steps_per_second': 1.599}\n",
      "Relation - source: Antwort auf, target: reply to\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315896606f294d07b43279043fea6f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.18, 'eval_loss': 3.1708714962005615, 'eval_runtime': 0.6367, 'eval_samples_per_second': 78.525, 'eval_steps_per_second': 1.57}\n",
      "Inversion - source: Werkliste, target: list of works\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b38fb099177499784e135d114e40cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.38, 'eval_loss': 3.405402421951294, 'eval_runtime': 0.6459, 'eval_samples_per_second': 77.41, 'eval_steps_per_second': 1.548}\n",
      "Relation - source: Nebenwirkung, target: side effect\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0990a22847a42fca174695fb15b9df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.26, 'eval_loss': 2.7538046836853027, 'eval_runtime': 0.6161, 'eval_samples_per_second': 81.154, 'eval_steps_per_second': 1.623}\n",
      "Inversion - source: interagiert physikalisch mit, target: physically interacts with\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc82b9dfb034bef83cbb35a1a3003e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.3, 'eval_loss': 2.077516794204712, 'eval_runtime': 0.6318, 'eval_samples_per_second': 79.141, 'eval_steps_per_second': 1.583}\n",
      "Relation - source: Punkt/Treffer erzielt durch, target: points/goal scored by\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb019733a5d41ff8eebbde04b7d2204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 1.0, 'eval_loss': 0.1565113216638565, 'eval_runtime': 0.6337, 'eval_samples_per_second': 78.904, 'eval_steps_per_second': 1.578}\n",
      "Inversion - source: Demonym zu, target: demonym of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168453402e5b45c88e2f6bb0c1945e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.16, 'eval_loss': 3.843660354614258, 'eval_runtime': 0.6198, 'eval_samples_per_second': 80.669, 'eval_steps_per_second': 1.613}\n",
      "Relation - source: Kriegseinsatz, target: conflict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08728bec5604a16898a346d625c59ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.02, 'eval_loss': 6.092104911804199, 'eval_runtime': 0.6327, 'eval_samples_per_second': 79.032, 'eval_steps_per_second': 1.581}\n",
      "Inversion - source: Kultur, target: culture\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857daf595c4e4086bbf552cfcaf46c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.32, 'eval_loss': 6.859859466552734, 'eval_runtime': 0.5991, 'eval_samples_per_second': 83.459, 'eval_steps_per_second': 1.669}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Symmetry per Relation\n",
    "evaluation_inversion(trainer, tokenizer, relations, source_language, copy.deepcopy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214e34f",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "- Is every relation inverted now? what about relations that aren't part of the training?\n",
    "- Pretrained?\n",
    "- Target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b37f3489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "080c503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Medina taxonomischer Rang Italie', 'Italie Notfalleinrichtungen Medina', 'Invasion taxonomischer Rang Bora', 'Bora Notfalleinrichtungen Invasion', 'Burke taxonomischer Rang Hus', 'Hus Notfalleinrichtungen Burke', 'Drama taxonomischer Rang epi', 'epi Notfalleinrichtungen Drama', 'Master taxonomischer Rang Wilfried', 'Wilfried Notfalleinrichtungen Master', 'Dari taxonomischer Rang Fach', 'Fach Notfalleinrichtungen Dari', 'Chihuahua taxonomischer Rang Inge', 'Inge Notfalleinrichtungen Chihuahua', 'EP taxonomischer Rang Elite', 'Elite Notfalleinrichtungen EP', 'Chase taxonomischer Rang Portland', 'Portland Notfalleinrichtungen Chase', 'Worcester taxonomischer Rang Eliza', 'Eliza Notfalleinrichtungen Worcester', 'Albert taxonomischer Rang Weir', 'Weir Notfalleinrichtungen Albert', 'Ibiza taxonomischer Rang Antoine', 'Antoine Notfalleinrichtungen Ibiza', 'Câmara taxonomischer Rang Universitas', 'Universitas Notfalleinrichtungen Câmara', 'Eleanor taxonomischer Rang Collins', 'Collins Notfalleinrichtungen Eleanor', 'Campus taxonomischer Rang Condor', 'Condor Notfalleinrichtungen Campus', 'Schaus taxonomischer Rang Gospel', 'Gospel Notfalleinrichtungen Schaus', 'Chaos taxonomischer Rang MS', 'MS Notfalleinrichtungen Chaos', 'Alice taxonomischer Rang Ramsey', 'Ramsey Notfalleinrichtungen Alice', 'Franklin taxonomischer Rang Katz', 'Katz Notfalleinrichtungen Franklin', 'Colombo taxonomischer Rang Saussure', 'Saussure Notfalleinrichtungen Colombo', 'Graf taxonomischer Rang BRT', 'BRT Notfalleinrichtungen Graf', 'Darwin taxonomischer Rang Carioca', 'Carioca Notfalleinrichtungen Darwin', 'Nisan taxonomischer Rang Note', 'Note Notfalleinrichtungen Nisan', 'Hartman taxonomischer Rang Ultra', 'Ultra Notfalleinrichtungen Hartman', 'Lur taxonomischer Rang Né', 'Né Notfalleinrichtungen Lur', 'Fiesta taxonomischer Rang Fidel', 'Fidel Notfalleinrichtungen Fiesta', 'Alexandra taxonomischer Rang Einer', 'Einer Notfalleinrichtungen Alexandra', 'Mérida taxonomischer Rang Everybody', 'Everybody Notfalleinrichtungen Mérida', 'Shire taxonomischer Rang Pizza', 'Pizza Notfalleinrichtungen Shire', 'Anthology taxonomischer Rang Koska', 'Koska Notfalleinrichtungen Anthology', 'Monroe taxonomischer Rang Ariane', 'Ariane Notfalleinrichtungen Monroe', 'Hell taxonomischer Rang Exil', 'Exil Notfalleinrichtungen Hell', 'Bandet taxonomischer Rang Holy', 'Holy Notfalleinrichtungen Bandet', 'Bertrand taxonomischer Rang Haar', 'Haar Notfalleinrichtungen Bertrand', 'Drum taxonomischer Rang Ringo', 'Ringo Notfalleinrichtungen Drum', 'IBM taxonomischer Rang Theresa', 'Theresa Notfalleinrichtungen IBM', 'Christchurch taxonomischer Rang Zone', 'Zone Notfalleinrichtungen Christchurch', 'Broadcast taxonomischer Rang Benth', 'Benth Notfalleinrichtungen Broadcast', 'Clay taxonomischer Rang Baja', 'Baja Notfalleinrichtungen Clay', 'Sound taxonomischer Rang Ruska', 'Ruska Notfalleinrichtungen Sound', 'Liga taxonomischer Rang Seymour', 'Seymour Notfalleinrichtungen Liga', 'Vatican taxonomischer Rang ECW', 'ECW Notfalleinrichtungen Vatican', 'Libro taxonomischer Rang Bez', 'Bez Notfalleinrichtungen Libro', 'Vanderbilt taxonomischer Rang Ortes', 'Ortes Notfalleinrichtungen Vanderbilt', 'Austrian taxonomischer Rang Jessica', 'Jessica Notfalleinrichtungen Austrian', 'Varese taxonomischer Rang Zoom', 'Zoom Notfalleinrichtungen Varese', 'Ninja taxonomischer Rang Telephone', 'Telephone Notfalleinrichtungen Ninja', 'Lafayette taxonomischer Rang A', 'A Notfalleinrichtungen Lafayette', 'Catalina taxonomischer Rang Barra', 'Barra Notfalleinrichtungen Catalina', 'Companion taxonomischer Rang Lori', 'Lori Notfalleinrichtungen Companion', 'Marshal taxonomischer Rang Reeves', 'Reeves Notfalleinrichtungen Marshal', 'Molina taxonomischer Rang Wagen', 'Wagen Notfalleinrichtungen Molina', 'Dow taxonomischer Rang Sound', 'Sound Notfalleinrichtungen Dow', 'Platte taxonomischer Rang JR', 'JR Notfalleinrichtungen Platte', 'Giant taxonomischer Rang Trail', 'Trail Notfalleinrichtungen Giant', 'Mulder taxonomischer Rang Concordia', 'Concordia Notfalleinrichtungen Mulder', 'Hancock taxonomischer Rang Windows', 'Windows Notfalleinrichtungen Hancock', 'Deutschland taxonomischer Rang Poco', 'Poco Notfalleinrichtungen Deutschland', 'Della taxonomischer Rang Suba', 'Suba Notfalleinrichtungen Della', 'Niko taxonomischer Rang Bartlett', 'Bartlett Notfalleinrichtungen Niko', 'Romans taxonomischer Rang Agent', 'Agent Notfalleinrichtungen Romans', 'Salman taxonomischer Rang Compton', 'Compton Notfalleinrichtungen Salman', 'Medi taxonomischer Rang Marshal', 'Marshal Notfalleinrichtungen Medi', 'Monaco taxonomischer Rang RF', 'RF Notfalleinrichtungen Monaco', 'Release taxonomischer Rang Lorenzo', 'Lorenzo Notfalleinrichtungen Release', 'Daughter taxonomischer Rang Cherry', 'Cherry Notfalleinrichtungen Daughter', 'Kendall taxonomischer Rang Namibia', 'Namibia Notfalleinrichtungen Kendall', 'PCR taxonomischer Rang FAA', 'FAA Notfalleinrichtungen PCR', 'Lorena taxonomischer Rang Kantonen', 'Kantonen Notfalleinrichtungen Lorena', 'Twain taxonomischer Rang Dunkerque', 'Dunkerque Notfalleinrichtungen Twain', 'Head taxonomischer Rang Demon', 'Demon Notfalleinrichtungen Head', 'Porta taxonomischer Rang Bears', 'Bears Notfalleinrichtungen Porta', 'Yunan taxonomischer Rang Paradise', 'Paradise Notfalleinrichtungen Yunan', 'MGM taxonomischer Rang Sidney', 'Sidney Notfalleinrichtungen MGM', 'Queste taxonomischer Rang ACM', 'ACM Notfalleinrichtungen Queste', 'Geld taxonomischer Rang Guanajuato', 'Guanajuato Notfalleinrichtungen Geld', 'Jesus taxonomischer Rang Bees', 'Bees Notfalleinrichtungen Jesus', 'Potter taxonomischer Rang Edwards', 'Edwards Notfalleinrichtungen Potter', 'Crown taxonomischer Rang Friesland', 'Friesland Notfalleinrichtungen Crown', 'Adi taxonomischer Rang Palencia', 'Palencia Notfalleinrichtungen Adi', 'Sinn taxonomischer Rang Baldwin', 'Baldwin Notfalleinrichtungen Sinn', 'Vacelet taxonomischer Rang Plans', 'Plans Notfalleinrichtungen Vacelet', 'Sagan taxonomischer Rang Hurley', 'Hurley Notfalleinrichtungen Sagan', 'Madsen taxonomischer Rang Schools', 'Schools Notfalleinrichtungen Madsen', 'Robinson taxonomischer Rang IX', 'IX Notfalleinrichtungen Robinson', 'Fayette taxonomischer Rang Alessandro', 'Alessandro Notfalleinrichtungen Fayette', 'Grant taxonomischer Rang Bürger', 'Bürger Notfalleinrichtungen Grant', 'Julian taxonomischer Rang Collegiate', 'Collegiate Notfalleinrichtungen Julian', 'Carrillo taxonomischer Rang Move', 'Move Notfalleinrichtungen Carrillo', 'Aus taxonomischer Rang KM', 'KM Notfalleinrichtungen Aus', 'BBC taxonomischer Rang Petra', 'Petra Notfalleinrichtungen BBC', 'Kongo taxonomischer Rang Nightmare', 'Nightmare Notfalleinrichtungen Kongo', 'Gaur taxonomischer Rang Krüger', 'Krüger Notfalleinrichtungen Gaur', 'Shining taxonomischer Rang Rota', 'Rota Notfalleinrichtungen Shining', 'Simpson taxonomischer Rang Face', 'Face Notfalleinrichtungen Simpson', 'Douglas taxonomischer Rang Martínez', 'Martínez Notfalleinrichtungen Douglas', 'Continental taxonomischer Rang Quinta', 'Quinta Notfalleinrichtungen Continental', 'West taxonomischer Rang Damit', 'Damit Notfalleinrichtungen West', 'Haas taxonomischer Rang Oper', 'Oper Notfalleinrichtungen Haas', 'Lynch taxonomischer Rang Frédéric', 'Frédéric Notfalleinrichtungen Lynch', 'Cornwall taxonomischer Rang Akdeniz', 'Akdeniz Notfalleinrichtungen Cornwall', 'Jalan taxonomischer Rang UC', 'UC Notfalleinrichtungen Jalan', 'Dhaka taxonomischer Rang Plan', 'Plan Notfalleinrichtungen Dhaka', 'Reading taxonomischer Rang Sailor', 'Sailor Notfalleinrichtungen Reading', 'Auckland taxonomischer Rang Words', 'Words Notfalleinrichtungen Auckland', 'Chancellor taxonomischer Rang Waterloo', 'Waterloo Notfalleinrichtungen Chancellor', 'Dakota taxonomischer Rang Peters', 'Peters Notfalleinrichtungen Dakota', 'Arles taxonomischer Rang Boyle', 'Boyle Notfalleinrichtungen Arles', 'Loan taxonomischer Rang Remixes', 'Remixes Notfalleinrichtungen Loan', 'Stab taxonomischer Rang Vienna', 'Vienna Notfalleinrichtungen Stab', 'Golden taxonomischer Rang Talent', 'Talent Notfalleinrichtungen Golden', 'Jakiel taxonomischer Rang Switch', 'Switch Notfalleinrichtungen Jakiel', 'Wade taxonomischer Rang Greatest', 'Greatest Notfalleinrichtungen Wade', 'Ebro taxonomischer Rang Shannon', 'Shannon Notfalleinrichtungen Ebro', 'Dorothea taxonomischer Rang Irena', 'Irena Notfalleinrichtungen Dorothea', 'Urban taxonomischer Rang Arabic', 'Arabic Notfalleinrichtungen Urban', 'HF taxonomischer Rang Membre', 'Membre Notfalleinrichtungen HF', 'Spectrum taxonomischer Rang Condor', 'Condor Notfalleinrichtungen Spectrum', 'Cass taxonomischer Rang Extra', 'Extra Notfalleinrichtungen Cass', 'Riva taxonomischer Rang Bil', 'Bil Notfalleinrichtungen Riva', 'Winston taxonomischer Rang Menor', 'Menor Notfalleinrichtungen Winston', 'Punk taxonomischer Rang Sprint', 'Sprint Notfalleinrichtungen Punk', 'SMS taxonomischer Rang Free', 'Free Notfalleinrichtungen SMS', 'Paglinawan taxonomischer Rang Ut', 'Ut Notfalleinrichtungen Paglinawan', 'McKay taxonomischer Rang Transfer', 'Transfer Notfalleinrichtungen McKay', 'Harmony taxonomischer Rang Batavia', 'Batavia Notfalleinrichtungen Harmony', 'Chantal taxonomischer Rang Das', 'Das Notfalleinrichtungen Chantal', 'Taurus taxonomischer Rang Lowe', 'Lowe Notfalleinrichtungen Taurus', 'Crosby taxonomischer Rang Flat', 'Flat Notfalleinrichtungen Crosby', 'GSC taxonomischer Rang Barry', 'Barry Notfalleinrichtungen GSC', 'Moonlight taxonomischer Rang Latreille', 'Latreille Notfalleinrichtungen Moonlight', 'Goodman taxonomischer Rang Bala', 'Bala Notfalleinrichtungen Goodman', 'Stéphane taxonomischer Rang Büyük', 'Büyük Notfalleinrichtungen Stéphane', 'Kader taxonomischer Rang Mittel', 'Mittel Notfalleinrichtungen Kader', 'Ferro taxonomischer Rang König', 'König Notfalleinrichtungen Ferro', 'Anglo taxonomischer Rang Yates', 'Yates Notfalleinrichtungen Anglo', 'Flesh taxonomischer Rang AFC', 'AFC Notfalleinrichtungen Flesh', 'Ubuntu taxonomischer Rang Matrix', 'Matrix Notfalleinrichtungen Ubuntu', 'GMT taxonomischer Rang Ottawa', 'Ottawa Notfalleinrichtungen GMT', 'Quintana taxonomischer Rang Tha', 'Tha Notfalleinrichtungen Quintana', 'Lublin taxonomischer Rang Shane', 'Shane Notfalleinrichtungen Lublin', 'Nagar taxonomischer Rang PCR', 'PCR Notfalleinrichtungen Nagar', 'Dictionary taxonomischer Rang Jazz', 'Jazz Notfalleinrichtungen Dictionary', 'Cats taxonomischer Rang Bismarck', 'Bismarck Notfalleinrichtungen Cats', 'Beyoncé taxonomischer Rang Carlton', 'Carlton Notfalleinrichtungen Beyoncé', 'Mendoza taxonomischer Rang George', 'George Notfalleinrichtungen Mendoza', 'Roja taxonomischer Rang Urban', 'Urban Notfalleinrichtungen Roja', 'Beckett taxonomischer Rang Samba', 'Samba Notfalleinrichtungen Beckett', 'Toulouse taxonomischer Rang Molina', 'Molina Notfalleinrichtungen Toulouse', 'Summit taxonomischer Rang Rivas', 'Rivas Notfalleinrichtungen Summit', 'Champion taxonomischer Rang Braga', 'Braga Notfalleinrichtungen Champion', 'Twins taxonomischer Rang Olav', 'Olav Notfalleinrichtungen Twins', 'Halo taxonomischer Rang Stephan', 'Stephan Notfalleinrichtungen Halo', 'Deborah taxonomischer Rang McKamey', 'McKamey Notfalleinrichtungen Deborah', 'SF taxonomischer Rang Burke', 'Burke Notfalleinrichtungen SF', 'Commonwealth taxonomischer Rang Luik', 'Luik Notfalleinrichtungen Commonwealth', 'Dublin taxonomischer Rang Arena', 'Arena Notfalleinrichtungen Dublin', 'Apocalypse taxonomischer Rang Oviedo', 'Oviedo Notfalleinrichtungen Apocalypse', 'Drake taxonomischer Rang Syracuse', 'Syracuse Notfalleinrichtungen Drake', 'Padang taxonomischer Rang Piazza', 'Piazza Notfalleinrichtungen Padang', 'Royal taxonomischer Rang Slag', 'Slag Notfalleinrichtungen Royal', 'Quick taxonomischer Rang Delft', 'Delft Notfalleinrichtungen Quick', 'Remote taxonomischer Rang Romagna', 'Romagna Notfalleinrichtungen Remote', 'Player taxonomischer Rang Partner', 'Partner Notfalleinrichtungen Player', 'Laba taxonomischer Rang Benton', 'Benton Notfalleinrichtungen Laba', 'Wiener taxonomischer Rang Quiet', 'Quiet Notfalleinrichtungen Wiener', 'Tripoli taxonomischer Rang Dexter', 'Dexter Notfalleinrichtungen Tripoli', 'Glasgow taxonomischer Rang Scientist', 'Scientist Notfalleinrichtungen Glasgow', 'Hansen taxonomischer Rang Maestro', 'Maestro Notfalleinrichtungen Hansen', 'Ortes taxonomischer Rang Busch', 'Busch Notfalleinrichtungen Ortes', 'Mozilla taxonomischer Rang Napoca', 'Napoca Notfalleinrichtungen Mozilla', 'Mur taxonomischer Rang Ball', 'Ball Notfalleinrichtungen Mur', 'Iberia taxonomischer Rang Lindsay', 'Lindsay Notfalleinrichtungen Iberia', 'Vir taxonomischer Rang Hector', 'Hector Notfalleinrichtungen Vir', 'Prato taxonomischer Rang IBM', 'IBM Notfalleinrichtungen Prato', 'Ferris taxonomischer Rang Dur', 'Dur Notfalleinrichtungen Ferris', 'Racing taxonomischer Rang Tanner', 'Tanner Notfalleinrichtungen Racing', 'Steiner taxonomischer Rang pr', 'pr Notfalleinrichtungen Steiner', 'Sala taxonomischer Rang Ensemble', 'Ensemble Notfalleinrichtungen Sala', 'Connection taxonomischer Rang Ester', 'Ester Notfalleinrichtungen Connection', 'Hastings taxonomischer Rang Hoya', 'Hoya Notfalleinrichtungen Hastings', 'Siegfried taxonomischer Rang Gauss', 'Gauss Notfalleinrichtungen Siegfried', 'Kale taxonomischer Rang Heine', 'Heine Notfalleinrichtungen Kale', 'Fourier taxonomischer Rang Madeira', 'Madeira Notfalleinrichtungen Fourier', 'Pada taxonomischer Rang Golf', 'Golf Notfalleinrichtungen Pada', 'Roberto taxonomischer Rang Pike', 'Pike Notfalleinrichtungen Roberto', 'Struggle taxonomischer Rang Helga', 'Helga Notfalleinrichtungen Struggle', 'Elijah taxonomischer Rang Down', 'Down Notfalleinrichtungen Elijah', 'Rote taxonomischer Rang Firefox', 'Firefox Notfalleinrichtungen Rote', 'Christi taxonomischer Rang Novel', 'Novel Notfalleinrichtungen Christi', 'Berge taxonomischer Rang Addison', 'Addison Notfalleinrichtungen Berge', 'Wimbledon taxonomischer Rang Loving', 'Loving Notfalleinrichtungen Wimbledon', 'Bees taxonomischer Rang Desse', 'Desse Notfalleinrichtungen Bees', 'Palm taxonomischer Rang Middlesex', 'Middlesex Notfalleinrichtungen Palm', 'Nor taxonomischer Rang Beth', 'Beth Notfalleinrichtungen Nor', 'Maria taxonomischer Rang Angoulême', 'Angoulême Notfalleinrichtungen Maria', 'Stadio taxonomischer Rang Grimaldi', 'Grimaldi Notfalleinrichtungen Stadio', 'Belo taxonomischer Rang Tobias', 'Tobias Notfalleinrichtungen Belo', 'Boga taxonomischer Rang Tat', 'Tat Notfalleinrichtungen Boga', 'NSW taxonomischer Rang Barry', 'Barry Notfalleinrichtungen NSW', 'Roller taxonomischer Rang Regno', 'Regno Notfalleinrichtungen Roller', 'León taxonomischer Rang CAD', 'CAD Notfalleinrichtungen León', 'Rollen taxonomischer Rang Machado', 'Machado Notfalleinrichtungen Rollen', 'Agama taxonomischer Rang Reich', 'Reich Notfalleinrichtungen Agama', 'Seminary taxonomischer Rang Choice', 'Choice Notfalleinrichtungen Seminary', 'Ensemble taxonomischer Rang Argentine', 'Argentine Notfalleinrichtungen Ensemble', 'Blade taxonomischer Rang Huntington', 'Huntington Notfalleinrichtungen Blade', 'Cochrane taxonomischer Rang Monk', 'Monk Notfalleinrichtungen Cochrane', 'Indiana taxonomischer Rang Mines', 'Mines Notfalleinrichtungen Indiana', 'Carvalho taxonomischer Rang Weston', 'Weston Notfalleinrichtungen Carvalho', 'René taxonomischer Rang CE', 'CE Notfalleinrichtungen René', 'Mask taxonomischer Rang Sint', 'Sint Notfalleinrichtungen Mask', 'Dacia taxonomischer Rang Lego', 'Lego Notfalleinrichtungen Dacia', 'Cea taxonomischer Rang Amour', 'Amour Notfalleinrichtungen Cea', 'Bulgaria taxonomischer Rang Martinez', 'Martinez Notfalleinrichtungen Bulgaria', 'Rocket taxonomischer Rang Ida', 'Ida Notfalleinrichtungen Rocket', 'Pro taxonomischer Rang Julie', 'Julie Notfalleinrichtungen Pro', 'Portugal taxonomischer Rang KK', 'KK Notfalleinrichtungen Portugal', 'Blanco taxonomischer Rang Trouble', 'Trouble Notfalleinrichtungen Blanco', 'Addison taxonomischer Rang Partido', 'Partido Notfalleinrichtungen Addison', 'Como taxonomischer Rang Jess', 'Jess Notfalleinrichtungen Como', 'Suomi taxonomischer Rang Keller', 'Keller Notfalleinrichtungen Suomi', 'Alt taxonomischer Rang Österreich', 'Österreich Notfalleinrichtungen Alt', 'Esther taxonomischer Rang Britten', 'Britten Notfalleinrichtungen Esther', 'Sick taxonomischer Rang Dacia', 'Dacia Notfalleinrichtungen Sick', 'Bowman taxonomischer Rang Pt', 'Pt Notfalleinrichtungen Bowman', 'Wells taxonomischer Rang Ekim', 'Ekim Notfalleinrichtungen Wells', 'NT taxonomischer Rang Cavendish', 'Cavendish Notfalleinrichtungen NT', 'Titanic taxonomischer Rang Durban', 'Durban Notfalleinrichtungen Titanic', 'Chamber taxonomischer Rang Garland', 'Garland Notfalleinrichtungen Chamber', 'Satellite taxonomischer Rang Check', 'Check Notfalleinrichtungen Satellite', 'Niels taxonomischer Rang Thor', 'Thor Notfalleinrichtungen Niels', 'Borneo taxonomischer Rang Remixes', 'Remixes Notfalleinrichtungen Borneo', 'Tigre taxonomischer Rang Pierce', 'Pierce Notfalleinrichtungen Tigre', 'Madagascar taxonomischer Rang Band', 'Band Notfalleinrichtungen Madagascar', 'Vincent taxonomischer Rang Baird', 'Baird Notfalleinrichtungen Vincent', 'Midlands taxonomischer Rang Sia', 'Sia Notfalleinrichtungen Midlands', 'Siam taxonomischer Rang Züge', 'Züge Notfalleinrichtungen Siam', 'Uit taxonomischer Rang Clyde', 'Clyde Notfalleinrichtungen Uit', 'CDC taxonomischer Rang Underground', 'Underground Notfalleinrichtungen CDC', 'Sawyer taxonomischer Rang Luxemburg', 'Luxemburg Notfalleinrichtungen Sawyer', 'Ranking taxonomischer Rang Sørensen', 'Sørensen Notfalleinrichtungen Ranking', 'Babylon taxonomischer Rang Esse', 'Esse Notfalleinrichtungen Babylon', 'Côte taxonomischer Rang Malay', 'Malay Notfalleinrichtungen Côte', 'IS taxonomischer Rang Wish', 'Wish Notfalleinrichtungen IS', 'Frost taxonomischer Rang Voyager', 'Voyager Notfalleinrichtungen Frost', 'Mariana taxonomischer Rang Holocaust', 'Holocaust Notfalleinrichtungen Mariana', 'Baza taxonomischer Rang LP', 'LP Notfalleinrichtungen Baza', 'Washington taxonomischer Rang Medley', 'Medley Notfalleinrichtungen Washington', 'Giles taxonomischer Rang Buena', 'Buena Notfalleinrichtungen Giles', 'Benton taxonomischer Rang Ponte', 'Ponte Notfalleinrichtungen Benton', 'Balázs taxonomischer Rang Ávila', 'Ávila Notfalleinrichtungen Balázs', 'Pays taxonomischer Rang Cullen', 'Cullen Notfalleinrichtungen Pays', 'Alman taxonomischer Rang Les', 'Les Notfalleinrichtungen Alman', 'Hitchcock taxonomischer Rang Gerard', 'Gerard Notfalleinrichtungen Hitchcock', 'Danube taxonomischer Rang ISS', 'ISS Notfalleinrichtungen Danube', 'Quattro taxonomischer Rang Trees', 'Trees Notfalleinrichtungen Quattro', 'Troy taxonomischer Rang Franjo', 'Franjo Notfalleinrichtungen Troy', 'Hector taxonomischer Rang Vas', 'Vas Notfalleinrichtungen Hector', 'Hip taxonomischer Rang Pioneer', 'Pioneer Notfalleinrichtungen Hip', 'Face taxonomischer Rang Isles', 'Isles Notfalleinrichtungen Face', 'Funk taxonomischer Rang Qu', 'Qu Notfalleinrichtungen Funk', 'Hazel taxonomischer Rang Cesar', 'Cesar Notfalleinrichtungen Hazel', 'Romawi taxonomischer Rang Satan', 'Satan Notfalleinrichtungen Romawi', 'Straits taxonomischer Rang EC', 'EC Notfalleinrichtungen Straits', 'Put taxonomischer Rang Delft', 'Delft Notfalleinrichtungen Put', 'ATP taxonomischer Rang Ter', 'Ter Notfalleinrichtungen ATP', 'Abucay taxonomischer Rang Burma', 'Burma Notfalleinrichtungen Abucay', 'Spor taxonomischer Rang Freie', 'Freie Notfalleinrichtungen Spor', 'Cathedral taxonomischer Rang Bride', 'Bride Notfalleinrichtungen Cathedral', 'Vijay taxonomischer Rang Espagne', 'Espagne Notfalleinrichtungen Vijay', 'Géza taxonomischer Rang Thành', 'Thành Notfalleinrichtungen Géza', 'Werk taxonomischer Rang Bunker', 'Bunker Notfalleinrichtungen Werk', 'Jungen taxonomischer Rang Italiana', 'Italiana Notfalleinrichtungen Jungen', 'Harri taxonomischer Rang Bad', 'Bad Notfalleinrichtungen Harri', 'Wood taxonomischer Rang Kamera', 'Kamera Notfalleinrichtungen Wood', 'XP taxonomischer Rang Beatrice', 'Beatrice Notfalleinrichtungen XP', 'Henning taxonomischer Rang Henri', 'Henri Notfalleinrichtungen Henning', 'Daniel taxonomischer Rang Mound', 'Mound Notfalleinrichtungen Daniel', 'Lac taxonomischer Rang Space', 'Space Notfalleinrichtungen Lac', 'Niger taxonomischer Rang USSR', 'USSR Notfalleinrichtungen Niger', 'Bon taxonomischer Rang Seigneur', 'Seigneur Notfalleinrichtungen Bon', 'Hess taxonomischer Rang Comte', 'Comte Notfalleinrichtungen Hess', 'Asunción taxonomischer Rang Díaz', 'Díaz Notfalleinrichtungen Asunción', 'ba taxonomischer Rang Wallace', 'Wallace Notfalleinrichtungen ba', 'Roll taxonomischer Rang Nepal', 'Nepal Notfalleinrichtungen Roll', 'Nada taxonomischer Rang Platnick', 'Platnick Notfalleinrichtungen Nada', 'Saale taxonomischer Rang FF', 'FF Notfalleinrichtungen Saale', 'Minden taxonomischer Rang Mickey', 'Mickey Notfalleinrichtungen Minden', 'Imperial taxonomischer Rang Railroad', 'Railroad Notfalleinrichtungen Imperial', 'Moreau taxonomischer Rang Samo', 'Samo Notfalleinrichtungen Moreau', 'Provence taxonomischer Rang Bach', 'Bach Notfalleinrichtungen Provence', 'Archer taxonomischer Rang Batman', 'Batman Notfalleinrichtungen Archer', 'JNA taxonomischer Rang Tucson', 'Tucson Notfalleinrichtungen JNA', 'Punjabi taxonomischer Rang Driver', 'Driver Notfalleinrichtungen Punjabi', 'Tarragona taxonomischer Rang Rat', 'Rat Notfalleinrichtungen Tarragona', 'Pirates taxonomischer Rang Hastings', 'Hastings Notfalleinrichtungen Pirates', 'Mat taxonomischer Rang Providence', 'Providence Notfalleinrichtungen Mat', 'Berlin taxonomischer Rang Ja', 'Ja Notfalleinrichtungen Berlin', 'Thành taxonomischer Rang Finale', 'Finale Notfalleinrichtungen Thành', 'Vance taxonomischer Rang Rex', 'Rex Notfalleinrichtungen Vance', 'Banja taxonomischer Rang Ace', 'Ace Notfalleinrichtungen Banja', 'Champ taxonomischer Rang Saskatchewan', 'Saskatchewan Notfalleinrichtungen Champ', 'Trieste taxonomischer Rang Sigurd', 'Sigurd Notfalleinrichtungen Trieste', 'Pi taxonomischer Rang Potsdam', 'Potsdam Notfalleinrichtungen Pi', 'Cisco taxonomischer Rang Irvine', 'Irvine Notfalleinrichtungen Cisco', 'Cologne taxonomischer Rang Coleman', 'Coleman Notfalleinrichtungen Cologne', 'Banks taxonomischer Rang Phil', 'Phil Notfalleinrichtungen Banks', 'MotoGP taxonomischer Rang Sul', 'Sul Notfalleinrichtungen MotoGP', 'Spiegel taxonomischer Rang Danh', 'Danh Notfalleinrichtungen Spiegel', 'Dara taxonomischer Rang Little', 'Little Notfalleinrichtungen Dara', 'Platinum taxonomischer Rang Basso', 'Basso Notfalleinrichtungen Platinum', 'Calvin taxonomischer Rang Gamma', 'Gamma Notfalleinrichtungen Calvin', 'Gauss taxonomischer Rang Dana', 'Dana Notfalleinrichtungen Gauss', 'Lena taxonomischer Rang Harbor', 'Harbor Notfalleinrichtungen Lena', 'Voltaire taxonomischer Rang Bears', 'Bears Notfalleinrichtungen Voltaire', 'Polar taxonomischer Rang Bristol', 'Bristol Notfalleinrichtungen Polar', 'Depot taxonomischer Rang Rusi', 'Rusi Notfalleinrichtungen Depot', 'Petit taxonomischer Rang Maxime', 'Maxime Notfalleinrichtungen Petit', 'Einstein taxonomischer Rang Jagger', 'Jagger Notfalleinrichtungen Einstein', 'Bay taxonomischer Rang Swan', 'Swan Notfalleinrichtungen Bay', 'Skin taxonomischer Rang Lord', 'Lord Notfalleinrichtungen Skin', 'Donna taxonomischer Rang SF', 'SF Notfalleinrichtungen Donna', 'Pasteur taxonomischer Rang Coffee', 'Coffee Notfalleinrichtungen Pasteur', 'Seul taxonomischer Rang Seele', 'Seele Notfalleinrichtungen Seul', 'Middlesex taxonomischer Rang Victorian', 'Victorian Notfalleinrichtungen Middlesex', 'Closer taxonomischer Rang Blanchard', 'Blanchard Notfalleinrichtungen Closer', 'Hollow taxonomischer Rang Happy', 'Happy Notfalleinrichtungen Hollow', 'Malang taxonomischer Rang Rae', 'Rae Notfalleinrichtungen Malang', 'KPD taxonomischer Rang Bitte', 'Bitte Notfalleinrichtungen KPD', 'Palma taxonomischer Rang Beyaz', 'Beyaz Notfalleinrichtungen Palma', 'AFL taxonomischer Rang Hügel', 'Hügel Notfalleinrichtungen AFL', 'Accademia taxonomischer Rang Free', 'Free Notfalleinrichtungen Accademia', 'Leafs taxonomischer Rang Cynthia', 'Cynthia Notfalleinrichtungen Leafs', 'Principal taxonomischer Rang Tông', 'Tông Notfalleinrichtungen Principal', 'Carla taxonomischer Rang Bouchet', 'Bouchet Notfalleinrichtungen Carla', 'Rally taxonomischer Rang Barbosa', 'Barbosa Notfalleinrichtungen Rally', 'Lilla taxonomischer Rang Assembly', 'Assembly Notfalleinrichtungen Lilla', 'Gamble taxonomischer Rang Stranger', 'Stranger Notfalleinrichtungen Gamble', 'Alec taxonomischer Rang Valls', 'Valls Notfalleinrichtungen Alec', 'Medvedev taxonomischer Rang Fresno', 'Fresno Notfalleinrichtungen Medvedev', 'Nino taxonomischer Rang NME', 'NME Notfalleinrichtungen Nino', 'Willard taxonomischer Rang Rover', 'Rover Notfalleinrichtungen Willard', 'Seoul taxonomischer Rang DS', 'DS Notfalleinrichtungen Seoul', 'Bala taxonomischer Rang Ja', 'Ja Notfalleinrichtungen Bala', 'Stift taxonomischer Rang Wenn', 'Wenn Notfalleinrichtungen Stift', 'Albania taxonomischer Rang Boone', 'Boone Notfalleinrichtungen Albania', 'Grund taxonomischer Rang Indonesia', 'Indonesia Notfalleinrichtungen Grund', 'Indre taxonomischer Rang Generation', 'Generation Notfalleinrichtungen Indre', 'Poslední taxonomischer Rang Avon', 'Avon Notfalleinrichtungen Poslední', 'Switch taxonomischer Rang Steen', 'Steen Notfalleinrichtungen Switch', 'Coral taxonomischer Rang Kobayashi', 'Kobayashi Notfalleinrichtungen Coral', 'Ethel taxonomischer Rang Ng', 'Ng Notfalleinrichtungen Ethel', 'Charlie taxonomischer Rang Cine', 'Cine Notfalleinrichtungen Charlie', 'Warren taxonomischer Rang Massacre', 'Massacre Notfalleinrichtungen Warren', 'Irvine taxonomischer Rang Sprint', 'Sprint Notfalleinrichtungen Irvine', 'Quinta taxonomischer Rang Aziz', 'Aziz Notfalleinrichtungen Quinta', 'Up taxonomischer Rang Hang', 'Hang Notfalleinrichtungen Up', 'Heft taxonomischer Rang Tiempo', 'Tiempo Notfalleinrichtungen Heft', 'Cap taxonomischer Rang Senat', 'Senat Notfalleinrichtungen Cap', 'NN taxonomischer Rang Dunia', 'Dunia Notfalleinrichtungen NN', 'Romas taxonomischer Rang Independencia', 'Independencia Notfalleinrichtungen Romas', 'Benedict taxonomischer Rang Genocide', 'Genocide Notfalleinrichtungen Benedict', 'Zurich taxonomischer Rang Atlanta', 'Atlanta Notfalleinrichtungen Zurich', 'Capitaine taxonomischer Rang Nash', 'Nash Notfalleinrichtungen Capitaine', 'Camilla taxonomischer Rang Lys', 'Lys Notfalleinrichtungen Camilla', 'Theatre taxonomischer Rang Valley', 'Valley Notfalleinrichtungen Theatre', 'Eo taxonomischer Rang Ingles', 'Ingles Notfalleinrichtungen Eo', 'Kelley taxonomischer Rang Kensington', 'Kensington Notfalleinrichtungen Kelley', 'Ola taxonomischer Rang Remo', 'Remo Notfalleinrichtungen Ola', 'Gibraltar taxonomischer Rang Henning', 'Henning Notfalleinrichtungen Gibraltar', 'Alabama taxonomischer Rang Rocca', 'Rocca Notfalleinrichtungen Alabama', 'Revue taxonomischer Rang Cynthia', 'Cynthia Notfalleinrichtungen Revue', 'Duran taxonomischer Rang Tech', 'Tech Notfalleinrichtungen Duran', 'Guía taxonomischer Rang ag', 'ag Notfalleinrichtungen Guía', 'Wanda taxonomischer Rang Odessa', 'Odessa Notfalleinrichtungen Wanda', 'Piemonte taxonomischer Rang Rhapsody', 'Rhapsody Notfalleinrichtungen Piemonte', 'Impact taxonomischer Rang Carlos', 'Carlos Notfalleinrichtungen Impact', 'Vene taxonomischer Rang Maka', 'Maka Notfalleinrichtungen Vene', 'KBS taxonomischer Rang Larry', 'Larry Notfalleinrichtungen KBS', 'Emery taxonomischer Rang Neki', 'Neki Notfalleinrichtungen Emery', 'Alexandria taxonomischer Rang Brisbane', 'Brisbane Notfalleinrichtungen Alexandria', 'Bey taxonomischer Rang Each', 'Each Notfalleinrichtungen Bey', 'Studi taxonomischer Rang Guerre', 'Guerre Notfalleinrichtungen Studi', 'Coll taxonomischer Rang Alta', 'Alta Notfalleinrichtungen Coll', 'Jeffries taxonomischer Rang Margareta', 'Margareta Notfalleinrichtungen Jeffries', 'Forst taxonomischer Rang Vacelet', 'Vacelet Notfalleinrichtungen Forst', 'Casablanca taxonomischer Rang Dol', 'Dol Notfalleinrichtungen Casablanca', 'Kati taxonomischer Rang Kirche', 'Kirche Notfalleinrichtungen Kati', 'Kort taxonomischer Rang Bambino', 'Bambino Notfalleinrichtungen Kort', 'Teluk taxonomischer Rang Royal', 'Royal Notfalleinrichtungen Teluk', 'Farrell taxonomischer Rang ABS', 'ABS Notfalleinrichtungen Farrell', 'Ehren taxonomischer Rang Madison', 'Madison Notfalleinrichtungen Ehren', 'Tampere taxonomischer Rang Ostrava', 'Ostrava Notfalleinrichtungen Tampere', 'Kepler taxonomischer Rang Díaz', 'Díaz Notfalleinrichtungen Kepler', 'Madeleine taxonomischer Rang Gina', 'Gina Notfalleinrichtungen Madeleine', 'Milan taxonomischer Rang Viru', 'Viru Notfalleinrichtungen Milan', 'Translation taxonomischer Rang Wellington', 'Wellington Notfalleinrichtungen Translation', 'Koch taxonomischer Rang Icarus', 'Icarus Notfalleinrichtungen Koch', 'Butler taxonomischer Rang Yunan', 'Yunan Notfalleinrichtungen Butler', 'Neuchâtel taxonomischer Rang Slot', 'Slot Notfalleinrichtungen Neuchâtel', 'Delaware taxonomischer Rang Grammar', 'Grammar Notfalleinrichtungen Delaware', 'Wittenberg taxonomischer Rang Sân', 'Sân Notfalleinrichtungen Wittenberg', 'Cidade taxonomischer Rang NN', 'NN Notfalleinrichtungen Cidade', 'Murphy taxonomischer Rang Haag', 'Haag Notfalleinrichtungen Murphy', 'su taxonomischer Rang Cash', 'Cash Notfalleinrichtungen su', 'Ses taxonomischer Rang Basso', 'Basso Notfalleinrichtungen Ses', 'Progreso taxonomischer Rang Gibson', 'Gibson Notfalleinrichtungen Progreso', 'Curie taxonomischer Rang Nota', 'Nota Notfalleinrichtungen Curie', 'Ned taxonomischer Rang Hour', 'Hour Notfalleinrichtungen Ned', 'Zealand taxonomischer Rang Loch', 'Loch Notfalleinrichtungen Zealand', 'Bennett taxonomischer Rang Atatürk', 'Atatürk Notfalleinrichtungen Bennett', 'Porsche taxonomischer Rang México', 'México Notfalleinrichtungen Porsche', 'Villiers taxonomischer Rang Algarve', 'Algarve Notfalleinrichtungen Villiers', 'Niño taxonomischer Rang Generation', 'Generation Notfalleinrichtungen Niño', 'Balance taxonomischer Rang Rhein', 'Rhein Notfalleinrichtungen Balance', 'Barth taxonomischer Rang Brod', 'Brod Notfalleinrichtungen Barth', 'Robot taxonomischer Rang Dem', 'Dem Notfalleinrichtungen Robot', 'Sinh taxonomischer Rang Nigel', 'Nigel Notfalleinrichtungen Sinh', 'Gillespie taxonomischer Rang Holloway', 'Holloway Notfalleinrichtungen Gillespie', 'Titan taxonomischer Rang IX', 'IX Notfalleinrichtungen Titan', 'Tierra taxonomischer Rang Stal', 'Stal Notfalleinrichtungen Tierra', 'ID taxonomischer Rang Sato', 'Sato Notfalleinrichtungen ID', 'WWF taxonomischer Rang FX', 'FX Notfalleinrichtungen WWF', 'Azur taxonomischer Rang Vries', 'Vries Notfalleinrichtungen Azur', 'Reason taxonomischer Rang Fusion', 'Fusion Notfalleinrichtungen Reason', 'Luke taxonomischer Rang Terminal', 'Terminal Notfalleinrichtungen Luke', 'Trees taxonomischer Rang Maa', 'Maa Notfalleinrichtungen Trees', 'Morris taxonomischer Rang Bayreuth', 'Bayreuth Notfalleinrichtungen Morris', 'Monate taxonomischer Rang Catharina', 'Catharina Notfalleinrichtungen Monate', 'Norway taxonomischer Rang Naomi', 'Naomi Notfalleinrichtungen Norway', 'Han taxonomischer Rang Drew', 'Drew Notfalleinrichtungen Han', 'Cuban taxonomischer Rang Path', 'Path Notfalleinrichtungen Cuban', 'Melbourne taxonomischer Rang Hector', 'Hector Notfalleinrichtungen Melbourne', 'Rooma taxonomischer Rang Colle', 'Colle Notfalleinrichtungen Rooma', 'Ad taxonomischer Rang Inspector', 'Inspector Notfalleinrichtungen Ad', 'Christ taxonomischer Rang WDR', 'WDR Notfalleinrichtungen Christ', 'PSA taxonomischer Rang Nizza', 'Nizza Notfalleinrichtungen PSA', 'Omer taxonomischer Rang Camino', 'Camino Notfalleinrichtungen Omer', 'THE taxonomischer Rang Noche', 'Noche Notfalleinrichtungen THE', 'Hand taxonomischer Rang Patria', 'Patria Notfalleinrichtungen Hand', 'Urgell taxonomischer Rang Rees', 'Rees Notfalleinrichtungen Urgell', 'Liv taxonomischer Rang Tucumán', 'Tucumán Notfalleinrichtungen Liv', 'Bonaparte taxonomischer Rang Copper', 'Copper Notfalleinrichtungen Bonaparte', 'Tempo taxonomischer Rang KBS', 'KBS Notfalleinrichtungen Tempo', 'Abel taxonomischer Rang Sporting', 'Sporting Notfalleinrichtungen Abel', 'Gegen taxonomischer Rang Irvine', 'Irvine Notfalleinrichtungen Gegen', 'Hoy taxonomischer Rang Lilly', 'Lilly Notfalleinrichtungen Hoy', 'Stil taxonomischer Rang Amalia', 'Amalia Notfalleinrichtungen Stil', 'CDP taxonomischer Rang Güney', 'Güney Notfalleinrichtungen CDP', 'Kanal taxonomischer Rang Kepler', 'Kepler Notfalleinrichtungen Kanal', 'Izrael taxonomischer Rang Bala', 'Bala Notfalleinrichtungen Izrael', 'Genesis taxonomischer Rang Questa', 'Questa Notfalleinrichtungen Genesis', 'Eylül taxonomischer Rang Roberts', 'Roberts Notfalleinrichtungen Eylül', 'Vivaldi taxonomischer Rang Doom', 'Doom Notfalleinrichtungen Vivaldi', 'Mario taxonomischer Rang Halk', 'Halk Notfalleinrichtungen Mario', 'Power taxonomischer Rang Duo', 'Duo Notfalleinrichtungen Power', 'Saussure taxonomischer Rang Dub', 'Dub Notfalleinrichtungen Saussure', 'Muir taxonomischer Rang Emily', 'Emily Notfalleinrichtungen Muir', 'Alonso taxonomischer Rang Walls', 'Walls Notfalleinrichtungen Alonso', 'Largo taxonomischer Rang Claude', 'Claude Notfalleinrichtungen Largo', 'Phi taxonomischer Rang Court', 'Court Notfalleinrichtungen Phi', 'While taxonomischer Rang Down', 'Down Notfalleinrichtungen While', 'Rain taxonomischer Rang Crash', 'Crash Notfalleinrichtungen Rain', 'Canary taxonomischer Rang Gibson', 'Gibson Notfalleinrichtungen Canary', 'Arms taxonomischer Rang Kappa', 'Kappa Notfalleinrichtungen Arms', 'Bil taxonomischer Rang Cruz', 'Cruz Notfalleinrichtungen Bil', 'Cause taxonomischer Rang Nino', 'Nino Notfalleinrichtungen Cause', 'Hitler taxonomischer Rang Hubbard', 'Hubbard Notfalleinrichtungen Hitler', 'Kimberly taxonomischer Rang Georgetown', 'Georgetown Notfalleinrichtungen Kimberly', 'Swan taxonomischer Rang Host', 'Host Notfalleinrichtungen Swan', 'Parker taxonomischer Rang Sato', 'Sato Notfalleinrichtungen Parker', 'WDR taxonomischer Rang UCD', 'UCD Notfalleinrichtungen WDR', 'Gama taxonomischer Rang Vancouver', 'Vancouver Notfalleinrichtungen Gama', 'Pam taxonomischer Rang CR', 'CR Notfalleinrichtungen Pam', 'Leif taxonomischer Rang Dacia', 'Dacia Notfalleinrichtungen Leif', 'IM taxonomischer Rang Hop', 'Hop Notfalleinrichtungen IM', 'Comte taxonomischer Rang Lino', 'Lino Notfalleinrichtungen Comte', 'Stella taxonomischer Rang Wonderland', 'Wonderland Notfalleinrichtungen Stella', 'Speedway taxonomischer Rang Yates', 'Yates Notfalleinrichtungen Speedway', 'Linda taxonomischer Rang Sabbath', 'Sabbath Notfalleinrichtungen Linda', 'Dok taxonomischer Rang DM', 'DM Notfalleinrichtungen Dok', 'Earth taxonomischer Rang Imre', 'Imre Notfalleinrichtungen Earth', 'Ele taxonomischer Rang Marea', 'Marea Notfalleinrichtungen Ele', 'Limited taxonomischer Rang Interview', 'Interview Notfalleinrichtungen Limited', 'GM taxonomischer Rang Lennox', 'Lennox Notfalleinrichtungen GM', 'Isola taxonomischer Rang Thing', 'Thing Notfalleinrichtungen Isola', 'Kahn taxonomischer Rang Erzurum', 'Erzurum Notfalleinrichtungen Kahn', 'Slavic taxonomischer Rang Vanderbilt', 'Vanderbilt Notfalleinrichtungen Slavic', 'Subway taxonomischer Rang Murat', 'Murat Notfalleinrichtungen Subway', 'Tracy taxonomischer Rang Anthem', 'Anthem Notfalleinrichtungen Tracy', 'Stage taxonomischer Rang Saints', 'Saints Notfalleinrichtungen Stage', 'Rady taxonomischer Rang Nieto', 'Nieto Notfalleinrichtungen Rady', 'Tanner taxonomischer Rang Higher', 'Higher Notfalleinrichtungen Tanner', 'Newport taxonomischer Rang Andere', 'Andere Notfalleinrichtungen Newport', 'Holt taxonomischer Rang Each', 'Each Notfalleinrichtungen Holt', 'Ion taxonomischer Rang Hara', 'Hara Notfalleinrichtungen Ion', 'Amigos taxonomischer Rang Albert', 'Albert Notfalleinrichtungen Amigos', 'Bruges taxonomischer Rang Hazel', 'Hazel Notfalleinrichtungen Bruges', 'Astra taxonomischer Rang Sun', 'Sun Notfalleinrichtungen Astra', 'KK taxonomischer Rang Oud', 'Oud Notfalleinrichtungen KK', 'Alliance taxonomischer Rang Sheppard', 'Sheppard Notfalleinrichtungen Alliance', 'Dana taxonomischer Rang Haley', 'Haley Notfalleinrichtungen Dana', 'Rep taxonomischer Rang Schultz', 'Schultz Notfalleinrichtungen Rep', 'Albin taxonomischer Rang Cinta', 'Cinta Notfalleinrichtungen Albin', 'Nissan taxonomischer Rang Madagascar', 'Madagascar Notfalleinrichtungen Nissan', 'Aberdeen taxonomischer Rang Rond', 'Rond Notfalleinrichtungen Aberdeen', 'Az taxonomischer Rang Romawi', 'Romawi Notfalleinrichtungen Az', 'Jenna taxonomischer Rang Australian', 'Australian Notfalleinrichtungen Jenna', 'Hamlet taxonomischer Rang Humboldt', 'Humboldt Notfalleinrichtungen Hamlet', 'Pretoria taxonomischer Rang Sono', 'Sono Notfalleinrichtungen Pretoria', 'Latin taxonomischer Rang Engagement', 'Engagement Notfalleinrichtungen Latin', 'ESPN taxonomischer Rang Pleasure', 'Pleasure Notfalleinrichtungen ESPN', 'Florence taxonomischer Rang Gallagher', 'Gallagher Notfalleinrichtungen Florence', 'Philippe taxonomischer Rang Ryan', 'Ryan Notfalleinrichtungen Philippe', 'Phelps taxonomischer Rang Klub', 'Klub Notfalleinrichtungen Phelps', 'Zoom taxonomischer Rang Ken', 'Ken Notfalleinrichtungen Zoom', 'Isto taxonomischer Rang Lotto', 'Lotto Notfalleinrichtungen Isto', 'Sporting taxonomischer Rang Dante', 'Dante Notfalleinrichtungen Sporting', 'Star taxonomischer Rang Space', 'Space Notfalleinrichtungen Star', 'Padre taxonomischer Rang Closer', 'Closer Notfalleinrichtungen Padre', 'Perth taxonomischer Rang Cynthia', 'Cynthia Notfalleinrichtungen Perth', 'Cecil taxonomischer Rang Rain', 'Rain Notfalleinrichtungen Cecil', 'Rec taxonomischer Rang Fontana', 'Fontana Notfalleinrichtungen Rec', 'Katrina taxonomischer Rang Fisch', 'Fisch Notfalleinrichtungen Katrina', 'Lincoln taxonomischer Rang Truman', 'Truman Notfalleinrichtungen Lincoln', 'Xuân taxonomischer Rang Resident', 'Resident Notfalleinrichtungen Xuân', 'Pet taxonomischer Rang Meet', 'Meet Notfalleinrichtungen Pet', 'Hora taxonomischer Rang Aragón', 'Aragón Notfalleinrichtungen Hora', 'Heads taxonomischer Rang Albion', 'Albion Notfalleinrichtungen Heads', 'Coimbra taxonomischer Rang Faber', 'Faber Notfalleinrichtungen Coimbra', 'Magister taxonomischer Rang Bot', 'Bot Notfalleinrichtungen Magister', 'Kristen taxonomischer Rang Lodge', 'Lodge Notfalleinrichtungen Kristen', 'Benson taxonomischer Rang Jos', 'Jos Notfalleinrichtungen Benson', 'ES taxonomischer Rang Sunset', 'Sunset Notfalleinrichtungen ES', 'Ayn taxonomischer Rang Kita', 'Kita Notfalleinrichtungen Ayn', 'Cu taxonomischer Rang Hilton', 'Hilton Notfalleinrichtungen Cu', 'Champs taxonomischer Rang Abby', 'Abby Notfalleinrichtungen Champs', 'Lydia taxonomischer Rang Alicante', 'Alicante Notfalleinrichtungen Lydia', 'Greg taxonomischer Rang Ratu', 'Ratu Notfalleinrichtungen Greg', 'Fortaleza taxonomischer Rang Barre', 'Barre Notfalleinrichtungen Fortaleza', 'TSV taxonomischer Rang Bells', 'Bells Notfalleinrichtungen TSV', 'Bulu taxonomischer Rang Ramsay', 'Ramsay Notfalleinrichtungen Bulu', 'Finale taxonomischer Rang Berge', 'Berge Notfalleinrichtungen Finale', 'Mitt taxonomischer Rang Denne', 'Denne Notfalleinrichtungen Mitt', 'Lyman taxonomischer Rang Schumann', 'Schumann Notfalleinrichtungen Lyman', 'Zeeland taxonomischer Rang Sweeney', 'Sweeney Notfalleinrichtungen Zeeland', 'Algeria taxonomischer Rang Nisan', 'Nisan Notfalleinrichtungen Algeria', 'Dada taxonomischer Rang Buddy', 'Buddy Notfalleinrichtungen Dada', 'Sheridan taxonomischer Rang Verder', 'Verder Notfalleinrichtungen Sheridan', 'Pinto taxonomischer Rang Jared', 'Jared Notfalleinrichtungen Pinto', 'Kelly taxonomischer Rang Tabachnick', 'Tabachnick Notfalleinrichtungen Kelly', 'Soria taxonomischer Rang Maxime', 'Maxime Notfalleinrichtungen Soria', 'Fairfax taxonomischer Rang Ship', 'Ship Notfalleinrichtungen Fairfax', 'Tutte taxonomischer Rang Quito', 'Quito Notfalleinrichtungen Tutte', 'Cent taxonomischer Rang Mer', 'Mer Notfalleinrichtungen Cent', 'AG taxonomischer Rang Zeta', 'Zeta Notfalleinrichtungen AG', 'Fund taxonomischer Rang Mesa', 'Mesa Notfalleinrichtungen Fund', 'Louisville taxonomischer Rang Lyndon', 'Lyndon Notfalleinrichtungen Louisville', 'Veracruz taxonomischer Rang Globo', 'Globo Notfalleinrichtungen Veracruz', 'Sabbath taxonomischer Rang Fortune', 'Fortune Notfalleinrichtungen Sabbath', 'Duck taxonomischer Rang PKK', 'PKK Notfalleinrichtungen Duck', 'Jack taxonomischer Rang Swami', 'Swami Notfalleinrichtungen Jack', 'IRAS taxonomischer Rang Rotterdam', 'Rotterdam Notfalleinrichtungen IRAS', 'Cherokee taxonomischer Rang Tin', 'Tin Notfalleinrichtungen Cherokee', 'Oper taxonomischer Rang Abrams', 'Abrams Notfalleinrichtungen Oper', 'Raphaël taxonomischer Rang Hampton', 'Hampton Notfalleinrichtungen Raphaël', 'Women taxonomischer Rang Paso', 'Paso Notfalleinrichtungen Women', 'Patton taxonomischer Rang Appleton', 'Appleton Notfalleinrichtungen Patton', 'Al taxonomischer Rang Pool', 'Pool Notfalleinrichtungen Al', 'Oaxaca taxonomischer Rang Appleton', 'Appleton Notfalleinrichtungen Oaxaca', 'Direito taxonomischer Rang CDC', 'CDC Notfalleinrichtungen Direito', 'Province taxonomischer Rang Bent', 'Bent Notfalleinrichtungen Province', 'Grimaldi taxonomischer Rang Tales', 'Tales Notfalleinrichtungen Grimaldi', 'Ruska taxonomischer Rang Alzheimer', 'Alzheimer Notfalleinrichtungen Ruska', 'Jess taxonomischer Rang Venecia', 'Venecia Notfalleinrichtungen Jess', 'Lugar taxonomischer Rang Lobo', 'Lobo Notfalleinrichtungen Lugar', 'Paul taxonomischer Rang Dre', 'Dre Notfalleinrichtungen Paul', 'CAD taxonomischer Rang Jerome', 'Jerome Notfalleinrichtungen CAD', 'Adrian taxonomischer Rang Jamaica', 'Jamaica Notfalleinrichtungen Adrian', 'Color taxonomischer Rang Melo', 'Melo Notfalleinrichtungen Color', 'Thornton taxonomischer Rang Auvergne', 'Auvergne Notfalleinrichtungen Thornton', 'Kampung taxonomischer Rang Kirkwood', 'Kirkwood Notfalleinrichtungen Kampung', 'Britten taxonomischer Rang Washington', 'Washington Notfalleinrichtungen Britten', 'Eagle taxonomischer Rang Conway', 'Conway Notfalleinrichtungen Eagle', 'Spirit taxonomischer Rang Montreal', 'Montreal Notfalleinrichtungen Spirit', 'Command taxonomischer Rang Dio', 'Dio Notfalleinrichtungen Command', 'Day taxonomischer Rang Sand', 'Sand Notfalleinrichtungen Day', 'America taxonomischer Rang Lobo', 'Lobo Notfalleinrichtungen America', 'Blanca taxonomischer Rang Mu', 'Mu Notfalleinrichtungen Blanca', 'Sherlock taxonomischer Rang Raider', 'Raider Notfalleinrichtungen Sherlock', 'Qara taxonomischer Rang Petit', 'Petit Notfalleinrichtungen Qara', 'Enterprise taxonomischer Rang Brandt', 'Brandt Notfalleinrichtungen Enterprise', 'Pedra taxonomischer Rang Operator', 'Operator Notfalleinrichtungen Pedra', 'Milli taxonomischer Rang Flame', 'Flame Notfalleinrichtungen Milli', 'Lost taxonomischer Rang Webb', 'Webb Notfalleinrichtungen Lost', 'Johnny taxonomischer Rang PhD', 'PhD Notfalleinrichtungen Johnny', 'Macau taxonomischer Rang Challenger', 'Challenger Notfalleinrichtungen Macau', 'Manhattan taxonomischer Rang Morro', 'Morro Notfalleinrichtungen Manhattan', 'Monde taxonomischer Rang Invasion', 'Invasion Notfalleinrichtungen Monde', 'Turquia taxonomischer Rang Doubs', 'Doubs Notfalleinrichtungen Turquia', 'Cast taxonomischer Rang Olympique', 'Olympique Notfalleinrichtungen Cast', 'Balkan taxonomischer Rang Cast', 'Cast Notfalleinrichtungen Balkan', 'Mountain taxonomischer Rang Witt', 'Witt Notfalleinrichtungen Mountain', 'Madonna taxonomischer Rang Remo', 'Remo Notfalleinrichtungen Madonna', 'Huelva taxonomischer Rang Shuttle', 'Shuttle Notfalleinrichtungen Huelva', 'Brady taxonomischer Rang Jacqueline', 'Jacqueline Notfalleinrichtungen Brady', 'Mustang taxonomischer Rang Dawn', 'Dawn Notfalleinrichtungen Mustang', 'Caen taxonomischer Rang Monique', 'Monique Notfalleinrichtungen Caen', 'Wert taxonomischer Rang Rogers', 'Rogers Notfalleinrichtungen Wert', 'Savage taxonomischer Rang Hopper', 'Hopper Notfalleinrichtungen Savage', 'WBA taxonomischer Rang Wilson', 'Wilson Notfalleinrichtungen WBA', 'Elias taxonomischer Rang Friedrich', 'Friedrich Notfalleinrichtungen Elias', 'Houten taxonomischer Rang Alus', 'Alus Notfalleinrichtungen Houten', 'Rogers taxonomischer Rang Sieger', 'Sieger Notfalleinrichtungen Rogers', 'Change taxonomischer Rang Allison', 'Allison Notfalleinrichtungen Change', 'WRC taxonomischer Rang Dame', 'Dame Notfalleinrichtungen WRC', 'Gia taxonomischer Rang Interview', 'Interview Notfalleinrichtungen Gia', 'Jason taxonomischer Rang Larry', 'Larry Notfalleinrichtungen Jason', 'Th taxonomischer Rang Klaus', 'Klaus Notfalleinrichtungen Th', 'Prairie taxonomischer Rang Ferns', 'Ferns Notfalleinrichtungen Prairie', 'Tyne taxonomischer Rang Sanders', 'Sanders Notfalleinrichtungen Tyne', 'Culture taxonomischer Rang Bridge', 'Bridge Notfalleinrichtungen Culture', 'Rang taxonomischer Rang Odessa', 'Odessa Notfalleinrichtungen Rang', 'Ph taxonomischer Rang Hien', 'Hien Notfalleinrichtungen Ph', 'Macbeth taxonomischer Rang Harriet', 'Harriet Notfalleinrichtungen Macbeth', 'KHL taxonomischer Rang Kenneth', 'Kenneth Notfalleinrichtungen KHL', 'Escape taxonomischer Rang ITF', 'ITF Notfalleinrichtungen Escape', 'Racine taxonomischer Rang Sinh', 'Sinh Notfalleinrichtungen Racine', 'Alus taxonomischer Rang Pe', 'Pe Notfalleinrichtungen Alus', 'Jenny taxonomischer Rang ITV', 'ITV Notfalleinrichtungen Jenny', 'Conquest taxonomischer Rang FN', 'FN Notfalleinrichtungen Conquest', 'Bambino taxonomischer Rang Fran', 'Fran Notfalleinrichtungen Bambino', 'Hagen taxonomischer Rang Babylon', 'Babylon Notfalleinrichtungen Hagen', 'Police taxonomischer Rang PhD', 'PhD Notfalleinrichtungen Police', 'Buster taxonomischer Rang Bishop', 'Bishop Notfalleinrichtungen Buster', 'Cherbourg taxonomischer Rang Duty', 'Duty Notfalleinrichtungen Cherbourg', 'Piper taxonomischer Rang Stanton', 'Stanton Notfalleinrichtungen Piper', 'Mexico taxonomischer Rang Gesundheit', 'Gesundheit Notfalleinrichtungen Mexico', 'Edda taxonomischer Rang Espagne', 'Espagne Notfalleinrichtungen Edda', 'Yoshida taxonomischer Rang Loan', 'Loan Notfalleinrichtungen Yoshida', 'Door taxonomischer Rang Wilhelmina', 'Wilhelmina Notfalleinrichtungen Door', 'Lenny taxonomischer Rang Norsk', 'Norsk Notfalleinrichtungen Lenny', 'Copa taxonomischer Rang Damon', 'Damon Notfalleinrichtungen Copa', 'NF taxonomischer Rang Bassi', 'Bassi Notfalleinrichtungen NF', 'Sá taxonomischer Rang Vertreter', 'Vertreter Notfalleinrichtungen Sá', 'Raw taxonomischer Rang Zug', 'Zug Notfalleinrichtungen Raw', 'Salta taxonomischer Rang Mivel', 'Mivel Notfalleinrichtungen Salta', 'Turnier taxonomischer Rang Nicholson', 'Nicholson Notfalleinrichtungen Turnier', 'Sheppard taxonomischer Rang Abrams', 'Abrams Notfalleinrichtungen Sheppard', 'Garden taxonomischer Rang Dominic', 'Dominic Notfalleinrichtungen Garden', 'Wiley taxonomischer Rang Humbert', 'Humbert Notfalleinrichtungen Wiley', 'Sanremo taxonomischer Rang González', 'González Notfalleinrichtungen Sanremo', 'Tri taxonomischer Rang Hate', 'Hate Notfalleinrichtungen Tri', 'Spain taxonomischer Rang Carlisle', 'Carlisle Notfalleinrichtungen Spain', 'Orient taxonomischer Rang Frey', 'Frey Notfalleinrichtungen Orient', 'Puis taxonomischer Rang Primera', 'Primera Notfalleinrichtungen Puis', 'Syracuse taxonomischer Rang NL', 'NL Notfalleinrichtungen Syracuse', 'Assembly taxonomischer Rang Xuân', 'Xuân Notfalleinrichtungen Assembly', 'Giang taxonomischer Rang Malcolm', 'Malcolm Notfalleinrichtungen Giang', 'Yokohama taxonomischer Rang Melvin', 'Melvin Notfalleinrichtungen Yokohama', 'Amiens taxonomischer Rang Design', 'Design Notfalleinrichtungen Amiens', 'Tat taxonomischer Rang Rhine', 'Rhine Notfalleinrichtungen Tat', 'Hara taxonomischer Rang Bishop', 'Bishop Notfalleinrichtungen Hara', 'Space taxonomischer Rang Sweeney', 'Sweeney Notfalleinrichtungen Space', 'Trang taxonomischer Rang Carnaval', 'Carnaval Notfalleinrichtungen Trang', 'Fisher taxonomischer Rang Cassandra', 'Cassandra Notfalleinrichtungen Fisher', 'Titre taxonomischer Rang Hal', 'Hal Notfalleinrichtungen Titre', 'Animals taxonomischer Rang Hara', 'Hara Notfalleinrichtungen Animals', 'Codex taxonomischer Rang Paulo', 'Paulo Notfalleinrichtungen Codex', 'Feria taxonomischer Rang Selva', 'Selva Notfalleinrichtungen Feria', 'Corazón taxonomischer Rang Sicilia', 'Sicilia Notfalleinrichtungen Corazón', 'Russie taxonomischer Rang AF', 'AF Notfalleinrichtungen Russie', 'IPA taxonomischer Rang Knowles', 'Knowles Notfalleinrichtungen IPA', 'Borough taxonomischer Rang EE', 'EE Notfalleinrichtungen Borough', 'Nigel taxonomischer Rang Branca', 'Branca Notfalleinrichtungen Nigel', 'Dans taxonomischer Rang Slater', 'Slater Notfalleinrichtungen Dans', 'Papa taxonomischer Rang Dupont', 'Dupont Notfalleinrichtungen Papa', 'Bonnie taxonomischer Rang Sens', 'Sens Notfalleinrichtungen Bonnie', 'Leicester taxonomischer Rang Römer', 'Römer Notfalleinrichtungen Leicester', 'Jørgensen taxonomischer Rang Roi', 'Roi Notfalleinrichtungen Jørgensen', 'Gallia taxonomischer Rang Mendoza', 'Mendoza Notfalleinrichtungen Gallia', 'Ulysses taxonomischer Rang Fairfield', 'Fairfield Notfalleinrichtungen Ulysses', 'Souza taxonomischer Rang Eugenio', 'Eugenio Notfalleinrichtungen Souza', 'Dixon taxonomischer Rang Novel', 'Novel Notfalleinrichtungen Dixon', 'Revolución taxonomischer Rang Quelle', 'Quelle Notfalleinrichtungen Revolución', 'Aire taxonomischer Rang Hector', 'Hector Notfalleinrichtungen Aire', 'Gerd taxonomischer Rang Scotland', 'Scotland Notfalleinrichtungen Gerd', 'Battalion taxonomischer Rang Township', 'Township Notfalleinrichtungen Battalion', 'Sans taxonomischer Rang Senat', 'Senat Notfalleinrichtungen Sans', 'Pampa taxonomischer Rang Wiener', 'Wiener Notfalleinrichtungen Pampa', 'Borussia taxonomischer Rang Allier', 'Allier Notfalleinrichtungen Borussia', 'Edit taxonomischer Rang Prema', 'Prema Notfalleinrichtungen Edit', 'Mei taxonomischer Rang Michel', 'Michel Notfalleinrichtungen Mei', 'Mille taxonomischer Rang Spieler', 'Spieler Notfalleinrichtungen Mille', 'Tore taxonomischer Rang Radar', 'Radar Notfalleinrichtungen Tore', 'MAC taxonomischer Rang Break', 'Break Notfalleinrichtungen MAC', 'Island taxonomischer Rang Pass', 'Pass Notfalleinrichtungen Island', 'Liam taxonomischer Rang Brenner', 'Brenner Notfalleinrichtungen Liam', 'Bones taxonomischer Rang Bismarck', 'Bismarck Notfalleinrichtungen Bones', 'Alta taxonomischer Rang Cunha', 'Cunha Notfalleinrichtungen Alta', 'Kobayashi taxonomischer Rang Kamen', 'Kamen Notfalleinrichtungen Kobayashi', 'Rams taxonomischer Rang Mesa', 'Mesa Notfalleinrichtungen Rams', 'du taxonomischer Rang Verso', 'Verso Notfalleinrichtungen du', 'Een taxonomischer Rang Rota', 'Rota Notfalleinrichtungen Een', 'Khmer taxonomischer Rang Bauer', 'Bauer Notfalleinrichtungen Khmer', 'Gina taxonomischer Rang Vivian', 'Vivian Notfalleinrichtungen Gina', 'Anna taxonomischer Rang Daisy', 'Daisy Notfalleinrichtungen Anna', 'Dag taxonomischer Rang Genus', 'Genus Notfalleinrichtungen Dag', 'Secrets taxonomischer Rang Dunia', 'Dunia Notfalleinrichtungen Secrets', 'Pace taxonomischer Rang Oaxaca', 'Oaxaca Notfalleinrichtungen Pace', 'Christopher taxonomischer Rang Seychelles', 'Seychelles Notfalleinrichtungen Christopher', 'RCA taxonomischer Rang Fargo', 'Fargo Notfalleinrichtungen RCA', 'Quest taxonomischer Rang Titan', 'Titan Notfalleinrichtungen Quest', 'Kirby taxonomischer Rang Robot', 'Robot Notfalleinrichtungen Kirby', 'Reich taxonomischer Rang Thornton', 'Thornton Notfalleinrichtungen Reich', 'Geiger taxonomischer Rang Arad', 'Arad Notfalleinrichtungen Geiger', 'Pass taxonomischer Rang Penguin', 'Penguin Notfalleinrichtungen Pass', 'Sigurd taxonomischer Rang TNA', 'TNA Notfalleinrichtungen Sigurd', 'Mighty taxonomischer Rang Zombie', 'Zombie Notfalleinrichtungen Mighty', 'Ng taxonomischer Rang Bethlehem', 'Bethlehem Notfalleinrichtungen Ng', 'Nil taxonomischer Rang Cinq', 'Cinq Notfalleinrichtungen Nil', 'Senat taxonomischer Rang Goldstein', 'Goldstein Notfalleinrichtungen Senat', 'Herrschaft taxonomischer Rang Kuna', 'Kuna Notfalleinrichtungen Herrschaft', 'Poison taxonomischer Rang Cantor', 'Cantor Notfalleinrichtungen Poison', 'Buddy taxonomischer Rang Trend', 'Trend Notfalleinrichtungen Buddy', 'Finance taxonomischer Rang Clock', 'Clock Notfalleinrichtungen Finance', 'Idol taxonomischer Rang Princesa', 'Princesa Notfalleinrichtungen Idol', 'Brighton taxonomischer Rang Una', 'Una Notfalleinrichtungen Brighton', 'Mississippi taxonomischer Rang Rue', 'Rue Notfalleinrichtungen Mississippi', 'Pero taxonomischer Rang Jenny', 'Jenny Notfalleinrichtungen Pero', 'Acer taxonomischer Rang Antoine', 'Antoine Notfalleinrichtungen Acer', 'Shane taxonomischer Rang Colony', 'Colony Notfalleinrichtungen Shane', 'Stig taxonomischer Rang Thornton', 'Thornton Notfalleinrichtungen Stig', 'Walther taxonomischer Rang te', 'te Notfalleinrichtungen Walther', 'Schönberg taxonomischer Rang Fredrik', 'Fredrik Notfalleinrichtungen Schönberg', 'Po taxonomischer Rang Königsberg', 'Königsberg Notfalleinrichtungen Po', 'Sino taxonomischer Rang Glory', 'Glory Notfalleinrichtungen Sino', 'Levante taxonomischer Rang Cuenca', 'Cuenca Notfalleinrichtungen Levante', 'Princess taxonomischer Rang Ain', 'Ain Notfalleinrichtungen Princess', 'Greenwood taxonomischer Rang WRC', 'WRC Notfalleinrichtungen Greenwood', 'Gate taxonomischer Rang Blackmore', 'Blackmore Notfalleinrichtungen Gate', 'ARM taxonomischer Rang Salazar', 'Salazar Notfalleinrichtungen ARM', 'Shelby taxonomischer Rang Tun', 'Tun Notfalleinrichtungen Shelby', 'Townsend taxonomischer Rang Branko', 'Branko Notfalleinrichtungen Townsend', 'Helena taxonomischer Rang Bauer', 'Bauer Notfalleinrichtungen Helena', 'Murad taxonomischer Rang Coca', 'Coca Notfalleinrichtungen Murad', 'Basse taxonomischer Rang Petit', 'Petit Notfalleinrichtungen Basse', 'Sharks taxonomischer Rang Alternate', 'Alternate Notfalleinrichtungen Sharks', 'Sainte taxonomischer Rang Showtime', 'Showtime Notfalleinrichtungen Sainte', 'Vergine taxonomischer Rang Shaun', 'Shaun Notfalleinrichtungen Vergine', 'CAS taxonomischer Rang Camilla', 'Camilla Notfalleinrichtungen CAS', 'Weiler taxonomischer Rang Rosa', 'Rosa Notfalleinrichtungen Weiler', 'IN taxonomischer Rang Malaya', 'Malaya Notfalleinrichtungen IN', 'Vida taxonomischer Rang Generation', 'Generation Notfalleinrichtungen Vida', 'Goebbels taxonomischer Rang Lowry', 'Lowry Notfalleinrichtungen Goebbels', 'Congo taxonomischer Rang Masters', 'Masters Notfalleinrichtungen Congo', 'Esperanza taxonomischer Rang Porter', 'Porter Notfalleinrichtungen Esperanza', 'Loyola taxonomischer Rang Carlton', 'Carlton Notfalleinrichtungen Loyola', 'Exeter taxonomischer Rang Organ', 'Organ Notfalleinrichtungen Exeter', 'Jamaica taxonomischer Rang Orson', 'Orson Notfalleinrichtungen Jamaica', 'Yale taxonomischer Rang Mirko', 'Mirko Notfalleinrichtungen Yale', 'Boyd taxonomischer Rang Classe', 'Classe Notfalleinrichtungen Boyd', 'Fargo taxonomischer Rang ABD', 'ABD Notfalleinrichtungen Fargo', 'Mühle taxonomischer Rang Salto', 'Salto Notfalleinrichtungen Mühle', 'Darkness taxonomischer Rang Bar', 'Bar Notfalleinrichtungen Darkness', 'Ronda taxonomischer Rang Salud', 'Salud Notfalleinrichtungen Ronda', 'Palestine taxonomischer Rang Pi', 'Pi Notfalleinrichtungen Palestine', 'Remo taxonomischer Rang Courtney', 'Courtney Notfalleinrichtungen Remo', 'Orlando taxonomischer Rang Ballet', 'Ballet Notfalleinrichtungen Orlando', 'Hume taxonomischer Rang Schleswig', 'Schleswig Notfalleinrichtungen Hume', 'Aube taxonomischer Rang Lebanon', 'Lebanon Notfalleinrichtungen Aube', 'Prin taxonomischer Rang Hannah', 'Hannah Notfalleinrichtungen Prin', 'Amiga taxonomischer Rang Point', 'Point Notfalleinrichtungen Amiga', 'Jos taxonomischer Rang Benedict', 'Benedict Notfalleinrichtungen Jos', 'Harper taxonomischer Rang Blood', 'Blood Notfalleinrichtungen Harper', 'Birds taxonomischer Rang SAP', 'SAP Notfalleinrichtungen Birds', 'Murcia taxonomischer Rang VL', 'VL Notfalleinrichtungen Murcia', 'SP taxonomischer Rang Bleu', 'Bleu Notfalleinrichtungen SP', 'Guns taxonomischer Rang Titus', 'Titus Notfalleinrichtungen Guns', 'Albany taxonomischer Rang Brooklyn', 'Brooklyn Notfalleinrichtungen Albany', 'Dakar taxonomischer Rang FCC', 'FCC Notfalleinrichtungen Dakar', 'ASCII taxonomischer Rang Brazil', 'Brazil Notfalleinrichtungen ASCII', 'Maynard taxonomischer Rang Sébastien', 'Sébastien Notfalleinrichtungen Maynard', 'Agnes taxonomischer Rang Gama', 'Gama Notfalleinrichtungen Agnes', 'UCB taxonomischer Rang Porsche', 'Porsche Notfalleinrichtungen UCB', 'Schultz taxonomischer Rang Guardia', 'Guardia Notfalleinrichtungen Schultz', 'Hook taxonomischer Rang Galatasaray', 'Galatasaray Notfalleinrichtungen Hook', 'White taxonomischer Rang Siegel', 'Siegel Notfalleinrichtungen White', 'Carrier taxonomischer Rang Indie', 'Indie Notfalleinrichtungen Carrier', 'Oko taxonomischer Rang VL', 'VL Notfalleinrichtungen Oko', 'Carpenter taxonomischer Rang Kosovo', 'Kosovo Notfalleinrichtungen Carpenter', 'Mato taxonomischer Rang Bet', 'Bet Notfalleinrichtungen Mato', 'Trial taxonomischer Rang Hook', 'Hook Notfalleinrichtungen Trial', 'Rock taxonomischer Rang Maja', 'Maja Notfalleinrichtungen Rock', 'Yüksek taxonomischer Rang Krupp', 'Krupp Notfalleinrichtungen Yüksek', 'Vas taxonomischer Rang Knowles', 'Knowles Notfalleinrichtungen Vas', 'Pack taxonomischer Rang Card', 'Card Notfalleinrichtungen Pack', 'Midland taxonomischer Rang Bürger', 'Bürger Notfalleinrichtungen Midland', 'Chef taxonomischer Rang Junior', 'Junior Notfalleinrichtungen Chef', 'Concilio taxonomischer Rang Surat', 'Surat Notfalleinrichtungen Concilio', 'Linares taxonomischer Rang Pulau', 'Pulau Notfalleinrichtungen Linares', 'Jay taxonomischer Rang Uit', 'Uit Notfalleinrichtungen Jay', 'Payne taxonomischer Rang McKinley', 'McKinley Notfalleinrichtungen Payne', 'FN taxonomischer Rang Highway', 'Highway Notfalleinrichtungen FN', 'Pizza taxonomischer Rang Jupiter', 'Jupiter Notfalleinrichtungen Pizza', 'Monat taxonomischer Rang Tudor', 'Tudor Notfalleinrichtungen Monat', 'Rincón taxonomischer Rang Wrong', 'Wrong Notfalleinrichtungen Rincón', 'Gandhi taxonomischer Rang Ariane', 'Ariane Notfalleinrichtungen Gandhi', 'WBC taxonomischer Rang Malta', 'Malta Notfalleinrichtungen WBC', 'Grupa taxonomischer Rang Gilmore', 'Gilmore Notfalleinrichtungen Grupa', 'Fury taxonomischer Rang Product', 'Product Notfalleinrichtungen Fury', 'Advance taxonomischer Rang Reed', 'Reed Notfalleinrichtungen Advance', 'Brunnen taxonomischer Rang Catharina', 'Catharina Notfalleinrichtungen Brunnen', 'Trinity taxonomischer Rang Evan', 'Evan Notfalleinrichtungen Trinity', 'Ex taxonomischer Rang Reflections', 'Reflections Notfalleinrichtungen Ex', 'Fox taxonomischer Rang Victorian', 'Victorian Notfalleinrichtungen Fox', 'Natal taxonomischer Rang Ost', 'Ost Notfalleinrichtungen Natal', 'RTL taxonomischer Rang Roosevelt', 'Roosevelt Notfalleinrichtungen RTL', 'Carthage taxonomischer Rang Titan', 'Titan Notfalleinrichtungen Carthage', 'María taxonomischer Rang Jane', 'Jane Notfalleinrichtungen María', 'tu taxonomischer Rang Register', 'Register Notfalleinrichtungen tu', 'UDP taxonomischer Rang Margareta', 'Margareta Notfalleinrichtungen UDP', 'Jake taxonomischer Rang Caldwell', 'Caldwell Notfalleinrichtungen Jake', 'Vivian taxonomischer Rang Rumble', 'Rumble Notfalleinrichtungen Vivian', 'sy taxonomischer Rang RPM', 'RPM Notfalleinrichtungen sy', 'Mobile taxonomischer Rang Boa', 'Boa Notfalleinrichtungen Mobile', 'Andere taxonomischer Rang Kirchner', 'Kirchner Notfalleinrichtungen Andere', 'Grammar taxonomischer Rang Riley', 'Riley Notfalleinrichtungen Grammar', 'Doom taxonomischer Rang Ut', 'Ut Notfalleinrichtungen Doom', 'SBS taxonomischer Rang Lager', 'Lager Notfalleinrichtungen SBS', 'asa taxonomischer Rang Söhne', 'Söhne Notfalleinrichtungen asa', 'Randolph taxonomischer Rang PMC', 'PMC Notfalleinrichtungen Randolph', 'Richter taxonomischer Rang Roja', 'Roja Notfalleinrichtungen Richter', 'Room taxonomischer Rang Coral', 'Coral Notfalleinrichtungen Room', 'Vis taxonomischer Rang Sono', 'Sono Notfalleinrichtungen Vis', 'Bengal taxonomischer Rang Reis', 'Reis Notfalleinrichtungen Bengal', 'Flynn taxonomischer Rang IGE', 'IGE Notfalleinrichtungen Flynn', 'Weston taxonomischer Rang Letters', 'Letters Notfalleinrichtungen Weston', 'Segura taxonomischer Rang Darmstadt', 'Darmstadt Notfalleinrichtungen Segura', 'Gets taxonomischer Rang Swami', 'Swami Notfalleinrichtungen Gets', 'Snow taxonomischer Rang Brabant', 'Brabant Notfalleinrichtungen Snow', 'NRW taxonomischer Rang Temps', 'Temps Notfalleinrichtungen NRW', 'Perry taxonomischer Rang Christensen', 'Christensen Notfalleinrichtungen Perry', 'Ribera taxonomischer Rang Bethlehem', 'Bethlehem Notfalleinrichtungen Ribera', 'Batavia taxonomischer Rang Gegen', 'Gegen Notfalleinrichtungen Batavia', 'Criminal taxonomischer Rang Giovanna', 'Giovanna Notfalleinrichtungen Criminal', 'Abby taxonomischer Rang Cornwall', 'Cornwall Notfalleinrichtungen Abby', 'Hatch taxonomischer Rang Pool', 'Pool Notfalleinrichtungen Hatch', 'Cáceres taxonomischer Rang Varese', 'Varese Notfalleinrichtungen Cáceres', 'Namibia taxonomischer Rang Melanie', 'Melanie Notfalleinrichtungen Namibia', 'Ferdinand taxonomischer Rang Ter', 'Ter Notfalleinrichtungen Ferdinand', 'Helen taxonomischer Rang Neckar', 'Neckar Notfalleinrichtungen Helen', 'Collegiate taxonomischer Rang Grenoble', 'Grenoble Notfalleinrichtungen Collegiate', 'Milne taxonomischer Rang Gia', 'Gia Notfalleinrichtungen Milne', 'BRT taxonomischer Rang Sims', 'Sims Notfalleinrichtungen BRT', 'U taxonomischer Rang Jackson', 'Jackson Notfalleinrichtungen U', 'Toulon taxonomischer Rang CPU', 'CPU Notfalleinrichtungen Toulon', 'Elvis taxonomischer Rang Interview', 'Interview Notfalleinrichtungen Elvis', 'Damon taxonomischer Rang Animal', 'Animal Notfalleinrichtungen Damon', 'Socorro taxonomischer Rang Europa', 'Europa Notfalleinrichtungen Socorro', 'Newell taxonomischer Rang Prestige', 'Prestige Notfalleinrichtungen Newell', 'Car taxonomischer Rang Weir', 'Weir Notfalleinrichtungen Car', 'Quincy taxonomischer Rang Shock', 'Shock Notfalleinrichtungen Quincy', 'Erfurt taxonomischer Rang Hora', 'Hora Notfalleinrichtungen Erfurt', 'Gattung taxonomischer Rang Qi', 'Qi Notfalleinrichtungen Gattung', 'Chiara taxonomischer Rang Cliff', 'Cliff Notfalleinrichtungen Chiara', 'Davida taxonomischer Rang Ka', 'Ka Notfalleinrichtungen Davida', 'Valencia taxonomischer Rang Apollo', 'Apollo Notfalleinrichtungen Valencia', 'Maja taxonomischer Rang Darat', 'Darat Notfalleinrichtungen Maja', 'Cinta taxonomischer Rang Ellis', 'Ellis Notfalleinrichtungen Cinta', 'Sergei taxonomischer Rang Benoit', 'Benoit Notfalleinrichtungen Sergei', 'Bangalore taxonomischer Rang Linha', 'Linha Notfalleinrichtungen Bangalore', 'Lorenz taxonomischer Rang Dessa', 'Dessa Notfalleinrichtungen Lorenz', 'Eye taxonomischer Rang Brooklyn', 'Brooklyn Notfalleinrichtungen Eye', 'Díaz taxonomischer Rang Yokohama', 'Yokohama Notfalleinrichtungen Díaz', 'Pamplona taxonomischer Rang Ragnar', 'Ragnar Notfalleinrichtungen Pamplona', 'Alicante taxonomischer Rang Spezia', 'Spezia Notfalleinrichtungen Alicante', 'Mouse taxonomischer Rang Rogers', 'Rogers Notfalleinrichtungen Mouse', 'Welle taxonomischer Rang Noise', 'Noise Notfalleinrichtungen Welle', 'Beast taxonomischer Rang Cotton', 'Cotton Notfalleinrichtungen Beast', 'CDATA taxonomischer Rang Geld', 'Geld Notfalleinrichtungen CDATA', 'WM taxonomischer Rang ac', 'ac Notfalleinrichtungen WM', 'Hull taxonomischer Rang Pay', 'Pay Notfalleinrichtungen Hull', 'Lauren taxonomischer Rang Padre', 'Padre Notfalleinrichtungen Lauren', 'Gers taxonomischer Rang Oslo', 'Oslo Notfalleinrichtungen Gers', 'Archie taxonomischer Rang avoir', 'avoir Notfalleinrichtungen Archie', 'Hunter taxonomischer Rang Oslo', 'Oslo Notfalleinrichtungen Hunter', 'Angels taxonomischer Rang Mont', 'Mont Notfalleinrichtungen Angels', 'Lance taxonomischer Rang Davidson', 'Davidson Notfalleinrichtungen Lance', 'Jet taxonomischer Rang cal', 'cal Notfalleinrichtungen Jet', 'Trent taxonomischer Rang Hitchcock', 'Hitchcock Notfalleinrichtungen Trent', 'Pleasure taxonomischer Rang Jurist', 'Jurist Notfalleinrichtungen Pleasure', 'SVT taxonomischer Rang Scoble', 'Scoble Notfalleinrichtungen SVT', 'Fermi taxonomischer Rang Trung', 'Trung Notfalleinrichtungen Fermi', 'Sweden taxonomischer Rang Lowe', 'Lowe Notfalleinrichtungen Sweden', 'Taylor taxonomischer Rang Stara', 'Stara Notfalleinrichtungen Taylor', 'Chez taxonomischer Rang Format', 'Format Notfalleinrichtungen Chez', 'Lennon taxonomischer Rang Jesús', 'Jesús Notfalleinrichtungen Lennon', 'Junie taxonomischer Rang Staat', 'Staat Notfalleinrichtungen Junie', 'Haji taxonomischer Rang Raymond', 'Yahoo taxonomischer Rang Chinese', 'Stal taxonomischer Rang West', 'FC taxonomischer Rang Rhode', 'Dad taxonomischer Rang Libro', 'Kenia taxonomischer Rang Weaver', 'CCD taxonomischer Rang Limited', 'Riau taxonomischer Rang NO', 'Ky taxonomischer Rang Frères', 'Billie taxonomischer Rang Li', 'Elbe taxonomischer Rang Pie', 'Paraíso taxonomischer Rang DSM', 'TD taxonomischer Rang Björn', 'Luther taxonomischer Rang Elsevier', 'Roi taxonomischer Rang Isabel', 'Pole taxonomischer Rang Valence', 'Page taxonomischer Rang Townsend', 'Baron taxonomischer Rang Levant', 'Libia taxonomischer Rang Khan', 'Cuenca taxonomischer Rang Ward', 'Kálmán taxonomischer Rang Valladolid', 'ET taxonomischer Rang Kristen', 'Mainstream taxonomischer Rang Allende', 'Agency taxonomischer Rang Malden', 'Mata taxonomischer Rang Ekim', 'Mineral taxonomischer Rang Norris', 'Figaro taxonomischer Rang Entangled', 'Trung taxonomischer Rang Nico', 'Sabha taxonomischer Rang NME', 'Guimarães taxonomischer Rang Christi', 'Disneyland taxonomischer Rang Laurel', 'Wes taxonomischer Rang Hammer', 'Gesù taxonomischer Rang Desse', 'Cinq taxonomischer Rang Albany', 'Silla taxonomischer Rang Hollow', 'JR taxonomischer Rang Music', 'Bryant taxonomischer Rang Munro', 'Klaus taxonomischer Rang Loving', 'Kleiner taxonomischer Rang Sun', 'Fort taxonomischer Rang Ros', 'City taxonomischer Rang Marie', 'Márquez taxonomischer Rang Libération', 'Yer taxonomischer Rang Mass', 'Bernard taxonomischer Rang Liu', 'Diaz taxonomischer Rang Belgium', 'Madre taxonomischer Rang Edouard', 'Asturias taxonomischer Rang te', 'FF taxonomischer Rang Brabant', 'Dis taxonomischer Rang Quick', 'Lorentz taxonomischer Rang Primavera', 'Tito Notfalleinrichtungen SM', 'Jubilee Notfalleinrichtungen Sempre', 'Nassau Notfalleinrichtungen Portsmouth', 'Nexus Notfalleinrichtungen Irena', 'Hiroshima Notfalleinrichtungen Oaks', 'Jenny Notfalleinrichtungen Fighting', 'Cardoso Notfalleinrichtungen Bilbao', 'Rusi Notfalleinrichtungen Sharon', 'Silvio Notfalleinrichtungen Allium', 'Scots Notfalleinrichtungen Haiti', 'Helmut Notfalleinrichtungen Isabella', 'Novel Notfalleinrichtungen Olsson', 'KM Notfalleinrichtungen Pure', 'Industria Notfalleinrichtungen Patrol', 'ap Notfalleinrichtungen Siege', 'Frères Notfalleinrichtungen Galiza', 'RN Notfalleinrichtungen Lucky', 'Gilbert Notfalleinrichtungen Pfeiffer', 'Bachelor Notfalleinrichtungen FX', 'GNU Notfalleinrichtungen Energy', 'UE Notfalleinrichtungen Joanne', 'Hamm Notfalleinrichtungen Erik', 'Rua Notfalleinrichtungen EM', 'Ike Notfalleinrichtungen Raum', 'Velvet Notfalleinrichtungen VL', 'Levant Notfalleinrichtungen Sabina', 'Spin Notfalleinrichtungen Krupp', 'Franz Notfalleinrichtungen October', 'SAR Notfalleinrichtungen Davidson', 'Taurus Notfalleinrichtungen Allah', 'Denne Notfalleinrichtungen Larva', 'Pizarro Notfalleinrichtungen Khu', 'Sunrise Notfalleinrichtungen Lahore', 'Burlington Notfalleinrichtungen Dorset', 'Invasion Notfalleinrichtungen Stay', 'Wiesbaden Notfalleinrichtungen Sit', 'Oregon Notfalleinrichtungen Wayne', 'Jess Notfalleinrichtungen Flag', 'Trees Notfalleinrichtungen Ferns', 'Norris Notfalleinrichtungen Jaguar', 'Raoul Notfalleinrichtungen Brenda', 'Neu Notfalleinrichtungen Mina', 'Gia Notfalleinrichtungen Harding', 'Pisa Notfalleinrichtungen Giants', 'State Notfalleinrichtungen Buta', 'Hidden Notfalleinrichtungen Faye', 'Mill Notfalleinrichtungen Generation', 'Everett Notfalleinrichtungen Schottland', 'Birinci Notfalleinrichtungen USD', 'Sparks Notfalleinrichtungen Eu', 'Medina Farbe Alice']\n"
     ]
    }
   ],
   "source": [
    "print(train_dict['sample'][:1901])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38896e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SM taxon rank Tito',\n",
       " 'Sempre taxon rank Jubilee',\n",
       " 'Portsmouth taxon rank Nassau',\n",
       " 'Irena taxon rank Nexus',\n",
       " 'Oaks taxon rank Hiroshima',\n",
       " 'Fighting taxon rank Jenny',\n",
       " 'Bilbao taxon rank Cardoso',\n",
       " 'Sharon taxon rank Rusi',\n",
       " 'Allium taxon rank Silvio',\n",
       " 'Haiti taxon rank Scots',\n",
       " 'Isabella taxon rank Helmut',\n",
       " 'Olsson taxon rank Novel',\n",
       " 'Pure taxon rank KM',\n",
       " 'Patrol taxon rank Industria',\n",
       " 'Siege taxon rank ap',\n",
       " 'Galiza taxon rank Frères',\n",
       " 'Lucky taxon rank RN',\n",
       " 'Pfeiffer taxon rank Gilbert',\n",
       " 'FX taxon rank Bachelor',\n",
       " 'Energy taxon rank GNU',\n",
       " 'Joanne taxon rank UE',\n",
       " 'Erik taxon rank Hamm',\n",
       " 'EM taxon rank Rua',\n",
       " 'Raum taxon rank Ike',\n",
       " 'VL taxon rank Velvet',\n",
       " 'Sabina taxon rank Levant',\n",
       " 'Krupp taxon rank Spin',\n",
       " 'October taxon rank Franz',\n",
       " 'Davidson taxon rank SAR',\n",
       " 'Allah taxon rank Taurus',\n",
       " 'Larva taxon rank Denne',\n",
       " 'Khu taxon rank Pizarro',\n",
       " 'Lahore taxon rank Sunrise',\n",
       " 'Dorset taxon rank Burlington',\n",
       " 'Stay taxon rank Invasion',\n",
       " 'Sit taxon rank Wiesbaden',\n",
       " 'Wayne taxon rank Oregon',\n",
       " 'Flag taxon rank Jess',\n",
       " 'Ferns taxon rank Trees',\n",
       " 'Jaguar taxon rank Norris',\n",
       " 'Brenda taxon rank Raoul',\n",
       " 'Mina taxon rank Neu',\n",
       " 'Harding taxon rank Gia',\n",
       " 'Giants taxon rank Pisa',\n",
       " 'Buta taxon rank State',\n",
       " 'Faye taxon rank Hidden',\n",
       " 'Generation taxon rank Mill',\n",
       " 'Schottland taxon rank Everett',\n",
       " 'USD taxon rank Birinci',\n",
       " 'Eu taxon rank Sparks',\n",
       " 'Raymond emergency services Haji',\n",
       " 'Chinese emergency services Yahoo',\n",
       " 'West emergency services Stal',\n",
       " 'Rhode emergency services FC',\n",
       " 'Libro emergency services Dad',\n",
       " 'Weaver emergency services Kenia',\n",
       " 'Limited emergency services CCD',\n",
       " 'NO emergency services Riau',\n",
       " 'Frères emergency services Ky',\n",
       " 'Li emergency services Billie',\n",
       " 'Pie emergency services Elbe',\n",
       " 'DSM emergency services Paraíso',\n",
       " 'Björn emergency services TD',\n",
       " 'Elsevier emergency services Luther',\n",
       " 'Isabel emergency services Roi',\n",
       " 'Valence emergency services Pole',\n",
       " 'Townsend emergency services Page',\n",
       " 'Levant emergency services Baron',\n",
       " 'Khan emergency services Libia',\n",
       " 'Ward emergency services Cuenca',\n",
       " 'Valladolid emergency services Kálmán',\n",
       " 'Kristen emergency services ET',\n",
       " 'Allende emergency services Mainstream',\n",
       " 'Malden emergency services Agency',\n",
       " 'Ekim emergency services Mata',\n",
       " 'Norris emergency services Mineral',\n",
       " 'Entangled emergency services Figaro',\n",
       " 'Nico emergency services Trung',\n",
       " 'NME emergency services Sabha',\n",
       " 'Christi emergency services Guimarães',\n",
       " 'Laurel emergency services Disneyland',\n",
       " 'Hammer emergency services Wes',\n",
       " 'Desse emergency services Gesù',\n",
       " 'Albany emergency services Cinq',\n",
       " 'Hollow emergency services Silla',\n",
       " 'Music emergency services JR',\n",
       " 'Munro emergency services Bryant',\n",
       " 'Loving emergency services Klaus',\n",
       " 'Sun emergency services Kleiner',\n",
       " 'Ros emergency services Fort',\n",
       " 'Marie emergency services City',\n",
       " 'Libération emergency services Márquez',\n",
       " 'Mass emergency services Yer',\n",
       " 'Liu emergency services Bernard',\n",
       " 'Belgium emergency services Diaz',\n",
       " 'Edouard emergency services Madre',\n",
       " 'te emergency services Asturias',\n",
       " 'Brabant emergency services FF',\n",
       " 'Quick emergency services Dis',\n",
       " 'Primavera emergency services Lorentz',\n",
       " 'SM color Carpenter',\n",
       " 'Sempre color Calderón',\n",
       " 'Portsmouth color RN',\n",
       " 'Irena color Miracle',\n",
       " 'Oaks color Alpi',\n",
       " 'Fighting color President',\n",
       " 'Bilbao color Caesar',\n",
       " 'Sharon color Cádiz',\n",
       " 'Allium color Animal',\n",
       " 'Haiti color Powell',\n",
       " 'Isabella color Penelope',\n",
       " 'Olsson color Braga',\n",
       " 'Pure color Kamen',\n",
       " 'Patrol color Lindl',\n",
       " 'Siege color Roberts',\n",
       " 'Galiza color Sofia',\n",
       " 'Lucky color Pitt',\n",
       " 'Pfeiffer color Esther',\n",
       " 'FX color Villiers',\n",
       " 'Energy color CAS',\n",
       " 'Joanne color Rutherford',\n",
       " 'Erik color Alvin',\n",
       " 'EM color Fritz',\n",
       " 'Raum color Ferreira',\n",
       " 'VL color Imperium',\n",
       " 'Sabina color Rollins',\n",
       " 'Krupp color Peck',\n",
       " 'October color Osman',\n",
       " 'Davidson color Cidade',\n",
       " 'Allah color Regional',\n",
       " 'Larva color Gunung',\n",
       " 'Khu color Cu',\n",
       " 'Lahore color Holocaust',\n",
       " 'Dorset color Return',\n",
       " 'Stay color Kemp',\n",
       " 'Sit color Late',\n",
       " 'Wayne color Rayon',\n",
       " 'Flag color Joy',\n",
       " 'Ferns color Graves',\n",
       " 'Jaguar color Bavaria',\n",
       " 'Brenda color Norwich',\n",
       " 'Mina color Advance',\n",
       " 'Harding color Illusion',\n",
       " 'Giants color Nice',\n",
       " 'Buta color BL',\n",
       " 'Faye color Kinos',\n",
       " 'Generation color Severus',\n",
       " 'Schottland color Lowell',\n",
       " 'USD color Ole',\n",
       " 'Eu color Platinum',\n",
       " 'Eugenia expected completeness Haji',\n",
       " 'Ocak expected completeness Yahoo',\n",
       " 'Astor expected completeness Stal',\n",
       " 'Piero expected completeness FC',\n",
       " 'Juli expected completeness Dad',\n",
       " 'Nina expected completeness Kenia',\n",
       " 'Tower expected completeness CCD',\n",
       " 'Sigurd expected completeness Riau',\n",
       " 'Lopes expected completeness Ky',\n",
       " 'Bertram expected completeness Billie',\n",
       " 'Billy expected completeness Elbe',\n",
       " 'Man expected completeness Paraíso',\n",
       " 'Naga expected completeness TD',\n",
       " 'Antonio expected completeness Luther',\n",
       " 'Shadows expected completeness Roi',\n",
       " 'CPU expected completeness Pole',\n",
       " 'Maxime expected completeness Page',\n",
       " 'Angel expected completeness Baron',\n",
       " 'Norsk expected completeness Libia',\n",
       " 'Bron expected completeness Cuenca',\n",
       " 'Wagen expected completeness Kálmán',\n",
       " 'Nella expected completeness ET',\n",
       " 'Issue expected completeness Mainstream',\n",
       " 'Sono expected completeness Agency',\n",
       " 'Sherwood expected completeness Mata',\n",
       " 'Freddie expected completeness Mineral',\n",
       " 'ANC expected completeness Figaro',\n",
       " 'Mama expected completeness Trung',\n",
       " 'Mills expected completeness Sabha',\n",
       " 'Juventus expected completeness Guimarães',\n",
       " 'TM expected completeness Disneyland',\n",
       " 'Hammer expected completeness Wes',\n",
       " 'Ferguson expected completeness Gesù',\n",
       " 'Primer expected completeness Cinq',\n",
       " 'Dublin expected completeness Silla',\n",
       " 'Ballet expected completeness JR',\n",
       " 'Scholar expected completeness Bryant',\n",
       " 'Squadron expected completeness Klaus',\n",
       " 'Kur expected completeness Kleiner',\n",
       " 'Twain expected completeness Fort',\n",
       " 'Alexa expected completeness City',\n",
       " 'Choice expected completeness Márquez',\n",
       " 'Teater expected completeness Yer',\n",
       " 'England expected completeness Bernard',\n",
       " 'Lopes expected completeness Diaz',\n",
       " 'Nirvana expected completeness Madre',\n",
       " 'Home expected completeness Asturias',\n",
       " 'Rookie expected completeness FF',\n",
       " 'Jesus expected completeness Dis',\n",
       " 'Nota expected completeness Lorentz',\n",
       " 'SM measured physical quantity Bassa',\n",
       " 'Sempre measured physical quantity Bush',\n",
       " 'Portsmouth measured physical quantity Train',\n",
       " 'Irena measured physical quantity Marea',\n",
       " 'Oaks measured physical quantity Mahler',\n",
       " 'Fighting measured physical quantity Kati',\n",
       " 'Bilbao measured physical quantity Lyle',\n",
       " 'Sharon measured physical quantity Preto',\n",
       " 'Allium measured physical quantity Königsberg',\n",
       " 'Haiti measured physical quantity Uit',\n",
       " 'Isabella measured physical quantity Farm',\n",
       " 'Olsson measured physical quantity Midden',\n",
       " 'Pure measured physical quantity Chicago',\n",
       " 'Patrol measured physical quantity Dol',\n",
       " 'Siege measured physical quantity Pero',\n",
       " 'Galiza measured physical quantity Toten',\n",
       " 'Lucky measured physical quantity Tears',\n",
       " 'Pfeiffer measured physical quantity Minister',\n",
       " 'FX measured physical quantity Valenciana',\n",
       " 'Energy measured physical quantity Dunkerque',\n",
       " 'Joanne measured physical quantity Sartre',\n",
       " 'Erik measured physical quantity Front',\n",
       " 'EM measured physical quantity CAF',\n",
       " 'Raum measured physical quantity WR',\n",
       " 'VL measured physical quantity Donovan',\n",
       " 'Sabina measured physical quantity Bey',\n",
       " 'Krupp measured physical quantity Alexa',\n",
       " 'October measured physical quantity Porter',\n",
       " 'Davidson measured physical quantity Euro',\n",
       " 'Allah measured physical quantity Shri',\n",
       " 'Larva measured physical quantity Atelier',\n",
       " 'Khu measured physical quantity Classic',\n",
       " 'Lahore measured physical quantity Noche',\n",
       " 'Dorset measured physical quantity Cavendish',\n",
       " 'Stay measured physical quantity Sinh',\n",
       " 'Sit measured physical quantity Sports',\n",
       " 'Wayne measured physical quantity Denne',\n",
       " 'Flag measured physical quantity Sonntag',\n",
       " 'Ferns measured physical quantity Terceira',\n",
       " 'Jaguar measured physical quantity Au',\n",
       " 'Brenda measured physical quantity Jammu',\n",
       " 'Mina measured physical quantity Carnaval',\n",
       " 'Harding measured physical quantity Ferreira',\n",
       " 'Giants measured physical quantity Fenner',\n",
       " 'Buta measured physical quantity Vader',\n",
       " 'Faye measured physical quantity Bray',\n",
       " 'Generation measured physical quantity Barnard',\n",
       " 'Schottland measured physical quantity Earl',\n",
       " 'USD measured physical quantity Manu',\n",
       " 'Eu measured physical quantity Dara',\n",
       " 'Zo open period from Haji',\n",
       " 'Odin open period from Yahoo',\n",
       " 'Cardiff open period from Stal',\n",
       " 'Linh open period from FC',\n",
       " 'Plains open period from Dad',\n",
       " 'Irena open period from Kenia',\n",
       " 'Montpellier open period from CCD',\n",
       " 'Manor open period from Riau',\n",
       " 'Trung open period from Ky',\n",
       " 'Frères open period from Billie',\n",
       " 'Cash open period from Elbe',\n",
       " 'Far open period from Paraíso',\n",
       " 'Pleasure open period from TD',\n",
       " 'Aid open period from Luther',\n",
       " 'Rioja open period from Roi',\n",
       " 'UD open period from Pole',\n",
       " 'CC open period from Page',\n",
       " 'Cambridge open period from Baron',\n",
       " 'Teddy open period from Libia',\n",
       " 'Crown open period from Cuenca',\n",
       " 'Bing open period from Kálmán',\n",
       " 'Face open period from ET',\n",
       " 'Marne open period from Mainstream',\n",
       " 'Hawker open period from Agency',\n",
       " 'Parry open period from Mata',\n",
       " 'Borussia open period from Mineral',\n",
       " 'Greece open period from Figaro',\n",
       " 'Telephone open period from Trung',\n",
       " 'MBA open period from Sabha',\n",
       " 'Latreille open period from Guimarães',\n",
       " 'Principal open period from Disneyland',\n",
       " 'Dad open period from Wes',\n",
       " 'Belfast open period from Gesù',\n",
       " 'Ages open period from Cinq',\n",
       " 'Massacre open period from Silla',\n",
       " 'Hub open period from JR',\n",
       " 'Newton open period from Bryant',\n",
       " 'Stand open period from Klaus',\n",
       " 'McKamey open period from Kleiner',\n",
       " 'Powers open period from Fort',\n",
       " 'Leta open period from City',\n",
       " 'Mad open period from Márquez',\n",
       " 'Samba open period from Yer',\n",
       " 'Senators open period from Bernard',\n",
       " 'Fel open period from Diaz',\n",
       " 'Hughes open period from Madre',\n",
       " 'Potok open period from Asturias',\n",
       " 'Maxim open period from FF',\n",
       " 'Winter open period from Dis',\n",
       " 'Morley open period from Lorentz',\n",
       " 'SM platform NH',\n",
       " 'Sempre platform Door',\n",
       " 'Portsmouth platform Profil',\n",
       " 'Irena platform Words',\n",
       " 'Oaks platform PE',\n",
       " 'Fighting platform Hamar',\n",
       " 'Bilbao platform Ribera',\n",
       " 'Sharon platform Crash',\n",
       " 'Allium platform Stella',\n",
       " 'Haiti platform Béla',\n",
       " 'Isabella platform Ferro',\n",
       " 'Olsson platform Writer',\n",
       " 'Pure platform Aviation',\n",
       " 'Patrol platform da',\n",
       " 'Siege platform Hercules',\n",
       " 'Galiza platform Elmi',\n",
       " 'Lucky platform Heide',\n",
       " 'Pfeiffer platform Armstrong',\n",
       " 'FX platform Remote',\n",
       " 'Energy platform Nagar',\n",
       " 'Joanne platform Moro',\n",
       " 'Erik platform Ami',\n",
       " 'EM platform CL',\n",
       " 'Raum platform Toren',\n",
       " 'VL platform Ph',\n",
       " 'Sabina platform Margaretha',\n",
       " 'Krupp platform Heide',\n",
       " 'October platform Banja',\n",
       " 'Davidson platform Spot',\n",
       " 'Allah platform Elias',\n",
       " 'Larva platform LP',\n",
       " 'Khu platform ML',\n",
       " 'Lahore platform Wait',\n",
       " 'Dorset platform Rome',\n",
       " 'Stay platform Citizen',\n",
       " 'Sit platform Allier',\n",
       " 'Wayne platform Hava',\n",
       " 'Flag platform Sada',\n",
       " 'Ferns platform Era',\n",
       " 'Jaguar platform Graham',\n",
       " 'Brenda platform Tone',\n",
       " 'Mina platform Madeleine',\n",
       " 'Harding platform Midnight',\n",
       " 'Giants platform GDP',\n",
       " 'Buta platform Salt',\n",
       " 'Faye platform Sima',\n",
       " 'Generation platform Steinicke',\n",
       " 'Schottland platform Nové',\n",
       " 'USD platform Checa',\n",
       " 'Eu platform Stanley',\n",
       " 'Wesley legislative committee Haji',\n",
       " 'Hai legislative committee Yahoo',\n",
       " 'Wilder legislative committee Stal',\n",
       " 'Porter legislative committee FC',\n",
       " 'Mirage legislative committee Dad',\n",
       " 'Dharma legislative committee Kenia',\n",
       " 'Santo legislative committee CCD',\n",
       " 'Josep legislative committee Riau',\n",
       " 'Burgess legislative committee Ky',\n",
       " 'Mortimer legislative committee Billie',\n",
       " 'Zbigniew legislative committee Elbe',\n",
       " 'Design legislative committee Paraíso',\n",
       " 'Hotel legislative committee TD',\n",
       " 'Robot legislative committee Luther',\n",
       " 'Guayaquil legislative committee Roi',\n",
       " 'Laurie legislative committee Pole',\n",
       " 'Vittoria legislative committee Page',\n",
       " 'Motion legislative committee Baron',\n",
       " 'Cor legislative committee Libia',\n",
       " 'Naga legislative committee Cuenca',\n",
       " 'Alexa legislative committee Kálmán',\n",
       " 'WBC legislative committee ET',\n",
       " 'Collins legislative committee Mainstream',\n",
       " 'Akademie legislative committee Agency',\n",
       " 'Faber legislative committee Mata',\n",
       " 'Cullen legislative committee Mineral',\n",
       " 'Hiroshima legislative committee Figaro',\n",
       " 'Auckland legislative committee Trung',\n",
       " 'Belmont legislative committee Sabha',\n",
       " 'RCA legislative committee Guimarães',\n",
       " 'Träger legislative committee Disneyland',\n",
       " 'CC legislative committee Wes',\n",
       " 'Deus legislative committee Gesù',\n",
       " 'Mustang legislative committee Cinq',\n",
       " 'Benjamin legislative committee Silla',\n",
       " 'Ronnie legislative committee JR',\n",
       " 'AD legislative committee Bryant',\n",
       " 'Oru legislative committee Klaus',\n",
       " 'Baronet legislative committee Kleiner',\n",
       " 'Nos legislative committee Fort',\n",
       " 'Certain legislative committee City',\n",
       " 'Kid legislative committee Márquez',\n",
       " 'Ami legislative committee Yer',\n",
       " 'AVN legislative committee Bernard',\n",
       " 'Maya legislative committee Diaz',\n",
       " 'Oasis legislative committee Madre',\n",
       " 'Ordu legislative committee Asturias',\n",
       " 'FAA legislative committee FF',\n",
       " 'Automatic legislative committee Dis',\n",
       " 'Chronicle legislative committee Lorentz',\n",
       " 'SM media franchise Kensley',\n",
       " 'Sempre media franchise Bartlett',\n",
       " 'Portsmouth media franchise Aves',\n",
       " 'Irena media franchise Phantom',\n",
       " 'Oaks media franchise Lori',\n",
       " 'Fighting media franchise Lors',\n",
       " 'Bilbao media franchise Gutenberg',\n",
       " 'Sharon media franchise Dänemark',\n",
       " 'Allium media franchise Sá',\n",
       " 'Haiti media franchise Oman',\n",
       " 'Isabella media franchise EN',\n",
       " 'Olsson media franchise Gate',\n",
       " 'Pure media franchise Ticino',\n",
       " 'Patrol media franchise Eclipse',\n",
       " 'Siege media franchise UCB',\n",
       " 'Galiza media franchise Arcade',\n",
       " 'Lucky media franchise Buzz',\n",
       " 'Pfeiffer media franchise Renault',\n",
       " 'FX media franchise Laval',\n",
       " 'Energy media franchise Molina',\n",
       " 'Joanne media franchise RN',\n",
       " 'Erik media franchise Monmouth',\n",
       " 'EM media franchise Turin',\n",
       " 'Raum media franchise Smith',\n",
       " 'VL media franchise Hume',\n",
       " 'Sabina media franchise Gan',\n",
       " 'Krupp media franchise Rotten',\n",
       " 'October media franchise Poet',\n",
       " 'Davidson media franchise Assembly',\n",
       " 'Allah media franchise Toren',\n",
       " 'Larva media franchise Roberto',\n",
       " 'Khu media franchise Beatrice',\n",
       " 'Lahore media franchise Mario',\n",
       " 'Dorset media franchise SR',\n",
       " 'Stay media franchise Marge',\n",
       " 'Sit media franchise Weir',\n",
       " 'Wayne media franchise Adobe',\n",
       " 'Flag media franchise Hallan',\n",
       " 'Ferns media franchise Viscount',\n",
       " 'Jaguar media franchise Dante',\n",
       " 'Brenda media franchise Sarmiento',\n",
       " 'Mina media franchise Nikolaj',\n",
       " 'Harding media franchise Lamarck',\n",
       " 'Giants media franchise Safe',\n",
       " 'Buta media franchise UN',\n",
       " 'Faye media franchise Edmonton',\n",
       " 'Generation media franchise Zelda',\n",
       " 'Schottland media franchise Planck',\n",
       " 'USD media franchise Ne',\n",
       " 'Eu media franchise Isle',\n",
       " 'Canadá type of lens Haji',\n",
       " 'Garden type of lens Yahoo',\n",
       " 'Walters type of lens Stal',\n",
       " 'Salon type of lens FC',\n",
       " 'Canada type of lens Dad',\n",
       " 'Cambridge type of lens Kenia',\n",
       " 'Az type of lens CCD',\n",
       " 'Ne type of lens Riau',\n",
       " 'Bulu type of lens Ky',\n",
       " 'Cramer type of lens Billie',\n",
       " 'Speedway type of lens Elbe',\n",
       " 'Paderborn type of lens Paraíso',\n",
       " 'Hazel type of lens TD',\n",
       " 'Ipswich type of lens Luther',\n",
       " 'Nowe type of lens Roi',\n",
       " 'Foi type of lens Pole',\n",
       " 'Powers type of lens Page',\n",
       " 'Ingles type of lens Baron',\n",
       " 'Welsh type of lens Libia',\n",
       " 'Ludwik type of lens Cuenca',\n",
       " 'Wedding type of lens Kálmán',\n",
       " 'Ludovic type of lens ET',\n",
       " 'Genus type of lens Mainstream',\n",
       " 'Fortuna type of lens Agency',\n",
       " 'Norris type of lens Mata',\n",
       " 'MC type of lens Mineral',\n",
       " 'Mass type of lens Figaro',\n",
       " 'Thái type of lens Trung',\n",
       " 'Fresno type of lens Sabha',\n",
       " 'Bresse type of lens Guimarães',\n",
       " 'Essex type of lens Disneyland',\n",
       " 'Conte type of lens Wes',\n",
       " 'Deeds type of lens Gesù',\n",
       " 'Woods type of lens Cinq',\n",
       " 'Lara type of lens Silla',\n",
       " 'Perdana type of lens JR',\n",
       " 'Shanghai type of lens Bryant',\n",
       " 'IRAS type of lens Klaus',\n",
       " 'Music type of lens Kleiner',\n",
       " 'Aiken type of lens Fort',\n",
       " 'Zion type of lens City',\n",
       " 'Hardy type of lens Márquez',\n",
       " 'While type of lens Yer',\n",
       " 'Cuenca type of lens Bernard',\n",
       " 'Wittgenstein type of lens Diaz',\n",
       " 'Chatham type of lens Madre',\n",
       " 'Hertfordshire type of lens Asturias',\n",
       " 'Em type of lens FF',\n",
       " 'Velázquez type of lens Dis',\n",
       " 'Barrett type of lens Lorentz',\n",
       " 'SM natural reservoir of AFL',\n",
       " 'Sempre natural reservoir of Irving',\n",
       " 'Portsmouth natural reservoir of Bunker',\n",
       " 'Irena natural reservoir of Mateo',\n",
       " 'Oaks natural reservoir of Grâce',\n",
       " 'Fighting natural reservoir of Third',\n",
       " 'Bilbao natural reservoir of Nicholson',\n",
       " 'Sharon natural reservoir of Graf',\n",
       " 'Allium natural reservoir of Surrey',\n",
       " 'Haiti natural reservoir of Records',\n",
       " 'Isabella natural reservoir of Napoli',\n",
       " 'Olsson natural reservoir of SMP',\n",
       " 'Pure natural reservoir of Monica',\n",
       " 'Patrol natural reservoir of Marian',\n",
       " 'Siege natural reservoir of Ty',\n",
       " 'Galiza natural reservoir of Naomi',\n",
       " 'Lucky natural reservoir of Paterson',\n",
       " 'Pfeiffer natural reservoir of Houston',\n",
       " 'FX natural reservoir of Gamma',\n",
       " 'Energy natural reservoir of Auburn',\n",
       " 'Joanne natural reservoir of Fernández',\n",
       " 'Erik natural reservoir of Templo',\n",
       " 'EM natural reservoir of Noord',\n",
       " 'Raum natural reservoir of Frederico',\n",
       " 'VL natural reservoir of Jalisco',\n",
       " 'Sabina natural reservoir of Eugenio',\n",
       " 'Krupp natural reservoir of Nightmare',\n",
       " 'October natural reservoir of Maxime',\n",
       " 'Davidson natural reservoir of Lola',\n",
       " 'Allah natural reservoir of Masse',\n",
       " 'Larva natural reservoir of Sent',\n",
       " 'Khu natural reservoir of Jackson',\n",
       " 'Lahore natural reservoir of Oakland',\n",
       " 'Dorset natural reservoir of Jason',\n",
       " 'Stay natural reservoir of GRN',\n",
       " 'Sit natural reservoir of Principal',\n",
       " 'Wayne natural reservoir of Bet',\n",
       " 'Flag natural reservoir of Cheryl',\n",
       " 'Ferns natural reservoir of Hip',\n",
       " 'Jaguar natural reservoir of Elvis',\n",
       " 'Brenda natural reservoir of Transvaal',\n",
       " 'Mina natural reservoir of Ty',\n",
       " 'Harding natural reservoir of Luzern',\n",
       " 'Giants natural reservoir of Syn',\n",
       " 'Buta natural reservoir of Doyle',\n",
       " 'Faye natural reservoir of Fischer',\n",
       " 'Generation natural reservoir of Banja',\n",
       " 'Schottland natural reservoir of Andrzej',\n",
       " 'USD natural reservoir of Cherbourg',\n",
       " 'Eu natural reservoir of Meet',\n",
       " 'Copeland facial hair Haji',\n",
       " 'Oosten facial hair Yahoo',\n",
       " 'Lilla facial hair Stal',\n",
       " 'Egypt facial hair FC',\n",
       " 'Mem facial hair Dad',\n",
       " 'Honour facial hair Kenia',\n",
       " 'Vegas facial hair CCD',\n",
       " 'Arm facial hair Riau',\n",
       " 'Crescent facial hair Ky',\n",
       " 'Tokyo facial hair Billie',\n",
       " 'Hewitt facial hair Elbe',\n",
       " 'Uri facial hair Paraíso',\n",
       " 'Klein facial hair TD',\n",
       " 'Márquez facial hair Luther',\n",
       " 'Stalin facial hair Roi',\n",
       " 'Abraham facial hair Pole',\n",
       " 'Regno facial hair Page',\n",
       " 'Amelia facial hair Baron',\n",
       " 'Ark facial hair Libia',\n",
       " 'MS facial hair Cuenca',\n",
       " 'Angkatan facial hair Kálmán',\n",
       " 'Marea facial hair ET',\n",
       " 'Colección facial hair Mainstream',\n",
       " 'Kaufman facial hair Agency',\n",
       " 'Audio facial hair Mata',\n",
       " 'Seine facial hair Mineral',\n",
       " 'Fulda facial hair Figaro',\n",
       " 'Maha facial hair Trung',\n",
       " 'Pour facial hair Sabha',\n",
       " 'God facial hair Guimarães',\n",
       " 'Fletcher facial hair Disneyland',\n",
       " 'Mildred facial hair Wes',\n",
       " 'Robaina facial hair Gesù',\n",
       " 'Chef facial hair Cinq',\n",
       " 'Manhattan facial hair Silla',\n",
       " 'Donato facial hair JR',\n",
       " 'Carpenter facial hair Bryant',\n",
       " 'Grupa facial hair Klaus',\n",
       " 'Lara facial hair Kleiner',\n",
       " 'Sharks facial hair Fort',\n",
       " 'Syn facial hair City',\n",
       " 'Copeland facial hair Márquez',\n",
       " 'Blanco facial hair Yer',\n",
       " 'Lawrence facial hair Bernard',\n",
       " 'CRC facial hair Diaz',\n",
       " 'Direito facial hair Madre',\n",
       " 'Oakland facial hair Asturias',\n",
       " 'Pre facial hair FF',\n",
       " 'Goiás facial hair Dis',\n",
       " 'Magnum facial hair Lorentz',\n",
       " 'SM reply to Beyoncé',\n",
       " 'Sempre reply to Cor',\n",
       " 'Portsmouth reply to Edouard',\n",
       " 'Irena reply to Ventura',\n",
       " 'Oaks reply to Escola',\n",
       " 'Fighting reply to Scholar',\n",
       " 'Bilbao reply to About',\n",
       " 'Sharon reply to Jessica',\n",
       " 'Allium reply to Cheyenne',\n",
       " 'Haiti reply to Huis',\n",
       " 'Isabella reply to Kiel',\n",
       " 'Olsson reply to Creek',\n",
       " 'Pure reply to Twilight',\n",
       " 'Patrol reply to Aria',\n",
       " 'Siege reply to A',\n",
       " 'Galiza reply to Manhattan',\n",
       " 'Lucky reply to Nile',\n",
       " 'Pfeiffer reply to Bumi',\n",
       " 'FX reply to Chiara',\n",
       " 'Energy reply to Storm',\n",
       " 'Joanne reply to Bulu',\n",
       " 'Erik reply to Soria',\n",
       " 'EM reply to Aner',\n",
       " 'Raum reply to Cicero',\n",
       " 'VL reply to Alpi',\n",
       " 'Sabina reply to Bee',\n",
       " 'Krupp reply to Kassel',\n",
       " 'October reply to Maxi',\n",
       " 'Davidson reply to Abby',\n",
       " 'Allah reply to Socorro',\n",
       " 'Larva reply to Holly',\n",
       " 'Khu reply to Macedonia',\n",
       " 'Lahore reply to Eaton',\n",
       " 'Dorset reply to Mineral',\n",
       " 'Stay reply to Juli',\n",
       " 'Sit reply to Clash',\n",
       " 'Wayne reply to National',\n",
       " 'Flag reply to Schönberg',\n",
       " 'Ferns reply to Pedra',\n",
       " 'Jaguar reply to Anthony',\n",
       " 'Brenda reply to Bora',\n",
       " 'Mina reply to Eure',\n",
       " 'Harding reply to Freire',\n",
       " 'Giants reply to Southampton',\n",
       " 'Buta reply to Hector',\n",
       " 'Faye reply to Glen',\n",
       " 'Generation reply to Bruxelles',\n",
       " 'Schottland reply to Porsche',\n",
       " 'USD reply to Mulder',\n",
       " 'Eu reply to Zee',\n",
       " 'Signal list of works Haji',\n",
       " 'Loire list of works Yahoo',\n",
       " 'Chelsea list of works Stal',\n",
       " 'Rada list of works FC',\n",
       " 'Henry list of works Dad',\n",
       " 'Nicaragua list of works Kenia',\n",
       " 'Robot list of works CCD',\n",
       " 'Erwin list of works Riau',\n",
       " 'Louise list of works Ky',\n",
       " 'Canis list of works Billie',\n",
       " 'Panorama list of works Elbe',\n",
       " 'Mercer list of works Paraíso',\n",
       " 'Villiers list of works TD',\n",
       " 'Mayotte list of works Luther',\n",
       " 'Sally list of works Roi',\n",
       " 'Cheryl list of works Pole',\n",
       " 'Laurel list of works Page',\n",
       " 'Stephenson list of works Baron',\n",
       " 'Denmark list of works Libia',\n",
       " 'Ice list of works Cuenca',\n",
       " 'Rumble list of works Kálmán',\n",
       " 'Line list of works ET',\n",
       " 'Anne list of works Mainstream',\n",
       " 'Junction list of works Agency',\n",
       " 'Branko list of works Mata',\n",
       " 'Cicero list of works Mineral',\n",
       " 'WDR list of works Figaro',\n",
       " 'Highlands list of works Trung',\n",
       " 'RN list of works Sabha',\n",
       " 'Trento list of works Guimarães',\n",
       " 'Ga list of works Disneyland',\n",
       " 'Az list of works Wes',\n",
       " 'Blum list of works Gesù',\n",
       " 'Vries list of works Cinq',\n",
       " 'Camilla list of works Silla',\n",
       " 'Sheriff list of works JR',\n",
       " 'Higgins list of works Bryant',\n",
       " 'Manor list of works Klaus',\n",
       " 'NFC list of works Kleiner',\n",
       " 'Joe list of works Fort',\n",
       " 'Dok list of works City',\n",
       " 'Dewey list of works Márquez',\n",
       " 'Vida list of works Yer',\n",
       " 'Gotham list of works Bernard',\n",
       " 'Pegasus list of works Diaz',\n",
       " 'Chief list of works Madre',\n",
       " 'Gilman list of works Asturias',\n",
       " 'Cada list of works FF',\n",
       " 'APG list of works Dis',\n",
       " 'Palmer list of works Lorentz',\n",
       " 'SM side effect Barbara',\n",
       " 'Sempre side effect Section',\n",
       " 'Portsmouth side effect Dorothy',\n",
       " 'Irena side effect Junie',\n",
       " 'Oaks side effect Freddie',\n",
       " 'Fighting side effect Bosch',\n",
       " 'Bilbao side effect Wien',\n",
       " 'Sharon side effect Medley',\n",
       " 'Allium side effect Den',\n",
       " 'Haiti side effect Silent',\n",
       " 'Isabella side effect Issue',\n",
       " 'Olsson side effect Hispania',\n",
       " 'Pure side effect UHF',\n",
       " 'Patrol side effect Llwyd',\n",
       " 'Siege side effect Man',\n",
       " 'Galiza side effect Tato',\n",
       " 'Lucky side effect NES',\n",
       " 'Pfeiffer side effect Khu',\n",
       " 'FX side effect Hilbert',\n",
       " 'Energy side effect Graham',\n",
       " 'Joanne side effect Fruit',\n",
       " 'Erik side effect Eugenio',\n",
       " 'EM side effect Broadway',\n",
       " 'Raum side effect Strong',\n",
       " 'VL side effect Struggle',\n",
       " 'Sabina side effect Alger',\n",
       " 'Krupp side effect Vogue',\n",
       " 'October side effect Selena',\n",
       " 'Davidson side effect Pt',\n",
       " 'Allah side effect Lola',\n",
       " 'Larva side effect Viborg',\n",
       " 'Khu side effect Prima',\n",
       " 'Lahore side effect Heide',\n",
       " 'Dorset side effect Feld',\n",
       " 'Stay side effect Lok',\n",
       " 'Sit side effect Sprint',\n",
       " 'Wayne side effect Ponte',\n",
       " 'Flag side effect Common',\n",
       " 'Ferns side effect Spiel',\n",
       " 'Jaguar side effect Southwest',\n",
       " 'Brenda side effect Cinta',\n",
       " 'Mina side effect Badminton',\n",
       " 'Harding side effect Nara',\n",
       " 'Giants side effect Minden',\n",
       " 'Buta side effect Nevada',\n",
       " 'Faye side effect MIT',\n",
       " 'Generation side effect ISS',\n",
       " 'Schottland side effect Chicago',\n",
       " 'USD side effect Mifflin',\n",
       " 'Eu side effect Herder',\n",
       " 'Gama physically interacts with Haji',\n",
       " 'Pavia physically interacts with Yahoo',\n",
       " 'Gaming physically interacts with Stal',\n",
       " 'Zola physically interacts with FC',\n",
       " 'Alaska physically interacts with Dad',\n",
       " 'Savoy physically interacts with Kenia',\n",
       " 'Like physically interacts with CCD',\n",
       " 'Cost physically interacts with Riau',\n",
       " 'Rabbit physically interacts with Ky',\n",
       " 'Ole physically interacts with Billie',\n",
       " 'Carnegie physically interacts with Elbe',\n",
       " 'Augusti physically interacts with Paraíso',\n",
       " 'Assad physically interacts with TD',\n",
       " 'Disney physically interacts with Luther',\n",
       " 'Suiza physically interacts with Roi',\n",
       " 'Torpedo physically interacts with Pole',\n",
       " 'October physically interacts with Page',\n",
       " 'Freak physically interacts with Baron',\n",
       " 'Mickey physically interacts with Libia',\n",
       " 'Rowan physically interacts with Cuenca',\n",
       " 'Sitze physically interacts with Kálmán',\n",
       " 'Georgetown physically interacts with ET',\n",
       " 'Tema physically interacts with Mainstream',\n",
       " 'Bron physically interacts with Agency',\n",
       " 'Friedrich physically interacts with Mata',\n",
       " 'Chair physically interacts with Mineral',\n",
       " 'Suite physically interacts with Figaro',\n",
       " 'Forbes physically interacts with Trung',\n",
       " 'Jake physically interacts with Sabha',\n",
       " 'Berkshire physically interacts with Guimarães',\n",
       " 'Mana physically interacts with Disneyland',\n",
       " 'Amiga physically interacts with Wes',\n",
       " 'Rumble physically interacts with Gesù',\n",
       " 'Iron physically interacts with Cinq',\n",
       " 'Medan physically interacts with Silla',\n",
       " 'Root physically interacts with JR',\n",
       " 'Ve physically interacts with Bryant',\n",
       " 'Chandler physically interacts with Klaus',\n",
       " 'Mendelssohn physically interacts with Kleiner',\n",
       " 'Morrison physically interacts with Fort',\n",
       " 'Sent physically interacts with City',\n",
       " 'Kostel physically interacts with Márquez',\n",
       " 'Grenoble physically interacts with Yer',\n",
       " 'Templo physically interacts with Bernard',\n",
       " 'Northeast physically interacts with Diaz',\n",
       " 'Usa physically interacts with Madre',\n",
       " 'Belle physically interacts with Asturias',\n",
       " 'Silva physically interacts with FF',\n",
       " 'Atatürk physically interacts with Dis',\n",
       " 'Ebben physically interacts with Lorentz',\n",
       " 'SM points/goal scored by Klein',\n",
       " 'Sempre points/goal scored by Monza',\n",
       " 'Portsmouth points/goal scored by Godzilla',\n",
       " 'Irena points/goal scored by Files',\n",
       " 'Oaks points/goal scored by Amy',\n",
       " 'Fighting points/goal scored by Greco',\n",
       " 'Bilbao points/goal scored by Artois',\n",
       " 'Sharon points/goal scored by Asturias',\n",
       " 'Allium points/goal scored by Danilo',\n",
       " 'Haiti points/goal scored by Yunan',\n",
       " 'Isabella points/goal scored by Béla',\n",
       " 'Olsson points/goal scored by March',\n",
       " 'Pure points/goal scored by Kelas',\n",
       " 'Patrol points/goal scored by Benedict',\n",
       " 'Siege points/goal scored by Premier',\n",
       " 'Galiza points/goal scored by Haynes',\n",
       " 'Lucky points/goal scored by Monate',\n",
       " 'Pfeiffer points/goal scored by Buchanan',\n",
       " 'FX points/goal scored by Forst',\n",
       " 'Energy points/goal scored by String',\n",
       " 'Joanne points/goal scored by Bar',\n",
       " 'Erik points/goal scored by Dharma',\n",
       " 'EM points/goal scored by Boga',\n",
       " 'Raum points/goal scored by Quartier',\n",
       " 'VL points/goal scored by Buchanan',\n",
       " 'Sabina points/goal scored by Saw',\n",
       " 'Krupp points/goal scored by Am',\n",
       " 'October points/goal scored by Nowe',\n",
       " 'Davidson points/goal scored by Sabine',\n",
       " 'Allah points/goal scored by Schatten',\n",
       " 'Larva points/goal scored by Henriette',\n",
       " 'Khu points/goal scored by Finale',\n",
       " 'Lahore points/goal scored by Adventure',\n",
       " 'Dorset points/goal scored by Vosges',\n",
       " 'Stay points/goal scored by Vizier',\n",
       " 'Sit points/goal scored by Saguenay',\n",
       " 'Wayne points/goal scored by Pt',\n",
       " 'Flag points/goal scored by Dana',\n",
       " 'Ferns points/goal scored by Epic',\n",
       " 'Jaguar points/goal scored by Milano',\n",
       " 'Brenda points/goal scored by Fire',\n",
       " 'Mina points/goal scored by Tech',\n",
       " 'Harding points/goal scored by Jacobi',\n",
       " 'Giants points/goal scored by Disney',\n",
       " 'Buta points/goal scored by Scream',\n",
       " 'Faye points/goal scored by Halk',\n",
       " 'Generation points/goal scored by Sabbath',\n",
       " 'Schottland points/goal scored by Oba',\n",
       " 'USD points/goal scored by Aden',\n",
       " 'Eu points/goal scored by Chile',\n",
       " 'México demonym of Haji',\n",
       " 'Louvain demonym of Yahoo',\n",
       " 'Science demonym of Stal',\n",
       " 'Continental demonym of FC',\n",
       " 'Bosco demonym of Dad',\n",
       " 'Mirko demonym of Kenia',\n",
       " 'Clements demonym of CCD',\n",
       " 'Poitou demonym of Riau',\n",
       " 'Ruggiero demonym of Ky',\n",
       " 'Santiago demonym of Billie',\n",
       " 'Pierce demonym of Elbe',\n",
       " 'Mutter demonym of Paraíso',\n",
       " 'Erika demonym of TD',\n",
       " 'Helden demonym of Luther',\n",
       " 'Aurora demonym of Roi',\n",
       " 'Drop demonym of Pole',\n",
       " 'Hague demonym of Page',\n",
       " 'Vockeroth demonym of Baron',\n",
       " 'Pest demonym of Libia',\n",
       " 'Bach demonym of Cuenca',\n",
       " 'Una demonym of Kálmán',\n",
       " 'Menor demonym of ET',\n",
       " 'Elke demonym of Mainstream',\n",
       " 'Lourdes demonym of Agency',\n",
       " 'Bosch demonym of Mata',\n",
       " 'Ellis demonym of Mineral',\n",
       " 'Dundee demonym of Figaro',\n",
       " 'Tampere demonym of Trung',\n",
       " 'Kohl demonym of Sabha',\n",
       " 'Prensa demonym of Guimarães',\n",
       " 'Browning demonym of Disneyland',\n",
       " 'Garland demonym of Wes',\n",
       " 'Champion demonym of Gesù',\n",
       " 'Batavia demonym of Cinq',\n",
       " 'Gutiérrez demonym of Silla',\n",
       " 'Horne demonym of JR',\n",
       " 'Jerusalén demonym of Bryant',\n",
       " 'Tito demonym of Klaus',\n",
       " 'Neuen demonym of Kleiner',\n",
       " 'Sieger demonym of Fort',\n",
       " 'Muslimani demonym of City',\n",
       " 'Alison demonym of Márquez',\n",
       " 'Syn demonym of Yer',\n",
       " 'Valentin demonym of Bernard',\n",
       " 'Monterey demonym of Diaz',\n",
       " 'Fabricius demonym of Madre',\n",
       " 'Korn demonym of Asturias',\n",
       " 'Ezra demonym of FF',\n",
       " 'Dubois demonym of Dis',\n",
       " 'Jamal demonym of Lorentz',\n",
       " 'SM conflict Stakes',\n",
       " 'Sempre conflict Titanic',\n",
       " 'Portsmouth conflict Ros',\n",
       " 'Irena conflict Dominic',\n",
       " 'Oaks conflict Bertram',\n",
       " 'Fighting conflict AF',\n",
       " 'Bilbao conflict Gillespie',\n",
       " 'Sharon conflict Crash',\n",
       " 'Allium conflict Gutenberg',\n",
       " 'Haiti conflict Hatch',\n",
       " 'Isabella conflict Decock',\n",
       " 'Olsson conflict Franklin',\n",
       " 'Pure conflict Juventus',\n",
       " 'Patrol conflict Rice',\n",
       " 'Siege conflict Première',\n",
       " 'Galiza conflict Sala',\n",
       " 'Lucky conflict Robertson',\n",
       " 'Pfeiffer conflict Liber',\n",
       " 'FX conflict Watson',\n",
       " 'Energy conflict Tornado',\n",
       " 'Joanne conflict Kunst',\n",
       " 'Erik conflict Rex',\n",
       " 'EM conflict Maybe',\n",
       " 'Raum conflict Campione',\n",
       " 'VL conflict Valencia',\n",
       " 'Sabina conflict Stakes',\n",
       " 'Krupp conflict Lino',\n",
       " 'October conflict Brady',\n",
       " 'Davidson conflict Rivière',\n",
       " 'Allah conflict Creta',\n",
       " 'Larva conflict Alice',\n",
       " 'Khu conflict Gerd',\n",
       " 'Lahore conflict Ent',\n",
       " 'Dorset conflict Albatros',\n",
       " 'Stay conflict Strange',\n",
       " 'Sit conflict Dresden',\n",
       " 'Wayne conflict Berge',\n",
       " 'Flag conflict Pardo',\n",
       " 'Ferns conflict Dalla',\n",
       " 'Jaguar conflict Asylum',\n",
       " 'Brenda conflict Florencia',\n",
       " 'Mina conflict Alexandria',\n",
       " 'Harding conflict Vela',\n",
       " 'Giants conflict White',\n",
       " 'Buta conflict Sudan',\n",
       " 'Faye conflict Lucy',\n",
       " 'Generation conflict Parks',\n",
       " 'Schottland conflict Ulysses',\n",
       " 'USD conflict Emden',\n",
       " 'Eu conflict Alpine',\n",
       " 'Flamengo culture Haji',\n",
       " 'Björn culture Yahoo',\n",
       " 'Are culture Stal',\n",
       " 'MD culture FC',\n",
       " 'Esch culture Dad',\n",
       " 'Padre culture Kenia',\n",
       " 'Rady culture CCD',\n",
       " 'Sino culture Riau',\n",
       " 'Kelurahan culture Ky',\n",
       " 'Pacific culture Billie',\n",
       " 'Farrell culture Elbe',\n",
       " 'Greatest culture Paraíso',\n",
       " 'DNA culture TD',\n",
       " 'Trang culture Luther',\n",
       " 'Florence culture Roi',\n",
       " 'Côte culture Pole',\n",
       " 'Boa culture Page',\n",
       " 'Diana culture Baron',\n",
       " 'Portsmouth culture Libia',\n",
       " 'Fran culture Cuenca',\n",
       " 'Ritter culture Kálmán',\n",
       " 'Magazine culture ET',\n",
       " 'Robinson culture Mainstream',\n",
       " 'Tore culture Agency',\n",
       " 'Pays culture Mata',\n",
       " 'Molde culture Mineral',\n",
       " 'Put culture Figaro',\n",
       " 'Augusti culture Trung',\n",
       " 'Welch culture Sabha',\n",
       " 'Ulysses culture Guimarães',\n",
       " 'AR culture Disneyland',\n",
       " 'Beaver culture Wes',\n",
       " 'ME culture Gesù',\n",
       " 'Mines culture Cinq',\n",
       " 'Wenn culture Silla',\n",
       " 'Onthophagus culture JR',\n",
       " 'Sari culture Bryant',\n",
       " 'Monat culture Klaus',\n",
       " 'Gelo culture Kleiner',\n",
       " 'Borussia culture Fort',\n",
       " 'Sonja culture City',\n",
       " 'Ante culture Márquez',\n",
       " 'Sainte culture Yer',\n",
       " 'EUA culture Bernard',\n",
       " 'RCA culture Diaz',\n",
       " 'Sarah culture Madre',\n",
       " 'Boeing culture Asturias',\n",
       " 'Adams culture FF',\n",
       " 'Luke culture Dis',\n",
       " 'Jaguar culture Lorentz']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict['sample'][:1901]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb14c2d",
   "metadata": {},
   "source": [
    "#### -> (e, r_de, f) vs (f, s, e) (+ (f, s_de, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53a164b",
   "metadata": {},
   "source": [
    "Evaluate if for (e, r, f) we know more often (e, s_de, f) or (f, s, e), i.e. Knowledge Transfer vs inversion rule.\n",
    "This can also help us understand which way we get (f, r_de, e).\n",
    "\n",
    "Since when we train on (e, r_de, f), we rarely get (f, s_de, e), it already implies that we would go the way:\n",
    "(e, r, f) -RULE-> (f, s, e) -KT-> (f, s_de, e)\n",
    "\n",
    "1800 facts are training the rule (900<->900)\n",
    "1800-1900 are facts that are used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9599e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap(a, b):\n",
    "    a_multiset = Counter(a)\n",
    "    b_multiset = Counter(b)\n",
    "\n",
    "    overlap = list((a_multiset & b_multiset).elements())\n",
    "    \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1e2ff06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation: taxonomischer Rang\n",
      "Accuracy for (f, s, e): 0.35\n",
      "Accuracy for (e, r_t, f): 0.27\n",
      "Accuracy for (f, s_t, e): 0.17\n",
      "Size (f, r, e): 35\n",
      "Size (e, r_t, f): 27\n",
      "Overlap between (f, s, e) and (e, r_t, f): 6\n",
      "Transfer from (e, r_t, f) to (f, s_t, e): 0.14814814814814814\n",
      "Transfer from (f, s, e) to (f, s_t, e): 0.34285714285714286\n",
      "\n",
      "\n",
      "Relation: Farbe\n",
      "Accuracy for (f, s, e): 0.39\n",
      "Accuracy for (e, r_t, f): 0.57\n",
      "Accuracy for (f, s_t, e): 0.14\n",
      "Size (f, r, e): 39\n",
      "Size (e, r_t, f): 57\n",
      "Overlap between (f, s, e) and (e, r_t, f): 37\n",
      "Transfer from (e, r_t, f) to (f, s_t, e): 0.19298245614035087\n",
      "Transfer from (f, s, e) to (f, s_t, e): 0.2564102564102564\n",
      "\n",
      "\n",
      "Relation: gemessene physikalische Größe\n",
      "Accuracy for (f, s, e): 0.39\n",
      "Accuracy for (e, r_t, f): 0.11\n",
      "Accuracy for (f, s_t, e): 0.07\n",
      "Size (f, r, e): 39\n",
      "Size (e, r_t, f): 11\n",
      "Overlap between (f, s, e) and (e, r_t, f): 0\n",
      "Transfer from (e, r_t, f) to (f, s_t, e): 0.0\n",
      "Transfer from (f, s, e) to (f, s_t, e): 0.15384615384615385\n",
      "\n",
      "\n",
      "Relation: Plattform\n",
      "Accuracy for (f, s, e): 0.37\n",
      "Accuracy for (e, r_t, f): 0.16\n",
      "Accuracy for (f, s_t, e): 0.39\n",
      "Size (f, r, e): 37\n",
      "Size (e, r_t, f): 16\n",
      "Overlap between (f, s, e) and (e, r_t, f): 0\n",
      "Transfer from (e, r_t, f) to (f, s_t, e): 0.625\n",
      "Transfer from (f, s, e) to (f, s_t, e): 0.40540540540540543\n",
      "\n",
      "\n",
      "Relation: Medien-Franchise\n",
      "Accuracy for (f, s, e): 0.37\n",
      "Accuracy for (e, r_t, f): 0.15\n",
      "Accuracy for (f, s_t, e): 0.12\n",
      "Size (f, r, e): 37\n",
      "Size (e, r_t, f): 15\n",
      "Overlap between (f, s, e) and (e, r_t, f): 0\n",
      "Transfer from (e, r_t, f) to (f, s_t, e): 0.0\n",
      "Transfer from (f, s, e) to (f, s_t, e): 0.2702702702702703\n",
      "\n",
      "\n",
      "Relation: Erregerreservoir von\n",
      "Accuracy for (f, s, e): 0.41\n",
      "Accuracy for (e, r_t, f): 0.23\n",
      "Accuracy for (f, s_t, e): 0.19\n",
      "Size (f, r, e): 41\n",
      "Size (e, r_t, f): 23\n",
      "Overlap between (f, s, e) and (e, r_t, f): 2\n",
      "Transfer from (e, r_t, f) to (f, s_t, e): 0.08695652173913043\n",
      "Transfer from (f, s, e) to (f, s_t, e): 0.36585365853658536\n",
      "\n",
      "\n",
      "Relation: Antwort auf\n",
      "Accuracy for (f, s, e): 0.4\n",
      "Accuracy for (e, r_t, f): 0.25\n",
      "Accuracy for (f, s_t, e): 0.24\n",
      "Size (f, r, e): 40\n",
      "Size (e, r_t, f): 25\n",
      "Overlap between (f, s, e) and (e, r_t, f): 7\n",
      "Transfer from (e, r_t, f) to (f, s_t, e): 0.12\n",
      "Transfer from (f, s, e) to (f, s_t, e): 0.45\n",
      "\n",
      "\n",
      "Relation: Nebenwirkung\n",
      "Accuracy for (f, s, e): 0.36\n",
      "Accuracy for (e, r_t, f): 0.26\n",
      "Accuracy for (f, s_t, e): 0.15\n",
      "Size (f, r, e): 36\n",
      "Size (e, r_t, f): 26\n",
      "Overlap between (f, s, e) and (e, r_t, f): 10\n",
      "Transfer from (e, r_t, f) to (f, s_t, e): 0.15384615384615385\n",
      "Transfer from (f, s, e) to (f, s_t, e): 0.4166666666666667\n",
      "\n",
      "\n",
      "Relation: Punkt/Treffer erzielt durch\n",
      "Accuracy for (f, s, e): 0.43\n",
      "Accuracy for (e, r_t, f): 0.5\n",
      "Accuracy for (f, s_t, e): 0.21\n",
      "Size (f, r, e): 43\n",
      "Size (e, r_t, f): 50\n",
      "Overlap between (f, s, e) and (e, r_t, f): 42\n",
      "Transfer from (e, r_t, f) to (f, s_t, e): 0.16\n",
      "Transfer from (f, s, e) to (f, s_t, e): 0.18604651162790697\n",
      "\n",
      "\n",
      "Relation: Kriegseinsatz\n",
      "Accuracy for (f, s, e): 0.34\n",
      "Accuracy for (e, r_t, f): 0.27\n",
      "Accuracy for (f, s_t, e): 0.17\n",
      "Size (f, r, e): 34\n",
      "Size (e, r_t, f): 27\n",
      "Overlap between (f, s, e) and (e, r_t, f): 0\n",
      "Transfer from (e, r_t, f) to (f, s_t, e): 0.0\n",
      "Transfer from (f, s, e) to (f, s_t, e): 0.47058823529411764\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over relations, take the training samples that were trained on\n",
    "for i in range(n_relations):\n",
    "    trained_test = train_dict['sample'][1800+i*1900:(i+1)*1900]\n",
    "\n",
    "    acc_s = 0\n",
    "    correct_entities_s = []\n",
    "    \n",
    "    acc_rde = 0\n",
    "    correct_entities_rde = []\n",
    "    \n",
    "    acc_sde = 0\n",
    "    correct_entities_sde = []\n",
    "    \n",
    "    # Relation pairs!\n",
    "    r = relations[0]['de'].iloc[i]\n",
    "    r_t = relations[0]['en'].iloc[i]\n",
    "    s = relations[1]['de'].iloc[i]\n",
    "    s_t = relations[1]['en'].iloc[i]\n",
    "\n",
    "    for sample in trained_test:\n",
    "\n",
    "        # Test (f, r, e)\n",
    "        f = sample.rsplit(' ', 1)[1] \n",
    "        e = sample.split(' ', 1)[0]\n",
    "\n",
    "        label_token = tokenizer.convert_tokens_to_ids(e)\n",
    "\n",
    "        prompt = f + ' ' + s + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_s += 1\n",
    "            correct_entities_s.append(e)\n",
    "\n",
    "        # Test (e, r_de, f)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(f)\n",
    "\n",
    "        prompt = e + ' ' + r_t + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_rde += 1\n",
    "            correct_entities_rde.append(e)\n",
    "            \n",
    "        # Test (f, r_de, e)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(e)\n",
    "\n",
    "        prompt = f + ' ' + s_t + ' [MASK]'\n",
    "        # print(prompt)\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        # Pick the [MASK] candidates with the highest logits\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_sde += 1\n",
    "            correct_entities_sde.append(e)\n",
    "        \n",
    "\n",
    "    acc_s /= 100\n",
    "    acc_rde /= 100\n",
    "    acc_sde /= 100\n",
    "\n",
    "    print(f'Relation: {r}')\n",
    "    print(f'Accuracy for (f, s, e): {acc_s}')\n",
    "    print(f'Accuracy for (e, r_t, f): {acc_rde}')\n",
    "    print(f'Accuracy for (f, s_t, e): {acc_sde}')\n",
    "    print(f'Size (f, r, e): {len(correct_entities_s)}')\n",
    "    print(f'Size (e, r_t, f): {len(correct_entities_rde)}')\n",
    "    print(f'Overlap between (f, s, e) and (e, r_t, f): {len(compute_overlap(correct_entities_s, correct_entities_rde))}')\n",
    "    if len(correct_entities_rde) == 0:\n",
    "        print(f'Transfer from (e, r_t, f) to (f, s_t, e): {0}')\n",
    "    else:\n",
    "        print(f'Transfer from (e, r_t, f) to (f, s_t, e): {len(compute_overlap(correct_entities_rde, correct_entities_sde))/len(correct_entities_rde)}')\n",
    "    \n",
    "    if len(correct_entities_s) == 0:\n",
    "        print(f'Transfer from (f, s, e) to (f, s_t, e): {0}')\n",
    "    else:\n",
    "        print(f'Transfer from (f, s, e) to (f, s_t, e): {len(compute_overlap(correct_entities_s, correct_entities_sde))/len(correct_entities_s)}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a6343",
   "metadata": {},
   "source": [
    "#### -> does inversion overgeneralize?\n",
    "For (e, r, f ) in train, it may predict (f, s, e) (correct), but also (e, s, f ) and (f, r, e) (incorrect).\n",
    "and (e, s_de, f) and (f, r_de, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a75a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over relations, take the training samples that were trained on\n",
    "for i in range(n_relations):\n",
    "    trained_test = train_dict['sample'][1800+i*1900:(i+1)*1900]\n",
    "\n",
    "    # False positives, i.e. higher -> the more overgeneralized the model has\n",
    "    acc_s = 0\n",
    "    acc_r = 0\n",
    "    \n",
    "    acc_rde = 0\n",
    "    acc_sde = 0\n",
    "    \n",
    "    # Relation pairs!\n",
    "    r = relations[0]['de'].iloc[i]\n",
    "    r_de = relations[0]['de'].iloc[i]\n",
    "    s = relations[1]['en'].iloc[i]\n",
    "    s_de = relations[1]['de'].iloc[i]\n",
    "\n",
    "    for sample in trained_test:\n",
    "\n",
    "        # Take entities\n",
    "        e = sample.split(' ', 1)[0]\n",
    "        f = sample.rsplit(' ', 1)[1]\n",
    "\n",
    "        # (e, s, f)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(f)\n",
    "        prompt = e + ' ' + s + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_s += 1\n",
    "\n",
    "        # (f, r, e)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(e)\n",
    "\n",
    "        prompt = f + ' ' + r + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_r += 1\n",
    "            \n",
    "        # (e, s_de, f)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(f)\n",
    "\n",
    "        prompt = e + ' ' + s_de + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        # Pick the [MASK] candidates with the highest logits\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_sde += 1\n",
    "        \n",
    "        # (f, r_de, e)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(e)\n",
    "\n",
    "        prompt = f + ' ' + r_de + ' [MASK]'\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        # Pick the [MASK] candidates with the highest logits\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_rde += 1\n",
    "        \n",
    "\n",
    "    acc_r /= 100\n",
    "    acc_rde /= 100\n",
    "    acc_s /= 100\n",
    "    acc_sde /= 100\n",
    "\n",
    "    print(f'Relation: {r}')\n",
    "    print(f'Accuracy for (e, s, f): {acc_s}')\n",
    "    print(f'Accuracy for (f, r, e): {acc_r}')\n",
    "    print(f'Accuracy for (e, s_de, f): {acc_sde}')\n",
    "    print(f'Accuracy for (f, r_de, e): {acc_rde}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cf623",
   "metadata": {},
   "source": [
    "### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4920784d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9582\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "total = len(train_dict['sample'])\n",
    "i = 0\n",
    "\n",
    "for txt in train_dict['sample'][:10000]:\n",
    "    i += 1\n",
    "    \n",
    "    # Add [MASK] for object\n",
    "    sample = txt.rsplit(' ', 1)[0] + ' [MASK]'\n",
    "    label_token = tokenizer.convert_tokens_to_ids(txt.rsplit(' ', 1)[1])\n",
    "    \n",
    "    encoded_input = tokenizer(sample, return_tensors='pt')\n",
    "    token_logits = model(**encoded_input).logits\n",
    "    \n",
    "    mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    \n",
    "    # Pick the [MASK] candidates with the highest logits\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()\n",
    "    \n",
    "    if label_token in top_5_tokens:\n",
    "        k += 1\n",
    "print(k/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"lens manner of [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n>>> {tokenizer.decode([chunk])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ff4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_dict['sample']:\n",
    "    if 'Alex' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e8aa5",
   "metadata": {},
   "source": [
    "### Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548995a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
