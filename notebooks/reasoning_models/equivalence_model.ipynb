{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e4031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizer, TrainingArguments, Trainer, \\\n",
    "    DataCollatorForLanguageModeling, IntervalStrategy\n",
    "\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "from data_generation_relation import *\n",
    "from utils import *\n",
    "from custom_trainer import CustomTrainer\n",
    "from datasets import load_metric\n",
    "import logging\n",
    "from transformers import logging as tlogging\n",
    "import wandb\n",
    "import sys\n",
    "from utils import set_seed\n",
    "from transformers.integrations import WandbCallback, TensorBoardCallback\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735ddbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "run_name = 'EQUI_es_en_target_pretrained'\n",
    "epochs = 200\n",
    "batch_size = 256\n",
    "lr = 4e-5\n",
    "\n",
    "relation = 'equivalence'\n",
    "source_language = ['es']\n",
    "target_language = ['en']\n",
    "n_relations = 10\n",
    "n_facts = 1000\n",
    "\n",
    "use_random = False\n",
    "use_anti = False\n",
    "\n",
    "use_pretrained = True\n",
    "use_target = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cd5ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>en</th>\n",
       "      <th>en_alias</th>\n",
       "      <th>de</th>\n",
       "      <th>de_alias</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "      <th>fr_alias</th>\n",
       "      <th>es_alias</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>P4330</td>\n",
       "      <td>contains</td>\n",
       "      <td>has contents</td>\n",
       "      <td>enthält</td>\n",
       "      <td>lagert</td>\n",
       "      <td>contiene</td>\n",
       "      <td>contient</td>\n",
       "      <td>contenant de</td>\n",
       "      <td>alberga</td>\n",
       "      <td>8269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>P8738</td>\n",
       "      <td>permits</td>\n",
       "      <td>does not prohibit</td>\n",
       "      <td>erlaubt</td>\n",
       "      <td>autorisiert</td>\n",
       "      <td>permite</td>\n",
       "      <td>permet</td>\n",
       "      <td>autorise</td>\n",
       "      <td>autoriza</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>P2872</td>\n",
       "      <td>visitor centre</td>\n",
       "      <td>tourist office</td>\n",
       "      <td>Touristeninformation</td>\n",
       "      <td>Touristeninformation</td>\n",
       "      <td>oficina de turismo</td>\n",
       "      <td>office de tourisme</td>\n",
       "      <td>office du tourisme</td>\n",
       "      <td>centro de información turística</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>P915</td>\n",
       "      <td>filming location</td>\n",
       "      <td>filmed at</td>\n",
       "      <td>Drehort</td>\n",
       "      <td>gedreht in</td>\n",
       "      <td>lugar de filmación</td>\n",
       "      <td>lieu du tournage</td>\n",
       "      <td>lieu de tournage</td>\n",
       "      <td>lugar de rodaje</td>\n",
       "      <td>32299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>P1777</td>\n",
       "      <td>manner of</td>\n",
       "      <td>style of</td>\n",
       "      <td>nach Art von</td>\n",
       "      <td>Nachahmer von</td>\n",
       "      <td>a la manera de</td>\n",
       "      <td>à la manière de</td>\n",
       "      <td>style de</td>\n",
       "      <td>manera de</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>P2175</td>\n",
       "      <td>medical condition treated</td>\n",
       "      <td>disease treated</td>\n",
       "      <td>zur Behandlung von benutzt</td>\n",
       "      <td>behandelt</td>\n",
       "      <td>condición médica tratada</td>\n",
       "      <td>maladie traitée</td>\n",
       "      <td>traite</td>\n",
       "      <td>enfermedad tratada</td>\n",
       "      <td>6992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>P1429</td>\n",
       "      <td>has pet</td>\n",
       "      <td>owns pet</td>\n",
       "      <td>hat Haustier</td>\n",
       "      <td>Haustiere</td>\n",
       "      <td>animal de compañía</td>\n",
       "      <td>animal de compagnie</td>\n",
       "      <td>a un animal de compagnie</td>\n",
       "      <td>mascota (animal)</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>P69</td>\n",
       "      <td>educated at</td>\n",
       "      <td>studied at</td>\n",
       "      <td>besuchte Bildungseinrichtung</td>\n",
       "      <td>Hochschule</td>\n",
       "      <td>educado en</td>\n",
       "      <td>scolarité</td>\n",
       "      <td>formation</td>\n",
       "      <td>lugar de estudio</td>\n",
       "      <td>2269402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P3173</td>\n",
       "      <td>offers view on</td>\n",
       "      <td>has view of</td>\n",
       "      <td>bietet Sicht auf</td>\n",
       "      <td>Sicht auf</td>\n",
       "      <td>ofrece vista a</td>\n",
       "      <td>offre une vue sur</td>\n",
       "      <td>offre un panorama sur</td>\n",
       "      <td>tiene vista a</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>P2679</td>\n",
       "      <td>author of foreword</td>\n",
       "      <td>foreword by</td>\n",
       "      <td>Autor des Vorworts</td>\n",
       "      <td>Vorwort von</td>\n",
       "      <td>autor del prefacio</td>\n",
       "      <td>auteur de la préface</td>\n",
       "      <td>auteur de l'avant-propos</td>\n",
       "      <td>prefacio por</td>\n",
       "      <td>1543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       _id                         en           en_alias  \\\n",
       "84   P4330                   contains       has contents   \n",
       "258  P8738                    permits  does not prohibit   \n",
       "45   P2872             visitor centre     tourist office   \n",
       "175   P915           filming location          filmed at   \n",
       "143  P1777                  manner of           style of   \n",
       "125  P2175  medical condition treated    disease treated   \n",
       "224  P1429                    has pet           owns pet   \n",
       "118    P69                educated at         studied at   \n",
       "9    P3173             offers view on        has view of   \n",
       "90   P2679         author of foreword        foreword by   \n",
       "\n",
       "                               de              de_alias  \\\n",
       "84                        enthält                lagert   \n",
       "258                       erlaubt           autorisiert   \n",
       "45           Touristeninformation  Touristeninformation   \n",
       "175                       Drehort            gedreht in   \n",
       "143                  nach Art von         Nachahmer von   \n",
       "125    zur Behandlung von benutzt             behandelt   \n",
       "224                  hat Haustier             Haustiere   \n",
       "118  besuchte Bildungseinrichtung            Hochschule   \n",
       "9                bietet Sicht auf             Sicht auf   \n",
       "90             Autor des Vorworts           Vorwort von   \n",
       "\n",
       "                           es                    fr                  fr_alias  \\\n",
       "84                   contiene              contient              contenant de   \n",
       "258                   permite                permet                  autorise   \n",
       "45         oficina de turismo    office de tourisme        office du tourisme   \n",
       "175        lugar de filmación      lieu du tournage          lieu de tournage   \n",
       "143            a la manera de       à la manière de                  style de   \n",
       "125  condición médica tratada       maladie traitée                    traite   \n",
       "224        animal de compañía   animal de compagnie  a un animal de compagnie   \n",
       "118                educado en             scolarité                 formation   \n",
       "9              ofrece vista a     offre une vue sur     offre un panorama sur   \n",
       "90         autor del prefacio  auteur de la préface  auteur de l'avant-propos   \n",
       "\n",
       "                            es_alias    count  \n",
       "84                           alberga     8269  \n",
       "258                         autoriza       23  \n",
       "45   centro de información turística      554  \n",
       "175                  lugar de rodaje    32299  \n",
       "143                        manera de        9  \n",
       "125               enfermedad tratada     6992  \n",
       "224                 mascota (animal)      318  \n",
       "118                 lugar de estudio  2269402  \n",
       "9                      tiene vista a      905  \n",
       "90                      prefacio por     1543  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test, relations = generate_reasoning(relation=Relation(relation),\n",
    "                                            source_language=source_language,\n",
    "                                            target_language=target_language,\n",
    "                                            n_relations=n_relations,\n",
    "                                            n_facts=n_facts,\n",
    "                                            use_pretrained=use_pretrained,\n",
    "                                            use_target=use_target,\n",
    "                                            use_enhanced=False,\n",
    "                                            use_same_relations=False,\n",
    "                                            n_pairs=0)\n",
    "\n",
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5349baf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_random = []\n",
    "\n",
    "if use_random:\n",
    "    # Generate half/half\n",
    "    factor = 1.0\n",
    "    n_random = factor * n_facts\n",
    "\n",
    "    train_random, relations_random = generate_random(source_language, target_language, n_random, n_relations)\n",
    "    train += train_random\n",
    "\n",
    "relations_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81631d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# LOADING\n",
    "# Load mBERT model and Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# model = BertForMaskedLM.from_pretrained(\"./output/EQUI_en_de/models/checkpoint-6422\")\n",
    "\n",
    "# Load Data Collator for Prediction and Evaluation\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "eval_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5618d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679e14ffae1d4c37bd46ffa145a8b5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e609a48423f4c7c828bd8d983f5dcee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ~~ PRE-PROCESSING ~~\n",
    "train_dict = {'sample': train}\n",
    "test_dict = {'sample': flatten_dict2_list(copy.deepcopy(test))}\n",
    "train_ds = Dataset.from_dict(train_dict)\n",
    "test_ds = Dataset.from_dict(test_dict)\n",
    "\n",
    "# Tokenize Training and Test Data\n",
    "tokenized_train = tokenize(tokenizer, train_ds)  # Train is shuffled by Huggingface\n",
    "tokenized_test = tokenize(tokenizer, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58740d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Train and Test Data\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "test_complete_df = pd.DataFrame(test)\n",
    "test_flat_df = pd.DataFrame(test_dict)\n",
    "\n",
    "data_dir = './output/' + run_name + '/data/'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "train_df.to_csv(data_dir + 'train_set', index=False)\n",
    "test_complete_df.to_json(data_dir + 'test_set_complete')\n",
    "test_flat_df.to_csv(data_dir + 'test_set', index=False)\n",
    "\n",
    "if use_random:\n",
    "    train_random_df = pd.DataFrame({'sample': train_random})\n",
    "    train_random_df.to_csv(data_dir + 'train_random', index=False)\n",
    "\n",
    "if use_anti:\n",
    "    train_anti_df = pd.DataFrame({'sample': train_anti})\n",
    "    test_anti_df = pd.DataFrame({'sample': test_anti})\n",
    "\n",
    "    train_anti_df.to_csv(data_dir + 'train_anti_set', index=False)\n",
    "    test_anti_df.to_json(data_dir + 'test_anti_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da0d66d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='./output/' + run_name + '/models/',\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=128,\n",
    "        learning_rate=lr,\n",
    "        logging_dir='./output/' + run_name + '/tb_logs/',\n",
    "        logging_strategy=IntervalStrategy.EPOCH,\n",
    "        evaluation_strategy=IntervalStrategy.EPOCH,\n",
    "        save_strategy=IntervalStrategy.EPOCH,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    eval_data_collator=eval_data_collator,\n",
    "    compute_metrics=precision_at_one\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b90787af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19000\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7600\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7600' max='7600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7600/7600 1:48:48, Epoch 200/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.396800</td>\n",
       "      <td>9.514431</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.333500</td>\n",
       "      <td>9.271684</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.229900</td>\n",
       "      <td>9.171506</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.089200</td>\n",
       "      <td>9.109668</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.049200</td>\n",
       "      <td>9.072428</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.040600</td>\n",
       "      <td>9.035267</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.008200</td>\n",
       "      <td>8.996627</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.961700</td>\n",
       "      <td>8.963954</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.929300</td>\n",
       "      <td>8.950615</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.938300</td>\n",
       "      <td>8.934973</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.901700</td>\n",
       "      <td>8.913105</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.860000</td>\n",
       "      <td>8.883851</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.848100</td>\n",
       "      <td>8.870687</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.888800</td>\n",
       "      <td>8.861097</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.896000</td>\n",
       "      <td>8.843496</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.814100</td>\n",
       "      <td>8.830283</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.827900</td>\n",
       "      <td>8.799067</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.827500</td>\n",
       "      <td>8.796995</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.795500</td>\n",
       "      <td>8.757575</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.813300</td>\n",
       "      <td>8.757878</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.862800</td>\n",
       "      <td>8.749780</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.753900</td>\n",
       "      <td>8.722605</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.811300</td>\n",
       "      <td>8.688846</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.702200</td>\n",
       "      <td>8.681074</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.752100</td>\n",
       "      <td>8.675900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.764300</td>\n",
       "      <td>8.660770</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.801600</td>\n",
       "      <td>8.626243</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.734500</td>\n",
       "      <td>8.609982</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.641400</td>\n",
       "      <td>8.603963</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.677500</td>\n",
       "      <td>8.585603</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.620800</td>\n",
       "      <td>8.558127</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.601700</td>\n",
       "      <td>8.526957</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.548000</td>\n",
       "      <td>8.511396</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.517600</td>\n",
       "      <td>8.490067</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.473000</td>\n",
       "      <td>8.466951</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.489700</td>\n",
       "      <td>8.434923</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.419600</td>\n",
       "      <td>8.420118</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.363300</td>\n",
       "      <td>8.404585</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.362200</td>\n",
       "      <td>8.384037</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.277300</td>\n",
       "      <td>8.346393</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.291500</td>\n",
       "      <td>8.318776</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.264100</td>\n",
       "      <td>8.305205</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.178100</td>\n",
       "      <td>8.294795</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.139500</td>\n",
       "      <td>8.272041</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.124900</td>\n",
       "      <td>8.269509</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.091300</td>\n",
       "      <td>8.234408</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.005600</td>\n",
       "      <td>8.225928</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.957000</td>\n",
       "      <td>8.185746</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.964700</td>\n",
       "      <td>8.163623</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.891100</td>\n",
       "      <td>8.138280</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.885800</td>\n",
       "      <td>8.113605</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.852600</td>\n",
       "      <td>8.093037</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.852200</td>\n",
       "      <td>8.073287</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.809900</td>\n",
       "      <td>8.041049</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.770200</td>\n",
       "      <td>8.016335</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.738600</td>\n",
       "      <td>7.982049</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.651000</td>\n",
       "      <td>7.961741</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.674200</td>\n",
       "      <td>7.919945</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.624500</td>\n",
       "      <td>7.919250</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.601600</td>\n",
       "      <td>7.874579</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.577000</td>\n",
       "      <td>7.884881</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.552000</td>\n",
       "      <td>7.831584</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.493700</td>\n",
       "      <td>7.830357</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.461400</td>\n",
       "      <td>7.773488</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.473700</td>\n",
       "      <td>7.752386</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.361900</td>\n",
       "      <td>7.733964</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.399400</td>\n",
       "      <td>7.682390</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.349400</td>\n",
       "      <td>7.667026</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.286000</td>\n",
       "      <td>7.666352</td>\n",
       "      <td>0.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.331700</td>\n",
       "      <td>7.627503</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.254900</td>\n",
       "      <td>7.560788</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.255800</td>\n",
       "      <td>7.551973</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.239000</td>\n",
       "      <td>7.500331</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.213400</td>\n",
       "      <td>7.467565</td>\n",
       "      <td>0.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.201500</td>\n",
       "      <td>7.418466</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.167400</td>\n",
       "      <td>7.410096</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.129500</td>\n",
       "      <td>7.363981</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.134000</td>\n",
       "      <td>7.366209</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.079500</td>\n",
       "      <td>7.295459</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.047600</td>\n",
       "      <td>7.282712</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.058500</td>\n",
       "      <td>7.260322</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.016400</td>\n",
       "      <td>7.222314</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.978500</td>\n",
       "      <td>7.224194</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.023100</td>\n",
       "      <td>7.139389</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.941700</td>\n",
       "      <td>7.088616</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.996900</td>\n",
       "      <td>7.082561</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.956500</td>\n",
       "      <td>7.047238</td>\n",
       "      <td>0.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>6.975941</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.917000</td>\n",
       "      <td>6.905331</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.919700</td>\n",
       "      <td>6.852310</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.950300</td>\n",
       "      <td>6.839068</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.891200</td>\n",
       "      <td>6.801317</td>\n",
       "      <td>0.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.863900</td>\n",
       "      <td>6.749243</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.861500</td>\n",
       "      <td>6.672566</td>\n",
       "      <td>0.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.872800</td>\n",
       "      <td>6.646703</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.783400</td>\n",
       "      <td>6.594380</td>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.832400</td>\n",
       "      <td>6.494370</td>\n",
       "      <td>0.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.827500</td>\n",
       "      <td>6.418345</td>\n",
       "      <td>0.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.775100</td>\n",
       "      <td>6.394498</td>\n",
       "      <td>0.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.768600</td>\n",
       "      <td>6.312959</td>\n",
       "      <td>0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.786300</td>\n",
       "      <td>6.273044</td>\n",
       "      <td>0.090000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>6.223544</td>\n",
       "      <td>0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.766400</td>\n",
       "      <td>6.166185</td>\n",
       "      <td>0.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.767300</td>\n",
       "      <td>6.125584</td>\n",
       "      <td>0.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.723300</td>\n",
       "      <td>6.042178</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.770300</td>\n",
       "      <td>5.948775</td>\n",
       "      <td>0.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.715500</td>\n",
       "      <td>5.903131</td>\n",
       "      <td>0.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.711400</td>\n",
       "      <td>5.813383</td>\n",
       "      <td>0.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.719900</td>\n",
       "      <td>5.852066</td>\n",
       "      <td>0.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.713900</td>\n",
       "      <td>5.700565</td>\n",
       "      <td>0.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.727200</td>\n",
       "      <td>5.629865</td>\n",
       "      <td>0.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>5.501058</td>\n",
       "      <td>0.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.677400</td>\n",
       "      <td>5.537011</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.719100</td>\n",
       "      <td>5.432525</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>5.396631</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.662100</td>\n",
       "      <td>5.290934</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.665400</td>\n",
       "      <td>5.265104</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.643300</td>\n",
       "      <td>5.205688</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>5.183731</td>\n",
       "      <td>0.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.651100</td>\n",
       "      <td>5.097285</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.652700</td>\n",
       "      <td>5.050526</td>\n",
       "      <td>0.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.615700</td>\n",
       "      <td>4.960560</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.617900</td>\n",
       "      <td>4.925427</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.600400</td>\n",
       "      <td>4.834658</td>\n",
       "      <td>0.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.629700</td>\n",
       "      <td>4.817384</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.601100</td>\n",
       "      <td>4.769546</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.605700</td>\n",
       "      <td>4.672717</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>4.628486</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.596700</td>\n",
       "      <td>4.504067</td>\n",
       "      <td>0.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.593900</td>\n",
       "      <td>4.447351</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.633300</td>\n",
       "      <td>4.432845</td>\n",
       "      <td>0.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>4.370050</td>\n",
       "      <td>0.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.582800</td>\n",
       "      <td>4.327803</td>\n",
       "      <td>0.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.594300</td>\n",
       "      <td>4.229449</td>\n",
       "      <td>0.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.629200</td>\n",
       "      <td>4.222831</td>\n",
       "      <td>0.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>4.169195</td>\n",
       "      <td>0.288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.585900</td>\n",
       "      <td>4.175768</td>\n",
       "      <td>0.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.574300</td>\n",
       "      <td>4.093826</td>\n",
       "      <td>0.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.575900</td>\n",
       "      <td>4.038976</td>\n",
       "      <td>0.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.574500</td>\n",
       "      <td>3.996206</td>\n",
       "      <td>0.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.586300</td>\n",
       "      <td>3.968747</td>\n",
       "      <td>0.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.583400</td>\n",
       "      <td>3.885052</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.581800</td>\n",
       "      <td>3.780198</td>\n",
       "      <td>0.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.574800</td>\n",
       "      <td>3.737179</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>3.703264</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.537300</td>\n",
       "      <td>3.652646</td>\n",
       "      <td>0.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>3.602982</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.522500</td>\n",
       "      <td>3.623492</td>\n",
       "      <td>0.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.566700</td>\n",
       "      <td>3.583311</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.548800</td>\n",
       "      <td>3.526824</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.604900</td>\n",
       "      <td>3.475476</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.528500</td>\n",
       "      <td>3.472714</td>\n",
       "      <td>0.366000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>3.486250</td>\n",
       "      <td>0.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.528500</td>\n",
       "      <td>3.387672</td>\n",
       "      <td>0.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>3.323955</td>\n",
       "      <td>0.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.556500</td>\n",
       "      <td>3.258899</td>\n",
       "      <td>0.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.529100</td>\n",
       "      <td>3.245477</td>\n",
       "      <td>0.406000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.508500</td>\n",
       "      <td>3.240286</td>\n",
       "      <td>0.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.561700</td>\n",
       "      <td>3.212657</td>\n",
       "      <td>0.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.518300</td>\n",
       "      <td>3.185895</td>\n",
       "      <td>0.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.561000</td>\n",
       "      <td>3.113083</td>\n",
       "      <td>0.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.532600</td>\n",
       "      <td>3.096083</td>\n",
       "      <td>0.434000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.515900</td>\n",
       "      <td>3.034920</td>\n",
       "      <td>0.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.541200</td>\n",
       "      <td>2.994260</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.555800</td>\n",
       "      <td>2.960593</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.535100</td>\n",
       "      <td>2.918937</td>\n",
       "      <td>0.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>2.906403</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.536600</td>\n",
       "      <td>2.893870</td>\n",
       "      <td>0.462000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.535100</td>\n",
       "      <td>2.862096</td>\n",
       "      <td>0.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.521600</td>\n",
       "      <td>2.810503</td>\n",
       "      <td>0.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.544900</td>\n",
       "      <td>2.768023</td>\n",
       "      <td>0.484000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.539900</td>\n",
       "      <td>2.749927</td>\n",
       "      <td>0.472000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.508400</td>\n",
       "      <td>2.768617</td>\n",
       "      <td>0.474000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.519800</td>\n",
       "      <td>2.782975</td>\n",
       "      <td>0.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.488500</td>\n",
       "      <td>2.728040</td>\n",
       "      <td>0.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.532500</td>\n",
       "      <td>2.698722</td>\n",
       "      <td>0.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.498900</td>\n",
       "      <td>2.692762</td>\n",
       "      <td>0.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.510100</td>\n",
       "      <td>2.667347</td>\n",
       "      <td>0.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.511500</td>\n",
       "      <td>2.657341</td>\n",
       "      <td>0.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.478300</td>\n",
       "      <td>2.644420</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>2.641091</td>\n",
       "      <td>0.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.459700</td>\n",
       "      <td>2.630129</td>\n",
       "      <td>0.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.504600</td>\n",
       "      <td>2.568494</td>\n",
       "      <td>0.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.527700</td>\n",
       "      <td>2.577049</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>2.554447</td>\n",
       "      <td>0.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.496300</td>\n",
       "      <td>2.535259</td>\n",
       "      <td>0.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.501800</td>\n",
       "      <td>2.518344</td>\n",
       "      <td>0.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.478500</td>\n",
       "      <td>2.506642</td>\n",
       "      <td>0.506000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.499700</td>\n",
       "      <td>2.502046</td>\n",
       "      <td>0.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.510700</td>\n",
       "      <td>2.491748</td>\n",
       "      <td>0.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.491900</td>\n",
       "      <td>2.491888</td>\n",
       "      <td>0.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.510700</td>\n",
       "      <td>2.462817</td>\n",
       "      <td>0.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.518600</td>\n",
       "      <td>2.437123</td>\n",
       "      <td>0.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.498400</td>\n",
       "      <td>2.437246</td>\n",
       "      <td>0.517000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.480700</td>\n",
       "      <td>2.434578</td>\n",
       "      <td>0.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.472100</td>\n",
       "      <td>2.429620</td>\n",
       "      <td>0.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.493000</td>\n",
       "      <td>2.426208</td>\n",
       "      <td>0.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>2.419310</td>\n",
       "      <td>0.527000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.528400</td>\n",
       "      <td>2.418321</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.517200</td>\n",
       "      <td>2.416515</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-38\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-38/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-38/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-38/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-38/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-76\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-76/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-76/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-76/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-76/special_tokens_map.json\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-114\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-114/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-114/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-114/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-114/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-76] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-152\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-152/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-152/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-152/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-152/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-114] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-190\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-190/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-190/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-190/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-190/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-152] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-228\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-228/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-228/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-228/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-228/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-38] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-266\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-266/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-266/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-266/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-266/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-190] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-304\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-304/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-304/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-304/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-304/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-266] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-342\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-342/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-342/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-342/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-342/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-304] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-380\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-380/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-380/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-380/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-380/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-342] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-418\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-418/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-418/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-418/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-418/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-380] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-456\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-456/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-456/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-456/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-456/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-418] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-494\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-494/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-494/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-494/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-494/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-456] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-532\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-532/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-532/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-532/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-532/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-494] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-570\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-570/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-570/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-570/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-570/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-532] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-608\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-608/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-608/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-608/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-608/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-228] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-646\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-646/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-646/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-646/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-646/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-570] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-684\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-684/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-684/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-684/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-684/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-646] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-722/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-722/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-722/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-722/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-684] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-760\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-760/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-760/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-760/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-760/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-722] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-798\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-798/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-798/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-798/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-798/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-760] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-836\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-836/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-836/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-836/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-836/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-798] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-874\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-874/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-874/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-874/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-874/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-836] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-912\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-912/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-912/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-912/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-912/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-874] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-950\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-950/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-950/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-912] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-988\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-988/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-988/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-988/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-988/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-950] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1026\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1026/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1026/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1026/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1026/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-988] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1064\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1064/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1064/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1064/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1064/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1026] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1102\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1102/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1102/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1102/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1102/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1064] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1140\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1140/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1140/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1140/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1140/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1102] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1178\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1178/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1178/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1178/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1178/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1140] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1216\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1216/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1216/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1216/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1216/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1178] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1254\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1254/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1254/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1254/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1254/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1216] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1292\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1292/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1292/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1292/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1292/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1254] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1330\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1330/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1330/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1330/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1330/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1292] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1368\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1368/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1368/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1368/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1368/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1330] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1406\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1406/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1406/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1406/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1406/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-608] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1444\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1444/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1444/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1444/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1444/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1368] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1482\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1482/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1482/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1482/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1482/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1444] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1520\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1520/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1520/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1520/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1520/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1482] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1558\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1558/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1558/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1558/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1558/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1520] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1596\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1596/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1596/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1596/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1596/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1558] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1634\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1634/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1634/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1634/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1634/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1596] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1672\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1672/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1672/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1672/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1672/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1634] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1710\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1710/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1710/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1710/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1710/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1406] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1748\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1748/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1748/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1748/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1748/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1672] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1786\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1786/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1786/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1786/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1786/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1748] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1824\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1824/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1824/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1824/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1824/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1786] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1862\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1862/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1862/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1862/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1862/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1824] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1900\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1900/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1900/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1862] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1938\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1938/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1938/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1938/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1938/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1900] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-1976\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1976/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1976/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1976/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-1976/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1938] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2014\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2014/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2014/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2014/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2014/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1976] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2052\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2052/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2052/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2052/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2052/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2014] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2090\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2090/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2090/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2090/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2090/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2052] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2128\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2128/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2128/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2128/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2128/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2090] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2166\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2166/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2166/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2166/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2166/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2128] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2204\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2204/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2204/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2204/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2204/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2166] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2242\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2242/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2242/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2242/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2242/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2204] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2280\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2280/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2280/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2280/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2280/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2242] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2318\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2318/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2318/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2318/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2318/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-1710] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2356\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2356/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2356/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2356/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2356/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2280] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2394\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2394/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2394/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2394/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2394/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2318] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2432\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2432/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2432/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2432/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2432/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2394] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2470\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2470/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2470/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2470/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2470/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2432] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2508\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2508/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2508/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2508/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2508/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2470] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2546\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2546/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2546/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2546/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2546/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2356] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2584\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2584/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2584/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2584/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2584/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2508] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2622\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2622/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2622/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2622/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2622/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2584] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2660\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2660/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2660/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2660/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2660/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2622] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2698\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2698/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2698/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2698/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2698/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2660] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2736\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2736/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2736/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2736/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2736/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2698] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2774\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2774/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2774/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2774/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2774/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2546] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2812\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2812/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2812/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2812/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2812/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2736] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2850\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2850/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2850/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2774] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2888\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2888/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2888/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2888/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2888/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2812] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2926\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2926/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2926/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2926/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2926/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2850] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-2964\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2964/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2964/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2964/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-2964/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2888] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3002\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3002/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3002/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3002/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3002/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2926] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3040\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3040/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3040/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3040/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3040/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-2964] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3078\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3078/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3078/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3078/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3078/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3002] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3116\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3116/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3116/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3116/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3116/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3040] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3154\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3154/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3154/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3154/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3154/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3116] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3192\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3192/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3192/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3192/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3192/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3078] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3230\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3230/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3230/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3230/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3230/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3154] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3268\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3268/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3268/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3268/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3268/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3192] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3306\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3306/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3306/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3306/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3306/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3230] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3344\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3344/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3344/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3344/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3344/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3306] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3382\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3382/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3382/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3382/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3382/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3268] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3420\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3420/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3420/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3420/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3420/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3344] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3458\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3458/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3458/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3458/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3458/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3382] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3496\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3496/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3496/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3496/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3496/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3420] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3534\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3534/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3534/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3534/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3534/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3458] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3572\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3572/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3572/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3572/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3572/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3496] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3610\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3610/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3610/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3610/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3610/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3534] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3648\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3648/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3648/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3648/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3648/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3572] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3686\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3686/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3686/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3686/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3686/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3610] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3724\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3724/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3724/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3724/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3724/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3648] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3762\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3762/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3762/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3762/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3762/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3686] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3800\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3800/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3800/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3724] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3838\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3838/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3838/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3838/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3838/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3762] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3876\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3876/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3876/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3876/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3876/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3800] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3914\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3914/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3914/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3914/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3914/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3838] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3952\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3952/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3952/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3952/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3952/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3876] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-3990\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3990/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3990/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3990/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-3990/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3914] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4028\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4028/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4028/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4028/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4028/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3952] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4066\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4066/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4066/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4066/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4066/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-3990] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4104\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4104/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4104/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4104/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4104/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4028] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4142\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4142/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4142/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4142/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4142/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4066] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4180\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4180/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4180/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4180/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4180/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4104] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4218\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4218/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4218/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4218/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4218/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4142] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4256\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4256/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4256/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4256/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4256/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4180] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4294\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4294/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4294/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4294/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4294/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4218] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4332\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4332/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4332/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4332/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4332/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4256] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4370\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4370/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4370/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4370/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4370/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4294] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4408\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4408/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4408/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4408/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4408/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4332] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4446\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4446/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4446/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4446/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4446/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4408] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4484\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4484/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4484/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4484/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4484/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4446] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4522\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4522/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4522/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4522/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4522/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4484] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4560\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4560/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4560/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4560/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4560/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4370] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4598\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4598/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4598/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4598/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4598/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4522] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4636\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4636/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4636/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4636/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4636/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4560] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4674\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4674/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4674/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4674/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4674/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4598] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4712\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4712/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4712/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4712/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4712/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4636] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4750\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4750/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4750/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4674] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4788\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4788/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4788/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4788/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4788/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4712] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4826\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4826/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4826/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4826/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4826/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4750] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4864\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4864/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4864/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4864/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4864/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4788] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4902\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4902/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4902/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4902/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4902/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4826] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4940\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4940/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4940/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4940/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4940/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4864] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-4978\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4978/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4978/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4978/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-4978/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4902] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5016\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5016/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5016/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5016/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5016/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4940] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5054\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5054/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5054/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5054/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5054/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-4978] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5092\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5092/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5092/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5092/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5092/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5016] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5130\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5130/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5130/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5130/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5130/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5054] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5168\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5168/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5168/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5168/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5168/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5092] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5206\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5206/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5206/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5206/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5206/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5130] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5244\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5244/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5244/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5244/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5244/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5168] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5282\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5282/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5282/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5282/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5282/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5206] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5320\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5320/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5320/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5320/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5320/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5282] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5358\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5358/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5358/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5358/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5358/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5320] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5396\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5396/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5396/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5396/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5396/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5244] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5434\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5434/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5434/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5434/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5434/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5358] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5472\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5472/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5472/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5472/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5472/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5396] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5510\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5510/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5510/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5510/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5510/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5434] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5548\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5548/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5548/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5548/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5548/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5472] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5586\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5586/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5586/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5586/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5586/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5510] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5624\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5624/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5624/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5624/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5624/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5548] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5662\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5662/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5662/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5662/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5662/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5586] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5700\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5700/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5624] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5738\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5738/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5738/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5738/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5738/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5662] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5776\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5776/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5776/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5776/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5776/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5738] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5814\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5814/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5814/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5814/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5814/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5776] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5852\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5852/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5852/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5852/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5852/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5700] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5890\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5890/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5890/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5890/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5890/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5814] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5928\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5928/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5928/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5928/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5928/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5852] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-5966\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5966/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5966/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5966/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-5966/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5890] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6004\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6004/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6004/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6004/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6004/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5928] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6042\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6042/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6042/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6042/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6042/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-5966] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6080\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6080/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6080/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6080/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6080/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6042] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6118\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6118/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6118/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6118/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6118/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6004] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6156\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6156/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6156/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6156/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6156/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6080] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6194\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6194/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6194/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6194/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6194/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6118] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6232\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6232/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6232/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6232/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6232/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6156] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6270\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6270/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6270/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6270/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6270/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6194] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6308\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6308/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6308/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6308/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6308/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6270] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6346\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6346/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6346/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6346/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6346/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6308] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6384\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6384/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6384/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6384/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6384/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6232] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6422\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6422/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6422/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6422/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6422/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6346] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6460\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6460/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6460/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6460/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6460/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6384] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6498\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6498/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6498/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6498/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6498/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6422] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6536\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6536/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6536/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6536/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6536/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6460] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6574\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6574/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6574/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6574/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6574/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6536] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6612\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6612/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6612/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6612/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6612/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6574] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6650\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6650/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6650/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6612] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6688\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6688/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6688/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6688/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6688/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6650] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6726\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6726/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6726/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6726/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6726/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6688] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6764\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6764/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6764/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6764/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6764/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6726] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6802\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6802/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6802/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6802/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6802/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6764] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6840\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6840/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6840/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6840/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6840/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6498] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6878\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6878/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6878/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6878/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6878/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6802] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6916\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6916/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6916/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6916/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6916/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6840] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6954\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6954/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6954/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6954/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6954/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6878] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-6992\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6992/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6992/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6992/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-6992/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6954] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7030\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7030/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7030/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7030/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7030/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6992] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7068\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7068/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7068/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7068/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7068/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7030] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7106\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7106/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7106/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7106/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7106/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-6916] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7144\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7144/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7144/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7144/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7144/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7068] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7182\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7182/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7182/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7182/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7182/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7144] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7220\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7220/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7220/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7220/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7220/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7106] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7258\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7258/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7258/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7258/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7258/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7182] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7296\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7296/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7296/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7296/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7296/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7220] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7334\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7334/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7334/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7334/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7334/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7258] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7372\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7372/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7372/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7372/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7372/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7296] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7410\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7410/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7410/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7410/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7410/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7334] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7448\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7448/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7448/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7448/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7448/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7372] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7486\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7486/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7486/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7486/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7486/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7410] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7524\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7524/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7524/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7524/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7524/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7448] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7562\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7562/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7562/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7562/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7562/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7486] due to args.save_total_limit\n",
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./output/EQUI_es_en_target_pretrained/models/checkpoint-7600\n",
      "Configuration saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7600/config.json\n",
      "Model weights saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7600/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/EQUI_es_en_target_pretrained/models/checkpoint-7600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/EQUI_es_en_target_pretrained/models/checkpoint-7524] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/EQUI_es_en_target_pretrained/models/checkpoint-7562 (score: 0.528).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7600, training_loss=1.2639215331328542, metrics={'train_runtime': 6530.5791, 'train_samples_per_second': 581.878, 'train_steps_per_second': 1.164, 'total_flos': 1.759952691e+16, 'train_loss': 1.2639215331328542, 'epoch': 200.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d1dc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_accuracy': 0.528,\n",
       " 'eval_loss': 2.418320655822754,\n",
       " 'eval_runtime': 1.3238,\n",
       " 'eval_samples_per_second': 755.386,\n",
       " 'eval_steps_per_second': 3.022,\n",
       " 'epoch': 200.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Test\n",
    "trainer.evaluate(eval_dataset=tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "360c6e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation - source: contiene, target: contains\n",
      "Alias - source: alberga, target: has contents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7bb67c1b224032ae542a2da847cfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.07, 'eval_loss': 5.154648303985596, 'eval_runtime': 0.5895, 'eval_samples_per_second': 169.625, 'eval_steps_per_second': 1.696}\n",
      "\n",
      "\n",
      "Relation - source: permite, target: permits\n",
      "Alias - source: autoriza, target: does not prohibit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6334c5a8e5b40bcba06fd73bf12ee31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.35, 'eval_loss': 3.030627965927124, 'eval_runtime': 0.5991, 'eval_samples_per_second': 166.918, 'eval_steps_per_second': 1.669}\n",
      "\n",
      "\n",
      "Relation - source: oficina de turismo, target: visitor centre\n",
      "Alias - source: centro de información turística, target: tourist office\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8a225620bf44c582f9866008202eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.25, 'eval_loss': 3.347647190093994, 'eval_runtime': 0.5947, 'eval_samples_per_second': 168.158, 'eval_steps_per_second': 1.682}\n",
      "\n",
      "\n",
      "Relation - source: lugar de filmación, target: filming location\n",
      "Alias - source: lugar de rodaje, target: filmed at\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d22b05efdb548eea48fc65991d5f3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.09, 'eval_loss': 5.401822566986084, 'eval_runtime': 0.6093, 'eval_samples_per_second': 164.132, 'eval_steps_per_second': 1.641}\n",
      "\n",
      "\n",
      "Relation - source: a la manera de, target: manner of\n",
      "Alias - source: manera de, target: style of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9435be359a4d1f9b237e12622b74af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.77, 'eval_loss': 1.325810194015503, 'eval_runtime': 0.5747, 'eval_samples_per_second': 174.013, 'eval_steps_per_second': 1.74}\n",
      "\n",
      "\n",
      "Relation - source: condición médica tratada, target: medical condition treated\n",
      "Alias - source: enfermedad tratada, target: disease treated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726c721c08ad4bd4a5557908bf61c25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.91, 'eval_loss': 0.518544614315033, 'eval_runtime': 0.5679, 'eval_samples_per_second': 176.074, 'eval_steps_per_second': 1.761}\n",
      "\n",
      "\n",
      "Relation - source: animal de compañía, target: has pet\n",
      "Alias - source: mascota (animal), target: owns pet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b05da626fdd4501a2482eb34bd08772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.76, 'eval_loss': 1.151837706565857, 'eval_runtime': 0.5663, 'eval_samples_per_second': 176.572, 'eval_steps_per_second': 1.766}\n",
      "\n",
      "\n",
      "Relation - source: educado en, target: educated at\n",
      "Alias - source: lugar de estudio, target: studied at\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b58767ea70495e859154d6245812ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.74, 'eval_loss': 1.2176198959350586, 'eval_runtime': 0.5878, 'eval_samples_per_second': 170.118, 'eval_steps_per_second': 1.701}\n",
      "\n",
      "\n",
      "Relation - source: ofrece vista a, target: offers view on\n",
      "Alias - source: tiene vista a, target: has view of\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1484803d5284542a14db1d61303ec18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.58, 'eval_loss': 1.9818114042282104, 'eval_runtime': 0.5739, 'eval_samples_per_second': 174.242, 'eval_steps_per_second': 1.742}\n",
      "\n",
      "\n",
      "Relation - source: autor del prefacio, target: author of foreword\n",
      "Alias - source: prefacio por, target: foreword by\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18d30c0bb574bb7847f5b0c359765c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurin/anaconda3/envs/master/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.76, 'eval_loss': 1.0528361797332764, 'eval_runtime': 0.5809, 'eval_samples_per_second': 172.161, 'eval_steps_per_second': 1.722}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Equivalence per Relation\n",
    "evaluation_equivalence_pretrained(trainer, tokenizer, relations, source_language, copy.deepcopy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214e34f",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "- Test my hypothesis if (f, r, e) or (e, r_de, f) exist more?\n",
    "- Is every relation symmetric now? What about relations that aren't part of the training?\n",
    "- If every relation is symmetric, try running with ANTI\n",
    "- And with General relations\n",
    "- Try Training with General and then evaluate general like on Anti!\n",
    "- Does that change the evaluation accuracy?\n",
    "- pretrained?\n",
    "- target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77216efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b37f3489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nirvana contains Ayn', 'Nirvana has contents Ayn', 'Bristol contains Cu', 'Bristol has contents Cu', 'Ark contains Champs', 'Ark has contents Champs', 'Ugo contains Seminary', 'Ugo has contents Seminary', 'Eure contains Lydia', 'Eure has contents Lydia', 'Goodman contains Greg', 'Goodman has contents Greg', 'Montero contains Fortaleza', 'Montero has contents Fortaleza', 'Nil contains TSV', 'Nil has contents TSV', 'Strike contains Bulu', 'Strike has contents Bulu', 'Marais contains Roll', 'Marais has contents Roll', 'Randolph contains Alec', 'Randolph has contents Alec', 'Highland contains Finale', 'Highland has contents Finale', 'Plans contains Mitt', 'Plans has contents Mitt', 'Principal contains Lyman', 'Principal has contents Lyman', 'Beatrice contains Bees', 'Beatrice has contents Bees', 'Polski contains Zeeland', 'Polski has contents Zeeland', 'Canada contains Algeria', 'Canada has contents Algeria', 'Mira contains Dada', 'Mira has contents Dada', 'Saga contains Sheridan', 'Saga has contents Sheridan', 'AF contains Pinto', 'AF has contents Pinto', 'Lakes contains Kelly', 'Lakes has contents Kelly', 'Oper contains Soria', 'Oper has contents Soria', 'Goodwin contains Fairfax', 'Goodwin has contents Fairfax', 'NDR contains Tutte', 'NDR has contents Tutte', 'Norton contains Cent', 'Norton has contents Cent', 'Hunter contains Lydia', 'Hunter has contents Lydia', 'Nuovo contains AG', 'Nuovo has contents AG', 'Goch contains Fund', 'Goch has contents Fund', 'Pretoria contains Louisville', 'Pretoria has contents Louisville', 'Oliver contains Veracruz', 'Oliver has contents Veracruz', 'Neustadt contains Sabbath', 'Neustadt has contents Sabbath', 'Mancha contains Duck', 'Mancha has contents Duck', 'Della contains Jack', 'Della has contents Jack', 'Ethel contains IRAS', 'Ethel has contents IRAS', 'ITF contains Cherokee', 'ITF has contents Cherokee', 'Rusi contains Oper', 'Rusi has contents Oper', 'Crescent contains Raphaël', 'Crescent has contents Raphaël', 'Thiessen contains Petit', 'Thiessen has contents Petit', 'Beauty contains Women', 'Beauty has contents Women', 'Irland contains Patton', 'Irland has contents Patton', 'Delgado contains Al', 'Delgado has contents Al', 'Klaus contains Oaxaca', 'Klaus has contents Oaxaca', 'Bhd contains Direito', 'Bhd has contents Direito', 'Dent contains Province', 'Dent has contents Province', 'Suiza contains Grimaldi', 'Suiza has contents Grimaldi', 'Alternate contains Ruska', 'Alternate has contents Ruska', 'ME contains Jess', 'ME has contents Jess', 'Slave contains Lugar', 'Slave has contents Lugar', 'Tore contains Paul', 'Tore has contents Paul', 'Reihe contains CAD', 'Reihe has contents CAD', 'Denmark contains Adrian', 'Denmark has contents Adrian', 'Teruel contains Color', 'Teruel has contents Color', 'TB contains Thornton', 'TB has contents Thornton', 'Dayton contains Madeleine', 'Dayton has contents Madeleine', 'Ming contains Kampung', 'Ming has contents Kampung', 'Männchen contains Britten', 'Männchen has contents Britten', 'Adventure contains Eagle', 'Adventure has contents Eagle', 'Sidi contains Spirit', 'Sidi has contents Spirit', 'AND contains Command', 'AND has contents Command', 'BSD contains Day', 'BSD has contents Day', 'Alexandria contains America', 'Alexandria has contents America', 'Marines contains Blanca', 'Marines has contents Blanca', 'He contains Sherlock', 'He has contents Sherlock', 'GC contains Qara', 'GC has contents Qara', 'Weiler contains Enterprise', 'Weiler has contents Enterprise', 'SM contains Pedra', 'SM has contents Pedra', 'Hollywood contains Milli', 'Hollywood has contents Milli', 'Sept contains Lost', 'Sept has contents Lost', 'Werner contains Johnny', 'Werner has contents Johnny', 'Griffith contains Vene', 'Griffith has contents Vene', 'Power contains Macau', 'Power has contents Macau', 'Yorkshire contains Manhattan', 'Yorkshire has contents Manhattan', 'Allier contains Monde', 'Allier has contents Monde', 'Catch contains Turquia', 'Catch has contents Turquia', 'Wait contains Cast', 'Wait has contents Cast', 'Bonaparte contains Balkan', 'Bonaparte has contents Balkan', 'London contains Mountain', 'London has contents Mountain', 'Ilha contains Madonna', 'Ilha has contents Madonna', 'Treviso contains Huelva', 'Treviso has contents Huelva', 'Daniels contains Brady', 'Daniels has contents Brady', 'Siria contains Mustang', 'Siria has contents Mustang', 'Fritz contains Caen', 'Fritz has contents Caen', 'Progreso contains Wert', 'Progreso has contents Wert', 'PlayStation contains Savage', 'PlayStation has contents Savage', 'Bulls contains WBA', 'Bulls has contents WBA', 'Senat contains Elias', 'Senat has contents Elias', 'Sra contains Houten', 'Sra has contents Houten', 'Oracle contains Rogers', 'Oracle has contents Rogers', 'Stern contains Change', 'Stern has contents Change', 'Midden contains WRC', 'Midden has contents WRC', 'Maka contains Gia', 'Maka has contents Gia', 'Wayne contains Jason', 'Wayne has contents Jason', 'Medalla contains Christi', 'Medalla has contents Christi', 'Copper contains Th', 'Copper has contents Th', 'Weiß contains Prairie', 'Weiß has contents Prairie', 'Leeds contains Tyne', 'Leeds has contents Tyne', 'Watch contains Culture', 'Watch has contents Culture', 'Data contains Rang', 'Data has contents Rang', 'Franche contains Ph', 'Franche has contents Ph', 'Birch contains Macbeth', 'Birch has contents Macbeth', 'Bello contains KHL', 'Bello has contents KHL', 'Cash contains Escape', 'Cash has contents Escape', 'Huis contains Racine', 'Huis has contents Racine', 'Celle contains Alus', 'Celle has contents Alus', 'Thành contains Jenny', 'Thành has contents Jenny', 'Cargo contains Conquest', 'Cargo has contents Conquest', 'Gruppo contains Loan', 'Gruppo has contents Loan', 'Middlesex contains Bambino', 'Middlesex has contents Bambino', 'Erie contains Theatre', 'Erie has contents Theatre', 'Males contains Hagen', 'Males has contents Hagen', 'Osiris contains Police', 'Osiris has contents Police', 'Chase contains Buster', 'Chase has contents Buster', 'Dunkerque contains Cherbourg', 'Dunkerque has contents Cherbourg', 'Bunker contains Isto', 'Bunker has contents Isto', 'Spiel contains Piper', 'Spiel has contents Piper', 'Ia contains Mexico', 'Ia has contents Mexico', 'Sagan contains Edda', 'Sagan has contents Edda', 'Golf contains Yoshida', 'Golf has contents Yoshida', 'Delia contains Door', 'Delia has contents Door', 'McGill contains Lenny', 'McGill has contents Lenny', 'DDR contains Copa', 'DDR has contents Copa', 'Siena contains NF', 'Siena has contents NF', 'Grupa contains Sá', 'Grupa has contents Sá', 'Dewan contains Raw', 'Dewan has contents Raw', 'BBC contains Salta', 'BBC has contents Salta', 'Amy contains Turnier', 'Amy has contents Turnier', 'Höhe contains Sheppard', 'Höhe has contents Sheppard', 'Blau contains Th', 'Blau has contents Th', 'Libertad contains Pam', 'Libertad has contents Pam', 'Zoo contains Garden', 'Zoo has contents Garden', 'Sera contains Wiley', 'Sera has contents Wiley', 'Newell contains Sanremo', 'Newell has contents Sanremo', 'Naomi contains Tri', 'Naomi has contents Tri', 'Superior contains Spain', 'Superior has contents Spain', 'ac contains Orient', 'ac has contents Orient', 'Bürgermeister contains Puis', 'Bürgermeister has contents Puis', 'Villiers contains Connection', 'Villiers has contents Connection', 'Fate contains Syracuse', 'Fate has contents Syracuse', 'Parks contains Assembly', 'Parks has contents Assembly', 'Lucy contains Giang', 'Lucy has contents Giang', 'FC contains Yokohama', 'FC has contents Yokohama', 'Mackenzie contains Amiens', 'Mackenzie has contents Amiens', 'Playhouse contains Tat', 'Playhouse has contents Tat', 'Path contains Hara', 'Path has contents Hara', 'Questa contains Space', 'Questa has contents Space', 'Lucas contains Trang', 'Lucas has contents Trang', 'Brüder contains Fisher', 'Brüder has contents Fisher', 'Sabine contains Roja', 'Sabine has contents Roja', 'Shanghai contains Titre', 'Shanghai has contents Titre', 'Wilhelmina contains Animals', 'Wilhelmina has contents Animals', 'Tone contains Bulu', 'Tone has contents Bulu', 'Hitchcock contains Willard', 'Hitchcock has contents Willard', 'Prometheus contains Codex', 'Prometheus has contents Codex', 'Buda contains Feria', 'Buda has contents Feria', 'Trend contains Corazón', 'Trend has contents Corazón', 'Roche contains Russie', 'Roche has contents Russie', 'Linden contains IPA', 'Linden has contents IPA', 'Baronet contains Borough', 'Baronet has contents Borough', 'Formosa contains Nigel', 'Formosa has contents Nigel', 'María contains Dans', 'María has contents Dans', 'Alma contains Papa', 'Alma has contents Papa', 'TX contains Bonnie', 'TX has contents Bonnie', 'Ruhr contains Leicester', 'Ruhr has contents Leicester', 'Keller contains Jørgensen', 'Keller has contents Jørgensen', 'Hy contains Gallia', 'Hy has contents Gallia', 'Inspector contains Ulysses', 'Inspector has contents Ulysses', 'Primera contains Souza', 'Primera has contents Souza', 'Fields contains Dixon', 'Fields has contents Dixon', 'Toro contains Revolución', 'Toro has contents Revolución', 'Snow contains Niger', 'Snow has contents Niger', 'Montenegro contains Aire', 'Montenegro has contents Aire', 'Helga contains Gerd', 'Helga has contents Gerd', 'Clarkson contains Battalion', 'Clarkson has contents Battalion', 'Neri contains Sans', 'Neri has contents Sans', 'Fed contains Pampa', 'Fed has contents Pampa', 'Welles contains Borussia', 'Welles has contents Borussia', 'Piper contains Edit', 'Piper has contents Edit', 'Mimi contains Mei', 'Mimi has contents Mei', 'Luzon contains Mille', 'Luzon has contents Mille', 'Bayreuth contains Tore', 'Bayreuth has contents Tore', 'Niger contains MAC', 'Niger has contents MAC', 'Madrid contains Island', 'Madrid has contents Island', 'Kawas contains Liam', 'Kawas has contents Liam', 'Henley contains Bones', 'Henley has contents Bones', 'Madagascar contains Alta', 'Madagascar has contents Alta', 'Dixie contains Kobayashi', 'Dixie has contents Kobayashi', 'Filadelfia contains Rams', 'Filadelfia has contents Rams', 'Give contains du', 'Give has contents du', 'Weser contains Een', 'Weser has contents Een', 'Leif contains Khmer', 'Leif has contents Khmer', 'Camus contains Gina', 'Camus has contents Gina', 'Santana contains Anna', 'Santana has contents Anna', 'EE contains Dag', 'EE has contents Dag', 'Haynes contains Secrets', 'Haynes has contents Secrets', 'Hart contains Pace', 'Hart has contents Pace', 'Nové contains Christopher', 'Nové has contents Christopher', 'Plessis contains Wiener', 'Plessis has contents Wiener', 'Ramsey contains RCA', 'Ramsey has contents RCA', 'Sturm contains Quest', 'Sturm has contents Quest', 'Marie contains Kirby', 'Marie has contents Kirby', 'MW contains Reich', 'MW has contents Reich', 'Rouge contains Geiger', 'Rouge has contents Geiger', 'Witt contains Pass', 'Witt has contents Pass', 'Branch contains Sigurd', 'Branch has contents Sigurd', 'Webber contains Mighty', 'Webber has contents Mighty', 'Auf contains Ng', 'Auf has contents Ng', 'Forma contains Nil', 'Forma has contents Nil', 'Bud contains Senat', 'Bud has contents Senat', 'Palmas contains Herrschaft', 'Palmas has contents Herrschaft', 'Hidalgo contains Poison', 'Hidalgo has contents Poison', 'Pizarro contains Swan', 'Pizarro has contents Swan', 'Sin contains Buddy', 'Sin has contents Buddy', 'Zelda contains Finance', 'Zelda has contents Finance', 'Wesley contains Idol', 'Wesley has contents Idol', 'Laba contains Brighton', 'Laba has contents Brighton', 'Ruska contains Principal', 'Ruska has contents Principal', 'RAF contains Mississippi', 'RAF has contents Mississippi', 'Padang contains Pero', 'Padang has contents Pero', 'Jump contains Acer', 'Jump has contents Acer', 'Bodø contains Shane', 'Bodø has contents Shane', 'Stift contains Stig', 'Stift has contents Stig', 'Hien contains Walther', 'Hien has contents Walther', 'Sabbath contains Schönberg', 'Sabbath has contents Schönberg', 'Auckland contains Po', 'Auckland has contents Po', 'Semana contains Sino', 'Semana has contents Sino', 'Sea contains Levante', 'Sea has contents Levante', 'Sicilia contains Princess', 'Sicilia has contents Princess', 'can contains Greenwood', 'can has contents Greenwood', 'Trio contains Gate', 'Trio has contents Gate', 'Milne contains ARM', 'Milne has contents ARM', 'Jacobus contains Shelby', 'Jacobus has contents Shelby', 'Norsk contains Townsend', 'Norsk has contents Townsend', 'Hollow contains Alonso', 'Hollow has contents Alonso', 'Pub contains Helena', 'Pub has contents Helena', 'AFL contains Murad', 'AFL has contents Murad', 'CK contains Basse', 'CK has contents Basse', 'Regio contains Sharks', 'Regio has contents Sharks', 'Justicia contains Sainte', 'Justicia has contents Sainte', 'Host contains Vergine', 'Host has contents Vergine', 'Hume contains CAS', 'Hume has contents CAS', 'Emperador contains Weiler', 'Emperador has contents Weiler', 'Genera contains IN', 'Genera has contents IN', 'Süleyman contains Murad', 'Süleyman has contents Murad', 'AFI contains Vida', 'AFI has contents Vida', 'ET contains Goebbels', 'ET has contents Goebbels', 'Order contains Roller', 'Order has contents Roller', 'Aube contains Shining', 'Aube has contents Shining', 'Rose contains Congo', 'Rose has contents Congo', 'Brücke contains Esperanza', 'Brücke has contents Esperanza', 'Belt contains Loyola', 'Belt has contents Loyola', 'Rok contains Exeter', 'Rok has contents Exeter', 'Bonus contains Jamaica', 'Bonus has contents Jamaica', 'Swing contains Yale', 'Swing has contents Yale', 'Sami contains Daniel', 'Sami has contents Daniel', 'Tema contains Boyd', 'Tema has contents Boyd', 'Bologna contains Fargo', 'Bologna has contents Fargo', 'Balance contains Mühle', 'Balance has contents Mühle', 'Luck contains Imperial', 'Luck has contents Imperial', 'Calderón contains Darkness', 'Calderón has contents Darkness', 'Seu contains Ronda', 'Seu has contents Ronda', 'Sessions contains Palestine', 'Sessions has contents Palestine', 'White contains Remo', 'White has contents Remo', 'Rodrigues contains Orlando', 'Rodrigues has contents Orlando', 'Jessica contains Hume', 'Jessica has contents Hume', 'McLaughlin contains Aube', 'McLaughlin has contents Aube', 'Brady contains Prin', 'Brady has contents Prin', 'Dur contains Amiga', 'Dur has contents Amiga', 'VOC contains Jos', 'VOC has contents Jos', 'Cardoso contains Harper', 'Cardoso has contents Harper', 'Katharina contains Birds', 'Katharina has contents Birds', 'SE contains Murcia', 'SE has contents Murcia', 'Congress contains SP', 'Congress has contents SP', 'Bumi contains Guns', 'Bumi has contents Guns', 'Bernstein contains Albany', 'Bernstein has contents Albany', 'Mitchell contains Dakar', 'Mitchell has contents Dakar', 'Alien contains ASCII', 'Alien has contents ASCII', 'Toppen contains Maynard', 'Toppen has contents Maynard', 'Blant contains Agnes', 'Blant has contents Agnes', 'Hispania contains UCB', 'Hispania has contents UCB', 'Ebert contains Schultz', 'Ebert has contents Schultz', 'Mario contains Hook', 'Mario has contents Hook', 'Santo contains White', 'Santo has contents White', 'Chronicle contains Eleanor', 'Chronicle has contents Eleanor', 'Parc contains Carrier', 'Parc has contents Carrier', 'Seat contains Padre', 'Seat has contents Padre', 'Hoy contains Oko', 'Hoy has contents Oko', 'Bears contains Carpenter', 'Bears has contents Carpenter', 'Coral contains Mato', 'Coral has contents Mato', 'Ashes contains Remo', 'Ashes has contents Remo', 'Mondadori contains Trial', 'Mondadori has contents Trial', 'Rooms contains Rock', 'Rooms has contents Rock', 'Lynn contains Yüksek', 'Lynn has contents Yüksek', 'Monaco contains Vas', 'Monaco has contents Vas', 'Queste contains Pack', 'Queste has contents Pack', 'Genel contains Midland', 'Genel has contents Midland', 'Cham contains Chef', 'Cham has contents Chef', 'Più contains Oko', 'Più has contents Oko', 'Macau contains Concilio', 'Macau has contents Concilio', 'Jeunesse contains Linares', 'Jeunesse has contents Linares', 'Mercury contains Jay', 'Mercury has contents Jay', 'Fantasia contains Payne', 'Fantasia has contents Payne', 'Lowry contains FN', 'Lowry has contents FN', 'Nelle contains Pizza', 'Nelle has contents Pizza', 'Enemy contains Monat', 'Enemy has contents Monat', 'Neu contains Rincón', 'Neu has contents Rincón', 'Campione contains Gandhi', 'Campione has contents Gandhi', 'Carnival contains WBC', 'Carnival has contents WBC', 'Yukon contains Grupa', 'Yukon has contents Grupa', 'Peace contains Fury', 'Peace has contents Fury', 'Ken contains Advance', 'Ken has contents Advance', 'Transvaal contains Brunnen', 'Transvaal has contents Brunnen', 'Tudor contains Amigos', 'Tudor has contents Amigos', 'Prema contains Trinity', 'Prema has contents Trinity', 'Az contains Ex', 'Az has contents Ex', 'oed contains Fox', 'oed has contents Fox', 'Noche contains Natal', 'Noche has contents Natal', 'Ludovic contains RTL', 'Ludovic has contents RTL', 'Schröder contains Liga', 'Schröder has contents Liga', 'Waves contains Sinh', 'Waves has contents Sinh', 'Hector contains Carthage', 'Hector has contents Carthage', 'Llwyd contains María', 'Llwyd has contents María', 'Lego contains tu', 'Lego has contents tu', 'Heikki contains UDP', 'Heikki has contents UDP', 'DSM contains Jake', 'DSM has contents Jake', 'Sendai contains Vivian', 'Sendai has contents Vivian', 'MSK contains sy', 'MSK has contents sy', 'Bismarck contains Mobile', 'Bismarck has contents Mobile', 'Basse contains Andere', 'Basse has contents Andere', 'Marburg contains Grammar', 'Marburg has contents Grammar', 'Radar contains Doom', 'Radar has contents Doom', 'Sunrise contains Catalina', 'Sunrise has contents Catalina', 'Darling contains SBS', 'Darling has contents SBS', 'Flesh contains asa', 'Flesh has contents asa', 'Ny contains Randolph', 'Ny has contents Randolph', 'Edouard contains Richter', 'Edouard has contents Richter', 'Peterborough contains Room', 'Peterborough has contents Room', 'Ivy contains Vis', 'Ivy has contents Vis', 'Medi contains Bengal', 'Medi has contents Bengal', 'Rocca contains Flynn', 'Rocca has contents Flynn', 'Elisabeth contains Weston', 'Elisabeth has contents Weston', 'Johansen contains Segura', 'Johansen has contents Segura', 'Morro contains Gets', 'Morro has contents Gets', 'Shakespeare contains Snow', 'Shakespeare has contents Snow', 'Regular contains NRW', 'Regular has contents NRW', 'Graves contains WRC', 'Graves has contents WRC', 'Cathedral contains Perry', 'Cathedral has contents Perry', 'Metro contains Ribera', 'Metro has contents Ribera', 'Sally contains Batavia', 'Sally has contents Batavia', 'Raum contains Sawyer', 'Raum has contents Sawyer', 'Checa contains Criminal', 'Checa has contents Criminal', 'Marlene contains Delaware', 'Marlene has contents Delaware', 'Gauss contains Abby', 'Gauss has contents Abby', 'Goebbels contains Hatch', 'Goebbels has contents Hatch', 'Mesa contains Cáceres', 'Mesa has contents Cáceres', 'MBC contains Namibia', 'MBC has contents Namibia', 'lt contains Ferdinand', 'lt has contents Ferdinand', 'Giant contains Hastings', 'Giant has contents Hastings', 'Fantasy contains Helen', 'Fantasy has contents Helen', 'Juin contains Collegiate', 'Juin has contents Collegiate', 'Kanal contains Milne', 'Kanal has contents Milne', 'Benoit contains BRT', 'Benoit has contents BRT', 'Saints contains Sá', 'Saints has contents Sá', 'Yorker contains U', 'Yorker has contents U', 'Jill contains Toulon', 'Jill has contents Toulon', 'Illinois contains Elvis', 'Illinois has contents Elvis', 'Quattro contains Damon', 'Quattro has contents Damon', 'Solomon contains Socorro', 'Solomon has contents Socorro', 'Jerusalén contains Newell', 'Jerusalén has contents Newell', 'Ent contains Car', 'Ent has contents Car', 'Gender contains Quincy', 'Gender has contents Quincy', 'Juárez contains Bengal', 'Juárez has contents Bengal', 'Richards contains Erfurt', 'Richards has contents Erfurt', 'Bahia contains Gattung', 'Bahia has contents Gattung', 'Garnier contains Chiara', 'Garnier has contents Chiara', 'Rec contains Davida', 'Rec has contents Davida', 'Amadeus contains Valencia', 'Amadeus has contents Valencia', 'Parra contains Maja', 'Parra has contents Maja', 'Cabo contains Tarragona', 'Cabo has contents Tarragona', 'Tun contains Cinta', 'Tun has contents Cinta', 'Piemonte contains Sergei', 'Piemonte has contents Sergei', 'Siegen contains Bangalore', 'Siegen has contents Bangalore', 'Rudy contains Lorenz', 'Rudy has contents Lorenz', 'Poland contains Eye', 'Poland has contents Eye', 'Dresdner contains Díaz', 'Dresdner has contents Díaz', 'Barcellona contains Pamplona', 'Barcellona has contents Pamplona', 'Raja contains Alicante', 'Raja has contents Alicante', 'Guide contains ba', 'Guide has contents ba', 'Ketika contains Mouse', 'Ketika has contents Mouse', 'Ciudad contains Welle', 'Ciudad has contents Welle', 'Fiat contains Beast', 'Fiat has contents Beast', 'Shark contains CDATA', 'Shark has contents CDATA', 'Roberts contains WM', 'Roberts has contents WM', 'Bender contains Hull', 'Bender has contents Hull', 'Abend contains Lauren', 'Abend has contents Lauren', 'GND contains Gers', 'GND has contents Gers', 'Brief contains Archie', 'Brief has contents Archie', 'Junkers contains Hunter', 'Junkers has contents Hunter', 'Edison contains Angels', 'Edison has contents Angels', 'Denna contains Lance', 'Denna has contents Lance', 'Panny contains Jet', 'Panny has contents Jet', 'CAS contains Trent', 'CAS has contents Trent', 'Issue contains Pleasure', 'Issue has contents Pleasure', 'Katz contains SVT', 'Katz has contents SVT', 'Elvis contains Fermi', 'Elvis has contents Fermi', 'Poitou contains Sweden', 'Poitou has contents Sweden', 'Ragnar contains Taylor', 'Ragnar has contents Taylor', 'Decock contains Chez', 'Decock has contents Chez', 'Harvest contains Lennon', 'Harvest has contents Lennon', 'Saw contains Junie', 'Saw has contents Junie', 'Daimler contains Haji', 'Daimler has contents Haji', 'Egipte contains Yahoo', 'Egipte has contents Yahoo', 'Khu contains Kort', 'Khu has contents Kort', 'Euler contains Stal', 'Euler has contents Stal', 'Lodge contains Berlin', 'Lodge has contents Berlin', 'Lau contains FC', 'Lau has contents FC', 'Sanskrit contains Dad', 'Sanskrit has contents Dad', 'United contains Kenia', 'United has contents Kenia', 'Burma contains CCD', 'Burma has contents CCD', 'Sánchez contains Riau', 'Sánchez has contents Riau', 'SF contains Ky', 'SF has contents Ky', 'Hollanda contains Billie', 'Hollanda has contents Billie', 'PCI contains Arms', 'PCI has contents Arms', 'Cars contains Elbe', 'Cars has contents Elbe', 'Dari contains Paraíso', 'Dari has contents Paraíso', 'Raden contains TD', 'Raden has contents TD', 'Vita contains Luther', 'Vita has contents Luther', 'Damascus contains Roi', 'Damascus has contents Roi', 'Hornet contains Pole', 'Hornet has contents Pole', 'Excel contains Ulysses', 'Excel has contents Ulysses', 'Aquino contains Page', 'Aquino has contents Page', 'Gymnasium contains Baron', 'Gymnasium has contents Baron', 'Rochelle contains Libia', 'Rochelle has contents Libia', 'Albania contains Cuenca', 'Albania has contents Cuenca', 'Au contains Kálmán', 'Au has contents Kálmán', 'Georg contains ET', 'Georg has contents ET', 'Springs contains Mainstream', 'Springs has contents Mainstream', 'Guardian contains Pinto', 'Guardian has contents Pinto', 'Fiction contains Agency', 'Fiction has contents Agency', 'Northeast contains Goodman', 'Northeast has contents Goodman', 'Complete contains Mata', 'Complete has contents Mata', 'Acre contains Mineral', 'Acre has contents Mineral', 'Isis contains Carvalho', 'Isis has contents Carvalho', 'Bursa contains Barth', 'Bursa has contents Barth', 'Emden contains Figaro', 'Emden has contents Figaro', 'Viborg contains Trung', 'Viborg has contents Trung', 'Melvin contains Sabha', 'Melvin has contents Sabha', 'Simpson contains Eleanor', 'Simpson has contents Eleanor', 'Aner contains Guimarães', 'Aner has contents Guimarães', 'Bogor contains Disneyland', 'Bogor has contents Disneyland', 'Clint contains Leicester', 'Clint has contents Leicester', 'Amber contains Wes', 'Amber has contents Wes', 'Dictionary contains Gesù', 'Dictionary has contents Gesù', 'Altar contains Medi', 'Altar has contents Medi', 'Braunschweig contains Cinq', 'Braunschweig has contents Cinq', 'Coro contains Silla', 'Coro has contents Silla', 'Hélène contains JR', 'Hélène has contents JR', 'Delaware contains Bryant', 'Delaware has contents Bryant', 'Tracy contains Klaus', 'Tracy has contents Klaus', 'fann contains Kleiner', 'fann has contents Kleiner', 'Murcia contains Fort', 'Murcia has contents Fort', 'Oltre contains Ex', 'Oltre has contents Ex', 'Zadar contains City', 'Zadar has contents City', 'Counter contains Márquez', 'Counter has contents Márquez', 'Storia contains Yer', 'Storia has contents Yer', 'AN contains Bernard', 'AN has contents Bernard', 'Shoot contains Diaz', 'Shoot has contents Diaz', 'Springfield contains Madre', 'Springfield has contents Madre', 'Durango contains Asturias', 'Durango has contents Asturias', 'Casino contains FF', 'Casino has contents FF', 'Racing contains Ribera', 'Racing has contents Ribera', 'Opus contains Dis', 'Opus has contents Dis', 'Tokio contains Lorentz', 'Tokio has contents Lorentz', 'Rally contains SM', 'Rally has contents SM', 'Gloria contains Sempre', 'Gloria has contents Sempre', 'Allison contains Portsmouth', 'Allison has contents Portsmouth', 'Speyer contains Irena', 'Speyer has contents Irena', 'IX contains Oaks', 'IX has contents Oaks', 'Stand contains Fighting', 'Stand has contents Fighting', 'Bees contains Bilbao', 'Bees has contents Bilbao', 'Faust contains Sharon', 'Faust has contents Sharon', 'Find contains Allium', 'Find has contents Allium', 'Malang contains Haiti', 'Malang has contents Haiti', 'Vi contains Niño', 'Vi has contents Niño', 'Carl contains Isabella', 'Carl has contents Isabella', 'Frankie contains Olsson', 'Frankie has contents Olsson', 'Rivière contains Revue', 'Rivière has contents Revue', 'Federal contains Pure', 'Federal has contents Pure', 'Ola contains Patrol', 'Ola has contents Patrol', 'Kerk contains Potter', 'Kerk has contents Potter', 'Cumberland contains Siege', 'Cumberland has contents Siege', 'Trap contains Galiza', 'Trap has contents Galiza', 'Snyder contains Lucky', 'Snyder has contents Lucky', 'Goa contains Pfeiffer', 'Goa has contents Pfeiffer', 'Guthrie contains Irvine', 'Guthrie has contents Irvine', 'Anita contains FX', 'Anita has contents FX', 'Loan contains Energy', 'Loan has contents Energy', 'Punjabi contains Joanne', 'Punjabi has contents Joanne', 'Title contains Erik', 'Title has contents Erik', 'TBS contains RTL', 'TBS has contents RTL', 'Beck contains EM', 'Beck has contents EM', 'Elsevier contains Raum', 'Elsevier has contents Raum', 'Rep contains VL', 'Rep has contents VL', 'Stuart contains Sabina', 'Stuart has contents Sabina', 'Tripoli contains Krupp', 'Tripoli has contents Krupp', 'OM contains October', 'OM has contents October', 'Grau contains Davidson', 'Grau has contents Davidson', 'Kuala contains Allah', 'Kuala has contents Allah', 'Campus contains Larva', 'Campus has contents Larva', 'Esse contains Khu', 'Esse has contents Khu', 'Rootsi contains Idol', 'Rootsi has contents Idol', 'Pieter contains Kálmán', 'Pieter has contents Kálmán', 'Kita contains Lahore', 'Kita has contents Lahore', 'Gert contains Diaz', 'Gert has contents Diaz', 'Heide contains Dorset', 'Heide has contents Dorset', 'Hancock contains Stay', 'Hancock has contents Stay', 'soo contains Sit', 'soo has contents Sit', 'Esta contains Wayne', 'Esta has contents Wayne', 'Energy contains Flag', 'Energy has contents Flag', 'Mato contains Ferns', 'Mato has contents Ferns', 'Grenoble contains Jaguar', 'Grenoble has contents Jaguar', 'Velvet contains Brenda', 'Velvet has contents Brenda', 'IUE contains Mina', 'IUE has contents Mina', 'Hessen contains Harding', 'Hessen has contents Harding', 'Fury contains Giants', 'Fury has contents Giants', 'Bloom contains Buta', 'Bloom has contents Buta', 'Medina contains Faye', 'Medina has contents Faye', 'Invasion contains Generation', 'Invasion has contents Generation', 'Burke contains Schottland', 'Burke has contents Schottland', 'Drama contains USD', 'Drama has contents USD', 'Master contains Eu', 'Master has contents Eu', 'Chihuahua contains Italie', 'Chihuahua has contents Italie', 'EP contains Bora', 'EP has contents Bora', 'Worcester contains Hus', 'Worcester has contents Hus', 'Albert contains epi', 'Albert has contents epi', 'Ibiza contains Wilfried', 'Ibiza has contents Wilfried', 'Câmara contains Fach', 'Câmara has contents Fach', 'Eleanor contains Inge', 'Eleanor has contents Inge', 'Schaus contains Elite', 'Schaus has contents Elite', 'Chaos contains Portland', 'Chaos has contents Portland', 'Alice contains Eliza', 'Alice has contents Eliza', 'Franklin contains Weir', 'Franklin has contents Weir', 'Colombo contains Antoine', 'Colombo has contents Antoine', 'Graf contains Universitas', 'Graf has contents Universitas', 'Darwin contains Collins', 'Darwin has contents Collins', 'Nisan contains Condor', 'Nisan has contents Condor', 'Hartman contains Gospel', 'Hartman has contents Gospel', 'Lur contains MS', 'Lur has contents MS', 'Fiesta contains Ramsey', 'Fiesta has contents Ramsey', 'Alexandra contains Katz', 'Alexandra has contents Katz', 'Mérida contains Saussure', 'Mérida has contents Saussure', 'Shire contains BRT', 'Shire has contents BRT', 'Anthology contains Carioca', 'Anthology has contents Carioca', 'Monroe contains Note', 'Monroe has contents Note', 'Hell contains Ultra', 'Hell has contents Ultra', 'Bandet contains Né', 'Bandet has contents Né', 'Bertrand contains Fidel', 'Bertrand has contents Fidel', 'Drum contains Einer', 'Drum has contents Einer', 'IBM contains Everybody', 'IBM has contents Everybody', 'Christchurch contains Pizza', 'Christchurch has contents Pizza', 'Broadcast contains Koska', 'Broadcast has contents Koska', 'Clay contains Ariane', 'Clay has contents Ariane', 'Sound contains Exil', 'Sound has contents Exil', 'Liga contains Holy', 'Liga has contents Holy', 'Vatican contains Haar', 'Vatican has contents Haar', 'Libro contains Ringo', 'Libro has contents Ringo', 'Vanderbilt contains Theresa', 'Vanderbilt has contents Theresa', 'Austrian contains Zone', 'Austrian has contents Zone', 'Varese contains Benth', 'Varese has contents Benth', 'Ninja contains Baja', 'Ninja has contents Baja', 'Lafayette contains Ruska', 'Lafayette has contents Ruska', 'Catalina contains Seymour', 'Catalina has contents Seymour', 'Companion contains ECW', 'Companion has contents ECW', 'Marshal contains Bez', 'Marshal has contents Bez', 'Molina contains Ortes', 'Molina has contents Ortes', 'Dow contains Jessica', 'Dow has contents Jessica', 'Platte contains Zoom', 'Platte has contents Zoom', 'Mulder contains Telephone', 'Mulder has contents Telephone', 'Deutschland contains A', 'Deutschland has contents A', 'Niko contains Barra', 'Niko has contents Barra', 'Romans contains Lori', 'Romans has contents Lori', 'Salman contains Reeves', 'Salman has contents Reeves', 'Release contains Wagen', 'Release has contents Wagen', 'Daughter contains Sound', 'Daughter has contents Sound', 'Kendall contains JR', 'Kendall has contents JR', 'PCR contains Trail', 'PCR has contents Trail', 'Lorena contains Concordia', 'Lorena has contents Concordia', 'Twain contains Windows', 'Twain has contents Windows', 'Head contains Poco', 'Head has contents Poco', 'Porta contains Suba', 'Porta has contents Suba', 'Yunan contains Bartlett', 'Yunan has contents Bartlett', 'MGM contains Agent', 'MGM has contents Agent', 'Geld contains Compton', 'Geld has contents Compton', 'Jesus contains Marshal', 'Jesus has contents Marshal', 'Potter contains RF', 'Potter has contents RF', 'Crown contains Lorenzo', 'Crown has contents Lorenzo', 'Adi contains Cherry', 'Adi has contents Cherry', 'Sinn contains Namibia', 'Sinn has contents Namibia', 'Vacelet contains FAA', 'Vacelet has contents FAA', 'Madsen contains Kantonen', 'Madsen has contents Kantonen', 'Robinson contains Dunkerque', 'Robinson has contents Dunkerque', 'Fayette contains Demon', 'Fayette has contents Demon', 'Grant contains Bears', 'Grant has contents Bears', 'Julian contains Paradise', 'Julian has contents Paradise', 'Carrillo contains Sidney', 'Carrillo has contents Sidney', 'Aus contains ACM', 'Aus has contents ACM', 'Kongo contains Guanajuato', 'Kongo has contents Guanajuato', 'Gaur contains Bees', 'Gaur has contents Bees', 'Shining contains Edwards', 'Shining has contents Edwards', 'Douglas contains Friesland', 'Douglas has contents Friesland', 'Continental contains Palencia', 'Continental has contents Palencia', 'West contains Baldwin', 'West has contents Baldwin', 'Haas contains Plans', 'Haas has contents Plans', 'Lynch contains Hurley', 'Lynch has contents Hurley', 'Cornwall contains Schools', 'Cornwall has contents Schools', 'Jalan contains IX', 'Jalan has contents IX', 'Dhaka contains Alessandro', 'Dhaka has contents Alessandro', 'Reading contains Bürger', 'Reading has contents Bürger', 'Chancellor contains Collegiate', 'Chancellor has contents Collegiate', 'Dakota contains Move', 'Dakota has contents Move', 'Arles contains KM', 'Arles has contents KM', 'Stab contains Petra', 'Stab has contents Petra', 'Golden contains Nightmare', 'Golden has contents Nightmare', 'Jakiel contains Krüger', 'Jakiel has contents Krüger', 'Wade contains Rota', 'Wade has contents Rota', 'Ebro contains Face', 'Ebro has contents Face', 'Dorothea contains Martínez', 'Dorothea has contents Martínez', 'Urban contains Quinta', 'Urban has contents Quinta', 'HF contains Damit', 'HF has contents Damit', 'Spectrum contains Oper', 'Spectrum has contents Oper', 'Cass contains Frédéric', 'Cass has contents Frédéric', 'Riva contains Akdeniz', 'Riva has contents Akdeniz', 'Winston contains UC', 'Winston has contents UC', 'Punk contains Plan', 'Punk has contents Plan', 'SMS contains Sailor', 'SMS has contents Sailor', 'Paglinawan contains Words', 'Paglinawan has contents Words', 'McKay contains Waterloo', 'McKay has contents Waterloo', 'Harmony contains Peters', 'Harmony has contents Peters', 'Chantal contains Boyle', 'Chantal has contents Boyle', 'Taurus contains Remixes', 'Taurus has contents Remixes', 'Crosby contains Vienna', 'Crosby has contents Vienna', 'GSC contains Talent', 'GSC has contents Talent', 'Moonlight contains Switch', 'Moonlight has contents Switch', 'Stéphane contains Greatest', 'Stéphane has contents Greatest', 'Kader contains Shannon', 'Kader has contents Shannon', 'Ferro contains Irena', 'Ferro has contents Irena', 'Anglo contains Arabic', 'Anglo has contents Arabic', 'Ubuntu contains Membre', 'Ubuntu has contents Membre', 'GMT contains Condor', 'GMT has contents Condor', 'Quintana contains Extra', 'Quintana has contents Extra', 'Lublin contains Bil', 'Lublin has contents Bil', 'Nagar contains Menor', 'Nagar has contents Menor', 'Cats contains Sprint', 'Cats has contents Sprint', 'Beyoncé contains Free', 'Beyoncé has contents Free', 'Mendoza contains Ut', 'Mendoza has contents Ut', 'Roja contains Transfer', 'Roja has contents Transfer', 'Beckett contains Batavia', 'Beckett has contents Batavia', 'Toulouse contains Das', 'Toulouse has contents Das', 'Summit contains Lowe', 'Summit has contents Lowe', 'Champion contains Flat', 'Champion has contents Flat', 'Twins contains Barry', 'Twins has contents Barry', 'Halo contains Latreille', 'Halo has contents Latreille', 'Deborah contains Bala', 'Deborah has contents Bala', 'Commonwealth contains Büyük', 'Commonwealth has contents Büyük', 'Dublin contains Mittel', 'Dublin has contents Mittel', 'Apocalypse contains König', 'Apocalypse has contents König', 'Drake contains Yates', 'Drake has contents Yates', 'Royal contains AFC', 'Royal has contents AFC', 'Quick contains Matrix', 'Quick has contents Matrix', 'Remote contains Ottawa', 'Remote has contents Ottawa', 'Player contains Tha', 'Player has contents Tha', 'Wiener contains Shane', 'Wiener has contents Shane', 'Glasgow contains PCR', 'Glasgow has contents PCR', 'Hansen contains Jazz', 'Hansen has contents Jazz', 'Ortes contains Bismarck', 'Ortes has contents Bismarck', 'Mozilla contains Carlton', 'Mozilla has contents Carlton', 'Mur contains George', 'Mur has contents George', 'Iberia contains Urban', 'Iberia has contents Urban', 'Vir contains Samba', 'Vir has contents Samba', 'Prato contains Molina', 'Prato has contents Molina', 'Ferris contains Rivas', 'Ferris has contents Rivas', 'Steiner contains Braga', 'Steiner has contents Braga', 'Sala contains Olav', 'Sala has contents Olav', 'Connection contains Stephan', 'Connection has contents Stephan', 'Hastings contains McKamey', 'Hastings has contents McKamey', 'Siegfried contains Burke', 'Siegfried has contents Burke', 'Kale contains Luik', 'Kale has contents Luik', 'Fourier contains Arena', 'Fourier has contents Arena', 'Pada contains Oviedo', 'Pada has contents Oviedo', 'Roberto contains Syracuse', 'Roberto has contents Syracuse', 'Struggle contains Piazza', 'Struggle has contents Piazza', 'Elijah contains Slag', 'Elijah has contents Slag', 'Rote contains Delft', 'Rote has contents Delft', 'Christi contains Romagna', 'Christi has contents Romagna', 'Berge contains Partner', 'Berge has contents Partner', 'Wimbledon contains Benton', 'Wimbledon has contents Benton', 'Palm contains Quiet', 'Palm has contents Quiet', 'Nor contains Dexter', 'Nor has contents Dexter', 'Maria contains Scientist', 'Maria has contents Scientist', 'Stadio contains Maestro', 'Stadio has contents Maestro', 'Belo contains Busch', 'Belo has contents Busch', 'Boga contains Napoca', 'Boga has contents Napoca', 'NSW contains Ball', 'NSW has contents Ball', 'Roller contains Lindsay', 'Roller has contents Lindsay', 'León contains Hector', 'León has contents Hector', 'Rollen contains IBM', 'Rollen has contents IBM', 'Agama contains Dur', 'Agama has contents Dur', 'Seminary contains Tanner', 'Seminary has contents Tanner', 'Ensemble contains pr', 'Ensemble has contents pr', 'Blade contains Ensemble', 'Blade has contents Ensemble', 'Cochrane contains Ester', 'Cochrane has contents Ester', 'Indiana contains Hoya', 'Indiana has contents Hoya', 'Carvalho contains Gauss', 'Carvalho has contents Gauss', 'René contains Heine', 'René has contents Heine', 'Mask contains Madeira', 'Mask has contents Madeira', 'Dacia contains Golf', 'Dacia has contents Golf', 'Cea contains Pike', 'Cea has contents Pike', 'Bulgaria contains Helga', 'Bulgaria has contents Helga', 'Rocket contains Down', 'Rocket has contents Down', 'Pro contains Firefox', 'Pro has contents Firefox', 'Portugal contains Novel', 'Portugal has contents Novel', 'Blanco contains Addison', 'Blanco has contents Addison', 'Addison contains Loving', 'Addison has contents Loving', 'Como contains Desse', 'Como has contents Desse', 'Suomi contains Middlesex', 'Suomi has contents Middlesex', 'Alt contains Beth', 'Alt has contents Beth', 'Esther contains Angoulême', 'Esther has contents Angoulême', 'Sick contains Grimaldi', 'Sick has contents Grimaldi', 'Bowman contains Tobias', 'Bowman has contents Tobias', 'Wells contains Tat', 'Wells has contents Tat', 'NT contains Barry', 'NT has contents Barry', 'Titanic contains Regno', 'Titanic has contents Regno', 'Chamber contains CAD', 'Chamber has contents CAD', 'Satellite contains Machado', 'Satellite has contents Machado', 'Niels contains Reich', 'Niels has contents Reich', 'Borneo contains Choice', 'Borneo has contents Choice', 'Tigre contains Argentine', 'Tigre has contents Argentine', 'Vincent contains Huntington', 'Vincent has contents Huntington', 'Midlands contains Monk', 'Midlands has contents Monk', 'Siam contains Mines', 'Siam has contents Mines', 'Uit contains Weston', 'Uit has contents Weston', 'CDC contains CE', 'CDC has contents CE', 'Sawyer contains Sint', 'Sawyer has contents Sint', 'Ranking contains Lego', 'Ranking has contents Lego', 'Babylon contains Amour', 'Babylon has contents Amour', 'Côte contains Martinez', 'Côte has contents Martinez', 'IS contains Ida', 'IS has contents Ida', 'Frost contains Julie', 'Frost has contents Julie', 'Mariana contains KK', 'Mariana has contents KK', 'Baza contains Trouble', 'Baza has contents Trouble', 'Washington contains Partido', 'Washington has contents Partido', 'Giles contains Jess', 'Giles has contents Jess', 'Benton contains Keller', 'Benton has contents Keller', 'Balázs contains Österreich', 'Balázs has contents Österreich', 'Pays contains Britten', 'Pays has contents Britten', 'Alman contains Dacia', 'Alman has contents Dacia', 'Danube contains Pt', 'Danube has contents Pt', 'Troy contains Ekim', 'Troy has contents Ekim', 'Hip contains Cavendish', 'Hip has contents Cavendish', 'Face contains Durban', 'Face has contents Durban', 'Funk contains Garland', 'Funk has contents Garland', 'Hazel contains Check', 'Hazel has contents Check', 'Romawi contains Thor', 'Romawi has contents Thor', 'Straits contains Remixes', 'Straits has contents Remixes', 'Put contains Pierce', 'Put has contents Pierce', 'ATP contains Band', 'ATP has contents Band', 'Abucay contains Baird', 'Abucay has contents Baird', 'Spor contains Sia', 'Spor has contents Sia', 'Vijay contains Züge', 'Vijay has contents Züge', 'Géza contains Clyde', 'Géza has contents Clyde', 'Werk contains Underground', 'Werk has contents Underground', 'Jungen contains Luxemburg', 'Jungen has contents Luxemburg', 'Harri contains Sørensen', 'Harri has contents Sørensen', 'Wood contains Esse', 'Wood has contents Esse', 'XP contains Malay', 'XP has contents Malay', 'Henning contains Wish', 'Henning has contents Wish', 'Daniel contains Voyager', 'Daniel has contents Voyager', 'Lac contains Holocaust', 'Lac has contents Holocaust', 'Bon contains LP', 'Bon has contents LP', 'Hess contains Medley', 'Hess has contents Medley', 'Asunción contains Buena', 'Asunción has contents Buena', 'ba contains Ponte', 'ba has contents Ponte', 'Roll contains Ávila', 'Roll has contents Ávila', 'Nada contains Cullen', 'Nada has contents Cullen', 'Saale contains Les', 'Saale has contents Les', 'Minden contains Gerard', 'Minden has contents Gerard', 'Imperial contains ISS', 'Imperial has contents ISS', 'Moreau contains Trees', 'Moreau has contents Trees', 'Provence contains Franjo', 'Provence has contents Franjo', 'Archer contains Vas', 'Archer has contents Vas', 'JNA contains Pioneer', 'JNA has contents Pioneer', 'Tarragona contains Isles', 'Tarragona has contents Isles', 'Pirates contains Qu', 'Pirates has contents Qu', 'Mat contains Cesar', 'Mat has contents Cesar', 'Berlin contains Satan', 'Berlin has contents Satan', 'Vance contains EC', 'Vance has contents EC', 'Banja contains Delft', 'Banja has contents Delft', 'Champ contains Ter', 'Champ has contents Ter', 'Trieste contains Burma', 'Trieste has contents Burma', 'Pi contains Freie', 'Pi has contents Freie', 'Cisco contains Bride', 'Cisco has contents Bride', 'Cologne contains Espagne', 'Cologne has contents Espagne', 'Banks contains Thành', 'Banks has contents Thành', 'MotoGP contains Bunker', 'MotoGP has contents Bunker', 'Spiegel contains Italiana', 'Spiegel has contents Italiana', 'Dara contains Bad', 'Dara has contents Bad', 'Platinum contains Kamera', 'Platinum has contents Kamera', 'Calvin contains Beatrice', 'Calvin has contents Beatrice', 'Lena contains Henri', 'Lena has contents Henri', 'Voltaire contains Mound', 'Voltaire has contents Mound', 'Polar contains Space', 'Polar has contents Space', 'Depot contains USSR', 'Depot has contents USSR', 'Petit contains Seigneur', 'Petit has contents Seigneur', 'Einstein contains Comte', 'Einstein has contents Comte', 'Bay contains Díaz', 'Bay has contents Díaz', 'Skin contains Wallace', 'Skin has contents Wallace', 'Donna contains Nepal', 'Donna has contents Nepal', 'Pasteur contains Platnick', 'Pasteur has contents Platnick', 'Seul contains FF', 'Seul has contents FF', 'Closer contains Mickey', 'Closer has contents Mickey', 'KPD contains Railroad', 'KPD has contents Railroad', 'Palma contains Samo', 'Palma has contents Samo', 'Accademia contains Bach', 'Accademia has contents Bach', 'Leafs contains Batman', 'Leafs has contents Batman', 'Carla contains Tucson', 'Carla has contents Tucson', 'Lilla contains Driver', 'Lilla has contents Driver', 'Gamble contains Rat', 'Gamble has contents Rat', 'Alec contains Hastings', 'Alec has contents Hastings', 'Medvedev contains Providence', 'Medvedev has contents Providence', 'Nino contains Ja', 'Nino has contents Ja', 'Willard contains Finale', 'Willard has contents Finale', 'Seoul contains Rex', 'Seoul has contents Rex', 'Bala contains Ace', 'Bala has contents Ace', 'Grund contains Saskatchewan', 'Grund has contents Saskatchewan', 'Indre contains Sigurd', 'Indre has contents Sigurd', 'Poslední contains Potsdam', 'Poslední has contents Potsdam', 'Switch contains Irvine', 'Switch has contents Irvine', 'Charlie contains Coleman', 'Charlie has contents Coleman', 'Warren contains Phil', 'Warren has contents Phil', 'Irvine contains Sul', 'Irvine has contents Sul', 'Quinta contains Danh', 'Quinta has contents Danh', 'Up contains Little', 'Up has contents Little', 'Heft contains Basso', 'Heft has contents Basso', 'Cap contains Gamma', 'Cap has contents Gamma', 'NN contains Dana', 'NN has contents Dana', 'Romas contains Harbor', 'Romas has contents Harbor', 'Benedict contains Bears', 'Benedict has contents Bears', 'Zurich contains Bristol', 'Zurich has contents Bristol', 'Capitaine contains Rusi', 'Capitaine has contents Rusi', 'Camilla contains Maxime', 'Camilla has contents Maxime', 'Theatre contains Jagger', 'Theatre has contents Jagger', 'Eo contains Swan', 'Eo has contents Swan', 'Kelley contains Lord', 'Kelley has contents Lord', 'Gibraltar contains SF', 'Gibraltar has contents SF', 'Alabama contains Coffee', 'Alabama has contents Coffee', 'Revue contains Seele', 'Revue has contents Seele', 'Duran contains Victorian', 'Duran has contents Victorian', 'Guía contains Blanchard', 'Guía has contents Blanchard', 'Wanda contains Happy', 'Wanda has contents Happy', 'Impact contains Rae', 'Impact has contents Rae', 'Vene contains Bitte', 'Vene has contents Bitte', 'KBS contains Beyaz', 'KBS has contents Beyaz', 'Emery contains Hügel', 'Emery has contents Hügel', 'Bey contains Free', 'Bey has contents Free', 'Studi contains Cynthia', 'Studi has contents Cynthia', 'Coll contains Tông', 'Coll has contents Tông', 'Jeffries contains Bouchet', 'Jeffries has contents Bouchet', 'Forst contains Barbosa', 'Forst has contents Barbosa', 'Casablanca contains Assembly', 'Casablanca has contents Assembly', 'Kati contains Stranger', 'Kati has contents Stranger', 'Kort contains Valls', 'Kort has contents Valls', 'Teluk contains Fresno', 'Teluk has contents Fresno', 'Farrell contains NME', 'Farrell has contents NME', 'Ehren contains Rover', 'Ehren has contents Rover', 'Tampere contains DS', 'Tampere has contents DS', 'Kepler contains Ja', 'Kepler has contents Ja', 'Madeleine contains Wenn', 'Madeleine has contents Wenn', 'Milan contains Boone', 'Milan has contents Boone', 'Translation contains Indonesia', 'Translation has contents Indonesia', 'Koch contains Generation', 'Koch has contents Generation', 'Butler contains Avon', 'Butler has contents Avon', 'Neuchâtel contains Steen', 'Neuchâtel has contents Steen', 'Wittenberg contains Kobayashi', 'Wittenberg has contents Kobayashi', 'Cidade contains Ng', 'Cidade has contents Ng', 'Murphy contains Cine', 'Murphy has contents Cine', 'su contains Massacre', 'su has contents Massacre', 'Ses contains Sprint', 'Ses has contents Sprint', 'Curie contains Aziz', 'Curie has contents Aziz', 'Ned contains Hang', 'Ned has contents Hang', 'Zealand contains Tiempo', 'Zealand has contents Tiempo', 'Bennett contains Senat', 'Bennett has contents Senat', 'Porsche contains Dunia', 'Porsche has contents Dunia', 'Niño contains Independencia', 'Niño has contents Independencia', 'Barth contains Genocide', 'Barth has contents Genocide', 'Robot contains Atlanta', 'Robot has contents Atlanta', 'Sinh contains Nash', 'Sinh has contents Nash', 'Gillespie contains Lys', 'Gillespie has contents Lys', 'Titan contains Valley', 'Titan has contents Valley', 'Tierra contains Ingles', 'Tierra has contents Ingles', 'ID contains Kensington', 'ID has contents Kensington', 'WWF contains Remo', 'WWF has contents Remo', 'Azur enthält Henning', 'Reason enthält Rocca', 'Luke enthält Cynthia', 'Trees enthält Tech', 'Morris enthält ag', 'Monate enthält Odessa', 'Norway enthält Rhapsody', 'Han enthält Carlos', 'Cuban enthält Maka', 'Melbourne enthält Larry', 'Rooma enthält Neki', 'Ad enthält Brisbane', 'Christ enthält Each', 'PSA enthält Guerre', 'Omer enthält Alta', 'THE enthält Margareta', 'Hand enthält Vacelet', 'Urgell enthält Dol', 'Liv enthält Kirche', 'Tempo enthält Bambino', 'Abel enthält Royal', 'Gegen enthält ABS', 'Stil enthält Madison', 'CDP enthält Ostrava', 'Izrael enthält Díaz', 'Genesis enthält Gina', 'Eylül enthält Viru', 'Vivaldi enthält Wellington', 'Saussure enthält Icarus', 'Muir enthält Yunan', 'Alonso enthält Slot', 'Largo enthält Grammar', 'Phi enthält Sân', 'While enthält NN', 'Rain enthält Haag', 'Canary enthält Cash', 'Arms enthält Basso', 'Bil enthält Gibson', 'Cause enthält Nota', 'Hitler enthält Hour', 'Kimberly enthält Loch', 'Swan enthält Atatürk', 'Parker enthält México', 'WDR enthält Algarve', 'Gama enthält Generation', 'Pam enthält Rhein', 'IM enthält Brod', 'Comte enthält Dem', 'Stella enthält Nigel', 'Speedway enthält Holloway', 'Linda enthält IX', 'Dok enthält Stal', 'Earth enthält Sato', 'Ele enthält FX', 'Limited enthält Vries', 'GM enthält Fusion', 'Isola enthält Terminal', 'Kahn enthält Maa', 'Slavic enthält Bayreuth', 'Subway enthält Catharina', 'Stage enthält Naomi', 'Rady enthält Drew', 'Tanner enthält Path', 'Newport enthält Hector', 'Holt enthält Colle', 'Ion enthält Inspector', 'Amigos enthält WDR', 'Bruges enthält Nizza', 'Astra enthält Camino', 'KK enthält Noche', 'Alliance enthält Patria', 'Dana enthält Rees', 'Albin enthält Tucumán', 'Nissan enthält Copper', 'Aberdeen enthält KBS', 'Jenna enthält Sporting', 'Hamlet enthält Irvine', 'Latin enthält Lilly', 'ESPN enthält Amalia', 'Florence enthält Güney', 'Philippe enthält Kepler', 'Phelps enthält Bala', 'Zoom enthält Questa', 'Isto enthält Roberts', 'Sporting enthält Doom', 'Star enthält Halk', 'Padre enthält Duo', 'Perth enthält Dub', 'Cecil enthält Emily', 'Katrina enthält Walls', 'Lincoln enthält Claude', 'Xuân enthält Court', 'Pet enthält Down', 'Hora enthält Crash', 'Heads enthält Gibson', 'Coimbra enthält Kappa', 'Magister enthält Cruz', 'Kristen enthält Nino', 'Benson enthält Hubbard', 'ES enthält Georgetown', 'Nirvana permits Host']\n"
     ]
    }
   ],
   "source": [
    "print(train_dict['sample'][:1901])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86e4d3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Azur lagert Henning',\n",
       " 'Reason lagert Rocca',\n",
       " 'Luke lagert Cynthia',\n",
       " 'Trees lagert Tech',\n",
       " 'Morris lagert ag',\n",
       " 'Monate lagert Odessa',\n",
       " 'Norway lagert Rhapsody',\n",
       " 'Han lagert Carlos',\n",
       " 'Cuban lagert Maka',\n",
       " 'Melbourne lagert Larry',\n",
       " 'Rooma lagert Neki',\n",
       " 'Ad lagert Brisbane',\n",
       " 'Christ lagert Each',\n",
       " 'PSA lagert Guerre',\n",
       " 'Omer lagert Alta',\n",
       " 'THE lagert Margareta',\n",
       " 'Hand lagert Vacelet',\n",
       " 'Urgell lagert Dol',\n",
       " 'Liv lagert Kirche',\n",
       " 'Tempo lagert Bambino',\n",
       " 'Abel lagert Royal',\n",
       " 'Gegen lagert ABS',\n",
       " 'Stil lagert Madison',\n",
       " 'CDP lagert Ostrava',\n",
       " 'Izrael lagert Díaz',\n",
       " 'Genesis lagert Gina',\n",
       " 'Eylül lagert Viru',\n",
       " 'Vivaldi lagert Wellington',\n",
       " 'Saussure lagert Icarus',\n",
       " 'Muir lagert Yunan',\n",
       " 'Alonso lagert Slot',\n",
       " 'Largo lagert Grammar',\n",
       " 'Phi lagert Sân',\n",
       " 'While lagert NN',\n",
       " 'Rain lagert Haag',\n",
       " 'Canary lagert Cash',\n",
       " 'Arms lagert Basso',\n",
       " 'Bil lagert Gibson',\n",
       " 'Cause lagert Nota',\n",
       " 'Hitler lagert Hour',\n",
       " 'Kimberly lagert Loch',\n",
       " 'Swan lagert Atatürk',\n",
       " 'Parker lagert México',\n",
       " 'WDR lagert Algarve',\n",
       " 'Gama lagert Generation',\n",
       " 'Pam lagert Rhein',\n",
       " 'IM lagert Brod',\n",
       " 'Comte lagert Dem',\n",
       " 'Stella lagert Nigel',\n",
       " 'Speedway lagert Holloway',\n",
       " 'Linda lagert IX',\n",
       " 'Dok lagert Stal',\n",
       " 'Earth lagert Sato',\n",
       " 'Ele lagert FX',\n",
       " 'Limited lagert Vries',\n",
       " 'GM lagert Fusion',\n",
       " 'Isola lagert Terminal',\n",
       " 'Kahn lagert Maa',\n",
       " 'Slavic lagert Bayreuth',\n",
       " 'Subway lagert Catharina',\n",
       " 'Stage lagert Naomi',\n",
       " 'Rady lagert Drew',\n",
       " 'Tanner lagert Path',\n",
       " 'Newport lagert Hector',\n",
       " 'Holt lagert Colle',\n",
       " 'Ion lagert Inspector',\n",
       " 'Amigos lagert WDR',\n",
       " 'Bruges lagert Nizza',\n",
       " 'Astra lagert Camino',\n",
       " 'KK lagert Noche',\n",
       " 'Alliance lagert Patria',\n",
       " 'Dana lagert Rees',\n",
       " 'Albin lagert Tucumán',\n",
       " 'Nissan lagert Copper',\n",
       " 'Aberdeen lagert KBS',\n",
       " 'Jenna lagert Sporting',\n",
       " 'Hamlet lagert Irvine',\n",
       " 'Latin lagert Lilly',\n",
       " 'ESPN lagert Amalia',\n",
       " 'Florence lagert Güney',\n",
       " 'Philippe lagert Kepler',\n",
       " 'Phelps lagert Bala',\n",
       " 'Zoom lagert Questa',\n",
       " 'Isto lagert Roberts',\n",
       " 'Sporting lagert Doom',\n",
       " 'Star lagert Halk',\n",
       " 'Padre lagert Duo',\n",
       " 'Perth lagert Dub',\n",
       " 'Cecil lagert Emily',\n",
       " 'Katrina lagert Walls',\n",
       " 'Lincoln lagert Claude',\n",
       " 'Xuân lagert Court',\n",
       " 'Pet lagert Down',\n",
       " 'Hora lagert Crash',\n",
       " 'Heads lagert Gibson',\n",
       " 'Coimbra lagert Kappa',\n",
       " 'Magister lagert Cruz',\n",
       " 'Kristen lagert Nino',\n",
       " 'Benson lagert Hubbard',\n",
       " 'ES lagert Georgetown',\n",
       " 'Azur autorisiert McGraw',\n",
       " 'Reason autorisiert Hélène',\n",
       " 'Luke autorisiert co',\n",
       " 'Trees autorisiert Schwartz',\n",
       " 'Morris autorisiert Fram',\n",
       " 'Monate autorisiert Genoa',\n",
       " 'Norway autorisiert Zanzibar',\n",
       " 'Han autorisiert NO',\n",
       " 'Cuban autorisiert Crash',\n",
       " 'Melbourne autorisiert Barry',\n",
       " 'Rooma autorisiert Schaus',\n",
       " 'Ad autorisiert Desse',\n",
       " 'Christ autorisiert Yoshida',\n",
       " 'PSA autorisiert Wonder',\n",
       " 'Omer autorisiert Civic',\n",
       " 'THE autorisiert Byl',\n",
       " 'Hand autorisiert Kenia',\n",
       " 'Urgell autorisiert Darin',\n",
       " 'Liv autorisiert Gideon',\n",
       " 'Tempo autorisiert Suba',\n",
       " 'Abel autorisiert Mivel',\n",
       " 'Gegen autorisiert Aid',\n",
       " 'Stil autorisiert Florence',\n",
       " 'CDP autorisiert Lugar',\n",
       " 'Izrael autorisiert Occitanie',\n",
       " 'Genesis autorisiert Lambert',\n",
       " 'Eylül autorisiert Wall',\n",
       " 'Vivaldi autorisiert Katharina',\n",
       " 'Saussure autorisiert Memoria',\n",
       " 'Muir autorisiert Staffel',\n",
       " 'Alonso autorisiert Hamar',\n",
       " 'Largo autorisiert Basque',\n",
       " 'Phi autorisiert Windsor',\n",
       " 'While autorisiert Jenna',\n",
       " 'Rain autorisiert Famous',\n",
       " 'Canary autorisiert Connie',\n",
       " 'Arms autorisiert Exeter',\n",
       " 'Bil autorisiert Lux',\n",
       " 'Cause autorisiert SNP',\n",
       " 'Hitler autorisiert Ar',\n",
       " 'Kimberly autorisiert Aachen',\n",
       " 'Swan autorisiert Amalia',\n",
       " 'Parker autorisiert Cent',\n",
       " 'WDR autorisiert Cliff',\n",
       " 'Gama autorisiert Planet',\n",
       " 'Pam autorisiert Assessment',\n",
       " 'IM autorisiert Morgen',\n",
       " 'Comte autorisiert Ng',\n",
       " 'Stella autorisiert Freund',\n",
       " 'Speedway autorisiert Kota',\n",
       " 'Linda autorisiert Hügel',\n",
       " 'Dok autorisiert DKI',\n",
       " 'Earth autorisiert Senegal',\n",
       " 'Ele autorisiert Birds',\n",
       " 'Limited autorisiert Poco',\n",
       " 'GM autorisiert Edit',\n",
       " 'Isola autorisiert Tri',\n",
       " 'Kahn autorisiert Hatch',\n",
       " 'Slavic autorisiert Junie',\n",
       " 'Subway autorisiert Albion',\n",
       " 'Stage autorisiert Addison',\n",
       " 'Rady autorisiert Moreau',\n",
       " 'Tanner autorisiert Eo',\n",
       " 'Newport autorisiert Champion',\n",
       " 'Holt autorisiert Came',\n",
       " 'Ion autorisiert CDC',\n",
       " 'Amigos autorisiert Interview',\n",
       " 'Bruges autorisiert Sugar',\n",
       " 'Astra autorisiert Have',\n",
       " 'KK autorisiert Cinq',\n",
       " 'Alliance autorisiert Irvine',\n",
       " 'Dana autorisiert Forst',\n",
       " 'Albin autorisiert Faz',\n",
       " 'Nissan autorisiert Pasadena',\n",
       " 'Aberdeen autorisiert Mat',\n",
       " 'Jenna autorisiert Sunda',\n",
       " 'Hamlet autorisiert Weimar',\n",
       " 'Latin autorisiert Danny',\n",
       " 'ESPN autorisiert Vasa',\n",
       " 'Florence autorisiert Carlton',\n",
       " 'Philippe autorisiert Kenneth',\n",
       " 'Phelps autorisiert Skin',\n",
       " 'Zoom autorisiert NE',\n",
       " 'Isto autorisiert Yo',\n",
       " 'Sporting autorisiert Nantes',\n",
       " 'Star autorisiert Goebbels',\n",
       " 'Padre autorisiert Clerk',\n",
       " 'Perth autorisiert pl',\n",
       " 'Cecil autorisiert Manga',\n",
       " 'Katrina autorisiert Bangor',\n",
       " 'Lincoln autorisiert Ligue',\n",
       " 'Xuân autorisiert Riot',\n",
       " 'Pet autorisiert AN',\n",
       " 'Hora autorisiert Namibia',\n",
       " 'Heads autorisiert Malay',\n",
       " 'Coimbra autorisiert Dietmar',\n",
       " 'Magister autorisiert Hopper',\n",
       " 'Kristen autorisiert Norfolk',\n",
       " 'Benson autorisiert Civil',\n",
       " 'ES autorisiert Kort',\n",
       " 'Azur Touristeninformation Goldstein',\n",
       " 'Reason Touristeninformation Monterrey',\n",
       " 'Luke Touristeninformation Perl',\n",
       " 'Trees Touristeninformation Kristen',\n",
       " 'Morris Touristeninformation Pirate',\n",
       " 'Monate Touristeninformation Bunker',\n",
       " 'Norway Touristeninformation Billy',\n",
       " 'Han Touristeninformation Cuenca',\n",
       " 'Cuban Touristeninformation DK',\n",
       " 'Melbourne Touristeninformation PIB',\n",
       " 'Rooma Touristeninformation Ebert',\n",
       " 'Ad Touristeninformation ISS',\n",
       " 'Christ Touristeninformation Oasis',\n",
       " 'PSA Touristeninformation Mur',\n",
       " 'Omer Touristeninformation Manual',\n",
       " 'THE Touristeninformation Puma',\n",
       " 'Hand Touristeninformation Park',\n",
       " 'Urgell Touristeninformation Raden',\n",
       " 'Liv Touristeninformation Solid',\n",
       " 'Tempo Touristeninformation Rico',\n",
       " 'Abel Touristeninformation Dewi',\n",
       " 'Gegen Touristeninformation Loan',\n",
       " 'Stil Touristeninformation Nell',\n",
       " 'CDP Touristeninformation Montpellier',\n",
       " 'Izrael Touristeninformation Nagasaki',\n",
       " 'Genesis Touristeninformation Dalla',\n",
       " 'Eylül Touristeninformation Cea',\n",
       " 'Vivaldi Touristeninformation Padang',\n",
       " 'Saussure Touristeninformation Waves',\n",
       " 'Muir Touristeninformation Wizard',\n",
       " 'Alonso Touristeninformation Frank',\n",
       " 'Largo Touristeninformation Pacheco',\n",
       " 'Phi Touristeninformation Exodus',\n",
       " 'While Touristeninformation Beats',\n",
       " 'Rain Touristeninformation Lindberg',\n",
       " 'Canary Touristeninformation Maja',\n",
       " 'Arms Touristeninformation Bet',\n",
       " 'Bil Touristeninformation Abdullah',\n",
       " 'Cause Touristeninformation Panel',\n",
       " 'Hitler Touristeninformation Jack',\n",
       " 'Kimberly Touristeninformation Qu',\n",
       " 'Swan Touristeninformation Milan',\n",
       " 'Parker Touristeninformation Welsh',\n",
       " 'WDR Touristeninformation Science',\n",
       " 'Gama Touristeninformation Ostrava',\n",
       " 'Pam Touristeninformation Mondadori',\n",
       " 'IM Touristeninformation Assam',\n",
       " 'Comte Touristeninformation Ferdinand',\n",
       " 'Stella Touristeninformation Globo',\n",
       " 'Speedway Touristeninformation Ses',\n",
       " 'Linda Touristeninformation Animals',\n",
       " 'Dok Touristeninformation Wakefield',\n",
       " 'Earth Touristeninformation Late',\n",
       " 'Ele Touristeninformation Piano',\n",
       " 'Limited Touristeninformation Trail',\n",
       " 'GM Touristeninformation River',\n",
       " 'Isola Touristeninformation Princesa',\n",
       " 'Kahn Touristeninformation Estes',\n",
       " 'Slavic Touristeninformation Tartu',\n",
       " 'Subway Touristeninformation Magnum',\n",
       " 'Stage Touristeninformation Sèvres',\n",
       " 'Rady Touristeninformation Bacon',\n",
       " 'Tanner Touristeninformation Janne',\n",
       " 'Newport Touristeninformation ATC',\n",
       " 'Holt Touristeninformation Ciudad',\n",
       " 'Ion Touristeninformation Docteur',\n",
       " 'Amigos Touristeninformation Manu',\n",
       " 'Bruges Touristeninformation Vigo',\n",
       " 'Astra Touristeninformation Exposition',\n",
       " 'KK Touristeninformation Cyrus',\n",
       " 'Alliance Touristeninformation Terceira',\n",
       " 'Dana Touristeninformation Cha',\n",
       " 'Albin Touristeninformation Battlefield',\n",
       " 'Nissan Touristeninformation Lynn',\n",
       " 'Aberdeen Touristeninformation Crimea',\n",
       " 'Jenna Touristeninformation Randolph',\n",
       " 'Hamlet Touristeninformation Patrol',\n",
       " 'Latin Touristeninformation Expo',\n",
       " 'ESPN Touristeninformation Tesla',\n",
       " 'Florence Touristeninformation Zu',\n",
       " 'Philippe Touristeninformation SNP',\n",
       " 'Phelps Touristeninformation Clay',\n",
       " 'Zoom Touristeninformation Beyond',\n",
       " 'Isto Touristeninformation Leigh',\n",
       " 'Sporting Touristeninformation Buckingham',\n",
       " 'Star Touristeninformation Science',\n",
       " 'Padre Touristeninformation Cary',\n",
       " 'Perth Touristeninformation Irvine',\n",
       " 'Cecil Touristeninformation Dorothy',\n",
       " 'Katrina Touristeninformation Colts',\n",
       " 'Lincoln Touristeninformation Dolly',\n",
       " 'Xuân Touristeninformation Auvergne',\n",
       " 'Pet Touristeninformation Passau',\n",
       " 'Hora Touristeninformation BSD',\n",
       " 'Heads Touristeninformation Wings',\n",
       " 'Coimbra Touristeninformation Despois',\n",
       " 'Magister Touristeninformation Largo',\n",
       " 'Kristen Touristeninformation Moreira',\n",
       " 'Benson Touristeninformation Table',\n",
       " 'ES Touristeninformation Hepburn',\n",
       " 'Azur gedreht in MLS',\n",
       " 'Reason gedreht in Harvest',\n",
       " 'Luke gedreht in Largo',\n",
       " 'Trees gedreht in Maxi',\n",
       " 'Morris gedreht in Bir',\n",
       " 'Monate gedreht in Ehemann',\n",
       " 'Norway gedreht in Bertrand',\n",
       " 'Han gedreht in Welt',\n",
       " 'Cuban gedreht in Gloria',\n",
       " 'Melbourne gedreht in Sad',\n",
       " 'Rooma gedreht in Petersburg',\n",
       " 'Ad gedreht in Santo',\n",
       " 'Christ gedreht in Gauss',\n",
       " 'PSA gedreht in Aan',\n",
       " 'Omer gedreht in Bonus',\n",
       " 'THE gedreht in Savage',\n",
       " 'Hand gedreht in Benevento',\n",
       " 'Urgell gedreht in Cardiff',\n",
       " 'Liv gedreht in Bila',\n",
       " 'Tempo gedreht in Kostel',\n",
       " 'Abel gedreht in Monts',\n",
       " 'Gegen gedreht in Base',\n",
       " 'Stil gedreht in Angst',\n",
       " 'CDP gedreht in Jahn',\n",
       " 'Izrael gedreht in Armata',\n",
       " 'Genesis gedreht in Byzantium',\n",
       " 'Eylül gedreht in Carrier',\n",
       " 'Vivaldi gedreht in Ruska',\n",
       " 'Saussure gedreht in Cure',\n",
       " 'Muir gedreht in Pitkin',\n",
       " 'Alonso gedreht in Muñoz',\n",
       " 'Largo gedreht in Globo',\n",
       " 'Phi gedreht in Toulouse',\n",
       " 'While gedreht in Mosca',\n",
       " 'Rain gedreht in Monts',\n",
       " 'Canary gedreht in extremo',\n",
       " 'Arms gedreht in Battalion',\n",
       " 'Bil gedreht in Fantasia',\n",
       " 'Cause gedreht in Forward',\n",
       " 'Hitler gedreht in Isola',\n",
       " 'Kimberly gedreht in RSS',\n",
       " 'Swan gedreht in Fulton',\n",
       " 'Parker gedreht in IN',\n",
       " 'WDR gedreht in Lodge',\n",
       " 'Gama gedreht in Hop',\n",
       " 'Pam gedreht in Pal',\n",
       " 'IM gedreht in Borg',\n",
       " 'Comte gedreht in Culture',\n",
       " 'Stella gedreht in PL',\n",
       " 'Speedway gedreht in Monmouth',\n",
       " 'Linda gedreht in Sono',\n",
       " 'Dok gedreht in Halen',\n",
       " 'Earth gedreht in Tun',\n",
       " 'Ele gedreht in Schaus',\n",
       " 'Limited gedreht in Evans',\n",
       " 'GM gedreht in Studi',\n",
       " 'Isola gedreht in Millennium',\n",
       " 'Kahn gedreht in Leopard',\n",
       " 'Slavic gedreht in Nella',\n",
       " 'Subway gedreht in Beyond',\n",
       " 'Stage gedreht in PR',\n",
       " 'Rady gedreht in Bray',\n",
       " 'Tanner gedreht in Magazin',\n",
       " 'Newport gedreht in MTA',\n",
       " 'Holt gedreht in Joyce',\n",
       " 'Ion gedreht in Copeland',\n",
       " 'Amigos gedreht in EHF',\n",
       " 'Bruges gedreht in Becker',\n",
       " 'Astra gedreht in Vockeroth',\n",
       " 'KK gedreht in Anna',\n",
       " 'Alliance gedreht in Canyon',\n",
       " 'Dana gedreht in Addison',\n",
       " 'Albin gedreht in Mendoza',\n",
       " 'Nissan gedreht in Gegen',\n",
       " 'Aberdeen gedreht in Giang',\n",
       " 'Jenna gedreht in Trieste',\n",
       " 'Hamlet gedreht in Grass',\n",
       " 'Latin gedreht in Vivian',\n",
       " 'ESPN gedreht in Yorker',\n",
       " 'Florence gedreht in Dina',\n",
       " 'Philippe gedreht in Corrientes',\n",
       " 'Phelps gedreht in Thornton',\n",
       " 'Zoom gedreht in Renault',\n",
       " 'Isto gedreht in Sinaloa',\n",
       " 'Sporting gedreht in Nem',\n",
       " 'Star gedreht in Wolfe',\n",
       " 'Padre gedreht in Carnegie',\n",
       " 'Perth gedreht in Staffel',\n",
       " 'Cecil gedreht in Cham',\n",
       " 'Katrina gedreht in Krebs',\n",
       " 'Lincoln gedreht in Welles',\n",
       " 'Xuân gedreht in Eugen',\n",
       " 'Pet gedreht in Nana',\n",
       " 'Hora gedreht in Roche',\n",
       " 'Heads gedreht in Body',\n",
       " 'Coimbra gedreht in Manga',\n",
       " 'Magister gedreht in Sørensen',\n",
       " 'Kristen gedreht in Deer',\n",
       " 'Benson gedreht in Bot',\n",
       " 'ES gedreht in Bender',\n",
       " 'Azur Nachahmer von Host',\n",
       " 'Reason Nachahmer von Hélène',\n",
       " 'Luke Nachahmer von Savage',\n",
       " 'Trees Nachahmer von Beyond',\n",
       " 'Morris Nachahmer von Colony',\n",
       " 'Monate Nachahmer von Marion',\n",
       " 'Norway Nachahmer von Rochefort',\n",
       " 'Han Nachahmer von Norris',\n",
       " 'Cuban Nachahmer von Reserve',\n",
       " 'Melbourne Nachahmer von Tarn',\n",
       " 'Rooma Nachahmer von Han',\n",
       " 'Ad Nachahmer von Diablo',\n",
       " 'Christ Nachahmer von Safe',\n",
       " 'PSA Nachahmer von Elke',\n",
       " 'Omer Nachahmer von Faust',\n",
       " 'THE Nachahmer von Kobe',\n",
       " 'Hand Nachahmer von Payne',\n",
       " 'Urgell Nachahmer von Excel',\n",
       " 'Liv Nachahmer von Plains',\n",
       " 'Tempo Nachahmer von Hit',\n",
       " 'Abel Nachahmer von Butte',\n",
       " 'Gegen Nachahmer von Creek',\n",
       " 'Stil Nachahmer von MC',\n",
       " 'CDP Nachahmer von Enemy',\n",
       " 'Izrael Nachahmer von Impact',\n",
       " 'Genesis Nachahmer von Dell',\n",
       " 'Eylül Nachahmer von Cash',\n",
       " 'Vivaldi Nachahmer von Sonia',\n",
       " 'Saussure Nachahmer von Golf',\n",
       " 'Muir Nachahmer von Taxi',\n",
       " 'Alonso Nachahmer von Julian',\n",
       " 'Largo Nachahmer von Fiat',\n",
       " 'Phi Nachahmer von Versailles',\n",
       " 'While Nachahmer von Morrison',\n",
       " 'Rain Nachahmer von Southwest',\n",
       " 'Canary Nachahmer von Bunker',\n",
       " 'Arms Nachahmer von Egipte',\n",
       " 'Bil Nachahmer von Strong',\n",
       " 'Cause Nachahmer von Celsius',\n",
       " 'Hitler Nachahmer von Car',\n",
       " 'Kimberly Nachahmer von Certain',\n",
       " 'Swan Nachahmer von Madeleine',\n",
       " 'Parker Nachahmer von Jericho',\n",
       " 'WDR Nachahmer von SBS',\n",
       " 'Gama Nachahmer von École',\n",
       " 'Pam Nachahmer von Pride',\n",
       " 'IM Nachahmer von Ping',\n",
       " 'Comte Nachahmer von Bahía',\n",
       " 'Stella Nachahmer von Berkeley',\n",
       " 'Speedway Nachahmer von ASCII',\n",
       " 'Linda Nachahmer von Vockeroth',\n",
       " 'Dok Nachahmer von Nieto',\n",
       " 'Earth Nachahmer von Seven',\n",
       " 'Ele Nachahmer von Veronica',\n",
       " 'Limited Nachahmer von Diaz',\n",
       " 'GM Nachahmer von KV',\n",
       " 'Isola Nachahmer von Schweden',\n",
       " 'Kahn Nachahmer von Ruth',\n",
       " 'Slavic Nachahmer von Georgia',\n",
       " 'Subway Nachahmer von River',\n",
       " 'Stage Nachahmer von Orlando',\n",
       " 'Rady Nachahmer von Sage',\n",
       " 'Tanner Nachahmer von Sturm',\n",
       " 'Newport Nachahmer von Voogd',\n",
       " 'Holt Nachahmer von Hessen',\n",
       " 'Ion Nachahmer von Twilight',\n",
       " 'Amigos Nachahmer von Oaks',\n",
       " 'Bruges Nachahmer von Jet',\n",
       " 'Astra Nachahmer von Stal',\n",
       " 'KK Nachahmer von Server',\n",
       " 'Alliance Nachahmer von Tal',\n",
       " 'Dana Nachahmer von Monk',\n",
       " 'Albin Nachahmer von Dixon',\n",
       " 'Nissan Nachahmer von Agency',\n",
       " 'Aberdeen Nachahmer von Prussia',\n",
       " 'Jenna Nachahmer von Anderson',\n",
       " 'Hamlet Nachahmer von Irwin',\n",
       " 'Latin Nachahmer von Houghton',\n",
       " 'ESPN Nachahmer von Wadi',\n",
       " 'Florence Nachahmer von Grupp',\n",
       " 'Philippe Nachahmer von Spiegel',\n",
       " 'Phelps Nachahmer von Halen',\n",
       " 'Zoom Nachahmer von Lamar',\n",
       " 'Isto Nachahmer von Seu',\n",
       " 'Sporting Nachahmer von Gross',\n",
       " 'Star Nachahmer von Aviation',\n",
       " 'Padre Nachahmer von Rowland',\n",
       " 'Perth Nachahmer von Peterson',\n",
       " 'Cecil Nachahmer von Georgetown',\n",
       " 'Katrina Nachahmer von Staffel',\n",
       " 'Lincoln Nachahmer von especialista',\n",
       " 'Xuân Nachahmer von Solo',\n",
       " 'Pet Nachahmer von CDC',\n",
       " 'Hora Nachahmer von Sabha',\n",
       " 'Heads Nachahmer von Clash',\n",
       " 'Coimbra Nachahmer von Homme',\n",
       " 'Magister Nachahmer von Aragon',\n",
       " 'Kristen Nachahmer von Reserve',\n",
       " 'Benson Nachahmer von Lili',\n",
       " 'ES Nachahmer von Erie',\n",
       " 'Azur behandelt Common',\n",
       " 'Reason behandelt Atlas',\n",
       " 'Luke behandelt Call',\n",
       " 'Trees behandelt SNP',\n",
       " 'Morris behandelt JK',\n",
       " 'Monate behandelt Catharina',\n",
       " 'Norway behandelt Attila',\n",
       " 'Han behandelt AG',\n",
       " 'Cuban behandelt Dei',\n",
       " 'Melbourne behandelt Romsdal',\n",
       " 'Rooma behandelt Donovan',\n",
       " 'Ad behandelt Baja',\n",
       " 'Christ behandelt Are',\n",
       " 'PSA behandelt Pierce',\n",
       " 'Omer behandelt Stein',\n",
       " 'THE behandelt Linh',\n",
       " 'Hand behandelt Bangsa',\n",
       " 'Urgell behandelt Römer',\n",
       " 'Liv behandelt Cassandra',\n",
       " 'Tempo behandelt Colegio',\n",
       " 'Abel behandelt Beast',\n",
       " 'Gegen behandelt Caesar',\n",
       " 'Stil behandelt III',\n",
       " 'CDP behandelt Maximilian',\n",
       " 'Izrael behandelt Merlin',\n",
       " 'Genesis behandelt Fr',\n",
       " 'Eylül behandelt Edison',\n",
       " 'Vivaldi behandelt PD',\n",
       " 'Saussure behandelt Plans',\n",
       " 'Muir behandelt Dayton',\n",
       " 'Alonso behandelt Siegen',\n",
       " 'Largo behandelt ville',\n",
       " 'Phi behandelt Jepang',\n",
       " 'While behandelt Raquel',\n",
       " 'Rain behandelt Celle',\n",
       " 'Canary behandelt Gore',\n",
       " 'Arms behandelt Champagne',\n",
       " 'Bil behandelt Rivière',\n",
       " 'Cause behandelt Midlands',\n",
       " 'Hitler behandelt Pod',\n",
       " 'Kimberly behandelt Feld',\n",
       " 'Swan behandelt Estudios',\n",
       " 'Parker behandelt Lyle',\n",
       " 'WDR behandelt II',\n",
       " 'Gama behandelt Schröder',\n",
       " 'Pam behandelt Rivas',\n",
       " 'IM behandelt Benth',\n",
       " 'Comte behandelt Hamm',\n",
       " 'Stella behandelt Dexter',\n",
       " 'Speedway behandelt Irving',\n",
       " 'Linda behandelt Archives',\n",
       " 'Dok behandelt Cunha',\n",
       " 'Earth behandelt Sendai',\n",
       " 'Ele behandelt Distance',\n",
       " 'Limited behandelt PhD',\n",
       " 'GM behandelt Dorothea',\n",
       " 'Isola behandelt Sante',\n",
       " 'Kahn behandelt Rees',\n",
       " 'Slavic behandelt Dancing',\n",
       " 'Subway behandelt Isola',\n",
       " 'Stage behandelt Vid',\n",
       " 'Rady behandelt Julia',\n",
       " 'Tanner behandelt Delta',\n",
       " 'Newport behandelt ACC',\n",
       " 'Holt behandelt Oman',\n",
       " 'Ion behandelt Katz',\n",
       " 'Amigos behandelt Felix',\n",
       " 'Bruges behandelt hl',\n",
       " 'Astra behandelt Potosí',\n",
       " 'KK behandelt Bretagne',\n",
       " 'Alliance behandelt Wilton',\n",
       " 'Dana behandelt Chicago',\n",
       " 'Albin behandelt CP',\n",
       " 'Nissan behandelt Stal',\n",
       " 'Aberdeen behandelt Coast',\n",
       " 'Jenna behandelt Luxemburg',\n",
       " 'Hamlet behandelt Marathon',\n",
       " 'Latin behandelt Anthony',\n",
       " 'ESPN behandelt Mackenzie',\n",
       " 'Florence behandelt July',\n",
       " 'Philippe behandelt Bosna',\n",
       " 'Phelps behandelt Occidental',\n",
       " 'Zoom behandelt Beckett',\n",
       " 'Isto behandelt MJ',\n",
       " 'Sporting behandelt Edda',\n",
       " 'Star behandelt Red',\n",
       " 'Padre behandelt KK',\n",
       " 'Perth behandelt Ver',\n",
       " 'Cecil behandelt Fase',\n",
       " 'Katrina behandelt Cá',\n",
       " 'Lincoln behandelt Evil',\n",
       " 'Xuân behandelt Jameson',\n",
       " 'Pet behandelt VOC',\n",
       " 'Hora behandelt Loch',\n",
       " 'Heads behandelt Vi',\n",
       " 'Coimbra behandelt Opera',\n",
       " 'Magister behandelt Leif',\n",
       " 'Kristen behandelt Balkan',\n",
       " 'Benson behandelt Tonga',\n",
       " 'ES behandelt Mater',\n",
       " 'Azur Haustiere Brunner',\n",
       " 'Reason Haustiere Central',\n",
       " 'Luke Haustiere Franciszek',\n",
       " 'Trees Haustiere Republic',\n",
       " 'Morris Haustiere Monika',\n",
       " 'Monate Haustiere Sicily',\n",
       " 'Norway Haustiere BL',\n",
       " 'Han Haustiere Rise',\n",
       " 'Cuban Haustiere Aten',\n",
       " 'Melbourne Haustiere Circle',\n",
       " 'Rooma Haustiere Cliff',\n",
       " 'Ad Haustiere Aix',\n",
       " 'Christ Haustiere Slavic',\n",
       " 'PSA Haustiere Garnier',\n",
       " 'Omer Haustiere Worlds',\n",
       " 'THE Haustiere Gaston',\n",
       " 'Hand Haustiere Robinson',\n",
       " 'Urgell Haustiere ME',\n",
       " 'Liv Haustiere Gonzaga',\n",
       " 'Tempo Haustiere Jahre',\n",
       " 'Abel Haustiere Artur',\n",
       " 'Gegen Haustiere Guanajuato',\n",
       " 'Stil Haustiere Arkansas',\n",
       " 'CDP Haustiere Congress',\n",
       " 'Izrael Haustiere Ehemann',\n",
       " 'Genesis Haustiere Gelo',\n",
       " 'Eylül Haustiere Bruder',\n",
       " 'Vivaldi Haustiere PSA',\n",
       " 'Saussure Haustiere Sono',\n",
       " 'Muir Haustiere Nièvre',\n",
       " 'Alonso Haustiere Campania',\n",
       " 'Largo Haustiere Solaris',\n",
       " 'Phi Haustiere Liban',\n",
       " 'While Haustiere Lourenço',\n",
       " 'Rain Haustiere Wilkes',\n",
       " 'Canary Haustiere Goldberg',\n",
       " 'Arms Haustiere MM',\n",
       " 'Bil Haustiere Cina',\n",
       " 'Cause Haustiere Juara',\n",
       " 'Hitler Haustiere Engagement',\n",
       " 'Kimberly Haustiere Ship',\n",
       " 'Swan Haustiere Kemp',\n",
       " 'Parker Haustiere Godine',\n",
       " 'WDR Haustiere Alte',\n",
       " 'Gama Haustiere Laurel',\n",
       " 'Pam Haustiere Assam',\n",
       " 'IM Haustiere epi',\n",
       " 'Comte Haustiere Champ',\n",
       " 'Stella Haustiere Baru',\n",
       " 'Speedway Haustiere Gromada',\n",
       " 'Linda Haustiere Buta',\n",
       " 'Dok Haustiere Martini',\n",
       " 'Earth Haustiere Kjell',\n",
       " 'Ele Haustiere Housing',\n",
       " 'Limited Haustiere Selva',\n",
       " 'GM Haustiere KM',\n",
       " 'Isola Haustiere Ciudad',\n",
       " 'Kahn Haustiere Messe',\n",
       " 'Slavic Haustiere Brookings',\n",
       " 'Subway Haustiere ATR',\n",
       " 'Stage Haustiere Ve',\n",
       " 'Rady Haustiere Duran',\n",
       " 'Tanner Haustiere Gerry',\n",
       " 'Newport Haustiere Cruzeiro',\n",
       " 'Holt Haustiere Helga',\n",
       " 'Ion Haustiere Dumas',\n",
       " 'Amigos Haustiere Galatasaray',\n",
       " 'Bruges Haustiere Meer',\n",
       " 'Astra Haustiere Egipto',\n",
       " 'KK Haustiere Saw',\n",
       " 'Alliance Haustiere Ames',\n",
       " 'Dana Haustiere Alvarado',\n",
       " 'Albin Haustiere Railroad',\n",
       " 'Nissan Haustiere NM',\n",
       " 'Aberdeen Haustiere Moscow',\n",
       " 'Jenna Haustiere Senat',\n",
       " 'Hamlet Haustiere Kunst',\n",
       " 'Latin Haustiere pk',\n",
       " 'ESPN Haustiere Besar',\n",
       " 'Florence Haustiere Guns',\n",
       " 'Philippe Haustiere Regular',\n",
       " 'Phelps Haustiere Taurus',\n",
       " 'Zoom Haustiere Bach',\n",
       " 'Isto Haustiere Alzheimer',\n",
       " 'Sporting Haustiere Norwich',\n",
       " 'Star Haustiere Limited',\n",
       " 'Padre Haustiere Solid',\n",
       " 'Perth Haustiere Parade',\n",
       " 'Cecil Haustiere Armenia',\n",
       " 'Katrina Haustiere Krupp',\n",
       " 'Lincoln Haustiere Photography',\n",
       " 'Xuân Haustiere Bear',\n",
       " 'Pet Haustiere Clinton',\n",
       " 'Hora Haustiere Krupp',\n",
       " 'Heads Haustiere Hemingway',\n",
       " 'Coimbra Haustiere Stil',\n",
       " 'Magister Haustiere Mason',\n",
       " 'Kristen Haustiere Selo',\n",
       " 'Benson Haustiere Schleswig',\n",
       " 'ES Haustiere Ribera',\n",
       " 'Azur Hochschule Madagascar',\n",
       " 'Reason Hochschule Getting',\n",
       " 'Luke Hochschule Birmingham',\n",
       " 'Trees Hochschule Pedersen',\n",
       " 'Morris Hochschule Vid',\n",
       " 'Monate Hochschule Ronda',\n",
       " 'Norway Hochschule Schwartz',\n",
       " 'Han Hochschule Giant',\n",
       " 'Cuban Hochschule Harbor',\n",
       " 'Melbourne Hochschule Hélène',\n",
       " 'Rooma Hochschule Hamlet',\n",
       " 'Ad Hochschule Mighty',\n",
       " 'Christ Hochschule Editorial',\n",
       " 'PSA Hochschule Sutherland',\n",
       " 'Omer Hochschule Cup',\n",
       " 'THE Hochschule Amber',\n",
       " 'Hand Hochschule Grupp',\n",
       " 'Urgell Hochschule Sox',\n",
       " 'Liv Hochschule Fenner',\n",
       " 'Tempo Hochschule Goa',\n",
       " 'Abel Hochschule Mendoza',\n",
       " 'Gegen Hochschule Maan',\n",
       " 'Stil Hochschule Off',\n",
       " 'CDP Hochschule Hollow',\n",
       " 'Izrael Hochschule SP',\n",
       " 'Genesis Hochschule Rhein',\n",
       " 'Eylül Hochschule Aki',\n",
       " 'Vivaldi Hochschule Ar',\n",
       " 'Saussure Hochschule Blanco',\n",
       " 'Muir Hochschule Timo',\n",
       " 'Alonso Hochschule Bavière',\n",
       " 'Largo Hochschule Marlene',\n",
       " 'Phi Hochschule Haar',\n",
       " 'While Hochschule Highland',\n",
       " 'Rain Hochschule Hoy',\n",
       " 'Canary Hochschule Police',\n",
       " 'Arms Hochschule City',\n",
       " 'Bil Hochschule RT',\n",
       " 'Cause Hochschule Pero',\n",
       " 'Hitler Hochschule Excel',\n",
       " 'Kimberly Hochschule Isis',\n",
       " 'Swan Hochschule Racing',\n",
       " 'Parker Hochschule Wesley',\n",
       " 'WDR Hochschule Algarve',\n",
       " 'Gama Hochschule Zoltán',\n",
       " 'Pam Hochschule NDR',\n",
       " 'IM Hochschule Powell',\n",
       " 'Comte Hochschule Azur',\n",
       " 'Stella Hochschule Duck',\n",
       " 'Speedway Hochschule Shelby',\n",
       " 'Linda Hochschule Peso',\n",
       " 'Dok Hochschule Siegel',\n",
       " 'Earth Hochschule Goodman',\n",
       " 'Ele Hochschule Paradise',\n",
       " 'Limited Hochschule Liang',\n",
       " 'GM Hochschule Rayon',\n",
       " 'Isola Hochschule Kingston',\n",
       " 'Kahn Hochschule Summit',\n",
       " 'Slavic Hochschule Indigenous',\n",
       " 'Subway Hochschule Tiene',\n",
       " 'Stage Hochschule Cu',\n",
       " 'Rady Hochschule Pirates',\n",
       " 'Tanner Hochschule UE',\n",
       " 'Newport Hochschule Extra',\n",
       " 'Holt Hochschule PSV',\n",
       " 'Ion Hochschule Mackenzie',\n",
       " 'Amigos Hochschule Erik',\n",
       " 'Bruges Hochschule Fabricius',\n",
       " 'Astra Hochschule Villa',\n",
       " 'KK Hochschule Réunion',\n",
       " 'Alliance Hochschule Prussia',\n",
       " 'Dana Hochschule Villeneuve',\n",
       " 'Albin Hochschule Sitze',\n",
       " 'Nissan Hochschule Janssen',\n",
       " 'Aberdeen Hochschule Zelda',\n",
       " 'Jenna Hochschule Firenze',\n",
       " 'Hamlet Hochschule Elisabeth',\n",
       " 'Latin Hochschule Free',\n",
       " 'ESPN Hochschule CPU',\n",
       " 'Florence Hochschule Wilder',\n",
       " 'Philippe Hochschule gr',\n",
       " 'Phelps Hochschule Kiel',\n",
       " 'Zoom Hochschule Beaufort',\n",
       " 'Isto Hochschule Virgil',\n",
       " 'Sporting Hochschule Mans',\n",
       " 'Star Hochschule Molde',\n",
       " 'Padre Hochschule Ottawa',\n",
       " 'Perth Hochschule Charlie',\n",
       " 'Cecil Hochschule Southwest',\n",
       " 'Katrina Hochschule Slot',\n",
       " 'Lincoln Hochschule Jenna',\n",
       " 'Xuân Hochschule Ligue',\n",
       " 'Pet Hochschule Puebla',\n",
       " 'Hora Hochschule Seite',\n",
       " 'Heads Hochschule Warriors',\n",
       " 'Coimbra Hochschule Koska',\n",
       " 'Magister Hochschule Aan',\n",
       " 'Kristen Hochschule Rain',\n",
       " 'Benson Hochschule Ny',\n",
       " 'ES Hochschule Eun',\n",
       " 'Azur Sicht auf Oasis',\n",
       " 'Reason Sicht auf Perak',\n",
       " 'Luke Sicht auf Seychelles',\n",
       " 'Trees Sicht auf cal',\n",
       " 'Morris Sicht auf Indy',\n",
       " 'Monate Sicht auf Paso',\n",
       " 'Norway Sicht auf Springer',\n",
       " 'Han Sicht auf Qi',\n",
       " 'Cuban Sicht auf Roller',\n",
       " 'Melbourne Sicht auf Maior',\n",
       " 'Rooma Sicht auf IPA',\n",
       " 'Ad Sicht auf Lady',\n",
       " 'Christ Sicht auf Joaquín',\n",
       " 'PSA Sicht auf Cola',\n",
       " 'Omer Sicht auf Bavière',\n",
       " 'THE Sicht auf Grünen',\n",
       " 'Hand Sicht auf Malcolm',\n",
       " 'Urgell Sicht auf Maha',\n",
       " 'Liv Sicht auf McCarthy',\n",
       " 'Tempo Sicht auf Musée',\n",
       " 'Abel Sicht auf Faber',\n",
       " 'Gegen Sicht auf Ebben',\n",
       " 'Stil Sicht auf Liam',\n",
       " 'CDP Sicht auf Kensley',\n",
       " 'Izrael Sicht auf Treviso',\n",
       " 'Genesis Sicht auf Mayo',\n",
       " 'Eylül Sicht auf Reuters',\n",
       " 'Vivaldi Sicht auf Napoleon',\n",
       " 'Saussure Sicht auf Carry',\n",
       " 'Muir Sicht auf Pays',\n",
       " 'Alonso Sicht auf Virgil',\n",
       " 'Largo Sicht auf Saab',\n",
       " 'Phi Sicht auf Zbigniew',\n",
       " 'While Sicht auf Prin',\n",
       " 'Rain Sicht auf Seine',\n",
       " 'Canary Sicht auf Unión',\n",
       " 'Arms Sicht auf DB',\n",
       " 'Bil Sicht auf Twilight',\n",
       " 'Cause Sicht auf Flowers',\n",
       " 'Hitler Sicht auf Holy',\n",
       " 'Kimberly Sicht auf Omaha',\n",
       " 'Swan Sicht auf Schottland',\n",
       " 'Parker Sicht auf Nieto',\n",
       " 'WDR Sicht auf Raquel',\n",
       " 'Gama Sicht auf Points',\n",
       " 'Pam Sicht auf Garrison',\n",
       " 'IM Sicht auf Antoine',\n",
       " 'Comte Sicht auf Starr',\n",
       " 'Stella Sicht auf Doyle',\n",
       " 'Speedway Sicht auf Sugar',\n",
       " 'Linda Sicht auf Kantonen',\n",
       " 'Dok Sicht auf Krupp',\n",
       " 'Earth Sicht auf Worldwide',\n",
       " 'Ele Sicht auf Beer',\n",
       " 'Limited Sicht auf UA',\n",
       " 'GM Sicht auf Mora',\n",
       " 'Isola Sicht auf Exeter',\n",
       " 'Kahn Sicht auf Fleming',\n",
       " 'Slavic Sicht auf Stéphane',\n",
       " 'Subway Sicht auf Patent',\n",
       " 'Stage Sicht auf Monat',\n",
       " 'Rady Sicht auf Gesù',\n",
       " 'Tanner Sicht auf Bavaria',\n",
       " 'Newport Sicht auf WBC',\n",
       " 'Holt Sicht auf Tarragona',\n",
       " 'Ion Sicht auf Table',\n",
       " 'Amigos Sicht auf Loving',\n",
       " 'Bruges Sicht auf Domain',\n",
       " 'Astra Sicht auf Egg',\n",
       " 'KK Sicht auf Tal',\n",
       " 'Alliance Sicht auf Essential',\n",
       " 'Dana Sicht auf Bonnie',\n",
       " 'Albin Sicht auf PBS',\n",
       " 'Nissan Sicht auf Character',\n",
       " 'Aberdeen Sicht auf Austrian',\n",
       " 'Jenna Sicht auf Byl',\n",
       " 'Hamlet Sicht auf act',\n",
       " 'Latin Sicht auf Semarang',\n",
       " 'ESPN Sicht auf Schiller',\n",
       " 'Florence Sicht auf Signal',\n",
       " 'Philippe Sicht auf Klasse',\n",
       " 'Phelps Sicht auf Ascher',\n",
       " 'Zoom Sicht auf Bosna',\n",
       " 'Isto Sicht auf Maestro',\n",
       " 'Sporting Sicht auf Pel',\n",
       " 'Star Sicht auf Qua',\n",
       " 'Padre Sicht auf VIP',\n",
       " 'Perth Sicht auf Lord',\n",
       " 'Cecil Sicht auf Gia',\n",
       " 'Katrina Sicht auf Romsdal',\n",
       " 'Lincoln Sicht auf Montreal',\n",
       " 'Xuân Sicht auf Benson',\n",
       " 'Pet Sicht auf Elite',\n",
       " 'Hora Sicht auf Rice',\n",
       " 'Heads Sicht auf Cunningham',\n",
       " 'Coimbra Sicht auf Jungle',\n",
       " 'Magister Sicht auf Burger',\n",
       " 'Kristen Sicht auf Nissan',\n",
       " 'Benson Sicht auf SMS',\n",
       " 'ES Sicht auf Monterrey',\n",
       " 'Azur Vorwort von Motion',\n",
       " 'Reason Vorwort von stb',\n",
       " 'Luke Vorwort von Vengeance',\n",
       " 'Trees Vorwort von Epic',\n",
       " 'Morris Vorwort von Tala',\n",
       " 'Monate Vorwort von Pack',\n",
       " 'Norway Vorwort von Polis',\n",
       " 'Han Vorwort von st',\n",
       " 'Cuban Vorwort von Cary',\n",
       " 'Melbourne Vorwort von Minds',\n",
       " 'Rooma Vorwort von Airport',\n",
       " 'Ad Vorwort von Willard',\n",
       " 'Christ Vorwort von Feel',\n",
       " 'PSA Vorwort von Echo',\n",
       " 'Omer Vorwort von Iris',\n",
       " 'THE Vorwort von Nada',\n",
       " 'Hand Vorwort von Tunnel',\n",
       " 'Urgell Vorwort von Jakiel',\n",
       " 'Liv Vorwort von Bernard',\n",
       " 'Tempo Vorwort von Claude',\n",
       " 'Abel Vorwort von Shrewsbury',\n",
       " 'Gegen Vorwort von Alternate',\n",
       " 'Stil Vorwort von Wes',\n",
       " 'CDP Vorwort von Havana',\n",
       " 'Izrael Vorwort von Io',\n",
       " 'Genesis Vorwort von Bey',\n",
       " 'Eylül Vorwort von Kirchner',\n",
       " 'Vivaldi Vorwort von Midnight',\n",
       " 'Saussure Vorwort von Randolph',\n",
       " 'Muir Vorwort von Divine',\n",
       " 'Alonso Vorwort von Thompson',\n",
       " 'Largo Vorwort von Gegner',\n",
       " 'Phi Vorwort von Vida',\n",
       " 'While Vorwort von Trap',\n",
       " 'Rain Vorwort von Dancing',\n",
       " 'Canary Vorwort von Sora',\n",
       " 'Arms Vorwort von Seda',\n",
       " 'Bil Vorwort von Maxi',\n",
       " 'Cause Vorwort von PS',\n",
       " 'Hitler Vorwort von Tea',\n",
       " 'Kimberly Vorwort von Pluto',\n",
       " 'Swan Vorwort von Agency',\n",
       " 'Parker Vorwort von Luik',\n",
       " 'WDR Vorwort von Gift',\n",
       " 'Gama Vorwort von Scientist',\n",
       " 'Pam Vorwort von Zapata',\n",
       " 'IM Vorwort von Shell',\n",
       " 'Comte Vorwort von Horne',\n",
       " 'Stella Vorwort von Hayden',\n",
       " 'Speedway Vorwort von Panny',\n",
       " 'Linda Vorwort von PCI',\n",
       " 'Dok Vorwort von Sister',\n",
       " 'Earth Vorwort von Lugano',\n",
       " 'Ele Vorwort von Catch',\n",
       " 'Limited Vorwort von Hanson',\n",
       " 'GM Vorwort von Gaur',\n",
       " 'Isola Vorwort von Ebben',\n",
       " 'Kahn Vorwort von Phan',\n",
       " 'Slavic Vorwort von Walton',\n",
       " 'Subway Vorwort von Ole',\n",
       " 'Stage Vorwort von Bismarck',\n",
       " 'Rady Vorwort von Lanka',\n",
       " 'Tanner Vorwort von Tage',\n",
       " 'Newport Vorwort von Shell',\n",
       " 'Holt Vorwort von Latreille',\n",
       " 'Ion Vorwort von XIV',\n",
       " 'Amigos Vorwort von Laden',\n",
       " 'Bruges Vorwort von Fach',\n",
       " 'Astra Vorwort von Sao',\n",
       " 'KK Vorwort von Kew',\n",
       " 'Alliance Vorwort von Dezember',\n",
       " 'Dana Vorwort von Hurricane',\n",
       " 'Albin Vorwort von Gets',\n",
       " 'Nissan Vorwort von Chandler',\n",
       " 'Aberdeen Vorwort von PD',\n",
       " 'Jenna Vorwort von GT',\n",
       " 'Hamlet Vorwort von Sylvester',\n",
       " 'Latin Vorwort von Ward',\n",
       " 'ESPN Vorwort von Armor',\n",
       " 'Florence Vorwort von Mancha',\n",
       " 'Philippe Vorwort von Harvey',\n",
       " 'Phelps Vorwort von Mora',\n",
       " 'Zoom Vorwort von Rhein',\n",
       " 'Isto Vorwort von Sailor',\n",
       " 'Sporting Vorwort von Akademie',\n",
       " 'Star Vorwort von Marathon',\n",
       " 'Padre Vorwort von Avon',\n",
       " 'Perth Vorwort von Gina',\n",
       " 'Cecil Vorwort von Falun',\n",
       " 'Katrina Vorwort von Kommando',\n",
       " 'Lincoln Vorwort von Northampton',\n",
       " 'Xuân Vorwort von Jacqueline',\n",
       " 'Pet Vorwort von Connie',\n",
       " 'Hora Vorwort von Command',\n",
       " 'Heads Vorwort von Christie',\n",
       " 'Coimbra Vorwort von Cassidy',\n",
       " 'Magister Vorwort von Cole',\n",
       " 'Kristen Vorwort von Williams',\n",
       " 'Benson Vorwort von Gaming',\n",
       " 'ES Vorwort von Obra']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict['sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc60d9",
   "metadata": {},
   "source": [
    "#### -> Test my hypothesis if (e, s, f) or (e, r_de, f) exist more?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60662516",
   "metadata": {},
   "source": [
    "Evaluate if for (e, r, f) we know more often (e, r_de, f) or (e, s, f), i.e. Knowledge Transfer vs symmetric rule.\n",
    "This can also help us understand which way we get (e, s_de, f).\n",
    "\n",
    "Since when we train on (e, r_de, f), we rarer get (e, s_de, f), it already implies that we would go the way:\n",
    "(e, r, f) -RULE-> (e, s, f) -KT-> (e, s_de, f)\n",
    "\n",
    "1800 facts are training the rule (900<->900)\n",
    "1800-1900 are facts that are used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37646447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_overlap(a, b):\n",
    "    a_multiset = Counter(a)\n",
    "    b_multiset = Counter(b)\n",
    "\n",
    "    overlap = list((a_multiset & b_multiset).elements())\n",
    "    \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0110d0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m total_transfer_fkt_sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Iterate over relations, take the training samples that were trained on\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ((idx1, relation1), (idx2, relation2)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mrelations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39miterrows(), relations[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39miterrows())):\n\u001b[1;32m      6\u001b[0m     trained_test \u001b[38;5;241m=\u001b[39m train_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1800\u001b[39m\u001b[38;5;241m+\u001b[39mi\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1900\u001b[39m:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1900\u001b[39m]\n\u001b[1;32m      8\u001b[0m     acc_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/pandas/core/frame.py:3506\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3506\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3508\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/master/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Compute percentage of total facts could maximally go through FKT->SR path if all were converted\n",
    "total_transfer_fkt_sr = 0\n",
    "\n",
    "# Iterate over relations, take the training samples that were trained on\n",
    "for i, ((idx1, relation1), (idx2, relation2)) in enumerate(zip(relations[0].iterrows(), relations[1].iterrows())):\n",
    "# for i, (idx, relation) in enumerate(relations.iterrows()):\n",
    "    trained_test = train_dict['sample'][1800+i*1900:(i+1)*1900]\n",
    "\n",
    "    acc_s = 0\n",
    "    correct_entities_s = []\n",
    "    \n",
    "    acc_rde = 0\n",
    "    correct_entities_rde = []\n",
    "    \n",
    "    acc_test = 0\n",
    "    correct_entities_test = []\n",
    "    \n",
    "    r = relation1['en']\n",
    "    r_t = relation1['de']\n",
    "    s = relation2['en']\n",
    "    s_t = relation2['de']\n",
    "    \n",
    "#     r = relation['en']\n",
    "#     r_t = relation['de']\n",
    "#     s = relation['en_alias']\n",
    "#     s_t = relation['de_alias']\n",
    "    \n",
    "    # trained_test are test-facts\n",
    "    for sample in trained_test:\n",
    "\n",
    "        # Test (e, s, f)\n",
    "        e = sample.split(' ', 1)[0]\n",
    "        f = sample.rsplit(' ', 1)[1]\n",
    "        \n",
    "        label_token = tokenizer.convert_tokens_to_ids(f)\n",
    "\n",
    "        prompt = e + ' ' + s + ' [MASK]'\n",
    "        # print(prompt)\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        # Pick the [MASK] candidates with the highest logits\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_s += 1\n",
    "            correct_entities_s.append(f)\n",
    "\n",
    "        # Test (e, r_de, f)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(f)\n",
    "\n",
    "        prompt = e + ' ' + r_t + ' [MASK]'\n",
    "        # print(prompt)\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        # Pick the [MASK] candidates with the highest logits\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_rde += 1\n",
    "            correct_entities_rde.append(f)\n",
    "            \n",
    "        # Test (e, s_de, f)\n",
    "        label_token = tokenizer.convert_tokens_to_ids(f)\n",
    "\n",
    "        prompt = e + ' ' + s_t + ' [MASK]'\n",
    "        # print(prompt)\n",
    "\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        token_logits = model(**encoded_input).logits\n",
    "\n",
    "        mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        # Pick the [MASK] candidates with the highest logits\n",
    "        top_1_token = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()[0]\n",
    "\n",
    "        if label_token == top_1_token:\n",
    "            acc_test += 1\n",
    "            correct_entities_test.append(f)\n",
    "        \n",
    "\n",
    "    acc_s /= 100\n",
    "    acc_rde /= 100\n",
    "    acc_test /= 100\n",
    "    total_transfer_fkt_sr += len(correct_entities_rde) * len(compute_overlap(correct_entities_rde, correct_entities_test))/len(correct_entities_rde)\n",
    "\n",
    "    print(f'Relation1: {r}')\n",
    "    print(f'Relation1 Target: {r_t}')\n",
    "    print(f'Relation2: {s}')\n",
    "    print(f'Relation2 Target: {s_t}')\n",
    "    \n",
    "    print(f'Accuracy for SR (e, s, f): {acc_s}')\n",
    "    print(f'Accuracy for FKT (e, r_t, f): {acc_rde}')\n",
    "    print(f'Accuracy for (e, s_t, f): {acc_test}')\n",
    "    print(f'Size (e, s, f): {len(correct_entities_s)}')\n",
    "    print(f'Size (e, r_t, f): {len(correct_entities_rde)}')\n",
    "    print(f'Overlap between (e, s, f) and (e, r_t, f): {len(compute_overlap(correct_entities_s, correct_entities_rde))}')\n",
    "    if len(correct_entities_rde) == 0:\n",
    "        print(f'Transfer from (e, r_t, f) to (e, s_t, f): {0}')\n",
    "    else:\n",
    "        print(f'Transfer from (e, r_t, f) to (e, s_t, f): {len(compute_overlap(correct_entities_rde, correct_entities_test))/len(correct_entities_rde)}')\n",
    "    \n",
    "    if len(correct_entities_s) == 0:\n",
    "        print(f'Transfer from (e, s, f) to (e, s_t, f): {0}')\n",
    "    else:\n",
    "        print(f'Transfer from (e, s, f) to (e, s_t, f): {len(compute_overlap(correct_entities_s, correct_entities_test))/len(correct_entities_s)}')\n",
    "    print('')\n",
    "\n",
    "print(f'Total transfer with upperbound on FKT->SR and implicit lowerbound on SR->FKT (1-res): {total_transfer_fkt_sr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554bc979",
   "metadata": {},
   "source": [
    "### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2accb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8829473684210526\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "total = len(train_dict['sample'])\n",
    "i = 0\n",
    "\n",
    "for txt in train_dict['sample'][:10000]:\n",
    "    i += 1\n",
    "    \n",
    "    # Add [MASK] for object\n",
    "    sample = txt.rsplit(' ', 1)[0] + ' [MASK]'\n",
    "    label_token = tokenizer.convert_tokens_to_ids(txt.rsplit(' ', 1)[1])\n",
    "    \n",
    "    encoded_input = tokenizer(sample, return_tensors='pt')\n",
    "    token_logits = model(**encoded_input).logits\n",
    "    \n",
    "    mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "    \n",
    "    # Pick the [MASK] candidates with the highest logits\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 1, dim=1).indices[0].tolist()\n",
    "    \n",
    "    if label_token in top_5_tokens:\n",
    "        k += 1\n",
    "print(k/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"lens manner of [MASK]\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "token_logits = model(**encoded_input).logits\n",
    "\n",
    "mask_token_index = torch.where(encoded_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for chunk in top_5_tokens:\n",
    "    print(f\"\\n>>> {tokenizer.decode([chunk])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbe90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_dict['sample']:\n",
    "    if 'Alex' in t:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ea4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
